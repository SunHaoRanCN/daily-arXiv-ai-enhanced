{"id": "2510.21872", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.21872", "abs": "https://arxiv.org/abs/2510.21872", "authors": ["Jackson Loth", "Pedro Sarmento", "Mark Sandler", "Mathieu Barthet"], "title": "GuitarFlow: Realistic Electric Guitar Synthesis From Tablatures via Flow Matching and Style Transfer", "comment": "To be published in Proceedings of the 17th International Symposium on\n  Computer Music and Multidisciplinary Research (CMMR)", "summary": "Music generation in the audio domain using artificial intelligence (AI) has\nwitnessed steady progress in recent years. However for some instruments,\nparticularly the guitar, controllable instrument synthesis remains limited in\nexpressivity. We introduce GuitarFlow, a model designed specifically for\nelectric guitar synthesis. The generative process is guided using tablatures,\nan ubiquitous and intuitive guitar-specific symbolic format. The tablature\nformat easily represents guitar-specific playing techniques (e.g. bends, muted\nstrings and legatos), which are more difficult to represent in other common\nmusic notation formats such as MIDI. Our model relies on an intermediary step\nof first rendering the tablature to audio using a simple sample-based virtual\ninstrument, then performing style transfer using Flow Matching in order to\ntransform the virtual instrument audio into more realistic sounding examples.\nThis results in a model that is quick to train and to perform inference,\nrequiring less than 6 hours of training data. We present the results of\nobjective evaluation metrics, together with a listening test, in which we show\nsignificant improvement in the realism of the generated guitar audio from\ntablatures."}
{"id": "2510.21748", "categories": ["eess.SP", "cs.SY", "eess.SY", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2510.21748", "abs": "https://arxiv.org/abs/2510.21748", "authors": ["Kiana Kiashemshaki", "Sina Samieirad", "Sarvenaz Erfani", "Aryan Jalaeianbanayan", "Nasibeh Asadi Isakan", "Hossein Najafzadeh"], "title": "Automated Tinnitus Detection Through Dual-Modality Neuroimaging: EEG Microstate Analysis and Resting-State fMRI Classification Using Deep Learning", "comment": null, "summary": "Objective: Tinnitus affects 10-15% of the population yet lacks objective\ndiagnostic biomarkers. This study applied machine learning to EEG and fMRI data\nto identify neural signatures distinguishing tinnitus patients from healthy\ncontrols. Methods: Two datasets were analyzed: 64-channel EEG recordings from\n80 participants (40 tinnitus, 40 controls) and resting-state fMRI data from 38\nparticipants (19 tinnitus, 19 controls). EEG analysis extracted microstate\nfeatures across four to seven clustering states and five frequency bands,\nproducing 440 features per subject. Global Field Power signals were also\ntransformed into wavelet images for deep learning. fMRI data were analyzed\nusing slice-wise convolutional neural networks and hybrid models combining\npre-trained architectures (VGG16, ResNet50) with Decision Tree, Random Forest,\nand SVM classifiers. Model performance was evaluated using 5-fold\ncross-validation based on accuracy, precision, recall, F1-score, and ROC-AUC.\nResults: EEG microstate analysis revealed altered network dynamics in tinnitus,\nparticularly reduced gamma-band microstate B occurrence (healthy: 56.56 vs\ntinnitus: 43.81, p < 0.001) and diminished alpha coverage. Tree-based\nclassifiers achieved up to 98.8% accuracy, while VGG16 on wavelet-transformed\nEEG yielded 95.4% and 94.1% accuracy for delta and alpha bands, respectively.\nfMRI analysis identified 12 high-performing axial slices (>=90% accuracy), with\nslice 17 reaching 99.0%. The hybrid VGG16-Decision Tree model achieved 98.95%\n+/- 2.94% accuracy. Conclusion: EEG and fMRI provided effective neural\nbiomarkers for tinnitus classification. Tree-based and hybrid models\ndemonstrated superior performance, highlighting tinnitus as a multi-network\ndisorder requiring multimodal analysis."}
{"id": "2510.22105", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2510.22105", "abs": "https://arxiv.org/abs/2510.22105", "authors": ["Yusong Wu", "Mason Wang", "Heidi Lei", "Stephen Brade", "Lancelot Blanchard", "Shih-Lun Wu", "Aaron Courville", "Anna Huang"], "title": "Streaming Generation for Music Accompaniment", "comment": null, "summary": "Music generation models can produce high-fidelity coherent accompaniment\ngiven complete audio input, but are limited to editing and loop-based\nworkflows. We study real-time audio-to-audio accompaniment: as a model hears an\ninput audio stream (e.g., a singer singing), it has to also simultaneously\ngenerate in real-time a coherent accompanying stream (e.g., a guitar\naccompaniment). In this work, we propose a model design considering inevitable\nsystem delays in practical deployment with two design variables: future\nvisibility $t_f$, the offset between the output playback time and the latest\ninput time used for conditioning, and output chunk duration $k$, the number of\nframes emitted per call. We train Transformer decoders across a grid of\n$(t_f,k)$ and show two consistent trade-offs: increasing effective $t_f$\nimproves coherence by reducing the recency gap, but requires faster inference\nto stay within the latency budget; increasing $k$ improves throughput but\nresults in degraded accompaniment due to a reduced update rate. Finally, we\nobserve that naive maximum-likelihood streaming training is insufficient for\ncoherent accompaniment where future context is not available, motivating\nadvanced anticipatory and agentic objectives for live jamming."}
{"id": "2510.21789", "categories": ["eess.SP", "cs.HC"], "pdf": "https://arxiv.org/pdf/2510.21789", "abs": "https://arxiv.org/abs/2510.21789", "authors": ["Beyazit Bestami Yuksel"], "title": "Monitoring Real-Time ECG Signals on Mobile Systems", "comment": "10 figure, 4 pages", "summary": "This study focuses on the connection of a development kit that enables\nreal-time monitoring of electrocardiogram (ECG) signals using a mobile system.\nA software developed on the Visual Studio .NET platform reads real-time ECG\nsignals from the human body through non invasive methods and displays them\ngraphically on the mobile system. ECG electrodes placed on specific areas of\nthe body using the method known as Einthoven's triangle. Subsequently, the\nsoftware initiates data flow through the serial port, and these data displayed\nas signal values on the mobile device's screen via a graphical interface. When\nthe monitored ECG signals fall below a certain threshold or reach a critical\nvalue, the system provides feedback with an alert based on medical data. The\ndeveloped system is fully portable. Additionally, the implemented system has\nthe potential to form the basis for a multi-purpose system in the future, such\nas online patient monitoring, patient location tracking, and even initial\nintervention using the defibrillation method."}
{"id": "2510.22172", "categories": ["cs.SD", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.22172", "abs": "https://arxiv.org/abs/2510.22172", "authors": ["Ruixiang Mao", "Xiangnan Ma", "Qing Yang", "Ziming Zhu", "Yucheng Qiao", "Yuan Ge", "Tong Xiao", "Shengxiang Gao", "Zhengtao Yu", "Jingbo Zhu"], "title": "M-CIF: Multi-Scale Alignment For CIF-Based Non-Autoregressive ASR", "comment": null, "summary": "The Continuous Integrate-and-Fire (CIF) mechanism provides effective\nalignment for non-autoregressive (NAR) speech recognition. This mechanism\ncreates a smooth and monotonic mapping from acoustic features to target tokens,\nachieving performance on Mandarin competitive with other NAR approaches.\nHowever, without finer-grained guidance, its stability degrades in some\nlanguages such as English and French. In this paper, we propose Multi-scale CIF\n(M-CIF), which performs multi-level alignment by integrating character and\nphoneme level supervision progressively distilled into subword representations,\nthereby enhancing robust acoustic-text alignment. Experiments show that M-CIF\nreduces WER compared to the Paraformer baseline, especially on CommonVoice by\n4.21% in German and 3.05% in French. To further investigate these gains, we\ndefine phonetic confusion errors (PE) and space-related segmentation errors\n(SE) as evaluation metrics. Analysis of these metrics across different M-CIF\nsettings reveals that the phoneme and character layers are essential for\nenhancing progressive CIF alignment."}
{"id": "2510.21969", "categories": ["eess.SP", "cs.LG", "cs.NE"], "pdf": "https://arxiv.org/pdf/2510.21969", "abs": "https://arxiv.org/abs/2510.21969", "authors": ["Weiyu Chen", "Arnaud Delorme"], "title": "Adaptive Split-MMD Training for Small-Sample Cross-Dataset P300 EEG Classification", "comment": "8 pages, 5 figures. Submitted to IEEE BIBM 2025 Workshop on Machine\n  Learning for EEG Signal Processing (MLESP)", "summary": "Detecting single-trial P300 from EEG is difficult when only a few labeled\ntrials are available. When attempting to boost a small target set with a large\nsource dataset through transfer learning, cross-dataset shift arises. To\naddress this challenge, we study transfer between two public visual-oddball ERP\ndatasets using five shared electrodes (Fz, Pz, P3, P4, Oz) under a strict\nsmall-sample regime (target: 10 trials/subject; source: 80 trials/subject). We\nintroduce Adaptive Split Maximum Mean Discrepancy Training (AS-MMD), which\ncombines (i) a target-weighted loss with warm-up tied to the square root of the\nsource/target size ratio, (ii) Split Batch Normalization (Split-BN) with shared\naffine parameters and per-domain running statistics, and (iii) a parameter-free\nlogit-level Radial Basis Function kernel Maximum Mean Discrepancy (RBF-MMD)\nterm using the median-bandwidth heuristic. Implemented on an EEG Conformer,\nAS-MMD is backbone-agnostic and leaves the inference-time model unchanged.\nAcross both transfer directions, it outperforms target-only and pooled training\n(Active Visual Oddball: accuracy/AUC 0.66/0.74; ERP CORE P3: 0.61/0.65), with\ngains over pooling significant under corrected paired t-tests. Ablations\nattribute improvements to all three components."}
{"id": "2510.22241", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2510.22241", "abs": "https://arxiv.org/abs/2510.22241", "authors": ["Parthasaarathy Sudarsanam", "Sebastian Braun", "Hannes Gamper"], "title": "FOA Tokenizer: Low-bitrate Neural Codec for First Order Ambisonics with Spatial Consistency Loss", "comment": "Submitted to ICASSP 2026", "summary": "Neural audio codecs have been widely studied for mono and stereo signals, but\nspatial audio remains largely unexplored. We present the first discrete neural\nspatial audio codec for first-order ambisonics (FOA). Building on the\nWavTokenizer architecture, we extend it to support four-channel FOA signals and\nintroduce a novel spatial consistency loss to preserve directional cues in the\nreconstructed signals under a highly compressed representation. Our codec\ncompresses 4-channel FOA audio at 24 kHz into 75 discrete tokens per second,\ncorresponding to a bit rate of 0.9 kbps. Evaluations on simulated reverberant\nmixtures, non-reverberant clean speech, and FOA mixtures with real room impulse\nresponses show accurate reconstruction, with mean angular errors of\n13.76{\\deg}, 3.96{\\deg}, and 25.83{\\deg}, respectively, across the three\nconditions. In addition, discrete latent representations derived from our codec\nprovide useful features for downstream spatial audio tasks, as demonstrated on\nsound event localization and detection with STARSS23 real recordings."}
{"id": "2510.22180", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.22180", "abs": "https://arxiv.org/abs/2510.22180", "authors": ["Maximilian Bauhofer", "Marcus Henninger", "Meik Kottkamp", "Lucas Giroto", "Philip Grill", "Alexander Felix", "Thorsten Wild", "Stephan ten Brink", "Silvio Mandelli"], "title": "Experimental Demonstration of Multi-Object Tracking in Integrated Sensing and Communication", "comment": null, "summary": "For a wide range of envisioned integrated sensing and communication (ISAC)\nuse cases, it is necessary to incorporate tracking techniques into cellular\ncommunication systems. While numerous multi-object tracking algorithms exist,\nthey have not yet been applied to real-world ISAC, with its challenges such as\nclutter and non-optimal hardware. In this work, we showcase multi-object\ntracking based on the probability hypothesis density (PHD) filter in the range\nand Doppler speed domain. The measurements are taken with a 5G compliant ISAC\nproof-of-concept in a real factory environment, where the pedestrian-like\nobjects are generated by a radar object emulator. We detail the complete\npipeline, from measurement acquisition to evaluation, with a focus on the\npost-processing of the raw captured data and the tracking itself. Our\nend-to-end evaluation and comparison to simulations show good multi-object\ntracking performance with mean absolute error <1.5m and detection rates >91%\nfor realistic but challenging scenarios."}
{"id": "2510.22183", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2510.22183", "abs": "https://arxiv.org/abs/2510.22183", "authors": ["Akira Omoto"], "title": "A Unified Framework for Direction and Diffuseness Estimation Using Tight-Frame Microphone Arrays", "comment": "36 pages including 14 files", "summary": "This work presents a unified framework for estimating both sound-field\ndirection and diffuseness using practical microphone arrays with different\nspatial configurations. Building on covariance-based diffuseness models, we\nformulate a velocity-only covariance approach that enables consistent\ndiffuseness evaluation across heterogeneous array geometries without requiring\nmode whitening or spherical-harmonic decomposition. Three array types -- an\nA-format array, a rigid-sphere array, and a newly proposed tight-frame array --\nare modeled and compared through both simulations and measurement-based\nexperiments. The results show that the tight-frame configuration achieves\nnear-isotropic directional sampling and reproduces diffuseness characteristics\ncomparable to those of higher-order spherical arrays, while maintaining a\ncompact physical structure. We further examine the accuracy of\ndirection-of-arrival estimation based on acoustic intensity within the same\nframework. These findings connect theoretical diffuseness analysis with\nimplementable array designs and support the development of robust, broadband\nmethods for spatial-sound-field characterization."}
{"id": "2510.22439", "categories": ["cs.SD", "cs.AI", "I.2.6, H.5.5"], "pdf": "https://arxiv.org/pdf/2510.22439", "abs": "https://arxiv.org/abs/2510.22439", "authors": ["Ali Vosoughi", "Yongyi Zang", "Qihui Yang", "Nathan Peak", "Randal Leistikow", "Chenliang Xu"], "title": "PromptReverb: Multimodal Room Impulse Response Generation Through Latent Rectified Flow Matching", "comment": "9 pages, 2 figures, 4 tables", "summary": "Room impulse response (RIR) generation remains a critical challenge for\ncreating immersive virtual acoustic environments. Current methods suffer from\ntwo fundamental limitations: the scarcity of full-band RIR datasets and the\ninability of existing models to generate acoustically accurate responses from\ndiverse input modalities. We present PromptReverb, a two-stage generative\nframework that addresses these challenges. Our approach combines a variational\nautoencoder that upsamples band-limited RIRs to full-band quality (48 kHz), and\na conditional diffusion transformer model based on rectified flow matching that\ngenerates RIRs from descriptions in natural language. Empirical evaluation\ndemonstrates that PromptReverb produces RIRs with superior perceptual quality\nand acoustic accuracy compared to existing methods, achieving 8.8% mean RT60\nerror compared to -37% for widely used baselines and yielding more realistic\nroom-acoustic parameters. Our method enables practical applications in virtual\nreality, architectural acoustics, and audio production where flexible,\nhigh-quality RIR synthesis is essential."}
{"id": "2510.22297", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.22297", "abs": "https://arxiv.org/abs/2510.22297", "authors": ["Alexander Felix", "Rudolf Hoffmann", "Marcus Henninger", "Stephan ten Brink", "Silvio Mandelli"], "title": "Angular Estimation Comparison with ISAC PoC", "comment": null, "summary": "The introduction of Integrated Sensing and Communications (ISAC) in cellular\nsystems is not expected to result in a shift away from the popular choice of\ncost- and energy-efficient analog or hybrid beamforming structures. However,\nthis comes at the cost of limiting the angular capabilities to a confined space\nper acquisitions. Thus, as a prerequisite for the successful implementation of\nnumerous ISAC use cases, the need for an optimal angular estimation of targets\nand their separation based on the minimal number of angular samples arises.\n  In this work, different approaches for angular estimation based on a minimal,\nDFT-based set of angular samples are evaluated. The samples are acquired\nthrough sweeping multiple beams of an ISAC proof of concept (PoC) in the\nindustrial scenario of the ARENA2036. The study's findings indicate that\ninterpolation approaches are more effective for generalizing across different\ntypes of angular scenarios. While the orthogonal matching pursuit (OMP)\napproach exhibits the most accurate estimation for a single, strong and clearly\ndiscriminable target, the DFT-based interpolation approach demonstrates the\nbest overall estimation performance."}
{"id": "2510.22237", "categories": ["eess.AS", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.22237", "abs": "https://arxiv.org/abs/2510.22237", "authors": ["Krishna Gurugubelli"], "title": "Bridging the Perceptual-Statistical Gap in Dysarthria Assessment: Why Machine Learning Still Falls Short", "comment": null, "summary": "Automated dysarthria detection and severity assessment from speech have\nattracted significant research attention due to their potential clinical\nimpact. Despite rapid progress in acoustic modeling and deep learning, models\nstill fall short of human expert performance. This manuscript provides a\ncomprehensive analysis of the reasons behind this gap, emphasizing a conceptual\ndivergence we term the ``perceptual-statistical gap''. We detail human expert\nperceptual processes, survey machine learning representations and methods,\nreview existing literature on feature sets and modeling strategies, and present\na theoretical analysis of limits imposed by label noise and inter-rater\nvariability. We further outline practical strategies to narrow the gap,\nperceptually motivated features, self-supervised pretraining, ASR-informed\nobjectives, multimodal fusion, human-in-the-loop training, and explainability\nmethods. Finally, we propose experimental protocols and evaluation metrics\naligned with clinical goals to guide future research toward clinically reliable\nand interpretable dysarthria assessment tools."}
{"id": "2510.22455", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.22455", "abs": "https://arxiv.org/abs/2510.22455", "authors": ["Brandon James Carone", "Iran R. Roman", "Pablo Ripollés"], "title": "Evaluating Multimodal Large Language Models on Core Music Perception Tasks", "comment": "Accepted to the NeurIPS 2025 Workshop on AI for Music (AI4Music), 16\n  pages, 1 figure, 3 tables", "summary": "Multimodal Large Language Models (LLMs) claim \"musical understanding\" via\nevaluations that conflate listening with score reading. We benchmark three SOTA\nLLMs (Gemini 2.5 Pro, Gemini 2.5 Flash, and Qwen2.5-Omni) across three core\nmusic skills: Syncopation Scoring, Transposition Detection, and Chord Quality\nIdentification. Moreover, we separate three sources of variability: (i)\nperceptual limitations (audio vs. MIDI inputs), (ii) exposure to examples\n(zero- vs. few-shot manipulations), and (iii) reasoning strategies (Standalone,\nCoT, LogicLM). For the latter we adapt LogicLM, a framework combining LLMs with\nsymbolic solvers to perform structured reasoning, to music. Results reveal a\nclear perceptual gap: models perform near ceiling on MIDI but show accuracy\ndrops on audio. Reasoning and few-shot prompting offer minimal gains. This is\nexpected for MIDI, where performance reaches saturation, but more surprising\nfor audio, where LogicLM, despite near-perfect MIDI accuracy, remains notably\nbrittle. Among models, Gemini Pro achieves the highest performance across most\nconditions. Overall, current systems reason well over symbols (MIDI) but do not\nyet \"listen\" reliably from audio. Our method and dataset make the\nperception-reasoning boundary explicit and offer actionable guidance for\nbuilding robust, audio-first music systems."}
{"id": "2510.22406", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.22406", "abs": "https://arxiv.org/abs/2510.22406", "authors": ["Anargyros Michaloliakos", "Benjamin J. Chang", "Lawrence A. Bergman", "Alexander F. Vakakis"], "title": "Data-driven, Wavelet-based Identification and Reduced-order Modeling of Linear Systems with Closely Spaced Modes", "comment": null, "summary": "This work presents a purely data-driven, wavelet-based framework for modal\nidentification and reduced-order modeling of mechanical systems with assumed\nlinear dynamics characterized by closely spaced modes with classical or\nnon-classical damping distribution. Traditional Fourier-based methods often\nfail to reliably identify closely spaced modes or accurately capture modal\ninteractions and complexities. To address these limitations, we propose a\nmethodology leveraging the enhanced time -frequency resolution capabilities of\nthe continuous wavelet transform (CWT). By selecting appropriate harmonic\nregions within the wavelet spectra, we effectively isolate modes, and then\ninvert them back in the temporal domain by applying the inverse CWT (ICWT). In\nthis way we reconstruct the corresponding modal dynamics in the time domain.\nUsing the Hilbert transform, instantaneous phases are extracted for each\nidentified mode, enabling the introduction of a complexified modal matrix which\nrobustly characterizes the system's modal properties, even under challenging\nperturbations such as noise and uncertainties due to modal interference and\nunmodeled effects. The identified modal parameters are utilized to reconstruct\nthe frequency response functions (FRFs) of the system and to develop a\nreduced-order model (ROM) that captures accurately the system's dominant\ndynamical behavior valid in a specified frequency range.. Validation of the\nmethodology is conducted both with a numerical non-classical damping and an\nexperimental testbed representing a model of an airplane structure. Results\ndemonstrate the effectiveness of the proposed approach in resolving intricate\nmodal interactions and accurately reproducing the dynamic response of complex\nstructural systems."}
{"id": "2510.22258", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2510.22258", "abs": "https://arxiv.org/abs/2510.22258", "authors": ["Sapir Goldring", "Zamir Ben Hur", "David Lou Alon", "Chad McKell", "Sebastian Prepelita", "Boaz Rafaely"], "title": "Binaural Signal Matching with Wearable Arrays for Near-Field Sources and Directional Focus", "comment": null, "summary": "This paper investigates the performance of Binaural Signal Matching (BSM)\nmethods for near-field sound reproduction using a wearable glasses-mounted\nmicrophone array. BSM is a flexible, signal-independent approach for binaural\nrendering with arbitrary arrays, but its conventional formulation assumes\nfar-field sources. In our previous work, we proposed a near-field extension of\nBSM (NF-BSM) that incorporates distance-dependent modeling and showed improved\nperformance over far-field BSM using analytic data, though degradation\npersisted for sources very close to the array. In this study, we extend that\nanalysis by using realistic simulated data of near-field Head-Related Transfer\nFunctions (HRTFs) and Acoustic Transfer Functions (ATFs) of the array,\naccounting for listener head rotation and evaluating binaural cues such as\ninteraural level and time differences (ILD and ITD). A key contribution is the\nintroduction of a Field of View (FoV) weighting, designed to emphasize\nperceptually relevant directions and improve robustness under challenging\nconditions. Results from both simulation and a listening test confirm that\nNF-BSM outperforms traditional far-field BSM in near-field scenarios, and that\nthe proposed NF-FoV-BSM method achieves the best perceptual and objective\nquality among all tested methods, particularly at close source distances and\nunder head rotation. These findings highlight the limitations for far-field\nmodels in near-field sources and demonstrate that incorporating source distance\nand directional weighting can significantly improve binaural reproduction\nperformance for wearable spatial audio systems."}
{"id": "2510.22795", "categories": ["cs.SD", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.22795", "abs": "https://arxiv.org/abs/2510.22795", "authors": ["Michael Ungersböck", "Florian Grötschla", "Luca A. Lanzendörfer", "June Young Yi", "Changho Choi", "Roger Wattenhofer"], "title": "SAO-Instruct: Free-form Audio Editing using Natural Language Instructions", "comment": "Accepted at NeurIPS 2025", "summary": "Generative models have made significant progress in synthesizing\nhigh-fidelity audio from short textual descriptions. However, editing existing\naudio using natural language has remained largely underexplored. Current\napproaches either require the complete description of the edited audio or are\nconstrained to predefined edit instructions that lack flexibility. In this\nwork, we introduce SAO-Instruct, a model based on Stable Audio Open capable of\nediting audio clips using any free-form natural language instruction. To train\nour model, we create a dataset of audio editing triplets (input audio, edit\ninstruction, output audio) using Prompt-to-Prompt, DDPM inversion, and a manual\nediting pipeline. Although partially trained on synthetic data, our model\ngeneralizes well to real in-the-wild audio clips and unseen edit instructions.\nWe demonstrate that SAO-Instruct achieves competitive performance on objective\nmetrics and outperforms other audio editing approaches in a subjective\nlistening study. To encourage future research, we release our code and model\nweights."}
{"id": "2510.22417", "categories": ["eess.SP", "cs.NE", "math.OC"], "pdf": "https://arxiv.org/pdf/2510.22417", "abs": "https://arxiv.org/abs/2510.22417", "authors": ["Laura Train", "Rodrigo Castellanos", "Miguel Gómez-López"], "title": "Genetic Optimization of a Software-Defined GNSS Receiver", "comment": null, "summary": "Commercial off-the-shelf (COTS) Global Navigation Satellite System (GNSS)\nreceivers face significant limitations under high-dynamic conditions,\nparticularly in high-acceleration environments such as those experienced by\nlaunch vehicles. These performance degradations, often observed as\ndiscontinuities in the navigation solution, arise from the inability of\ntraditional tracking loop bandwidths to cope with rapid variations in\nsynchronization parameters. Software-Defined Radio (SDR) receivers overcome\nthese constraints by enabling flexible reconfiguration of tracking loops;\nhowever, manual tuning involves a complex, multidimensional search and seldom\nensures optimal performance. This work introduces a genetic algorithm-based\noptimization framework that autonomously explores the receiver configuration\nspace to determine optimal loop parameters for phase, frequency, and delay\ntracking. The approach is validated within an SDR environment using\nrealistically simulated GPS L1 signals for three representative dynamic regimes\n-guided rocket flight, Low Earth Orbit (LEO) satellite, and static\nreceiver-processed with the open-source GNSS-SDR architecture. Results\ndemonstrate that evolutionary optimization enables SDR receivers to maintain\nrobust and accurate Position, Velocity, and Time (PVT) solutions across diverse\ndynamic conditions. The optimized configurations yielded maximum position and\nvelocity errors of approximately 6 m and 0.08 m/s for the static case, 12 m and\n2.5 m/s for the rocket case, and 5 m and 0.2 m/s for the LEO case."}
{"id": "2510.22263", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2510.22263", "abs": "https://arxiv.org/abs/2510.22263", "authors": ["Heejoon Koo", "Miika Toikkanen", "Yoon Tae Kim", "Soo Yong Kim", "June-Woo Kim"], "title": "Empowering Multimodal Respiratory Sound Classification with Counterfactual Adversarial Debiasing for Out-of-Distribution Robustness", "comment": "3 figures, 4 Tables, and 5 pages", "summary": "Multimodal respiratory sound classification offers promise for early\npulmonary disease detection by integrating bioacoustic signals with patient\nmetadata. Nevertheless, current approaches remain vulnerable to spurious\ncorrelations from attributes such as age, sex, or acquisition device, which\nhinder their generalization, especially under distribution shifts across\nclinical sites. To this end, we propose a counterfactual adversarial debiasing\nframework. First, we employ a causal graph-based counterfactual debiasing\nstrategy to suppress non-causal dependencies from patient metadata. Second, we\nintroduce adversarial debiasing to learn metadata-insensitive representations\nand reduce metadata-specific biases. Third, we design counterfactual metadata\naugmentation to mitigate spurious correlations further and strengthen\nmetadata-invariant representations. By doing so, our method consistently\noutperforms strong baselines in evaluations under both in-distribution and\ndistribution shifts. The code is available at\nhttps://github.com/RSC-Toolkit/BTS-CARD."}
{"id": "2510.23096", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2510.23096", "abs": "https://arxiv.org/abs/2510.23096", "authors": ["Jiyoung Hong", "Yoonseo Chung", "Seungyeon Oh", "Juntae Kim", "Jiyoung Lee", "Sookyung Kim", "Hyunsoo Cho"], "title": "TwinShift: Benchmarking Audio Deepfake Detection across Synthesizer and Speaker Shifts", "comment": "Submitted to ICASSP 2026", "summary": "Audio deepfakes pose a growing threat, already exploited in fraud and\nmisinformation. A key challenge is ensuring detectors remain robust to unseen\nsynthesis methods and diverse speakers, since generation techniques evolve\nquickly. Despite strong benchmark results, current systems struggle to\ngeneralize to new conditions limiting real-world reliability. To address this,\nwe introduce TWINSHIFT, a benchmark explicitly designed to evaluate detection\nrobustness under strictly unseen conditions. Our benchmark is constructed from\nsix different synthesis systems, each paired with disjoint sets of speakers,\nallowing for a rigorous assessment of how well detectors generalize when both\nthe generative model and the speaker identity change. Through extensive\nexperiments, we show that TWINSHIFT reveals important robustness gaps, uncover\noverlooked limitations, and provide principled guidance for developing ADD\nsystems. The TWINSHIFT benchmark can be accessed at\nhttps://github.com/intheMeantime/TWINSHIFT."}
{"id": "2510.22472", "categories": ["eess.SP", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.22472", "abs": "https://arxiv.org/abs/2510.22472", "authors": ["Yohei Kono", "Yoshiyuki Tajima"], "title": "Data-driven Exponential Framing for Pulsive Temporal Patterns without Repetition or Singularity", "comment": "16 pages", "summary": "Extracting pulsive temporal patterns from a small dataset without their\nrepetition or singularity shows significant importance in manufacturing\napplications but does not sufficiently attract scientific attention. We propose\nto quantify how long temporal patterns appear without relying on their\nrepetition or singularity, enabling to extract such temporal patterns from a\nsmall dataset. Inspired by the celebrated time delay embedding and data-driven\nHankel matrix analysis, we introduce a linear dynamical system model on the\ntime-delay coordinates behind the data to derive the discrete-time bases each\nof which has a distinct exponential decay constant. The derived bases are\nfitted onto subsequences that are extracted with a sliding window in order to\nquantify how long patterns are dominant in the set of subsequences. We call the\nquantification method Data-driven Exponential Framing (DEF). A toy model-based\nexperiment shows that DEF can identify multiple patterns with distinct lengths.\nDEF is also applied to electric current measurement on a punching machine,\nshowing its possibility to extract multiple patterns from real-world\noscillatory data."}
{"id": "2510.22588", "categories": ["eess.AS", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.22588", "abs": "https://arxiv.org/abs/2510.22588", "authors": ["Wenming Tu", "Guanrou Yang", "Ruiqi Yan", "Wenxi Chen", "Ziyang Ma", "Yipeng Kang", "Kai Yu", "Xie Chen", "Zilong Zheng"], "title": "UltraVoice: Scaling Fine-Grained Style-Controlled Speech Conversations for Spoken Dialogue Models", "comment": "23 pages, 4 figures", "summary": "Spoken dialogue models currently lack the ability for fine-grained speech\nstyle control, a critical capability for human-like interaction that is often\noverlooked in favor of purely functional capabilities like reasoning and\nquestion answering. To address this limitation, we introduce UltraVoice, the\nfirst large-scale speech dialogue dataset engineered for multiple fine-grained\nspeech style control. Encompassing over 830 hours of speech dialogues,\nUltraVoice provides instructions across six key speech stylistic dimensions:\nemotion, speed, volume, accent, language, and composite styles. Fine-tuning\nleading models such as SLAM-Omni and VocalNet on UltraVoice significantly\nenhances their fine-grained speech stylistic controllability without degrading\ncore conversational abilities. Specifically, our fine-tuned models achieve\nimprovements of 29.12-42.33% in Mean Opinion Score (MOS) and 14.61-40.09\npercentage points in Instruction Following Rate (IFR) on multi-dimensional\ncontrol tasks designed in the UltraVoice. Moreover, on the URO-Bench benchmark,\nour fine-tuned models demonstrate substantial gains in core understanding,\nreasoning, and conversational abilities, with average improvements of +10.84%\non the Basic setting and +7.87% on the Pro setting. Furthermore, the dataset's\nutility extends to training controllable Text-to-Speech (TTS) models,\nunderscoring its high quality and broad applicability for expressive speech\nsynthesis. The complete dataset and model checkpoints are available at:\nhttps://github.com/bigai-nlco/UltraVoice."}
{"id": "2510.23312", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.23312", "abs": "https://arxiv.org/abs/2510.23312", "authors": ["Kamil Wojcicki", "Yusuf Ziya Isik", "Laura Lechler", "Mansur Yesilbursa", "Ivana Balić", "Wolfgang Mack", "Rafał Łaganowski", "Guoqing Zhang", "Yossi Adi", "Minje Kim", "Shinji Watanabe"], "title": "Low-Resource Audio Codec (LRAC): 2025 Challenge Description", "comment": null, "summary": "While recent neural audio codecs deliver superior speech quality at ultralow\nbitrates over traditional methods, their practical adoption is hindered by\nobstacles related to low-resource operation and robustness to acoustic\ndistortions. Edge deployment scenarios demand codecs that operate under\nstringent compute constraints while maintaining low latency and bitrate. The\npresence of background noise and reverberation further necessitates designs\nthat are resilient to such degradations. The performance of neural codecs under\nthese constraints and their integration with speech enhancement remain largely\nunaddressed. To catalyze progress in this area, we introduce the 2025\nLow-Resource Audio Codec Challenge, which targets the development of neural and\nhybrid codecs for resource-constrained applications. Participants are supported\nwith a standardized training dataset, two baseline systems, and a comprehensive\nevaluation framework. The challenge is expected to yield valuable insights\napplicable to both codec design and related downstream audio tasks."}
{"id": "2510.22557", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.22557", "abs": "https://arxiv.org/abs/2510.22557", "authors": ["Wang Liu", "Cunhua Pan", "Hong Ren", "Wei Zhang", "Cheng-Xiang Wang", "Jiangzhou Wang"], "title": "Large-Model AI for Near Field Beam Prediction: A CNN-GPT2 Framework for 6G XL-MIMO", "comment": null, "summary": "The emergence of extremely large-scale antenna arrays (ELAA) in\nmillimeter-wave (mmWave) communications, particularly in high-mobility\nscenarios, highlights the importance of near-field beam prediction. Unlike the\nconventional far-field assumption, near-field beam prediction requires\ncodebooks that jointly sample the angular and distance domains, which leads to\na dramatic increase in pilot overhead. Moreover, unlike the far-field case\nwhere the optimal beam evolution is temporally smooth, the optimal near-field\nbeam index exhibits abrupt and nonlinear dynamics due to its joint dependence\non user angle and distance, posing significant challenges for temporal\nmodeling. To address these challenges, we propose a novel Convolutional Neural\nNetwork-Generative Pre-trained Transformer 2 (CNN-GPT2) based near-field beam\nprediction framework. Specifically, an uplink pilot transmission strategy is\ndesigned to enable efficient channel probing through widebeam analog precoding\nand frequency-varying digital precoding. The received pilot signals are\npreprocessed and passed through a CNN-based feature extractor, followed by a\nGPT-2 model that captures temporal dependencies across multiple frames and\ndirectly predicts the near-field beam index in an end-to-end manner."}
{"id": "2510.22603", "categories": ["eess.AS", "cs.CV", "cs.SD"], "pdf": "https://arxiv.org/pdf/2510.22603", "abs": "https://arxiv.org/abs/2510.22603", "authors": ["Anand", "Umberto Cappellazzo", "Stavros Petridis", "Maja Pantic"], "title": "Mitigating Attention Sinks and Massive Activations in Audio-Visual Speech Recognition with LLMS", "comment": "The code is available at\n  https://github.com/umbertocappellazzo/Llama-AVSR", "summary": "Large language models (LLMs) have recently advanced auditory speech\nrecognition (ASR), visual speech recognition (VSR), and audio-visual speech\nrecognition (AVSR). However, understanding of their internal dynamics under\nfine-tuning remains limited. In natural language processing, recent work has\nrevealed attention sinks, tokens that attract disproportionately high\nattention, and associated massive activations in which some features of sink\ntokens exhibit huge activation in LLMs. In this work, we are the first to study\nthese phenomena in multimodal speech recognition. Through a detailed analysis\nof audio-visual LLMs, we identify attention sinks and massive activations not\nonly at the BOS token but also at intermediate low-semantic tokens across ASR,\nVSR, and AVSR. We show that massive activations originate in the MLP layers and\ncorrespond to fixed feature indices across all sink tokens. We further show\nthat intermediate sink tokens exhibit high cosine similarity to the BOS token,\nthereby amplifying attention and activation. Building on these insights, we\nintroduce a simple decorrelation loss that reduces cosine similarity between\nBOS and other tokens, effectively mitigating intermediate sinks and massive\nactivations. Furthermore, our method improves word error rate (WER) under high\naudio-visual feature downsampling while remaining stable at lower downsampling\nrates."}
{"id": "2510.23530", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.23530", "abs": "https://arxiv.org/abs/2510.23530", "authors": ["Bernardo Torres", "Manuel Moussallam", "Gabriel Meseguer-Brocal"], "title": "Learning Linearity in Audio Consistency Autoencoders via Implicit Regularization", "comment": null, "summary": "Audio autoencoders learn useful, compressed audio representations, but their\nnon-linear latent spaces prevent intuitive algebraic manipulation such as\nmixing or scaling. We introduce a simple training methodology to induce\nlinearity in a high-compression Consistency Autoencoder (CAE) by using data\naugmentation, thereby inducing homogeneity (equivariance to scalar gain) and\nadditivity (the decoder preserves addition) without altering the model's\narchitecture or loss function. When trained with our method, the CAE exhibits\nlinear behavior in both the encoder and decoder while preserving reconstruction\nfidelity. We test the practical utility of our learned space on music source\ncomposition and separation via simple latent arithmetic. This work presents a\nstraightforward technique for constructing structured latent spaces, enabling\nmore intuitive and efficient audio processing."}
{"id": "2510.22621", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.22621", "abs": "https://arxiv.org/abs/2510.22621", "authors": ["Md. Shahriar Sadid", "Ali A. Nasir", "Saad Al-Ahmadi", "Samir Al-Ghadhban"], "title": "Parametric Channel Estimation and Design for Active-RIS-Assisted Communications", "comment": null, "summary": "Reconfigurable Intelligent Surface (RIS) technology has emerged as a key\nenabler for future wireless communications. However, its potential is\nconstrained by the difficulty of acquiring accurate user-to-RIS channel state\ninformation (CSI), due to the cascaded channel structure and the high pilot\noverhead of non-parametric methods. Unlike a passive RIS, where the reflected\nsignal suffers from multiplicative path loss, an active RIS amplifies the\nsignal, improving its practicality in real deployments. In this letter, we\npropose a parametric channel estimation method tailored for active RISs. The\nproposed approach integrates an active RIS model with an adaptive Maximum\nLikelihood Estimator (MLE) to recover the main channel parameters using a\nminimal number of pilots. To further enhance performance, an adaptive active\nRIS configuration strategy is employed, which refines the beam direction based\non an initial user location estimate. Moreover, an orthogonal angle-pair\ncodebook is used instead of the conventional Discrete Fourier Transform (DFT)\ncodebook, significantly reducing the codebook size and ensuring reliable\noperation for both far-field and near-field users. Extensive simulations\ndemonstrate that the proposed method achieves near-optimal performance with\nvery few pilots compared to non-parametric approaches. Its performance is also\nbenchmarked against that of a traditional passive RIS under the same total\npower budget to ensure fairness. Results show that active RIS yields higher\nspectral efficiency (SE) by eliminating the multiplicative fading inherent in\npassive RISs and allocating more resources to data transmission"}
{"id": "2510.22637", "categories": ["eess.AS", "eess.SP"], "pdf": "https://arxiv.org/pdf/2510.22637", "abs": "https://arxiv.org/abs/2510.22637", "authors": ["Yuval Bar Ilan", "Boaz Rafaely", "Vladimir Tourbabin"], "title": "HyBeam: Hybrid Microphone-Beamforming Array-Agnostic Speech Enhancement for Wearables", "comment": null, "summary": "Speech enhancement is a fundamental challenge in signal processing,\nparticularly when robustness is required across diverse acoustic conditions and\nmicrophone setups. Deep learning methods have been successful for speech\nenhancement, but often assume fixed array geometries, limiting their use in\nmobile, embedded, and wearable devices. Existing array-agnostic approaches\ntypically rely on either raw microphone signals or beamformer outputs, but both\nhave drawbacks under changing geometries. We introduce HyBeam, a hybrid\nframework that uses raw microphone signals at low frequencies and beamformer\nsignals at higher frequencies, exploiting their complementary strengths while\nremaining highly array-agnostic. Simulations across diverse rooms and wearable\narray configurations demonstrate that HyBeam consistently surpasses\nmicrophone-only and beamformer-only baselines in PESQ, STOI, and SI-SDR. A\nbandwise analysis shows that the hybrid approach leverages beamformer\ndirectivity at high frequencies and microphone cues at low frequencies,\noutperforming either method alone across all bands."}
{"id": "2510.23558", "categories": ["cs.SD", "cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.23558", "abs": "https://arxiv.org/abs/2510.23558", "authors": ["Bohan Li", "Wenbin Huang", "Yuhang Qiu", "Yiwei Guo", "Hankun Wang", "Zhihan Li", "Jing Peng", "Ziyang Ma", "Xie Chen", "Kai Yu"], "title": "ISA-Bench: Benchmarking Instruction Sensitivity for Large Audio Language Models", "comment": "submitted to icassp 2026", "summary": "Large Audio Language Models (LALMs), which couple acoustic perception with\nlarge language models (LLMs) to extract and understand diverse information from\naudio, have attracted intense interest from both academic and industrial\ncommunities. However, existing LALMs are highly sensitive to how instructions\nare phrased, affecting both (i) instruction-following rates and (ii) task\nperformance. Yet, no existing benchmarks offer a systematic and comprehensive\nevaluation of this sensitivity. We introduce ISA-Bench, a dynamic benchmark\nevaluating instruction sensitivity for LALMs along three axes: instruction\ndescription, output format, and task composition. We assess recent open-source\nand proprietary LALMs using ISA-Bench, profiling both compliance and accuracy\nunder controlled instruction variations. Experimental results reveal that even\nstate-of-the-art LALMs suffer significant instruction sensitivity, leading to\ndegraded performance on fundamental audio understanding tasks. To mitigate this\nissue, we fine-tune Qwen2-Audio on a specifically constructed complex\ninstruction-variant dataset, achieving a marked improvement in\ninstruction-following performance. However, this also induces nontrivial\ncatastrophic forgetting: the model loses some previously mastered task\ncapabilities when exposed to new instruction styles. Our benchmark provides a\nstandardized basis for assessing and improving instruction sensitivity in\nLALMs, underscoring the need for instruction-robust audio understanding in\nreal-world pipelines."}
{"id": "2510.22731", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.22731", "abs": "https://arxiv.org/abs/2510.22731", "authors": ["Yong Huang", "Wenjing Wang", "Dalong Zhang", "Junjie Wang", "Chen Chen", "Yan Cao", "Wei Wang"], "title": "Enhancing WiFi CSI Fingerprinting: A Deep Auxiliary Learning Approach", "comment": "To appear in the IEEE Internet of Things", "summary": "Radio frequency (RF) fingerprinting techniques provide a promising supplement\nto cryptography-based approaches but rely on dedicated equipment to capture\nin-phase and quadrature (IQ) samples, hindering their wide adoption. Recent\nadvances advocate easily obtainable channel state information (CSI) by\ncommercial WiFi devices for lightweight RF fingerprinting, while falling short\nin addressing the challenges of coarse granularity of CSI measurements in an\nopen-world setting. In this paper, we propose CSI2Q, a novel CSI fingerprinting\nsystem that achieves comparable performance to IQ-based approaches. Instead of\nextracting fingerprints directly from raw CSI measurements, CSI2Q first\ntransforms frequency-domain CSI measurements into time-domain signals that\nshare the same feature space with IQ samples. Then, we employ a deep auxiliary\nlearning strategy to transfer useful knowledge from an IQ fingerprinting model\nto the CSI counterpart. Finally, the trained CSI model is combined with an\nOpenMax function to estimate the likelihood of unknown ones. We evaluate CSI2Q\non one synthetic CSI dataset involving 85 devices and two real CSI datasets,\nincluding 10 and 25 WiFi routers, respectively. Our system achieves accuracy\nincreases of at least 16% on the synthetic CSI dataset, 20% on the in-lab CSI\ndataset, and 17% on the in-the-wild CSI dataset."}
{"id": "2510.22682", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2510.22682", "abs": "https://arxiv.org/abs/2510.22682", "authors": ["Bar Shaybet", "Vladimir Tourbabin", "Boaz Rafaely"], "title": "SRP-PHAT-NET: A Reliability-Driven DNN for Reverberant Speaker Localization", "comment": "In submission process to the IEEE Transactions on Audio, Speech and\n  Language Processing, 2025", "summary": "Accurate Direction-of-Arrival (DOA) estimation in reverberant environments\nremains a fundamental challenge for spatial audio applications. While deep\nlearning methods have shown strong performance in such conditions, they\ntypically lack a mechanism to assess the reliability of their predictions - an\nessential feature for real-world deployment. In this work, we present the\nSRP-PHAT-NET, a deep neural network framework that leverages SRP-PHAT\ndirectional maps as spatial features and introduces a built-in reliability\nestimation. To enable meaningful reliability scoring, the model is trained\nusing Gaussian-weighted labels centered around the true direction. We\nsystematically analyze the influence of label smoothing on accuracy and\nreliability, demonstrating that the choice of Gaussian kernel width can be\ntuned to application-specific requirements. Experimental results show that\nselectively using high-confidence predictions yields significantly improved\nlocalization accuracy, highlighting the practical benefits of integrating\nreliability into deep learning-based DOA estimation."}
{"id": "2510.08373", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2510.08373", "abs": "https://arxiv.org/abs/2510.08373", "authors": ["Hanke Xie", "Dake Guo", "Chengyou Wang", "Yue Li", "Wenjie Tian", "Xinfa Zhu", "Xinsheng Wang", "Xiulin Li", "Guanqiong Miao", "Bo Liu", "Lei Xie"], "title": "DialoSpeech: Dual-Speaker Dialogue Generation with LLM and Flow Matching", "comment": null, "summary": "Recent advances in text-to-speech (TTS) synthesis, particularly those\nleveraging large language models (LLMs), have significantly improved\nexpressiveness and naturalness. However, generating human-like, interactive\ndialogue speech remains challenging. Current systems face limitations due to\nthe scarcity of dual-track data and difficulties in achieving naturalness,\ncontextual coherence, and interactional dynamics, such as turn-taking,\noverlapping speech, and speaker consistency, in multi-turn conversations. To\naddress these challenges, we propose DialoSpeech, a dual-track architecture\ncombining a large language model with Chunked Flow Matching for expressive,\nhuman-like dialogue speech synthesis. DialoSpeech generates natural multi-turn\nconversations with coherent speaker turns and natural overlaps, supporting both\nChinese and English and cross-lingual speech synthesis. We introduce a data\nprocessing pipeline to construct dual-track dialogue datasets, facilitating\nscalable training and experimental validation. Experiments show that our model\noutperforms baselines, offering a solution for generating human-like spoken\ndialogues. Audio samples are available at\nhttps://tiamojames.github.io/DialoSpeech"}
{"id": "2510.22772", "categories": ["eess.SP", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.22772", "abs": "https://arxiv.org/abs/2510.22772", "authors": ["Yizhuo Wu", "Francesco Fioranelli", "Chang Gao"], "title": "Neural-HAR: A Dimension-Gated CNN Accelerator for Real-Time Radar Human Activity Recognition", "comment": null, "summary": "Radar-based human activity recognition (HAR) is attractive for unobtrusive\nand privacy-preserving monitoring, yet many CNN/RNN solutions remain too heavy\nfor edge deployment, and even lightweight ViT/SSM variants often exceed\npractical compute and memory budgets. We introduce Neural-HAR, a\ndimension-gated CNN accelerator tailored for real-time radar HAR on\nresource-constrained platforms. At its core is GateCNN, a parameter-efficient\nDoppler-temporal network that (i) embeds Doppler vectors to emphasize frequency\nevolution over time and (ii) applies dual-path gated convolutions that modulate\nDoppler-aware content features with temporal gates, complemented by a residual\npath for stable training. On the University of Glasgow UoG2020 continuous radar\ndataset, GateCNN attains 86.4% accuracy with only 2.7k parameters and 0.28M\nFLOPs per inference, comparable to CNN-BiGRU at a fraction of the complexity.\nOur FPGA prototype on Xilinx Zynq-7000 Z-7007S reaches 107.5 $\\mu$s latency and\n15 mW dynamic power using LUT-based ROM and distributed RAM only (zero\nDSP/BRAM), demonstrating real-time, energy-efficient edge inference. Code and\nHLS conversion scripts are available at https://github.com/lab-emi/AIRHAR."}
{"id": "2510.22950", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2510.22950", "abs": "https://arxiv.org/abs/2510.22950", "authors": ["Yuepeng Jiang", "Huakang Chen", "Ziqian Ning", "Jixun Yao", "Zerui Han", "Di Wu", "Meng Meng", "Jian Luan", "Zhonghua Fu", "Lei Xie"], "title": "DiffRhythm 2: Efficient and High Fidelity Song Generation via Block Flow Matching", "comment": null, "summary": "Generating full-length, high-quality songs is challenging, as it requires\nmaintaining long-term coherence both across text and music modalities and\nwithin the music modality itself. Existing non-autoregressive (NAR) frameworks,\nwhile capable of producing high-quality songs, often struggle with the\nalignment between lyrics and vocal. Concurrently, catering to diverse musical\npreferences necessitates reinforcement learning from human feedback (RLHF).\nHowever, existing methods often rely on merging multiple models during\nmulti-preference optimization, which results in significant performance\ndegradation. To address these challenges, we introduce DiffRhythm 2, an\nend-to-end framework designed for high-fidelity, controllable song generation.\nTo tackle the lyric alignment problem, DiffRhythm 2 employs a\nsemi-autoregressive architecture based on block flow matching. This design\nenables faithful alignment of lyrics to singing vocals without relying on\nexternal labels and constraints, all while preserving the high generation\nquality and efficiency of NAR models. To make this framework computationally\ntractable for long sequences, we implement a music variational autoencoder\n(VAE) that achieves a low frame rate of 5 Hz while still enabling high-fidelity\naudio reconstruction. In addition, to overcome the limitations of\nmulti-preference optimization in RLHF, we propose cross-pair preference\noptimization. This method effectively mitigates the performance drop typically\nassociated with model merging, allowing for more robust optimization across\ndiverse human preferences. We further enhance musicality and structural\ncoherence by introducing stochastic block representation alignment loss."}
{"id": "2510.22603", "categories": ["eess.AS", "cs.CV", "cs.SD"], "pdf": "https://arxiv.org/pdf/2510.22603", "abs": "https://arxiv.org/abs/2510.22603", "authors": ["Anand", "Umberto Cappellazzo", "Stavros Petridis", "Maja Pantic"], "title": "Mitigating Attention Sinks and Massive Activations in Audio-Visual Speech Recognition with LLMS", "comment": "The code is available at\n  https://github.com/umbertocappellazzo/Llama-AVSR", "summary": "Large language models (LLMs) have recently advanced auditory speech\nrecognition (ASR), visual speech recognition (VSR), and audio-visual speech\nrecognition (AVSR). However, understanding of their internal dynamics under\nfine-tuning remains limited. In natural language processing, recent work has\nrevealed attention sinks, tokens that attract disproportionately high\nattention, and associated massive activations in which some features of sink\ntokens exhibit huge activation in LLMs. In this work, we are the first to study\nthese phenomena in multimodal speech recognition. Through a detailed analysis\nof audio-visual LLMs, we identify attention sinks and massive activations not\nonly at the BOS token but also at intermediate low-semantic tokens across ASR,\nVSR, and AVSR. We show that massive activations originate in the MLP layers and\ncorrespond to fixed feature indices across all sink tokens. We further show\nthat intermediate sink tokens exhibit high cosine similarity to the BOS token,\nthereby amplifying attention and activation. Building on these insights, we\nintroduce a simple decorrelation loss that reduces cosine similarity between\nBOS and other tokens, effectively mitigating intermediate sinks and massive\nactivations. Furthermore, our method improves word error rate (WER) under high\naudio-visual feature downsampling while remaining stable at lower downsampling\nrates."}
{"id": "2510.22895", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.22895", "abs": "https://arxiv.org/abs/2510.22895", "authors": ["Wang Hao", "Kuang Zhang", "Hou Chengyu", "Yang Yifan", "Tan Chenxing", "Fu Weifeng"], "title": "Rmd: Robust Modal Decomposition with Constrained Bandwidth", "comment": null, "summary": "Modal decomposition techniques, such as Empirical Mode Decomposition (EMD),\nVariational Mode Decomposition (VMD), and Singular Spectrum Analysis (SSA),\nhave advanced time-frequency signal analysis since the early 21st century.\nThese methods are generally classified into two categories: numerical\noptimization-based methods (EMD, VMD) and spectral decomposition methods (SSA)\nthat consider the physical meaning of signals. The former can produce spurious\nmodes due to the lack of physical constraints, while the latter is more\nsensitive to noise and struggles with nonlinear signals. Despite continuous\nimprovements in these methods, a modal decomposition approach that effectively\ncombines the strengths of both categories remains elusive. This paper thus\nproposes a Robust Modal Decomposition (RMD) method with constrained bandwidth,\nwhich preserves the intrinsic structure of the signal by mapping the time\nseries into its trajectory-GRAM matrix in phase space. Moreover, the method\nincorporates bandwidth constraints during the decomposition process, enhancing\nnoise resistance. Extensive experiments on synthetic and real-world datasets,\nincluding millimeter-wave radar echoes, electrocardiogram (ECG),\nphonocardiogram (PCG), and bearing fault detection data, demonstrate the\nmethod's effectiveness and versatility. All code and dataset samples are\navailable on GitHub: https://github.com/Einstein-sworder/RMD."}
{"id": "2510.22961", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2510.22961", "abs": "https://arxiv.org/abs/2510.22961", "authors": ["Jing-Xuan Zhang", "Genshun Wan", "Jin Li", "Jianqing Gao"], "title": "Adapting Speech Foundation Models with Large Language Models for Unified Speech Recognition", "comment": "submitted to Pattern Recognition", "summary": "Unified speech recognition aims to perform auditory, visual, and audiovisual\nspeech recognition within a single model framework. While speech foundation\nmodels (SFMs) have demonstrated remarkable performance in auditory tasks, their\nadaptation to multimodal scenarios remains underexplored. This paper presents\nUASR-LLM, a novel framework that adapts frozen SFMs to unified VSR, ASR, and\nAVSR tasks by leveraging large language models (LLMs) as text decoders. Our\napproach introduces visual representations into multiple SFM layers through\nvisual injection modules, enabling multimodal input processing and unified\nhidden representations. The augmented SFMs connect with decoder-only LLMs via a\nfeed-forward adaptor, where concatenated representations and instruction\nprompts guide speech transcription. We implement a twostage training strategy:\nvisual injection pretraining followed by speech recognition finetuning. SFM\nparameters remain frozen throughout training, with only visual injection\nmodules optimized initially, and LLMs finetuned using LoRA parameters\nsubsequently. Experimental results demonstrate superior performance over\nstate-of-the-art baselines across VSR, ASR, and AVSR tasks under both clean and\nnoisy conditions. Ablation studies confirm generalization across various SFMs\nand LLMs, validating the proposed training strategy."}
{"id": "2510.23320", "categories": ["eess.AS", "cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2510.23320", "abs": "https://arxiv.org/abs/2510.23320", "authors": ["Máté Gedeon", "Péter Mihajlik"], "title": "LibriConvo: Simulating Conversations from Read Literature for ASR and Diarization", "comment": "Submitted to LREC 2026", "summary": "We introduce LibriConvo, a simulated multi-speaker conversational dataset\nbased on speaker-aware conversation simulation (SASC), designed to support\ntraining and evaluation of speaker diarization and automatic speech recognition\n(ASR) systems. Unlike prior resources that mostly rely on semantically\ndisconnected utterances and implausible temporal gaps, LibriConvo ensures\nsemantic coherence and realistic conversational timing. Our pipeline leverages\nCallHome with external VAD for reliable boundaries, applies compression to\nreduce unnaturally long silences, and organizes LibriTTS utterances by book to\nmaintain contextual consistency. Acoustic realism is enhanced via a novel room\nimpulse response selection procedure that ranks speaker-microphone\nconfigurations by spatial plausibility, balancing realism and diversity. The\ndataset comprises 240.1 hours across 1,496 dialogues with 830 unique speakers,\nsplit in a speaker-disjoint manner for robust evaluation. Baselines show that\nthe sortformer model outperforms the pyannote pipeline in diarization, while a\nfine-tuned Fast Conformer-CTC XLarge with Serialized Output Training achieves\n7.29\\% WER for ASR, surpassing zero-shot Whisper-large-v3. LibriConvo provides\na valuable resource for advancing multi-speaker speech processing research with\nrealistic conversational dynamics and controlled experimental conditions."}
{"id": "2510.22913", "categories": ["eess.SP", "cs.HC", "cs.LG", "cs.RO", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2510.22913", "abs": "https://arxiv.org/abs/2510.22913", "authors": ["Thanyanee Srichaisak", "Arissa Ieochai", "Aueaphum Aueawattthanaphisut"], "title": "Clinic-Oriented Feasibility of a Sensor-Fused Wearable for Upper-Limb Function", "comment": "19 pages, 7 figures, 5 Tables", "summary": "Background: Upper-limb weakness and tremor (4--12 Hz) limit activities of\ndaily living (ADL) and reduce adherence to home rehabilitation. Objective: To\nassess technical feasibility and clinician-relevant signals of a sensor-fused\nwearable targeting the triceps brachii and extensor pollicis brevis. Methods: A\nlightweight node integrates surface EMG (1 kHz), IMU (100--200 Hz), and\nflex/force sensors with on-device INT8 inference (Tiny 1D-CNN/Transformer) and\na safety-bounded assist policy (angle/torque/jerk limits; stall/time-out).\nHealthy adults (n = 12) performed three ADL-like tasks. Primary outcomes:\nTremor Index (TI), range of motion (ROM), repetitions (Reps min$^{-1}$).\nSecondary: EMG median-frequency slope (fatigue trend), closed-loop latency,\nsession completion, and device-related adverse events. Analyses used\nsubject-level paired medians with BCa 95\\% CIs; exact Wilcoxon $p$-values are\nreported in the Results. Results: Assistance was associated with lower tremor\nprominence and improved task throughput: TI decreased by $-0.092$ (95\\% CI\n[$-0.102$, $-0.079$]), ROM increased by $+12.65\\%$ (95\\% CI [$+8.43$,\n$+13.89$]), and Reps rose by $+2.99$ min$^{-1}$ (95\\% CI [$+2.61$, $+3.35$]).\nMedian on-device latency was 8.7 ms at a 100 Hz loop rate; all sessions were\ncompleted with no device-related adverse events. Conclusions: Multimodal\nsensing with low-latency, safety-bounded assistance produced improved movement\nquality (TI $\\downarrow$) and throughput (ROM, Reps $\\uparrow$) in a pilot\ntechnical-feasibility setting, supporting progression to IRB-approved patient\nstudies. Trial registration: Not applicable (pilot non-clinical)."}
{"id": "2510.23141", "categories": ["eess.AS", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.23141", "abs": "https://arxiv.org/abs/2510.23141", "authors": ["Sarabeth S. Mullins", "Georg Götz", "Eric Bezzam", "Steven Zheng", "Daniel Gert Nielsen"], "title": "Treble10: A high-quality dataset for far-field speech recognition, dereverberation, and enhancement", "comment": null, "summary": "Accurate far-field speech datasets are critical for tasks such as automatic\nspeech recognition (ASR), dereverberation, speech enhancement, and source\nseparation. However, current datasets are limited by the trade-off between\nacoustic realism and scalability. Measured corpora provide faithful physics but\nare expensive, low-coverage, and rarely include paired clean and reverberant\ndata. In contrast, most simulation-based datasets rely on simplified\ngeometrical acoustics, thus failing to reproduce key physical phenomena like\ndiffraction, scattering, and interference that govern sound propagation in\ncomplex environments. We introduce Treble10, a large-scale, physically accurate\nroom-acoustic dataset. Treble10 contains over 3000 broadband room impulse\nresponses (RIRs) simulated in 10 fully furnished real-world rooms, using a\nhybrid simulation paradigm implemented in the Treble SDK that combines a\nwave-based and geometrical acoustics solver. The dataset provides six\ncomplementary subsets, spanning mono, 8th-order Ambisonics, and 6-channel\ndevice RIRs, as well as pre-convolved reverberant speech scenes paired with\nLibriSpeech utterances. All signals are simulated at 32 kHz, accurately\nmodelling low-frequency wave effects and high-frequency reflections. Treble10\nbridges the realism gap between measurement and simulation, enabling\nreproducible, physically grounded evaluation and large-scale data augmentation\nfor far-field speech tasks. The dataset is openly available via the Hugging\nFace Hub, and is intended as both a benchmark and a template for\nnext-generation simulation-driven audio research."}
{"id": "2510.23541", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2510.23541", "abs": "https://arxiv.org/abs/2510.23541", "authors": ["Hanke Xie", "Haopeng Lin", "Wenxiao Cao", "Dake Guo", "Wenjie Tian", "Jun Wu", "Hanlin Wen", "Ruixuan Shang", "Hongmei Liu", "Zhiqi Jiang", "Yuepeng Jiang", "Wenxi Chen", "Ruiqi Yan", "Jiale Qian", "Yichao Yan", "Shunshun Yin", "Ming Tao", "Xie Chen", "Lei Xie", "Xinsheng Wang"], "title": "SoulX-Podcast: Towards Realistic Long-form Podcasts with Dialectal and Paralinguistic Diversity", "comment": null, "summary": "Recent advances in text-to-speech (TTS) synthesis have significantly improved\nspeech expressiveness and naturalness. However, most existing systems are\ntailored for single-speaker synthesis and fall short in generating coherent\nmulti-speaker conversational speech. This technical report presents\nSoulX-Podcast, a system designed for podcast-style multi-turn, multi-speaker\ndialogic speech generation, while also achieving state-of-the-art performance\nin conventional TTS tasks.\n  To meet the higher naturalness demands of multi-turn spoken dialogue,\nSoulX-Podcast integrates a range of paralinguistic controls and supports both\nMandarin and English, as well as several Chinese dialects, including\nSichuanese, Henanese, and Cantonese, enabling more personalized podcast-style\nspeech generation. Experimental results demonstrate that SoulX-Podcast can\ncontinuously produce over 90 minutes of conversation with stable speaker timbre\nand smooth speaker transitions. Moreover, speakers exhibit contextually\nadaptive prosody, reflecting natural rhythm and intonation changes as dialogues\nprogress. Across multiple evaluation metrics, SoulX-Podcast achieves\nstate-of-the-art performance in both monologue TTS and multi-turn\nconversational speech synthesis."}
{"id": "2510.22947", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.22947", "abs": "https://arxiv.org/abs/2510.22947", "authors": ["Yi Tao", "Zhen Gao", "Fangquan Ye", "Jingbo Xu", "Tao Song", "Weidong Li", "Yu Su", "Lu Peng", "Xiaomei Wu", "Tong Qin", "Zhongxiang Li", "Dezhi Zheng"], "title": "Intelligent Multimodal Multi-Sensor Fusion-Based UAV Identification, Localization, and Countermeasures for Safeguarding Low-Altitude Economy", "comment": null, "summary": "The development of the low-altitude economy has led to a growing prominence\nof uncrewed aerial vehicle (UAV) safety management issues. Therefore, accurate\nidentification, real-time localization, and effective countermeasures have\nbecome core challenges in airspace security assurance. This paper introduces an\nintegrated UAV management and control system based on deep learning, which\nintegrates multimodal multi-sensor fusion perception, precise positioning, and\ncollaborative countermeasures. By incorporating deep learning methods, the\nsystem combines radio frequency (RF) spectral feature analysis, radar\ndetection, electro-optical identification, and other methods at the detection\nlevel to achieve the identification and classification of UAVs. At the\nlocalization level, the system relies on multi-sensor data fusion and the\nair-space-ground integrated communication network to conduct real-time tracking\nand prediction of UAV flight status, providing support for early warning and\ndecision-making. At the countermeasure level, it adopts comprehensive measures\nthat integrate ``soft kill'' and ``hard kill'', including technologies such as\nelectromagnetic signal jamming, navigation spoofing, and physical interception,\nto form a closed-loop management and control process from early warning to\nfinal disposal, which significantly enhances the response efficiency and\ndisposal accuracy of low-altitude UAV management."}
{"id": "2510.23158", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2510.23158", "abs": "https://arxiv.org/abs/2510.23158", "authors": ["Philipp Götz", "Gloria Dal Santo", "Sebastian J. Schlecht", "Vesa Välimäki", "Emanuël A. P. Habets"], "title": "Matching Reverberant Speech Through Learned Acoustic Embeddings and Feedback Delay Networks", "comment": "Submitted to ICASSP 2026", "summary": "Reverberation conveys critical acoustic cues about the environment,\nsupporting spatial awareness and immersion. For auditory augmented reality\n(AAR) systems, generating perceptually plausible reverberation in real time\nremains a key challenge, especially when explicit acoustic measurements are\nunavailable. We address this by formulating blind estimation of artificial\nreverberation parameters as a reverberant signal matching task, leveraging a\nlearned room-acoustic prior. Furthermore, we propose a feedback delay network\n(FDN) structure that reproduces both frequency-dependent decay times and the\ndirect-to-reverberation ratio of a target space. Experimental evaluation\nagainst a leading automatic FDN tuning method demonstrates improvements in\nestimated room-acoustic parameters and perceptual plausibility of artificial\nreverberant speech. These results highlight the potential of our approach for\nefficient, perceptually consistent reverberation rendering in AAR applications."}
{"id": "2510.22948", "categories": ["eess.SP", "cs.AI", "cs.NI"], "pdf": "https://arxiv.org/pdf/2510.22948", "abs": "https://arxiv.org/abs/2510.22948", "authors": ["Zhaoming Hu", "Ruikang Zhong", "Xidong Mu", "Dengao Li", "Yuanwei Liu"], "title": "PASS-Enhanced MEC: Joint Optimization of Task Offloading and Uplink PASS Beamforming", "comment": null, "summary": "A pinching-antenna system (PASS)-enhanced mobile edge computing (MEC)\narchitecture is investigated to improve the task offloading efficiency and\nlatency performance in dynamic wireless environments. By leveraging dielectric\nwaveguides and flexibly adjustable pinching antennas, PASS establishes\nshort-distance line-of-sight (LoS) links while effectively mitigating the\nsignificant path loss and potential signal blockage, making it a promising\nsolution for high-frequency MEC systems. We formulate a network latency\nminimization problem to joint optimize uplink PASS beamforming and task\noffloading. The resulting problem is modeled as a Markov decision process (MDP)\nand solved via the deep reinforcement learning (DRL) method. To address the\ninstability introduced by the $\\max$ operator in the objective function, we\npropose a load balancing-aware proximal policy optimization (LBPPO) algorithm.\nLBPPO incorporates both node-level and waveguide-level load balancing\ninformation into the policy design, maintaining computational and transmission\ndelay equilibrium, respectively. Simulation results demonstrate that the\nproposed PASS-enhanced MEC with adaptive uplink PASS beamforming exhibit\nstronger convergence capability than fixed-PA baselines and conventional\nMIMO-assisted MEC, especially in scenarios with a large number of UEs or high\ntransmit power."}
{"id": "2510.23320", "categories": ["eess.AS", "cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2510.23320", "abs": "https://arxiv.org/abs/2510.23320", "authors": ["Máté Gedeon", "Péter Mihajlik"], "title": "LibriConvo: Simulating Conversations from Read Literature for ASR and Diarization", "comment": "Submitted to LREC 2026", "summary": "We introduce LibriConvo, a simulated multi-speaker conversational dataset\nbased on speaker-aware conversation simulation (SASC), designed to support\ntraining and evaluation of speaker diarization and automatic speech recognition\n(ASR) systems. Unlike prior resources that mostly rely on semantically\ndisconnected utterances and implausible temporal gaps, LibriConvo ensures\nsemantic coherence and realistic conversational timing. Our pipeline leverages\nCallHome with external VAD for reliable boundaries, applies compression to\nreduce unnaturally long silences, and organizes LibriTTS utterances by book to\nmaintain contextual consistency. Acoustic realism is enhanced via a novel room\nimpulse response selection procedure that ranks speaker-microphone\nconfigurations by spatial plausibility, balancing realism and diversity. The\ndataset comprises 240.1 hours across 1,496 dialogues with 830 unique speakers,\nsplit in a speaker-disjoint manner for robust evaluation. Baselines show that\nthe sortformer model outperforms the pyannote pipeline in diarization, while a\nfine-tuned Fast Conformer-CTC XLarge with Serialized Output Training achieves\n7.29\\% WER for ASR, surpassing zero-shot Whisper-large-v3. LibriConvo provides\na valuable resource for advancing multi-speaker speech processing research with\nrealistic conversational dynamics and controlled experimental conditions."}
{"id": "2510.23021", "categories": ["eess.SP", "cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.23021", "abs": "https://arxiv.org/abs/2510.23021", "authors": ["Xibin Jin", "Guoliang Li", "Shuai Wang", "Fan Liu", "Miaowen Wen", "Huseyin Arslan", "Derrick Wing Kwan Ng", "Chengzhong Xu"], "title": "Planning Oriented Integrated Sensing and Communication", "comment": null, "summary": "Integrated sensing and communication (ISAC) enables simultaneous\nlocalization, environment perception, and data exchange for connected\nautonomous vehicles. However, most existing ISAC designs prioritize sensing\naccuracy and communication throughput, treating all targets uniformly and\noverlooking the impact of critical obstacles on motion efficiency. To overcome\nthis limitation, we propose a planning-oriented ISAC (PISAC) framework that\nreduces the sensing uncertainty of planning-bottleneck obstacles and expands\nthe safe navigable path for the ego-vehicle, thereby bridging the gap between\nphysical-layer optimization and motion-level planning. The core of PISAC lies\nin deriving a closed-form safety bound that explicitly links ISAC transmit\npower to sensing uncertainty, based on the Cram\\'er-Rao Bound and occupancy\ninflation principles. Using this model, we formulate a bilevel power allocation\nand motion planning (PAMP) problem, where the inner layer optimizes the ISAC\nbeam power distribution and the outer layer computes a collision-free\ntrajectory under uncertainty-aware safety constraints. Comprehensive\nsimulations in high-fidelity urban driving environments demonstrate that PISAC\nachieves up to 40% higher success rates and over 5% shorter traversal times\nthan existing ISAC-based and communication-oriented benchmarks, validating its\neffectiveness in enhancing both safety and efficiency."}
{"id": "2510.23403", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2510.23403", "abs": "https://arxiv.org/abs/2510.23403", "authors": ["Ş. Ekmen", "H. Lee"], "title": "Evaluation of Spherical Wavelet Framework in Comparsion with Ambisonics", "comment": "13 pages, 8 figures. Submitted to IEEE TASLP", "summary": "Recently, the Spherical Wavelet Framework (SWF) was proposed to combine the\nbenefits of Ambisonics and Object-Based Audio (OBA) by utilising highly\nlocalised basis functions. SWF can enhance the sweet-spot area and reduce\nlocalisation blur while still enabling a sparse representation of the complete\nsound field, making storage and transmission more efficient. Initial vector\nanalysis and listening test of SWF have shown promising results; however, these\nfindings are limited to very specific conditions and do not include perceptual\nmetrics. The present study investigates SWF in greater detail, comparing it\nwith Ambisonics. The comparison was carried out using IACC, ITD, and ILD\nestimations, as well as listening tests with ecologically valid sound sources.\nVarious reproduction layouts: regular polyhedron, t-design, and Lebedev grid\nwith their corresponding Ambisonics orders and channel counts were evaluated.\nResults indicate that SWF is rated significantly more similar to the reference\nthan Ambisonics is, in terms of overall spatial and timbral fidelity; however,\nit is considerably dependent on the subdivison of the sphere. Moreover, it\ncannot natively represent a wave arriving at a continuous direction. Possible\nsolutions are proposed."}
{"id": "2510.23147", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.23147", "abs": "https://arxiv.org/abs/2510.23147", "authors": ["Parisa Kanani", "Mohammad Javad Omidi", "Mahmoud Modarres-Hashemi", "Halim Yanikomeroglu"], "title": "HAPS-ISAC for 6G: Architecture, Design Trade-offs, and a Practical Roadmap", "comment": null, "summary": "To meet the ambitious goals of next-generation 6G networks, including\nultra-high data rates and ubiquitous coverage, we propose a novel high-altitude\nplatform station (HAPS)-based integrated sensing and communication (ISAC)\narchitecture. Operating in the stratosphere, the HAPS functions as both a\npowerful communication hub and an advanced environmental sensor. Combined with\na fleet of cooperative uncrewed aerial vehicles (UAVs), this dual-purpose\nsystem forms a scalable and intelligent 3D network. Simulation results indicate\nthat this approach significantly boosts network performance, improves sensing\naccuracy, and ensures a fairer service distribution across users, outperforming\nconventional UAV-only baselines. We conclude by outlining the prospective\napplications and a deployment roadmap for this technology for smart cities and\nother large-scale environments."}
{"id": "2510.23541", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2510.23541", "abs": "https://arxiv.org/abs/2510.23541", "authors": ["Hanke Xie", "Haopeng Lin", "Wenxiao Cao", "Dake Guo", "Wenjie Tian", "Jun Wu", "Hanlin Wen", "Ruixuan Shang", "Hongmei Liu", "Zhiqi Jiang", "Yuepeng Jiang", "Wenxi Chen", "Ruiqi Yan", "Jiale Qian", "Yichao Yan", "Shunshun Yin", "Ming Tao", "Xie Chen", "Lei Xie", "Xinsheng Wang"], "title": "SoulX-Podcast: Towards Realistic Long-form Podcasts with Dialectal and Paralinguistic Diversity", "comment": null, "summary": "Recent advances in text-to-speech (TTS) synthesis have significantly improved\nspeech expressiveness and naturalness. However, most existing systems are\ntailored for single-speaker synthesis and fall short in generating coherent\nmulti-speaker conversational speech. This technical report presents\nSoulX-Podcast, a system designed for podcast-style multi-turn, multi-speaker\ndialogic speech generation, while also achieving state-of-the-art performance\nin conventional TTS tasks.\n  To meet the higher naturalness demands of multi-turn spoken dialogue,\nSoulX-Podcast integrates a range of paralinguistic controls and supports both\nMandarin and English, as well as several Chinese dialects, including\nSichuanese, Henanese, and Cantonese, enabling more personalized podcast-style\nspeech generation. Experimental results demonstrate that SoulX-Podcast can\ncontinuously produce over 90 minutes of conversation with stable speaker timbre\nand smooth speaker transitions. Moreover, speakers exhibit contextually\nadaptive prosody, reflecting natural rhythm and intonation changes as dialogues\nprogress. Across multiple evaluation metrics, SoulX-Podcast achieves\nstate-of-the-art performance in both monologue TTS and multi-turn\nconversational speech synthesis."}
{"id": "2510.23186", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.23186", "abs": "https://arxiv.org/abs/2510.23186", "authors": ["Lukas Henneke", "Frank Kurth"], "title": "Approaching Domain Generalization with Embeddings for Robust Discrimination and Recognition of RF Communication Signals", "comment": null, "summary": "Radio frequency (RF) signal recognition plays a critical role in modern\nwireless communication and security applications. Deep learning-based\napproaches have achieved strong performance but typically rely heavily on\nextensive training data and often fail to generalize to unseen signals. In this\npaper, we propose a method to learn discriminative embeddings without relying\non real-world RF signal recordings by training on signals of synthetic wireless\nprotocols. We validate the approach on a dataset of real RF signals and show\nthat the learned embeddings capture features enabling accurate discrimination\nof previously unseen real-world signals, highlighting its potential for robust\nRF signal classification and anomaly detection."}
{"id": "2510.21872", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.21872", "abs": "https://arxiv.org/abs/2510.21872", "authors": ["Jackson Loth", "Pedro Sarmento", "Mark Sandler", "Mathieu Barthet"], "title": "GuitarFlow: Realistic Electric Guitar Synthesis From Tablatures via Flow Matching and Style Transfer", "comment": "To be published in Proceedings of the 17th International Symposium on\n  Computer Music and Multidisciplinary Research (CMMR)", "summary": "Music generation in the audio domain using artificial intelligence (AI) has\nwitnessed steady progress in recent years. However for some instruments,\nparticularly the guitar, controllable instrument synthesis remains limited in\nexpressivity. We introduce GuitarFlow, a model designed specifically for\nelectric guitar synthesis. The generative process is guided using tablatures,\nan ubiquitous and intuitive guitar-specific symbolic format. The tablature\nformat easily represents guitar-specific playing techniques (e.g. bends, muted\nstrings and legatos), which are more difficult to represent in other common\nmusic notation formats such as MIDI. Our model relies on an intermediary step\nof first rendering the tablature to audio using a simple sample-based virtual\ninstrument, then performing style transfer using Flow Matching in order to\ntransform the virtual instrument audio into more realistic sounding examples.\nThis results in a model that is quick to train and to perform inference,\nrequiring less than 6 hours of training data. We present the results of\nobjective evaluation metrics, together with a listening test, in which we show\nsignificant improvement in the realism of the generated guitar audio from\ntablatures."}
{"id": "2510.23355", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.23355", "abs": "https://arxiv.org/abs/2510.23355", "authors": ["Pengyu Gao", "Qu Luo", "Jing Zhu", "Gaojie Chen", "Pei Xiao", "Chuan Heng Foh"], "title": "Uplink SCMA-empowered Uncoordinated Random Access for Future mMTC", "comment": null, "summary": "In this paper, a novel uncoordinated random access (URA) protocol is\npresented to address the pressing demand for massive connectivity with low\naccess latency in future massive machine type communication (mMTC) scenarios.\nThe proposed URA scheme integrates the classical slotted ALOHA (S-ALOHA)\nprotocol with sparse code multiple access (SCMA) technique, referred to as\nSCMA-empowered URA. Specifically, active users randomly choose an SCMA codebook\nto access the communication network in an arbitrary time slot whenever they\nwant without scheduling. However, due to the lack of central coordination in\nthe proposed URA scheme, SCMA codebook collisions become inevitable, making\ndecoding challenging and leading to increased access failures. To cope with the\ndecoding issue, an interference-canceling (IC) first decoding strategy is\nproposed at the access point (AP), which can partially tackles collision\nproblems, contributing to a higher system throughput. Taking the proposed\nIC-first decoding strategy into account, a closed-form theoretical expression\nof the throughput is derived. Moreover, to alleviate the throughput degradation\nunder the congested user traffic, a user barring mechanism is introduced to\nmanage the traffic load. Firstly, a closed-form expression of idle codebook\nprobability is developed to help indicate the system state, i.e., congested or\nnot. Then, in addition to the estimated real-time load, the AP adaptively\nadjusts the access probability and redistributes the actual access load.\nFinally, simulation results demonstrate that the proposed SCMA-empowered URA\nscheme enjoys higher maximum throughput, compared to the conventional\northogonal multiple access (OMA) based URA scheme. Moreover, the accuracy of\nthe presented theoretical analysis and the effectiveness of the user barring\nmechanism are verified."}
{"id": "2510.22455", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.22455", "abs": "https://arxiv.org/abs/2510.22455", "authors": ["Brandon James Carone", "Iran R. Roman", "Pablo Ripollés"], "title": "Evaluating Multimodal Large Language Models on Core Music Perception Tasks", "comment": "Accepted to the NeurIPS 2025 Workshop on AI for Music (AI4Music), 16\n  pages, 1 figure, 3 tables", "summary": "Multimodal Large Language Models (LLMs) claim \"musical understanding\" via\nevaluations that conflate listening with score reading. We benchmark three SOTA\nLLMs (Gemini 2.5 Pro, Gemini 2.5 Flash, and Qwen2.5-Omni) across three core\nmusic skills: Syncopation Scoring, Transposition Detection, and Chord Quality\nIdentification. Moreover, we separate three sources of variability: (i)\nperceptual limitations (audio vs. MIDI inputs), (ii) exposure to examples\n(zero- vs. few-shot manipulations), and (iii) reasoning strategies (Standalone,\nCoT, LogicLM). For the latter we adapt LogicLM, a framework combining LLMs with\nsymbolic solvers to perform structured reasoning, to music. Results reveal a\nclear perceptual gap: models perform near ceiling on MIDI but show accuracy\ndrops on audio. Reasoning and few-shot prompting offer minimal gains. This is\nexpected for MIDI, where performance reaches saturation, but more surprising\nfor audio, where LogicLM, despite near-perfect MIDI accuracy, remains notably\nbrittle. Among models, Gemini Pro achieves the highest performance across most\nconditions. Overall, current systems reason well over symbols (MIDI) but do not\nyet \"listen\" reliably from audio. Our method and dataset make the\nperception-reasoning boundary explicit and offer actionable guidance for\nbuilding robust, audio-first music systems."}
{"id": "2510.23440", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.23440", "abs": "https://arxiv.org/abs/2510.23440", "authors": ["Donatella Darsena", "Ivan Iudice", "Vincenzo Galdi", "Francesco Verde"], "title": "Randomized Space-Time Coded Stacked Intelligent Metasurfaces for Massive Multiuser Downlink Connectivity", "comment": "12 pages, 6 figures, 2 tables", "summary": "Stacked intelligent metasurfaces (SIMs) represent a key enabler for\nnext-generation wireless networks, offering beamforming gains while\nsignificantly reducing radio-frequency chain requirements. In conventional\nspace-only SIM architectures, the rate of reconfigurability of the SIM is equal\nto the inverse of the channel coherence time. This paper investigates a novel\nbeamforming strategy for massive downlink connectivity using a randomized\nspace-time (ST) coded SIM. In addition to conventional space-only metasurface\nlayers, the proposed design integrates a ST metasurface layer at the input\nstage of the SIM that introduces random time variations over each channel\ncoherence time interval. These artificial time variations enable opportunistic\nuser scheduling and exploitation of multiuser diversity under slow channel\ndynamics. To mitigate the prohibitive overhead associated with full channel\nstate information at the transmitter (CSIT), we propose a partial-CSIT-based\nbeamforming scheme that leverages randomized steering vectors and limited\nuser-side feedback based on signal quality measurements. Numerical results\ndemonstrate that the proposed ST-SIM architecture achieves satisfactory\nsum-rate performance while significantly reducing CSIT acquisition and feedback\noverhead, thereby enabling scalable downlink connectivity in dense networks."}
{"id": "2510.23312", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.23312", "abs": "https://arxiv.org/abs/2510.23312", "authors": ["Kamil Wojcicki", "Yusuf Ziya Isik", "Laura Lechler", "Mansur Yesilbursa", "Ivana Balić", "Wolfgang Mack", "Rafał Łaganowski", "Guoqing Zhang", "Yossi Adi", "Minje Kim", "Shinji Watanabe"], "title": "Low-Resource Audio Codec (LRAC): 2025 Challenge Description", "comment": null, "summary": "While recent neural audio codecs deliver superior speech quality at ultralow\nbitrates over traditional methods, their practical adoption is hindered by\nobstacles related to low-resource operation and robustness to acoustic\ndistortions. Edge deployment scenarios demand codecs that operate under\nstringent compute constraints while maintaining low latency and bitrate. The\npresence of background noise and reverberation further necessitates designs\nthat are resilient to such degradations. The performance of neural codecs under\nthese constraints and their integration with speech enhancement remain largely\nunaddressed. To catalyze progress in this area, we introduce the 2025\nLow-Resource Audio Codec Challenge, which targets the development of neural and\nhybrid codecs for resource-constrained applications. Participants are supported\nwith a standardized training dataset, two baseline systems, and a comprehensive\nevaluation framework. The challenge is expected to yield valuable insights\napplicable to both codec design and related downstream audio tasks."}
{"id": "2510.23467", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.23467", "abs": "https://arxiv.org/abs/2510.23467", "authors": ["Shreya Khisa", "Ali Amhaz", "Mohamed Elhattab", "Chadi Assi", "Sanaa Sharafeddine"], "title": "Joint Uplink and Downlink Resource Allocation and Antenna Activation for Pinching Antenna Systems", "comment": null, "summary": "In this paper, we explore a novel joint uplink and downlink framework\nutilizing a pinching antenna system (PASS). We consider two waveguides, one\ndedicated to transmission and one to reception, and both of them are connected\nto a base station (BS). Each type of waveguide consists of several pinching\nantennas (PAs) in some preconfigured positions. In this framework, we assume\nthe BS can serve downlink and uplink user equipments (UEs) at the same time\nusing the same spectrum resources through the presented PASS. In this aspect,\nwe formulate a sum rate optimization problem that jointly optimizes the antenna\nactivation factor, the BS transmit power, and the UE's transmit power, subject\nto power budget constraints for the BS and the UEs, as well as minimum rate\nrequirements for the UEs. The formulated problem is highly non-convex and\ndifficult to solve directly. Hence, we divide the main problem into two\nsub-problems: the antenna activation sub-problem and the power allocation\nsub-problem. Then, we solve the antenna activation problem utilizing a distance\nand spatial correlation-based algorithm. Meanwhile, the resource allocation\nproblem is solved using a successive convex approximation (SCA)-based\nalgorithm. Numerical results show that our proposed framework can achieve\naround 60-90\\% performance gains over its time division duplex (TDD) where the\nuplink and downlink transmissions are served in different orthogonal time\nslots."}
{"id": "2510.23530", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.23530", "abs": "https://arxiv.org/abs/2510.23530", "authors": ["Bernardo Torres", "Manuel Moussallam", "Gabriel Meseguer-Brocal"], "title": "Learning Linearity in Audio Consistency Autoencoders via Implicit Regularization", "comment": null, "summary": "Audio autoencoders learn useful, compressed audio representations, but their\nnon-linear latent spaces prevent intuitive algebraic manipulation such as\nmixing or scaling. We introduce a simple training methodology to induce\nlinearity in a high-compression Consistency Autoencoder (CAE) by using data\naugmentation, thereby inducing homogeneity (equivariance to scalar gain) and\nadditivity (the decoder preserves addition) without altering the model's\narchitecture or loss function. When trained with our method, the CAE exhibits\nlinear behavior in both the encoder and decoder while preserving reconstruction\nfidelity. We test the practical utility of our learned space on music source\ncomposition and separation via simple latent arithmetic. This work presents a\nstraightforward technique for constructing structured latent spaces, enabling\nmore intuitive and efficient audio processing."}
{"id": "2510.22637", "categories": ["eess.AS", "eess.SP"], "pdf": "https://arxiv.org/pdf/2510.22637", "abs": "https://arxiv.org/abs/2510.22637", "authors": ["Yuval Bar Ilan", "Boaz Rafaely", "Vladimir Tourbabin"], "title": "HyBeam: Hybrid Microphone-Beamforming Array-Agnostic Speech Enhancement for Wearables", "comment": null, "summary": "Speech enhancement is a fundamental challenge in signal processing,\nparticularly when robustness is required across diverse acoustic conditions and\nmicrophone setups. Deep learning methods have been successful for speech\nenhancement, but often assume fixed array geometries, limiting their use in\nmobile, embedded, and wearable devices. Existing array-agnostic approaches\ntypically rely on either raw microphone signals or beamformer outputs, but both\nhave drawbacks under changing geometries. We introduce HyBeam, a hybrid\nframework that uses raw microphone signals at low frequencies and beamformer\nsignals at higher frequencies, exploiting their complementary strengths while\nremaining highly array-agnostic. Simulations across diverse rooms and wearable\narray configurations demonstrate that HyBeam consistently surpasses\nmicrophone-only and beamformer-only baselines in PESQ, STOI, and SI-SDR. A\nbandwise analysis shows that the hybrid approach leverages beamformer\ndirectivity at high frequencies and microphone cues at low frequencies,\noutperforming either method alone across all bands."}
{"id": "2510.23558", "categories": ["cs.SD", "cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.23558", "abs": "https://arxiv.org/abs/2510.23558", "authors": ["Bohan Li", "Wenbin Huang", "Yuhang Qiu", "Yiwei Guo", "Hankun Wang", "Zhihan Li", "Jing Peng", "Ziyang Ma", "Xie Chen", "Kai Yu"], "title": "ISA-Bench: Benchmarking Instruction Sensitivity for Large Audio Language Models", "comment": "submitted to icassp 2026", "summary": "Large Audio Language Models (LALMs), which couple acoustic perception with\nlarge language models (LLMs) to extract and understand diverse information from\naudio, have attracted intense interest from both academic and industrial\ncommunities. However, existing LALMs are highly sensitive to how instructions\nare phrased, affecting both (i) instruction-following rates and (ii) task\nperformance. Yet, no existing benchmarks offer a systematic and comprehensive\nevaluation of this sensitivity. We introduce ISA-Bench, a dynamic benchmark\nevaluating instruction sensitivity for LALMs along three axes: instruction\ndescription, output format, and task composition. We assess recent open-source\nand proprietary LALMs using ISA-Bench, profiling both compliance and accuracy\nunder controlled instruction variations. Experimental results reveal that even\nstate-of-the-art LALMs suffer significant instruction sensitivity, leading to\ndegraded performance on fundamental audio understanding tasks. To mitigate this\nissue, we fine-tune Qwen2-Audio on a specifically constructed complex\ninstruction-variant dataset, achieving a marked improvement in\ninstruction-following performance. However, this also induces nontrivial\ncatastrophic forgetting: the model loses some previously mastered task\ncapabilities when exposed to new instruction styles. Our benchmark provides a\nstandardized basis for assessing and improving instruction sensitivity in\nLALMs, underscoring the need for instruction-robust audio understanding in\nreal-world pipelines."}

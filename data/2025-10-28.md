<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 22]
- [eess.AS](#eess.AS) [Total: 16]
- [cs.SD](#cs.SD) [Total: 11]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [Automated Tinnitus Detection Through Dual-Modality Neuroimaging: EEG Microstate Analysis and Resting-State fMRI Classification Using Deep Learning](https://arxiv.org/abs/2510.21748)
*Kiana Kiashemshaki,Sina Samieirad,Sarvenaz Erfani,Aryan Jalaeianbanayan,Nasibeh Asadi Isakan,Hossein Najafzadeh*

Main category: eess.SP

TL;DR: 本研究应用机器学习分析EEG和fMRI数据，发现耳鸣患者与健康对照的神经特征差异，树基分类器和混合模型在耳鸣分类中表现优异，准确率高达98.8%。


<details>
  <summary>Details</summary>
Motivation: 耳鸣影响10-15%的人口但缺乏客观诊断生物标志物，需要识别区分耳鸣患者与健康对照的神经特征。

Method: 分析64通道EEG记录（80人）和静息态fMRI数据（38人）。EEG分析提取微状态特征和全局场功率信号的小波图像；fMRI使用切片卷积神经网络和混合模型（VGG16、ResNet50结合决策树、随机森林、SVM）。

Result: EEG微状态分析显示耳鸣患者γ波段微状态B出现减少（健康：56.56 vs 耳鸣：43.81，p<0.001）和α覆盖减少。树基分类器达98.8%准确率，VGG16在小波变换EEG上达95.4%（δ波段）和94.1%（α波段）。fMRI分析识别12个高准确率轴向切片（≥90%），切片17达99.0%。混合VGG16-决策树模型达98.95%±2.94%准确率。

Conclusion: EEG和fMRI提供了有效的耳鸣分类神经生物标志物。树基和混合模型表现优异，表明耳鸣是需要多模态分析的多网络障碍。

Abstract: Objective: Tinnitus affects 10-15% of the population yet lacks objective
diagnostic biomarkers. This study applied machine learning to EEG and fMRI data
to identify neural signatures distinguishing tinnitus patients from healthy
controls. Methods: Two datasets were analyzed: 64-channel EEG recordings from
80 participants (40 tinnitus, 40 controls) and resting-state fMRI data from 38
participants (19 tinnitus, 19 controls). EEG analysis extracted microstate
features across four to seven clustering states and five frequency bands,
producing 440 features per subject. Global Field Power signals were also
transformed into wavelet images for deep learning. fMRI data were analyzed
using slice-wise convolutional neural networks and hybrid models combining
pre-trained architectures (VGG16, ResNet50) with Decision Tree, Random Forest,
and SVM classifiers. Model performance was evaluated using 5-fold
cross-validation based on accuracy, precision, recall, F1-score, and ROC-AUC.
Results: EEG microstate analysis revealed altered network dynamics in tinnitus,
particularly reduced gamma-band microstate B occurrence (healthy: 56.56 vs
tinnitus: 43.81, p < 0.001) and diminished alpha coverage. Tree-based
classifiers achieved up to 98.8% accuracy, while VGG16 on wavelet-transformed
EEG yielded 95.4% and 94.1% accuracy for delta and alpha bands, respectively.
fMRI analysis identified 12 high-performing axial slices (>=90% accuracy), with
slice 17 reaching 99.0%. The hybrid VGG16-Decision Tree model achieved 98.95%
+/- 2.94% accuracy. Conclusion: EEG and fMRI provided effective neural
biomarkers for tinnitus classification. Tree-based and hybrid models
demonstrated superior performance, highlighting tinnitus as a multi-network
disorder requiring multimodal analysis.

</details>


### [2] [Monitoring Real-Time ECG Signals on Mobile Systems](https://arxiv.org/abs/2510.21789)
*Beyazit Bestami Yuksel*

Main category: eess.SP

TL;DR: 开发了一个基于移动系统的实时心电图监测系统，通过非侵入方法读取ECG信号并在移动设备上图形化显示，当信号异常时提供医疗警报。


<details>
  <summary>Details</summary>
Motivation: 实现便携式实时心电图监测，为远程患者监护、位置跟踪和初步干预提供基础。

Method: 使用Visual Studio .NET平台开发软件，通过Einthoven三角法放置电极，通过串口传输数据并在移动设备上图形化显示ECG信号。

Result: 成功开发了完全便携的系统，能够实时监测ECG信号并在异常时发出警报。

Conclusion: 该系统为未来多用途医疗监测系统（如在线患者监护、位置跟踪和除颤干预）奠定了基础。

Abstract: This study focuses on the connection of a development kit that enables
real-time monitoring of electrocardiogram (ECG) signals using a mobile system.
A software developed on the Visual Studio .NET platform reads real-time ECG
signals from the human body through non invasive methods and displays them
graphically on the mobile system. ECG electrodes placed on specific areas of
the body using the method known as Einthoven's triangle. Subsequently, the
software initiates data flow through the serial port, and these data displayed
as signal values on the mobile device's screen via a graphical interface. When
the monitored ECG signals fall below a certain threshold or reach a critical
value, the system provides feedback with an alert based on medical data. The
developed system is fully portable. Additionally, the implemented system has
the potential to form the basis for a multi-purpose system in the future, such
as online patient monitoring, patient location tracking, and even initial
intervention using the defibrillation method.

</details>


### [3] [Adaptive Split-MMD Training for Small-Sample Cross-Dataset P300 EEG Classification](https://arxiv.org/abs/2510.21969)
*Weiyu Chen,Arnaud Delorme*

Main category: eess.SP

TL;DR: 提出AS-MMD方法解决小样本P300检测中的跨数据集迁移学习问题，结合目标加权损失、Split Batch Normalization和RBF-MMD正则化，在严格小样本条件下优于目标域单独训练和池化训练。


<details>
  <summary>Details</summary>
Motivation: 当只有少量标记试次可用时，单试次P300检测很困难。在通过迁移学习用小目标集增强大源数据集时，会出现跨数据集偏移问题。

Method: 提出自适应分割最大均值差异训练(AS-MMD)，结合：(i)与源/目标大小比平方根相关的目标加权损失和预热；(ii)具有共享仿射参数和每域运行统计的分割批归一化；(iii)使用中值带宽启发式的无参数logit级径向基函数核最大均值差异项。在EEG Conformer上实现。

Result: 在两个迁移方向上均优于目标域单独训练和池化训练（Active Visual Oddball：准确率/AUC 0.66/0.74；ERP CORE P3：0.61/0.65），经校正配对t检验显示相对于池化的增益显著。消融实验表明所有三个组件都贡献了改进。

Conclusion: AS-MMD方法有效解决了小样本P300检测中的跨数据集迁移学习问题，在严格小样本条件下表现出优越性能。

Abstract: Detecting single-trial P300 from EEG is difficult when only a few labeled
trials are available. When attempting to boost a small target set with a large
source dataset through transfer learning, cross-dataset shift arises. To
address this challenge, we study transfer between two public visual-oddball ERP
datasets using five shared electrodes (Fz, Pz, P3, P4, Oz) under a strict
small-sample regime (target: 10 trials/subject; source: 80 trials/subject). We
introduce Adaptive Split Maximum Mean Discrepancy Training (AS-MMD), which
combines (i) a target-weighted loss with warm-up tied to the square root of the
source/target size ratio, (ii) Split Batch Normalization (Split-BN) with shared
affine parameters and per-domain running statistics, and (iii) a parameter-free
logit-level Radial Basis Function kernel Maximum Mean Discrepancy (RBF-MMD)
term using the median-bandwidth heuristic. Implemented on an EEG Conformer,
AS-MMD is backbone-agnostic and leaves the inference-time model unchanged.
Across both transfer directions, it outperforms target-only and pooled training
(Active Visual Oddball: accuracy/AUC 0.66/0.74; ERP CORE P3: 0.61/0.65), with
gains over pooling significant under corrected paired t-tests. Ablations
attribute improvements to all three components.

</details>


### [4] [Experimental Demonstration of Multi-Object Tracking in Integrated Sensing and Communication](https://arxiv.org/abs/2510.22180)
*Maximilian Bauhofer,Marcus Henninger,Meik Kottkamp,Lucas Giroto,Philip Grill,Alexander Felix,Thorsten Wild,Stephan ten Brink,Silvio Mandelli*

Main category: eess.SP

TL;DR: 该论文展示了在5G兼容的集成感知与通信系统中，基于概率假设密度滤波器在距离和多普勒速度域的多目标跟踪方法，并在真实工厂环境中验证了性能。


<details>
  <summary>Details</summary>
Motivation: 将跟踪技术融入蜂窝通信系统对于广泛的集成感知与通信应用场景至关重要，但现有多目标跟踪算法尚未应用于面临杂波和非理想硬件挑战的真实ISAC环境。

Method: 使用5G兼容的ISAC概念验证系统，在真实工厂环境中采集数据，采用基于概率假设密度滤波器在距离和多普勒速度域进行多目标跟踪，详细描述了从测量采集到评估的完整处理流程。

Result: 端到端评估显示良好的多目标跟踪性能，在现实但具有挑战性的场景中，平均绝对误差<1.5米，检测率>91%。

Conclusion: 该方法成功展示了在真实ISAC环境中实现多目标跟踪的可行性，为集成感知与通信系统的实际应用提供了重要参考。

Abstract: For a wide range of envisioned integrated sensing and communication (ISAC)
use cases, it is necessary to incorporate tracking techniques into cellular
communication systems. While numerous multi-object tracking algorithms exist,
they have not yet been applied to real-world ISAC, with its challenges such as
clutter and non-optimal hardware. In this work, we showcase multi-object
tracking based on the probability hypothesis density (PHD) filter in the range
and Doppler speed domain. The measurements are taken with a 5G compliant ISAC
proof-of-concept in a real factory environment, where the pedestrian-like
objects are generated by a radar object emulator. We detail the complete
pipeline, from measurement acquisition to evaluation, with a focus on the
post-processing of the raw captured data and the tracking itself. Our
end-to-end evaluation and comparison to simulations show good multi-object
tracking performance with mean absolute error <1.5m and detection rates >91%
for realistic but challenging scenarios.

</details>


### [5] [Angular Estimation Comparison with ISAC PoC](https://arxiv.org/abs/2510.22297)
*Alexander Felix,Rudolf Hoffmann,Marcus Henninger,Stephan ten Brink,Silvio Mandelli*

Main category: eess.SP

TL;DR: 评估基于最小DFT角度采样的不同角度估计方法，发现插值方法在泛化性上更优，DFT插值整体性能最佳，而OMP在单个强目标时最准确。


<details>
  <summary>Details</summary>
Motivation: ISAC系统中使用模拟/混合波束成形限制了角度感知能力，需要基于最少角度样本实现最优角度估计和目标分离。

Method: 使用ISAC概念验证系统在工业场景中扫描多个波束获取最小DFT角度样本，比较不同角度估计方法。

Result: 插值方法在不同角度场景下泛化性更好，DFT插值整体估计性能最佳，OMP对单个强目标估计最准确。

Conclusion: DFT插值方法在ISAC角度估计中表现最优，为最小样本角度估计提供了有效解决方案。

Abstract: The introduction of Integrated Sensing and Communications (ISAC) in cellular
systems is not expected to result in a shift away from the popular choice of
cost- and energy-efficient analog or hybrid beamforming structures. However,
this comes at the cost of limiting the angular capabilities to a confined space
per acquisitions. Thus, as a prerequisite for the successful implementation of
numerous ISAC use cases, the need for an optimal angular estimation of targets
and their separation based on the minimal number of angular samples arises.
  In this work, different approaches for angular estimation based on a minimal,
DFT-based set of angular samples are evaluated. The samples are acquired
through sweeping multiple beams of an ISAC proof of concept (PoC) in the
industrial scenario of the ARENA2036. The study's findings indicate that
interpolation approaches are more effective for generalizing across different
types of angular scenarios. While the orthogonal matching pursuit (OMP)
approach exhibits the most accurate estimation for a single, strong and clearly
discriminable target, the DFT-based interpolation approach demonstrates the
best overall estimation performance.

</details>


### [6] [Data-driven, Wavelet-based Identification and Reduced-order Modeling of Linear Systems with Closely Spaced Modes](https://arxiv.org/abs/2510.22406)
*Anargyros Michaloliakos,Benjamin J. Chang,Lawrence A. Bergman,Alexander F. Vakakis*

Main category: eess.SP

TL;DR: 提出基于小波变换的数据驱动模态识别和降阶建模框架，有效解决传统傅里叶方法在密集模态识别中的局限性


<details>
  <summary>Details</summary>
Motivation: 传统傅里叶方法难以可靠识别密集模态和准确捕捉模态相互作用，需要开发更有效的模态识别方法

Method: 利用连续小波变换的时频分辨率优势，通过选择小波谱中的谐波区域分离模态，应用逆小波变换重构时域模态动态，结合希尔伯特变换提取瞬时相位

Result: 方法在数值非经典阻尼和飞机结构实验测试中验证有效，能够准确解析复杂模态相互作用并重现结构系统动态响应

Conclusion: 所提出的小波基方法能够稳健地表征系统模态特性，即使在噪声和模态干扰等挑战性扰动下也能准确识别模态参数

Abstract: This work presents a purely data-driven, wavelet-based framework for modal
identification and reduced-order modeling of mechanical systems with assumed
linear dynamics characterized by closely spaced modes with classical or
non-classical damping distribution. Traditional Fourier-based methods often
fail to reliably identify closely spaced modes or accurately capture modal
interactions and complexities. To address these limitations, we propose a
methodology leveraging the enhanced time -frequency resolution capabilities of
the continuous wavelet transform (CWT). By selecting appropriate harmonic
regions within the wavelet spectra, we effectively isolate modes, and then
invert them back in the temporal domain by applying the inverse CWT (ICWT). In
this way we reconstruct the corresponding modal dynamics in the time domain.
Using the Hilbert transform, instantaneous phases are extracted for each
identified mode, enabling the introduction of a complexified modal matrix which
robustly characterizes the system's modal properties, even under challenging
perturbations such as noise and uncertainties due to modal interference and
unmodeled effects. The identified modal parameters are utilized to reconstruct
the frequency response functions (FRFs) of the system and to develop a
reduced-order model (ROM) that captures accurately the system's dominant
dynamical behavior valid in a specified frequency range.. Validation of the
methodology is conducted both with a numerical non-classical damping and an
experimental testbed representing a model of an airplane structure. Results
demonstrate the effectiveness of the proposed approach in resolving intricate
modal interactions and accurately reproducing the dynamic response of complex
structural systems.

</details>


### [7] [Genetic Optimization of a Software-Defined GNSS Receiver](https://arxiv.org/abs/2510.22417)
*Laura Train,Rodrigo Castellanos,Miguel Gómez-López*

Main category: eess.SP

TL;DR: 提出基于遗传算法的优化框架，自动搜索GNSS接收机最优跟踪环参数，解决高动态环境下传统接收机性能下降问题。


<details>
  <summary>Details</summary>
Motivation: 商用GNSS接收机在高动态环境（如火箭发射）中性能严重下降，传统跟踪环带宽无法适应同步参数的快速变化。软件定义无线电接收机虽可灵活配置，但手动调参复杂且难以保证最优性能。

Method: 使用遗传算法优化框架，在SDR环境中自动探索接收机配置空间，优化相位、频率和延迟跟踪环参数。在GNSS-SDR开源架构中处理模拟的GPS L1信号，验证三种动态场景。

Result: 优化配置在不同动态条件下均能保持稳健准确的PVT解：静态场景最大位置/速度误差约6m/0.08m/s，火箭场景12m/2.5m/s，LEO场景5m/0.2m/s。

Conclusion: 进化优化使SDR接收机能够在多样化动态条件下维持鲁棒准确的导航性能，解决了高动态环境下的GNSS跟踪问题。

Abstract: Commercial off-the-shelf (COTS) Global Navigation Satellite System (GNSS)
receivers face significant limitations under high-dynamic conditions,
particularly in high-acceleration environments such as those experienced by
launch vehicles. These performance degradations, often observed as
discontinuities in the navigation solution, arise from the inability of
traditional tracking loop bandwidths to cope with rapid variations in
synchronization parameters. Software-Defined Radio (SDR) receivers overcome
these constraints by enabling flexible reconfiguration of tracking loops;
however, manual tuning involves a complex, multidimensional search and seldom
ensures optimal performance. This work introduces a genetic algorithm-based
optimization framework that autonomously explores the receiver configuration
space to determine optimal loop parameters for phase, frequency, and delay
tracking. The approach is validated within an SDR environment using
realistically simulated GPS L1 signals for three representative dynamic regimes
-guided rocket flight, Low Earth Orbit (LEO) satellite, and static
receiver-processed with the open-source GNSS-SDR architecture. Results
demonstrate that evolutionary optimization enables SDR receivers to maintain
robust and accurate Position, Velocity, and Time (PVT) solutions across diverse
dynamic conditions. The optimized configurations yielded maximum position and
velocity errors of approximately 6 m and 0.08 m/s for the static case, 12 m and
2.5 m/s for the rocket case, and 5 m and 0.2 m/s for the LEO case.

</details>


### [8] [Data-driven Exponential Framing for Pulsive Temporal Patterns without Repetition or Singularity](https://arxiv.org/abs/2510.22472)
*Yohei Kono,Yoshiyuki Tajima*

Main category: eess.SP

TL;DR: 提出了一种名为数据驱动指数框架（DEF）的方法，用于从小数据集中提取脉冲时间模式，无需依赖模式的重复性或奇异性。该方法基于时间延迟嵌入和Hankel矩阵分析，通过线性动力系统模型量化时间模式的主导时长。


<details>
  <summary>Details</summary>
Motivation: 在制造应用中，从小数据集中提取不重复或非奇异的脉冲时间模式具有重要意义，但尚未得到足够科学关注。需要一种不依赖模式重复性或奇异性的量化方法。

Method: 受时间延迟嵌入和数据驱动Hankel矩阵分析启发，在时间延迟坐标上建立线性动力系统模型，推导具有不同指数衰减常数的离散时间基函数。通过滑动窗口提取子序列，并将基函数拟合到子序列上来量化模式的主导时长。

Result: 玩具模型实验显示DEF能识别具有不同长度的多个模式。在冲压机电流测量应用中，DEF成功从真实世界振荡数据中提取了多个模式。

Conclusion: DEF方法能够有效量化时间模式的主导时长，无需依赖模式的重复性或奇异性，为小数据集中的脉冲时间模式提取提供了可行方案。

Abstract: Extracting pulsive temporal patterns from a small dataset without their
repetition or singularity shows significant importance in manufacturing
applications but does not sufficiently attract scientific attention. We propose
to quantify how long temporal patterns appear without relying on their
repetition or singularity, enabling to extract such temporal patterns from a
small dataset. Inspired by the celebrated time delay embedding and data-driven
Hankel matrix analysis, we introduce a linear dynamical system model on the
time-delay coordinates behind the data to derive the discrete-time bases each
of which has a distinct exponential decay constant. The derived bases are
fitted onto subsequences that are extracted with a sliding window in order to
quantify how long patterns are dominant in the set of subsequences. We call the
quantification method Data-driven Exponential Framing (DEF). A toy model-based
experiment shows that DEF can identify multiple patterns with distinct lengths.
DEF is also applied to electric current measurement on a punching machine,
showing its possibility to extract multiple patterns from real-world
oscillatory data.

</details>


### [9] [Large-Model AI for Near Field Beam Prediction: A CNN-GPT2 Framework for 6G XL-MIMO](https://arxiv.org/abs/2510.22557)
*Wang Liu,Cunhua Pan,Hong Ren,Wei Zhang,Cheng-Xiang Wang,Jiangzhou Wang*

Main category: eess.SP

TL;DR: 提出基于CNN-GPT2的框架解决毫米波通信中大规模天线阵列的近场波束预测问题，通过上行链路导频传输策略和深度学习模型预测波束索引。


<details>
  <summary>Details</summary>
Motivation: 大规模天线阵列在毫米波通信中面临近场波束预测挑战，传统远场假设不再适用，近场需要联合采样角度和距离域，导致导频开销剧增，且最优波束索引呈现非线性动态特性。

Method: 设计上行链路导频传输策略，采用宽波束模拟预编码和频率变化数字预编码进行高效信道探测；使用CNN特征提取器和GPT-2模型捕获时间依赖性，端到端预测近场波束索引。

Result: 提出的CNN-GPT2框架能够有效处理近场波束预测中的非线性动态特性，减少导频开销。

Conclusion: 该深度学习框架为毫米波通信中大规模天线阵列的近场波束预测提供了有效解决方案，克服了传统方法的局限性。

Abstract: The emergence of extremely large-scale antenna arrays (ELAA) in
millimeter-wave (mmWave) communications, particularly in high-mobility
scenarios, highlights the importance of near-field beam prediction. Unlike the
conventional far-field assumption, near-field beam prediction requires
codebooks that jointly sample the angular and distance domains, which leads to
a dramatic increase in pilot overhead. Moreover, unlike the far-field case
where the optimal beam evolution is temporally smooth, the optimal near-field
beam index exhibits abrupt and nonlinear dynamics due to its joint dependence
on user angle and distance, posing significant challenges for temporal
modeling. To address these challenges, we propose a novel Convolutional Neural
Network-Generative Pre-trained Transformer 2 (CNN-GPT2) based near-field beam
prediction framework. Specifically, an uplink pilot transmission strategy is
designed to enable efficient channel probing through widebeam analog precoding
and frequency-varying digital precoding. The received pilot signals are
preprocessed and passed through a CNN-based feature extractor, followed by a
GPT-2 model that captures temporal dependencies across multiple frames and
directly predicts the near-field beam index in an end-to-end manner.

</details>


### [10] [Parametric Channel Estimation and Design for Active-RIS-Assisted Communications](https://arxiv.org/abs/2510.22621)
*Md. Shahriar Sadid,Ali A. Nasir,Saad Al-Ahmadi,Samir Al-Ghadhban*

Main category: eess.SP

TL;DR: 提出了一种针对主动RIS的参数化信道估计方法，通过自适应MLE和主动RIS配置策略，使用少量导频实现近最优性能


<details>
  <summary>Details</summary>
Motivation: 传统RIS受限于级联信道结构和非参数方法的高导频开销，难以获取准确CSI。主动RIS通过信号放大提高了实际部署的可行性

Method: 结合主动RIS模型和自适应MLE，采用自适应主动RIS配置策略和正交角度对码本，减少码本大小并确保远场和近场用户的可靠操作

Result: 与被动RIS相比，主动RIS在相同总功率预算下获得更高的频谱效率，通过消除乘性衰落并将更多资源分配给数据传输

Conclusion: 所提方法使用极少导频即可实现近最优性能，显著优于非参数方法，为主动RIS的实际应用提供了有效的信道估计解决方案

Abstract: Reconfigurable Intelligent Surface (RIS) technology has emerged as a key
enabler for future wireless communications. However, its potential is
constrained by the difficulty of acquiring accurate user-to-RIS channel state
information (CSI), due to the cascaded channel structure and the high pilot
overhead of non-parametric methods. Unlike a passive RIS, where the reflected
signal suffers from multiplicative path loss, an active RIS amplifies the
signal, improving its practicality in real deployments. In this letter, we
propose a parametric channel estimation method tailored for active RISs. The
proposed approach integrates an active RIS model with an adaptive Maximum
Likelihood Estimator (MLE) to recover the main channel parameters using a
minimal number of pilots. To further enhance performance, an adaptive active
RIS configuration strategy is employed, which refines the beam direction based
on an initial user location estimate. Moreover, an orthogonal angle-pair
codebook is used instead of the conventional Discrete Fourier Transform (DFT)
codebook, significantly reducing the codebook size and ensuring reliable
operation for both far-field and near-field users. Extensive simulations
demonstrate that the proposed method achieves near-optimal performance with
very few pilots compared to non-parametric approaches. Its performance is also
benchmarked against that of a traditional passive RIS under the same total
power budget to ensure fairness. Results show that active RIS yields higher
spectral efficiency (SE) by eliminating the multiplicative fading inherent in
passive RISs and allocating more resources to data transmission

</details>


### [11] [Enhancing WiFi CSI Fingerprinting: A Deep Auxiliary Learning Approach](https://arxiv.org/abs/2510.22731)
*Yong Huang,Wenjing Wang,Dalong Zhang,Junjie Wang,Chen Chen,Yan Cao,Wei Wang*

Main category: eess.SP

TL;DR: CSI2Q是一个新颖的CSI指纹识别系统，通过将频域CSI测量转换为时域信号，并采用深度辅助学习策略从IQ指纹模型转移知识，在开放世界设置中实现了与IQ方法相当的性能。


<details>
  <summary>Details</summary>
Motivation: 传统RF指纹识别技术需要专用设备捕获IQ样本，限制了广泛应用。虽然商用WiFi设备可获取CSI，但在开放世界设置中面临CSI测量粒度粗糙的挑战。

Method: 首先将频域CSI测量转换为与IQ样本共享特征空间的时域信号，然后采用深度辅助学习策略从IQ指纹模型转移知识，最后结合OpenMax函数估计未知设备的可能性。

Result: 在包含85个设备的合成CSI数据集和两个真实CSI数据集（分别包含10个和25个WiFi路由器）上评估，准确率分别提高了至少16%、20%和17%。

Conclusion: CSI2Q系统能够有效利用商用WiFi设备的CSI实现高性能RF指纹识别，性能接近基于IQ的方法，具有广泛的应用潜力。

Abstract: Radio frequency (RF) fingerprinting techniques provide a promising supplement
to cryptography-based approaches but rely on dedicated equipment to capture
in-phase and quadrature (IQ) samples, hindering their wide adoption. Recent
advances advocate easily obtainable channel state information (CSI) by
commercial WiFi devices for lightweight RF fingerprinting, while falling short
in addressing the challenges of coarse granularity of CSI measurements in an
open-world setting. In this paper, we propose CSI2Q, a novel CSI fingerprinting
system that achieves comparable performance to IQ-based approaches. Instead of
extracting fingerprints directly from raw CSI measurements, CSI2Q first
transforms frequency-domain CSI measurements into time-domain signals that
share the same feature space with IQ samples. Then, we employ a deep auxiliary
learning strategy to transfer useful knowledge from an IQ fingerprinting model
to the CSI counterpart. Finally, the trained CSI model is combined with an
OpenMax function to estimate the likelihood of unknown ones. We evaluate CSI2Q
on one synthetic CSI dataset involving 85 devices and two real CSI datasets,
including 10 and 25 WiFi routers, respectively. Our system achieves accuracy
increases of at least 16% on the synthetic CSI dataset, 20% on the in-lab CSI
dataset, and 17% on the in-the-wild CSI dataset.

</details>


### [12] [Neural-HAR: A Dimension-Gated CNN Accelerator for Real-Time Radar Human Activity Recognition](https://arxiv.org/abs/2510.22772)
*Yizhuo Wu,Francesco Fioranelli,Chang Gao*

Main category: eess.SP

TL;DR: 提出Neural-HAR，一种维度门控CNN加速器，用于资源受限平台上的实时雷达人体活动识别。核心是GateCNN网络，通过双路径门控卷积和残差路径实现高效识别。


<details>
  <summary>Details</summary>
Motivation: 现有CNN/RNN解决方案在边缘部署时计算和内存开销过大，即使是轻量级ViT/SSM变体也超出实际预算，需要开发更高效的雷达活动识别方法。

Method: 使用GateCNN网络，将多普勒向量嵌入以强调频率随时间演化，应用双路径门控卷积调制多普勒感知内容特征，并辅以残差路径稳定训练。

Result: 在UoG2020数据集上达到86.4%准确率，仅需2.7k参数和0.28M FLOPs。FPGA原型在Xilinx Zynq-7000上实现107.5μs延迟和15mW动态功耗。

Conclusion: Neural-HAR展示了在资源受限边缘设备上实现实时、高能效雷达活动识别的可行性，为隐私保护监控提供了实用解决方案。

Abstract: Radar-based human activity recognition (HAR) is attractive for unobtrusive
and privacy-preserving monitoring, yet many CNN/RNN solutions remain too heavy
for edge deployment, and even lightweight ViT/SSM variants often exceed
practical compute and memory budgets. We introduce Neural-HAR, a
dimension-gated CNN accelerator tailored for real-time radar HAR on
resource-constrained platforms. At its core is GateCNN, a parameter-efficient
Doppler-temporal network that (i) embeds Doppler vectors to emphasize frequency
evolution over time and (ii) applies dual-path gated convolutions that modulate
Doppler-aware content features with temporal gates, complemented by a residual
path for stable training. On the University of Glasgow UoG2020 continuous radar
dataset, GateCNN attains 86.4% accuracy with only 2.7k parameters and 0.28M
FLOPs per inference, comparable to CNN-BiGRU at a fraction of the complexity.
Our FPGA prototype on Xilinx Zynq-7000 Z-7007S reaches 107.5 $\mu$s latency and
15 mW dynamic power using LUT-based ROM and distributed RAM only (zero
DSP/BRAM), demonstrating real-time, energy-efficient edge inference. Code and
HLS conversion scripts are available at https://github.com/lab-emi/AIRHAR.

</details>


### [13] [Rmd: Robust Modal Decomposition with Constrained Bandwidth](https://arxiv.org/abs/2510.22895)
*Wang Hao,Kuang Zhang,Hou Chengyu,Yang Yifan,Tan Chenxing,Fu Weifeng*

Main category: eess.SP

TL;DR: 提出了一种带约束带宽的鲁棒模态分解方法，通过将时间序列映射到相空间轨迹矩阵来保留信号内在结构，结合了数值优化和频谱分解方法的优势。


<details>
  <summary>Details</summary>
Motivation: 现有模态分解方法存在局限：数值优化方法缺乏物理约束会产生虚假模态，频谱分解方法对噪声敏感且难以处理非线性信号。需要一种能结合两者优势的方法。

Method: 提出RMD方法，将时间序列映射到相空间轨迹矩阵，在分解过程中加入带宽约束以增强抗噪性。

Result: 在合成和真实数据集上的实验验证了方法的有效性，包括毫米波雷达回波、心电图、心音图和轴承故障检测数据。

Conclusion: RMD方法成功结合了数值优化和频谱分解方法的优势，提供了鲁棒且通用的模态分解解决方案。

Abstract: Modal decomposition techniques, such as Empirical Mode Decomposition (EMD),
Variational Mode Decomposition (VMD), and Singular Spectrum Analysis (SSA),
have advanced time-frequency signal analysis since the early 21st century.
These methods are generally classified into two categories: numerical
optimization-based methods (EMD, VMD) and spectral decomposition methods (SSA)
that consider the physical meaning of signals. The former can produce spurious
modes due to the lack of physical constraints, while the latter is more
sensitive to noise and struggles with nonlinear signals. Despite continuous
improvements in these methods, a modal decomposition approach that effectively
combines the strengths of both categories remains elusive. This paper thus
proposes a Robust Modal Decomposition (RMD) method with constrained bandwidth,
which preserves the intrinsic structure of the signal by mapping the time
series into its trajectory-GRAM matrix in phase space. Moreover, the method
incorporates bandwidth constraints during the decomposition process, enhancing
noise resistance. Extensive experiments on synthetic and real-world datasets,
including millimeter-wave radar echoes, electrocardiogram (ECG),
phonocardiogram (PCG), and bearing fault detection data, demonstrate the
method's effectiveness and versatility. All code and dataset samples are
available on GitHub: https://github.com/Einstein-sworder/RMD.

</details>


### [14] [Clinic-Oriented Feasibility of a Sensor-Fused Wearable for Upper-Limb Function](https://arxiv.org/abs/2510.22913)
*Thanyanee Srichaisak,Arissa Ieochai,Aueaphum Aueawattthanaphisut*

Main category: eess.SP

TL;DR: 该研究开发了一种集成表面肌电、惯性测量单元和弯曲/力传感器的可穿戴设备，通过低延迟AI推理为手臂震颤和肌无力患者提供安全辅助，在12名健康成人中验证了其技术可行性。


<details>
  <summary>Details</summary>
Motivation: 上肢无力和震颤限制了日常生活活动能力并降低了家庭康复依从性，需要开发安全有效的辅助技术来改善运动质量和任务完成效率。

Method: 采用轻量级传感器节点集成表面肌电(1kHz)、IMU(100-200Hz)和弯曲/力传感器，使用INT8量化在设备端进行1D-CNN/Transformer推理，实施安全约束辅助策略(角度/扭矩/急动度限制；失速/超时保护)。

Result: 辅助显著降低了震颤指数(-0.092)，增加了活动范围(+12.65%)和每分钟重复次数(+2.99次)，设备端延迟中位数为8.7ms，所有测试均完成且无设备相关不良事件。

Conclusion: 多模态传感结合低延迟、安全约束辅助在技术可行性研究中改善了运动质量和任务完成效率，支持推进至IRB批准的临床患者研究。

Abstract: Background: Upper-limb weakness and tremor (4--12 Hz) limit activities of
daily living (ADL) and reduce adherence to home rehabilitation. Objective: To
assess technical feasibility and clinician-relevant signals of a sensor-fused
wearable targeting the triceps brachii and extensor pollicis brevis. Methods: A
lightweight node integrates surface EMG (1 kHz), IMU (100--200 Hz), and
flex/force sensors with on-device INT8 inference (Tiny 1D-CNN/Transformer) and
a safety-bounded assist policy (angle/torque/jerk limits; stall/time-out).
Healthy adults (n = 12) performed three ADL-like tasks. Primary outcomes:
Tremor Index (TI), range of motion (ROM), repetitions (Reps min$^{-1}$).
Secondary: EMG median-frequency slope (fatigue trend), closed-loop latency,
session completion, and device-related adverse events. Analyses used
subject-level paired medians with BCa 95\% CIs; exact Wilcoxon $p$-values are
reported in the Results. Results: Assistance was associated with lower tremor
prominence and improved task throughput: TI decreased by $-0.092$ (95\% CI
[$-0.102$, $-0.079$]), ROM increased by $+12.65\%$ (95\% CI [$+8.43$,
$+13.89$]), and Reps rose by $+2.99$ min$^{-1}$ (95\% CI [$+2.61$, $+3.35$]).
Median on-device latency was 8.7 ms at a 100 Hz loop rate; all sessions were
completed with no device-related adverse events. Conclusions: Multimodal
sensing with low-latency, safety-bounded assistance produced improved movement
quality (TI $\downarrow$) and throughput (ROM, Reps $\uparrow$) in a pilot
technical-feasibility setting, supporting progression to IRB-approved patient
studies. Trial registration: Not applicable (pilot non-clinical).

</details>


### [15] [Intelligent Multimodal Multi-Sensor Fusion-Based UAV Identification, Localization, and Countermeasures for Safeguarding Low-Altitude Economy](https://arxiv.org/abs/2510.22947)
*Yi Tao,Zhen Gao,Fangquan Ye,Jingbo Xu,Tao Song,Weidong Li,Yu Su,Lu Peng,Xiaomei Wu,Tong Qin,Zhongxiang Li,Dezhi Zheng*

Main category: eess.SP

TL;DR: 本文提出了一种基于深度学习的无人机综合管控系统，通过多模态传感器融合感知、精确定位和协同反制，实现了从预警到最终处置的闭环管理。


<details>
  <summary>Details</summary>
Motivation: 随着低空经济的发展，无人机安全管理问题日益突出，准确识别、实时定位和有效反制成为空域安全保障的核心挑战。

Method: 系统采用深度学习技术，在探测层面融合射频频谱特征分析、雷达探测、光电识别等方法；在定位层面利用多传感器数据融合和空天地一体化通信网络；在反制层面采用软硬杀伤结合的综合性措施。

Result: 该系统显著提升了低空无人机管控的响应效率和处置精度，形成了从预警到最终处置的闭环管理控制流程。

Conclusion: 基于深度学习的无人机综合管控系统能够有效解决低空无人机安全管理问题，为空域安全保障提供了可靠的技术支撑。

Abstract: The development of the low-altitude economy has led to a growing prominence
of uncrewed aerial vehicle (UAV) safety management issues. Therefore, accurate
identification, real-time localization, and effective countermeasures have
become core challenges in airspace security assurance. This paper introduces an
integrated UAV management and control system based on deep learning, which
integrates multimodal multi-sensor fusion perception, precise positioning, and
collaborative countermeasures. By incorporating deep learning methods, the
system combines radio frequency (RF) spectral feature analysis, radar
detection, electro-optical identification, and other methods at the detection
level to achieve the identification and classification of UAVs. At the
localization level, the system relies on multi-sensor data fusion and the
air-space-ground integrated communication network to conduct real-time tracking
and prediction of UAV flight status, providing support for early warning and
decision-making. At the countermeasure level, it adopts comprehensive measures
that integrate ``soft kill'' and ``hard kill'', including technologies such as
electromagnetic signal jamming, navigation spoofing, and physical interception,
to form a closed-loop management and control process from early warning to
final disposal, which significantly enhances the response efficiency and
disposal accuracy of low-altitude UAV management.

</details>


### [16] [PASS-Enhanced MEC: Joint Optimization of Task Offloading and Uplink PASS Beamforming](https://arxiv.org/abs/2510.22948)
*Zhaoming Hu,Ruikang Zhong,Xidong Mu,Dengao Li,Yuanwei Liu*

Main category: eess.SP

TL;DR: 提出了一种夹持天线系统增强的移动边缘计算架构，通过介电波导和可调夹持天线建立短距离视距链路，使用深度强化学习方法联合优化上行链路波束成形和任务卸载，以最小化网络延迟。


<details>
  <summary>Details</summary>
Motivation: 在动态无线环境中提高任务卸载效率和延迟性能，解决高频MEC系统中的显著路径损耗和潜在信号阻塞问题。

Method: 将网络延迟最小化问题建模为马尔可夫决策过程，提出负载平衡感知的近端策略优化算法，在策略设计中融入节点级和波导级负载平衡信息。

Result: 仿真结果表明，所提出的PASS增强MEC与自适应上行链路波束成形相比固定PA基线和传统MIMO辅助MEC具有更强的收敛能力，特别是在用户设备数量大或发射功率高的场景中。

Conclusion: PASS增强的MEC架构通过自适应波束成形和负载平衡优化，能够有效改善动态无线环境中的任务卸载效率和延迟性能。

Abstract: A pinching-antenna system (PASS)-enhanced mobile edge computing (MEC)
architecture is investigated to improve the task offloading efficiency and
latency performance in dynamic wireless environments. By leveraging dielectric
waveguides and flexibly adjustable pinching antennas, PASS establishes
short-distance line-of-sight (LoS) links while effectively mitigating the
significant path loss and potential signal blockage, making it a promising
solution for high-frequency MEC systems. We formulate a network latency
minimization problem to joint optimize uplink PASS beamforming and task
offloading. The resulting problem is modeled as a Markov decision process (MDP)
and solved via the deep reinforcement learning (DRL) method. To address the
instability introduced by the $\max$ operator in the objective function, we
propose a load balancing-aware proximal policy optimization (LBPPO) algorithm.
LBPPO incorporates both node-level and waveguide-level load balancing
information into the policy design, maintaining computational and transmission
delay equilibrium, respectively. Simulation results demonstrate that the
proposed PASS-enhanced MEC with adaptive uplink PASS beamforming exhibit
stronger convergence capability than fixed-PA baselines and conventional
MIMO-assisted MEC, especially in scenarios with a large number of UEs or high
transmit power.

</details>


### [17] [Planning Oriented Integrated Sensing and Communication](https://arxiv.org/abs/2510.23021)
*Xibin Jin,Guoliang Li,Shuai Wang,Fan Liu,Miaowen Wen,Huseyin Arslan,Derrick Wing Kwan Ng,Chengzhong Xu*

Main category: eess.SP

TL;DR: 提出了一种面向规划的集成感知与通信框架，通过降低规划瓶颈障碍物的感知不确定性来提升自动驾驶车辆的安全性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有ISAC设计主要关注感知精度和通信吞吐量，忽视了关键障碍物对运动效率的影响，需要将物理层优化与运动规划相结合。

Method: 基于克拉美-罗界和占用膨胀原理推导安全边界，构建双层功率分配与运动规划问题，内层优化ISAC波束功率分配，外层计算不确定性感知的安全约束下的无碰撞轨迹。

Result: 在高保真城市驾驶环境中的仿真表明，相比现有ISAC和通信导向基准方法，PISAC实现了高达40%的成功率和超过5%的缩短通行时间。

Conclusion: PISAC框架有效提升了自动驾驶车辆的安全性和效率，成功弥合了物理层优化与运动规划之间的差距。

Abstract: Integrated sensing and communication (ISAC) enables simultaneous
localization, environment perception, and data exchange for connected
autonomous vehicles. However, most existing ISAC designs prioritize sensing
accuracy and communication throughput, treating all targets uniformly and
overlooking the impact of critical obstacles on motion efficiency. To overcome
this limitation, we propose a planning-oriented ISAC (PISAC) framework that
reduces the sensing uncertainty of planning-bottleneck obstacles and expands
the safe navigable path for the ego-vehicle, thereby bridging the gap between
physical-layer optimization and motion-level planning. The core of PISAC lies
in deriving a closed-form safety bound that explicitly links ISAC transmit
power to sensing uncertainty, based on the Cram\'er-Rao Bound and occupancy
inflation principles. Using this model, we formulate a bilevel power allocation
and motion planning (PAMP) problem, where the inner layer optimizes the ISAC
beam power distribution and the outer layer computes a collision-free
trajectory under uncertainty-aware safety constraints. Comprehensive
simulations in high-fidelity urban driving environments demonstrate that PISAC
achieves up to 40% higher success rates and over 5% shorter traversal times
than existing ISAC-based and communication-oriented benchmarks, validating its
effectiveness in enhancing both safety and efficiency.

</details>


### [18] [HAPS-ISAC for 6G: Architecture, Design Trade-offs, and a Practical Roadmap](https://arxiv.org/abs/2510.23147)
*Parisa Kanani,Mohammad Javad Omidi,Mahmoud Modarres-Hashemi,Halim Yanikomeroglu*

Main category: eess.SP

TL;DR: 提出了一种基于高空平台站(HAPS)的集成感知与通信(ISAC)架构，结合无人机群构建可扩展的智能3D网络，以支持6G网络的超高数据速率和泛在覆盖目标。


<details>
  <summary>Details</summary>
Motivation: 为满足下一代6G网络的超高数据速率和泛在覆盖等宏伟目标，需要创新的网络架构解决方案。

Method: 采用在平流层运行的高空平台站(HAPS)作为强大的通信枢纽和先进环境传感器，结合协作无人机群形成双用途系统。

Result: 仿真结果表明，该方法显著提升了网络性能，提高了感知精度，并确保了更公平的用户服务分布，优于传统的仅无人机基准方案。

Conclusion: 该技术为智慧城市和其他大规模环境提供了有前景的应用和部署路线图。

Abstract: To meet the ambitious goals of next-generation 6G networks, including
ultra-high data rates and ubiquitous coverage, we propose a novel high-altitude
platform station (HAPS)-based integrated sensing and communication (ISAC)
architecture. Operating in the stratosphere, the HAPS functions as both a
powerful communication hub and an advanced environmental sensor. Combined with
a fleet of cooperative uncrewed aerial vehicles (UAVs), this dual-purpose
system forms a scalable and intelligent 3D network. Simulation results indicate
that this approach significantly boosts network performance, improves sensing
accuracy, and ensures a fairer service distribution across users, outperforming
conventional UAV-only baselines. We conclude by outlining the prospective
applications and a deployment roadmap for this technology for smart cities and
other large-scale environments.

</details>


### [19] [Approaching Domain Generalization with Embeddings for Robust Discrimination and Recognition of RF Communication Signals](https://arxiv.org/abs/2510.23186)
*Lukas Henneke,Frank Kurth*

Main category: eess.SP

TL;DR: 提出一种通过合成无线协议信号训练的方法，学习判别性嵌入，无需依赖真实RF信号记录，在未见过的真实信号上实现准确分类。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法依赖大量训练数据且难以泛化到未见信号，需要解决RF信号识别中的数据依赖和泛化问题。

Method: 使用合成无线协议信号进行训练，学习判别性嵌入表示，避免依赖真实RF信号记录。

Result: 在真实RF信号数据集上验证，学习到的嵌入能够准确区分未见过的真实世界信号。

Conclusion: 该方法为鲁棒的RF信号分类和异常检测提供了潜力，解决了数据依赖和泛化问题。

Abstract: Radio frequency (RF) signal recognition plays a critical role in modern
wireless communication and security applications. Deep learning-based
approaches have achieved strong performance but typically rely heavily on
extensive training data and often fail to generalize to unseen signals. In this
paper, we propose a method to learn discriminative embeddings without relying
on real-world RF signal recordings by training on signals of synthetic wireless
protocols. We validate the approach on a dataset of real RF signals and show
that the learned embeddings capture features enabling accurate discrimination
of previously unseen real-world signals, highlighting its potential for robust
RF signal classification and anomaly detection.

</details>


### [20] [Uplink SCMA-empowered Uncoordinated Random Access for Future mMTC](https://arxiv.org/abs/2510.23355)
*Pengyu Gao,Qu Luo,Jing Zhu,Gaojie Chen,Pei Xiao,Chuan Heng Foh*

Main category: eess.SP

TL;DR: 提出了一种基于SCMA的无协调随机接入协议，结合了时隙ALOHA和稀疏码多址技术，通过干扰消除解码策略和用户阻拦机制来提高系统吞吐量。


<details>
  <summary>Details</summary>
Motivation: 解决未来大规模机器类型通信场景中对大规模连接和低接入延迟的迫切需求，特别是在缺乏中央协调的情况下。

Method: 将经典时隙ALOHA协议与稀疏码多址技术相结合，提出干扰消除优先的解码策略，并引入用户阻拦机制来管理流量负载。

Result: 仿真结果表明，与传统的正交多址方案相比，所提出的SCMA赋能URA方案具有更高的最大吞吐量，理论分析和用户阻拦机制的有效性得到验证。

Conclusion: SCMA赋能的URA方案能够有效提高系统吞吐量，干扰消除解码策略和自适应用户阻拦机制能够缓解码本碰撞问题，适用于大规模机器通信场景。

Abstract: In this paper, a novel uncoordinated random access (URA) protocol is
presented to address the pressing demand for massive connectivity with low
access latency in future massive machine type communication (mMTC) scenarios.
The proposed URA scheme integrates the classical slotted ALOHA (S-ALOHA)
protocol with sparse code multiple access (SCMA) technique, referred to as
SCMA-empowered URA. Specifically, active users randomly choose an SCMA codebook
to access the communication network in an arbitrary time slot whenever they
want without scheduling. However, due to the lack of central coordination in
the proposed URA scheme, SCMA codebook collisions become inevitable, making
decoding challenging and leading to increased access failures. To cope with the
decoding issue, an interference-canceling (IC) first decoding strategy is
proposed at the access point (AP), which can partially tackles collision
problems, contributing to a higher system throughput. Taking the proposed
IC-first decoding strategy into account, a closed-form theoretical expression
of the throughput is derived. Moreover, to alleviate the throughput degradation
under the congested user traffic, a user barring mechanism is introduced to
manage the traffic load. Firstly, a closed-form expression of idle codebook
probability is developed to help indicate the system state, i.e., congested or
not. Then, in addition to the estimated real-time load, the AP adaptively
adjusts the access probability and redistributes the actual access load.
Finally, simulation results demonstrate that the proposed SCMA-empowered URA
scheme enjoys higher maximum throughput, compared to the conventional
orthogonal multiple access (OMA) based URA scheme. Moreover, the accuracy of
the presented theoretical analysis and the effectiveness of the user barring
mechanism are verified.

</details>


### [21] [Randomized Space-Time Coded Stacked Intelligent Metasurfaces for Massive Multiuser Downlink Connectivity](https://arxiv.org/abs/2510.23440)
*Donatella Darsena,Ivan Iudice,Vincenzo Galdi,Francesco Verde*

Main category: eess.SP

TL;DR: 提出了一种基于随机空时编码的堆叠智能超表面架构，通过引入人工时间变化来利用多用户分集，在慢速信道动态下实现可扩展的大规模下行连接。


<details>
  <summary>Details</summary>
Motivation: 传统空间超表面架构的重配置速率受限于信道相干时间，难以在慢速信道动态下有效利用多用户分集，且全信道状态信息获取开销过大。

Method: 在堆叠智能超表面输入端集成空时超表面层，在每个信道相干时间间隔内引入随机时间变化，结合基于部分信道状态信息的波束成形方案和有限用户侧反馈。

Result: 数值结果表明，所提出的空时堆叠智能超表面架构在显著降低信道状态信息获取和反馈开销的同时，实现了令人满意的总速率性能。

Conclusion: 该空时堆叠智能超表面架构能够实现可扩展的下行连接，特别适用于密集网络场景，有效平衡了性能和开销。

Abstract: Stacked intelligent metasurfaces (SIMs) represent a key enabler for
next-generation wireless networks, offering beamforming gains while
significantly reducing radio-frequency chain requirements. In conventional
space-only SIM architectures, the rate of reconfigurability of the SIM is equal
to the inverse of the channel coherence time. This paper investigates a novel
beamforming strategy for massive downlink connectivity using a randomized
space-time (ST) coded SIM. In addition to conventional space-only metasurface
layers, the proposed design integrates a ST metasurface layer at the input
stage of the SIM that introduces random time variations over each channel
coherence time interval. These artificial time variations enable opportunistic
user scheduling and exploitation of multiuser diversity under slow channel
dynamics. To mitigate the prohibitive overhead associated with full channel
state information at the transmitter (CSIT), we propose a partial-CSIT-based
beamforming scheme that leverages randomized steering vectors and limited
user-side feedback based on signal quality measurements. Numerical results
demonstrate that the proposed ST-SIM architecture achieves satisfactory
sum-rate performance while significantly reducing CSIT acquisition and feedback
overhead, thereby enabling scalable downlink connectivity in dense networks.

</details>


### [22] [Joint Uplink and Downlink Resource Allocation and Antenna Activation for Pinching Antenna Systems](https://arxiv.org/abs/2510.23467)
*Shreya Khisa,Ali Amhaz,Mohamed Elhattab,Chadi Assi,Sanaa Sharafeddine*

Main category: eess.SP

TL;DR: 提出了一种基于夹持天线系统的联合上下行框架，通过优化天线激活因子、基站发射功率和用户设备发射功率，实现同时同频上下行传输，相比TDD方案可获得60-90%的性能增益。


<details>
  <summary>Details</summary>
Motivation: 传统TDD方案中上下行传输需要在不同时隙进行，频谱利用率较低。本文旨在通过夹持天线系统实现同时同频上下行传输，提高频谱效率。

Method: 将非凸的求和速率优化问题分解为天线激活子问题和功率分配子问题。天线激活采用基于距离和空间相关性的算法，功率分配采用逐次凸逼近算法求解。

Result: 数值结果表明，所提框架相比TDD方案可获得60-90%的性能增益，显著提高了频谱利用率。

Conclusion: 夹持天线系统能够有效支持同时同频上下行传输，通过联合优化天线激活和功率分配，可大幅提升系统性能。

Abstract: In this paper, we explore a novel joint uplink and downlink framework
utilizing a pinching antenna system (PASS). We consider two waveguides, one
dedicated to transmission and one to reception, and both of them are connected
to a base station (BS). Each type of waveguide consists of several pinching
antennas (PAs) in some preconfigured positions. In this framework, we assume
the BS can serve downlink and uplink user equipments (UEs) at the same time
using the same spectrum resources through the presented PASS. In this aspect,
we formulate a sum rate optimization problem that jointly optimizes the antenna
activation factor, the BS transmit power, and the UE's transmit power, subject
to power budget constraints for the BS and the UEs, as well as minimum rate
requirements for the UEs. The formulated problem is highly non-convex and
difficult to solve directly. Hence, we divide the main problem into two
sub-problems: the antenna activation sub-problem and the power allocation
sub-problem. Then, we solve the antenna activation problem utilizing a distance
and spatial correlation-based algorithm. Meanwhile, the resource allocation
problem is solved using a successive convex approximation (SCA)-based
algorithm. Numerical results show that our proposed framework can achieve
around 60-90\% performance gains over its time division duplex (TDD) where the
uplink and downlink transmissions are served in different orthogonal time
slots.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [23] [A Unified Framework for Direction and Diffuseness Estimation Using Tight-Frame Microphone Arrays](https://arxiv.org/abs/2510.22183)
*Akira Omoto*

Main category: eess.AS

TL;DR: 提出了一个统一框架，用于使用不同空间配置的实用麦克风阵列估计声场方向和扩散度。基于协方差扩散度模型，开发了仅速度协方差方法，可在异构阵列几何结构上实现一致的扩散度评估。


<details>
  <summary>Details</summary>
Motivation: 现有方法需要模式白化或球谐分解，限制了实际应用。需要开发一种能在不同阵列几何结构上一致评估扩散度的实用方法。

Method: 基于协方差的扩散度模型，提出仅速度协方差方法。建模并比较了三种阵列类型：A格式阵列、刚性球阵列和新提出的紧框架阵列。通过仿真和测量实验进行验证。

Result: 紧框架配置实现了近乎各向同性的方向采样，其扩散度特性可与高阶球阵列相媲美，同时保持紧凑的物理结构。在同一框架内验证了声强方向估计的准确性。

Conclusion: 该研究将理论扩散度分析与可实现的阵列设计联系起来，支持开发用于空间声场表征的鲁棒宽带方法。

Abstract: This work presents a unified framework for estimating both sound-field
direction and diffuseness using practical microphone arrays with different
spatial configurations. Building on covariance-based diffuseness models, we
formulate a velocity-only covariance approach that enables consistent
diffuseness evaluation across heterogeneous array geometries without requiring
mode whitening or spherical-harmonic decomposition. Three array types -- an
A-format array, a rigid-sphere array, and a newly proposed tight-frame array --
are modeled and compared through both simulations and measurement-based
experiments. The results show that the tight-frame configuration achieves
near-isotropic directional sampling and reproduces diffuseness characteristics
comparable to those of higher-order spherical arrays, while maintaining a
compact physical structure. We further examine the accuracy of
direction-of-arrival estimation based on acoustic intensity within the same
framework. These findings connect theoretical diffuseness analysis with
implementable array designs and support the development of robust, broadband
methods for spatial-sound-field characterization.

</details>


### [24] [Bridging the Perceptual-Statistical Gap in Dysarthria Assessment: Why Machine Learning Still Falls Short](https://arxiv.org/abs/2510.22237)
*Krishna Gurugubelli*

Main category: eess.AS

TL;DR: 本文分析了自动构音障碍检测与严重程度评估中机器与人类专家性能差距的原因，提出了"感知-统计差距"概念，并探讨了缩小该差距的策略和实验方案。


<details>
  <summary>Details</summary>
Motivation: 尽管声学建模和深度学习快速发展，自动构音障碍检测模型仍无法达到人类专家水平，需要深入分析这一差距的根本原因。

Method: 详细分析人类专家感知过程，调查机器学习表示和方法，回顾现有特征集和建模策略，理论分析标签噪声和评分者变异性的限制。

Result: 识别出"感知-统计差距"是性能差距的主要原因，并提出了缩小差距的多种策略。

Conclusion: 需要开发与临床目标一致的实验协议和评估指标，以指导未来构建临床可靠且可解释的构音障碍评估工具。

Abstract: Automated dysarthria detection and severity assessment from speech have
attracted significant research attention due to their potential clinical
impact. Despite rapid progress in acoustic modeling and deep learning, models
still fall short of human expert performance. This manuscript provides a
comprehensive analysis of the reasons behind this gap, emphasizing a conceptual
divergence we term the ``perceptual-statistical gap''. We detail human expert
perceptual processes, survey machine learning representations and methods,
review existing literature on feature sets and modeling strategies, and present
a theoretical analysis of limits imposed by label noise and inter-rater
variability. We further outline practical strategies to narrow the gap,
perceptually motivated features, self-supervised pretraining, ASR-informed
objectives, multimodal fusion, human-in-the-loop training, and explainability
methods. Finally, we propose experimental protocols and evaluation metrics
aligned with clinical goals to guide future research toward clinically reliable
and interpretable dysarthria assessment tools.

</details>


### [25] [Binaural Signal Matching with Wearable Arrays for Near-Field Sources and Directional Focus](https://arxiv.org/abs/2510.22258)
*Sapir Goldring,Zamir Ben Hur,David Lou Alon,Chad McKell,Sebastian Prepelita,Boaz Rafaely*

Main category: eess.AS

TL;DR: 本文研究了近场双耳信号匹配方法在可穿戴眼镜麦克风阵列上的性能，提出了结合距离建模和视场加权的改进方法，在近场场景下显著优于传统远场方法。


<details>
  <summary>Details</summary>
Motivation: 传统双耳信号匹配方法假设声源在远场，但在可穿戴音频系统的近场应用中性能受限。需要开发能够处理近场声源和头部旋转的改进方法。

Method: 提出了近场双耳信号匹配扩展方法，结合距离相关建模和视场加权，使用真实的近场头相关传递函数和声学传递函数模拟数据，评估双耳线索。

Result: 仿真和听力测试结果表明，近场BSM优于传统远场BSM，而提出的NF-FoV-BSM方法在所有测试方法中获得了最佳感知和客观质量，特别是在近距离声源和头部旋转情况下。

Conclusion: 远场模型在近场声源应用中存在局限性，结合声源距离和方向加权可以显著提高可穿戴空间音频系统的双耳重放性能。

Abstract: This paper investigates the performance of Binaural Signal Matching (BSM)
methods for near-field sound reproduction using a wearable glasses-mounted
microphone array. BSM is a flexible, signal-independent approach for binaural
rendering with arbitrary arrays, but its conventional formulation assumes
far-field sources. In our previous work, we proposed a near-field extension of
BSM (NF-BSM) that incorporates distance-dependent modeling and showed improved
performance over far-field BSM using analytic data, though degradation
persisted for sources very close to the array. In this study, we extend that
analysis by using realistic simulated data of near-field Head-Related Transfer
Functions (HRTFs) and Acoustic Transfer Functions (ATFs) of the array,
accounting for listener head rotation and evaluating binaural cues such as
interaural level and time differences (ILD and ITD). A key contribution is the
introduction of a Field of View (FoV) weighting, designed to emphasize
perceptually relevant directions and improve robustness under challenging
conditions. Results from both simulation and a listening test confirm that
NF-BSM outperforms traditional far-field BSM in near-field scenarios, and that
the proposed NF-FoV-BSM method achieves the best perceptual and objective
quality among all tested methods, particularly at close source distances and
under head rotation. These findings highlight the limitations for far-field
models in near-field sources and demonstrate that incorporating source distance
and directional weighting can significantly improve binaural reproduction
performance for wearable spatial audio systems.

</details>


### [26] [Empowering Multimodal Respiratory Sound Classification with Counterfactual Adversarial Debiasing for Out-of-Distribution Robustness](https://arxiv.org/abs/2510.22263)
*Heejoon Koo,Miika Toikkanen,Yoon Tae Kim,Soo Yong Kim,June-Woo Kim*

Main category: eess.AS

TL;DR: 提出了一种反事实对抗去偏框架，通过因果图反事实去偏、对抗去偏和反事实元数据增强来抑制患者元数据中的虚假相关性，提高多模态呼吸音分类在分布偏移下的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前多模态呼吸音分类方法容易受到年龄、性别或采集设备等患者元数据的虚假相关性影响，限制了其在临床站点分布偏移下的泛化性能。

Method: 1. 基于因果图的反事实去偏策略抑制患者元数据的非因果依赖；2. 对抗去偏学习元数据不敏感表示；3. 反事实元数据增强进一步减轻虚假相关性。

Result: 该方法在分布内和分布偏移评估中均优于强基线模型。

Conclusion: 提出的反事实对抗去偏框架能有效提高多模态呼吸音分类的泛化能力，特别是在临床分布偏移场景下。

Abstract: Multimodal respiratory sound classification offers promise for early
pulmonary disease detection by integrating bioacoustic signals with patient
metadata. Nevertheless, current approaches remain vulnerable to spurious
correlations from attributes such as age, sex, or acquisition device, which
hinder their generalization, especially under distribution shifts across
clinical sites. To this end, we propose a counterfactual adversarial debiasing
framework. First, we employ a causal graph-based counterfactual debiasing
strategy to suppress non-causal dependencies from patient metadata. Second, we
introduce adversarial debiasing to learn metadata-insensitive representations
and reduce metadata-specific biases. Third, we design counterfactual metadata
augmentation to mitigate spurious correlations further and strengthen
metadata-invariant representations. By doing so, our method consistently
outperforms strong baselines in evaluations under both in-distribution and
distribution shifts. The code is available at
https://github.com/RSC-Toolkit/BTS-CARD.

</details>


### [27] [UltraVoice: Scaling Fine-Grained Style-Controlled Speech Conversations for Spoken Dialogue Models](https://arxiv.org/abs/2510.22588)
*Wenming Tu,Guanrou Yang,Ruiqi Yan,Wenxi Chen,Ziyang Ma,Yipeng Kang,Kai Yu,Xie Chen,Zilong Zheng*

Main category: eess.AS

TL;DR: UltraVoice是一个大规模语音对话数据集，专门用于细粒度语音风格控制，包含830小时语音数据，涵盖6个语音风格维度。微调SLAM-Omni和VocalNet模型后，在多维度控制任务中显著提升了MOS评分和指令遵循率。


<details>
  <summary>Details</summary>
Motivation: 当前语音对话模型缺乏细粒度语音风格控制能力，这是实现类人交互的关键能力，但往往被推理和问答等功能性能力所忽视。

Method: 构建UltraVoice大规模语音对话数据集，包含6个关键语音风格维度：情感、语速、音量、口音、语言和复合风格。在SLAM-Omni和VocalNet等领先模型上进行微调。

Result: 微调模型在多维度控制任务中MOS评分提升29.12-42.33%，指令遵循率提升14.61-40.09个百分点。在URO-Bench基准测试中，基础设置平均提升10.84%，专业设置提升7.87%。

Conclusion: UltraVoice数据集显著提升了语音模型的细粒度风格控制能力，同时保持核心对话能力，还可用于训练可控文本到语音模型，展示了高质量和广泛适用性。

Abstract: Spoken dialogue models currently lack the ability for fine-grained speech
style control, a critical capability for human-like interaction that is often
overlooked in favor of purely functional capabilities like reasoning and
question answering. To address this limitation, we introduce UltraVoice, the
first large-scale speech dialogue dataset engineered for multiple fine-grained
speech style control. Encompassing over 830 hours of speech dialogues,
UltraVoice provides instructions across six key speech stylistic dimensions:
emotion, speed, volume, accent, language, and composite styles. Fine-tuning
leading models such as SLAM-Omni and VocalNet on UltraVoice significantly
enhances their fine-grained speech stylistic controllability without degrading
core conversational abilities. Specifically, our fine-tuned models achieve
improvements of 29.12-42.33% in Mean Opinion Score (MOS) and 14.61-40.09
percentage points in Instruction Following Rate (IFR) on multi-dimensional
control tasks designed in the UltraVoice. Moreover, on the URO-Bench benchmark,
our fine-tuned models demonstrate substantial gains in core understanding,
reasoning, and conversational abilities, with average improvements of +10.84%
on the Basic setting and +7.87% on the Pro setting. Furthermore, the dataset's
utility extends to training controllable Text-to-Speech (TTS) models,
underscoring its high quality and broad applicability for expressive speech
synthesis. The complete dataset and model checkpoints are available at:
https://github.com/bigai-nlco/UltraVoice.

</details>


### [28] [Mitigating Attention Sinks and Massive Activations in Audio-Visual Speech Recognition with LLMS](https://arxiv.org/abs/2510.22603)
*Anand,Umberto Cappellazzo,Stavros Petridis,Maja Pantic*

Main category: eess.AS

TL;DR: 该研究首次在多模态语音识别中分析注意力汇聚和大量激活现象，发现不仅BOS标记存在这些现象，中间低语义标记也存在。研究提出一种简单的去相关损失来缓解这些问题，并在高降采样率下改善了词错误率。


<details>
  <summary>Details</summary>
Motivation: 理解多模态语音识别中大型语言模型在微调时的内部动态，特别是注意力汇聚和大量激活现象，这些在自然语言处理中已有研究但在多模态语音识别中尚未探索。

Method: 通过详细分析音频-视觉LLMs，识别注意力汇聚和大量激活现象，提出使用去相关损失来减少BOS标记与其他标记之间的余弦相似度。

Result: 发现中间汇聚标记与BOS标记具有高余弦相似度，提出的去相关损失能有效缓解中间汇聚和大量激活，并在高音频-视觉特征降采样下改善词错误率。

Conclusion: 多模态语音识别中存在注意力汇聚和大量激活现象，通过减少BOS与其他标记的相似度可以有效缓解这些问题，同时提升模型在高降采样率下的性能。

Abstract: Large language models (LLMs) have recently advanced auditory speech
recognition (ASR), visual speech recognition (VSR), and audio-visual speech
recognition (AVSR). However, understanding of their internal dynamics under
fine-tuning remains limited. In natural language processing, recent work has
revealed attention sinks, tokens that attract disproportionately high
attention, and associated massive activations in which some features of sink
tokens exhibit huge activation in LLMs. In this work, we are the first to study
these phenomena in multimodal speech recognition. Through a detailed analysis
of audio-visual LLMs, we identify attention sinks and massive activations not
only at the BOS token but also at intermediate low-semantic tokens across ASR,
VSR, and AVSR. We show that massive activations originate in the MLP layers and
correspond to fixed feature indices across all sink tokens. We further show
that intermediate sink tokens exhibit high cosine similarity to the BOS token,
thereby amplifying attention and activation. Building on these insights, we
introduce a simple decorrelation loss that reduces cosine similarity between
BOS and other tokens, effectively mitigating intermediate sinks and massive
activations. Furthermore, our method improves word error rate (WER) under high
audio-visual feature downsampling while remaining stable at lower downsampling
rates.

</details>


### [29] [HyBeam: Hybrid Microphone-Beamforming Array-Agnostic Speech Enhancement for Wearables](https://arxiv.org/abs/2510.22637)
*Yuval Bar Ilan,Boaz Rafaely,Vladimir Tourbabin*

Main category: eess.AS

TL;DR: HyBeam是一个混合语音增强框架，在低频使用原始麦克风信号，在高频使用波束形成器信号，以利用各自的优势并保持阵列无关性。


<details>
  <summary>Details</summary>
Motivation: 现有语音增强方法通常假设固定阵列几何结构，限制了在移动、嵌入式和可穿戴设备中的应用。现有的阵列无关方法要么依赖原始麦克风信号，要么依赖波束形成器输出，但在几何结构变化时都有缺点。

Method: 提出HyBeam混合框架，在低频使用原始麦克风信号，在高频使用波束形成器信号，结合两者的互补优势。

Result: 在不同房间和可穿戴阵列配置的模拟中，HyBeam在PESQ、STOI和SI-SDR指标上持续优于仅使用麦克风或仅使用波束形成器的基线方法。频带分析显示混合方法在高频利用波束形成器方向性，在低频利用麦克风线索。

Conclusion: HyBeam混合方法在所有频带上都优于单独使用任何一种方法，证明了在语音增强中结合原始麦克风信号和波束形成器信号的有效性。

Abstract: Speech enhancement is a fundamental challenge in signal processing,
particularly when robustness is required across diverse acoustic conditions and
microphone setups. Deep learning methods have been successful for speech
enhancement, but often assume fixed array geometries, limiting their use in
mobile, embedded, and wearable devices. Existing array-agnostic approaches
typically rely on either raw microphone signals or beamformer outputs, but both
have drawbacks under changing geometries. We introduce HyBeam, a hybrid
framework that uses raw microphone signals at low frequencies and beamformer
signals at higher frequencies, exploiting their complementary strengths while
remaining highly array-agnostic. Simulations across diverse rooms and wearable
array configurations demonstrate that HyBeam consistently surpasses
microphone-only and beamformer-only baselines in PESQ, STOI, and SI-SDR. A
bandwise analysis shows that the hybrid approach leverages beamformer
directivity at high frequencies and microphone cues at low frequencies,
outperforming either method alone across all bands.

</details>


### [30] [SRP-PHAT-NET: A Reliability-Driven DNN for Reverberant Speaker Localization](https://arxiv.org/abs/2510.22682)
*Bar Shaybet,Vladimir Tourbabin,Boaz Rafaely*

Main category: eess.AS

TL;DR: SRP-PHAT-NET：一种结合SRP-PHAT方向图作为空间特征的深度学习框架，内置可靠性评估机制，通过高斯加权标签训练实现DOA估计的置信度评分。


<details>
  <summary>Details</summary>
Motivation: 在混响环境中进行准确的DOA估计是空间音频应用的基本挑战。现有深度学习方法缺乏评估预测可靠性的机制，而这对实际部署至关重要。

Method: 利用SRP-PHAT方向图作为空间特征，使用以真实方向为中心的高斯加权标签进行训练，引入内置可靠性估计机制。

Result: 实验结果表明，选择性使用高置信度预测可显著提高定位精度，证明了在基于深度学习的DOA估计中集成可靠性的实际价值。

Conclusion: 通过调整高斯核宽度可满足特定应用需求，集成可靠性评估的深度学习框架为DOA估计提供了更实用的解决方案。

Abstract: Accurate Direction-of-Arrival (DOA) estimation in reverberant environments
remains a fundamental challenge for spatial audio applications. While deep
learning methods have shown strong performance in such conditions, they
typically lack a mechanism to assess the reliability of their predictions - an
essential feature for real-world deployment. In this work, we present the
SRP-PHAT-NET, a deep neural network framework that leverages SRP-PHAT
directional maps as spatial features and introduces a built-in reliability
estimation. To enable meaningful reliability scoring, the model is trained
using Gaussian-weighted labels centered around the true direction. We
systematically analyze the influence of label smoothing on accuracy and
reliability, demonstrating that the choice of Gaussian kernel width can be
tuned to application-specific requirements. Experimental results show that
selectively using high-confidence predictions yields significantly improved
localization accuracy, highlighting the practical benefits of integrating
reliability into deep learning-based DOA estimation.

</details>


### [31] [DialoSpeech: Dual-Speaker Dialogue Generation with LLM and Flow Matching](https://arxiv.org/abs/2510.08373)
*Hanke Xie,Dake Guo,Chengyou Wang,Yue Li,Wenjie Tian,Xinfa Zhu,Xinsheng Wang,Xiulin Li,Guanqiong Miao,Bo Liu,Lei Xie*

Main category: eess.AS

TL;DR: DialoSpeech是一个结合大语言模型和分块流匹配的双轨架构，用于生成表达力强、类人的对话语音合成，支持中英文及跨语言合成。


<details>
  <summary>Details</summary>
Motivation: 当前文本转语音系统在生成类人交互对话语音方面存在挑战，主要由于双轨数据稀缺以及难以在多轮对话中实现自然性、上下文连贯性和交互动态性（如话轮转换、重叠语音和说话人一致性）。

Method: 提出DialoSpeech双轨架构，结合大语言模型和分块流匹配技术，并引入数据处理流水线构建双轨对话数据集，支持可扩展的训练和实验验证。

Result: 实验表明，该模型在基准测试中表现优于基线方法，能够生成自然的多人对话，具有连贯的话轮转换和自然的重叠语音。

Conclusion: DialoSpeech为生成类人口语对话提供了有效解决方案，支持中英文及跨语言语音合成。

Abstract: Recent advances in text-to-speech (TTS) synthesis, particularly those
leveraging large language models (LLMs), have significantly improved
expressiveness and naturalness. However, generating human-like, interactive
dialogue speech remains challenging. Current systems face limitations due to
the scarcity of dual-track data and difficulties in achieving naturalness,
contextual coherence, and interactional dynamics, such as turn-taking,
overlapping speech, and speaker consistency, in multi-turn conversations. To
address these challenges, we propose DialoSpeech, a dual-track architecture
combining a large language model with Chunked Flow Matching for expressive,
human-like dialogue speech synthesis. DialoSpeech generates natural multi-turn
conversations with coherent speaker turns and natural overlaps, supporting both
Chinese and English and cross-lingual speech synthesis. We introduce a data
processing pipeline to construct dual-track dialogue datasets, facilitating
scalable training and experimental validation. Experiments show that our model
outperforms baselines, offering a solution for generating human-like spoken
dialogues. Audio samples are available at
https://tiamojames.github.io/DialoSpeech

</details>


### [32] [DiffRhythm 2: Efficient and High Fidelity Song Generation via Block Flow Matching](https://arxiv.org/abs/2510.22950)
*Yuepeng Jiang,Huakang Chen,Ziqian Ning,Jixun Yao,Zerui Han,Di Wu,Meng Meng,Jian Luan,Zhonghua Fu,Lei Xie*

Main category: eess.AS

TL;DR: DiffRhythm 2是一个端到端的高保真可控歌曲生成框架，通过半自回归块流匹配解决歌词对齐问题，使用音乐VAE降低帧率，并提出跨对偏好优化来改进RLHF性能。


<details>
  <summary>Details</summary>
Motivation: 现有非自回归框架在歌词与歌声对齐方面存在困难，且多偏好优化会导致性能下降，需要解决长序列生成中的模态对齐和多样性偏好问题。

Method: 采用基于块流匹配的半自回归架构实现歌词对齐；使用音乐VAE将帧率降至5Hz；提出跨对偏好优化方法避免模型合并的性能下降；引入随机块表示对齐损失增强音乐性。

Result: 该框架能够在保持非自回归模型高质量和高效生成的同时，实现歌词与歌声的忠实对齐，并支持多样化的音乐偏好优化。

Conclusion: DiffRhythm 2通过创新的半自回归架构和跨对偏好优化，有效解决了歌曲生成中的歌词对齐和多偏好优化挑战，为高质量可控音乐生成提供了可行方案。

Abstract: Generating full-length, high-quality songs is challenging, as it requires
maintaining long-term coherence both across text and music modalities and
within the music modality itself. Existing non-autoregressive (NAR) frameworks,
while capable of producing high-quality songs, often struggle with the
alignment between lyrics and vocal. Concurrently, catering to diverse musical
preferences necessitates reinforcement learning from human feedback (RLHF).
However, existing methods often rely on merging multiple models during
multi-preference optimization, which results in significant performance
degradation. To address these challenges, we introduce DiffRhythm 2, an
end-to-end framework designed for high-fidelity, controllable song generation.
To tackle the lyric alignment problem, DiffRhythm 2 employs a
semi-autoregressive architecture based on block flow matching. This design
enables faithful alignment of lyrics to singing vocals without relying on
external labels and constraints, all while preserving the high generation
quality and efficiency of NAR models. To make this framework computationally
tractable for long sequences, we implement a music variational autoencoder
(VAE) that achieves a low frame rate of 5 Hz while still enabling high-fidelity
audio reconstruction. In addition, to overcome the limitations of
multi-preference optimization in RLHF, we propose cross-pair preference
optimization. This method effectively mitigates the performance drop typically
associated with model merging, allowing for more robust optimization across
diverse human preferences. We further enhance musicality and structural
coherence by introducing stochastic block representation alignment loss.

</details>


### [33] [Adapting Speech Foundation Models with Large Language Models for Unified Speech Recognition](https://arxiv.org/abs/2510.22961)
*Jing-Xuan Zhang,Genshun Wan,Jin Li,Jianqing Gao*

Main category: eess.AS

TL;DR: UASR-LLM是一个统一语音识别框架，通过将冻结的语音基础模型与大型语言模型结合，实现了视觉、听觉和视听语音识别任务。


<details>
  <summary>Details</summary>
Motivation: 语音基础模型在听觉任务中表现出色，但在多模态场景中的应用仍待探索。本文旨在将冻结的SFMs适配到统一的VSR、ASR和AVSR任务中。

Method: 通过视觉注入模块将视觉表示引入多个SFM层，使用LLMs作为文本解码器，采用两阶段训练策略：视觉注入预训练和语音识别微调。

Result: 实验结果表明，在干净和嘈杂条件下，该方法在VSR、ASR和AVSR任务上均优于最先进的基线方法。

Conclusion: 该框架验证了将冻结SFMs与LLMs结合进行统一语音识别的有效性，消融研究证实了该方法在不同SFMs和LLMs上的泛化能力。

Abstract: Unified speech recognition aims to perform auditory, visual, and audiovisual
speech recognition within a single model framework. While speech foundation
models (SFMs) have demonstrated remarkable performance in auditory tasks, their
adaptation to multimodal scenarios remains underexplored. This paper presents
UASR-LLM, a novel framework that adapts frozen SFMs to unified VSR, ASR, and
AVSR tasks by leveraging large language models (LLMs) as text decoders. Our
approach introduces visual representations into multiple SFM layers through
visual injection modules, enabling multimodal input processing and unified
hidden representations. The augmented SFMs connect with decoder-only LLMs via a
feed-forward adaptor, where concatenated representations and instruction
prompts guide speech transcription. We implement a twostage training strategy:
visual injection pretraining followed by speech recognition finetuning. SFM
parameters remain frozen throughout training, with only visual injection
modules optimized initially, and LLMs finetuned using LoRA parameters
subsequently. Experimental results demonstrate superior performance over
state-of-the-art baselines across VSR, ASR, and AVSR tasks under both clean and
noisy conditions. Ablation studies confirm generalization across various SFMs
and LLMs, validating the proposed training strategy.

</details>


### [34] [LibriConvo: Simulating Conversations from Read Literature for ASR and Diarization](https://arxiv.org/abs/2510.23320)
*Máté Gedeon,Péter Mihajlik*

Main category: eess.AS

TL;DR: LibriConvo是一个基于说话人感知对话模拟的多说话人对话数据集，支持说话人日志化和自动语音识别系统的训练与评估，通过语义连贯性和真实对话时序提升数据质量。


<details>
  <summary>Details</summary>
Motivation: 现有数据集大多依赖语义不连贯的话语和不合理的时间间隔，缺乏真实对话的动态特性，需要构建更真实的模拟对话数据集来推动多说话人语音处理研究。

Method: 使用CallHome和外部VAD获取可靠边界，压缩减少不自然的静默，按书籍组织LibriTTS话语保持上下文一致性，通过新颖的房间脉冲响应选择程序增强声学真实性。

Result: 数据集包含240.1小时、1,496个对话、830个独特说话者；sortformer模型在说话人日志化中优于pyannote pipeline，微调的Fast Conformer-CTC XLarge在ASR上达到7.29% WER，超越零样本Whisper-large-v3。

Conclusion: LibriConvo为多说话人语音处理研究提供了具有真实对话动态和受控实验条件的宝贵资源，能够有效推动该领域的发展。

Abstract: We introduce LibriConvo, a simulated multi-speaker conversational dataset
based on speaker-aware conversation simulation (SASC), designed to support
training and evaluation of speaker diarization and automatic speech recognition
(ASR) systems. Unlike prior resources that mostly rely on semantically
disconnected utterances and implausible temporal gaps, LibriConvo ensures
semantic coherence and realistic conversational timing. Our pipeline leverages
CallHome with external VAD for reliable boundaries, applies compression to
reduce unnaturally long silences, and organizes LibriTTS utterances by book to
maintain contextual consistency. Acoustic realism is enhanced via a novel room
impulse response selection procedure that ranks speaker-microphone
configurations by spatial plausibility, balancing realism and diversity. The
dataset comprises 240.1 hours across 1,496 dialogues with 830 unique speakers,
split in a speaker-disjoint manner for robust evaluation. Baselines show that
the sortformer model outperforms the pyannote pipeline in diarization, while a
fine-tuned Fast Conformer-CTC XLarge with Serialized Output Training achieves
7.29\% WER for ASR, surpassing zero-shot Whisper-large-v3. LibriConvo provides
a valuable resource for advancing multi-speaker speech processing research with
realistic conversational dynamics and controlled experimental conditions.

</details>


### [35] [Treble10: A high-quality dataset for far-field speech recognition, dereverberation, and enhancement](https://arxiv.org/abs/2510.23141)
*Sarabeth S. Mullins,Georg Götz,Eric Bezzam,Steven Zheng,Daniel Gert Nielsen*

Main category: eess.AS

TL;DR: Treble10是一个大规模、物理精确的室内声学数据集，包含3000多个宽带房间脉冲响应，在10个真实房间中通过混合模拟方法生成，填补了测量和模拟之间的真实感差距。


<details>
  <summary>Details</summary>
Motivation: 当前远场语音数据集在声学真实性和可扩展性之间存在权衡。测量数据集物理真实但昂贵且覆盖范围小，而模拟数据集通常使用简化的几何声学方法，无法再现复杂环境中的关键物理现象。

Method: 使用Treble SDK中的混合模拟范式，结合基于波和几何声学的求解器，在10个真实装修房间中模拟生成房间脉冲响应，提供单声道、8阶Ambisonics和6通道设备RIR等多种格式。

Result: 生成了包含3000多个RIR的数据集，所有信号以32kHz采样率模拟，准确建模低频波效应和高频反射，提供了预卷积的混响语音场景与LibriSpeech语料的配对数据。

Conclusion: Treble10填补了测量和模拟之间的真实感差距，为远场语音任务提供了可重复、物理基础评估和大规模数据增强的能力，可作为基准和下一代模拟驱动音频研究的模板。

Abstract: Accurate far-field speech datasets are critical for tasks such as automatic
speech recognition (ASR), dereverberation, speech enhancement, and source
separation. However, current datasets are limited by the trade-off between
acoustic realism and scalability. Measured corpora provide faithful physics but
are expensive, low-coverage, and rarely include paired clean and reverberant
data. In contrast, most simulation-based datasets rely on simplified
geometrical acoustics, thus failing to reproduce key physical phenomena like
diffraction, scattering, and interference that govern sound propagation in
complex environments. We introduce Treble10, a large-scale, physically accurate
room-acoustic dataset. Treble10 contains over 3000 broadband room impulse
responses (RIRs) simulated in 10 fully furnished real-world rooms, using a
hybrid simulation paradigm implemented in the Treble SDK that combines a
wave-based and geometrical acoustics solver. The dataset provides six
complementary subsets, spanning mono, 8th-order Ambisonics, and 6-channel
device RIRs, as well as pre-convolved reverberant speech scenes paired with
LibriSpeech utterances. All signals are simulated at 32 kHz, accurately
modelling low-frequency wave effects and high-frequency reflections. Treble10
bridges the realism gap between measurement and simulation, enabling
reproducible, physically grounded evaluation and large-scale data augmentation
for far-field speech tasks. The dataset is openly available via the Hugging
Face Hub, and is intended as both a benchmark and a template for
next-generation simulation-driven audio research.

</details>


### [36] [SoulX-Podcast: Towards Realistic Long-form Podcasts with Dialectal and Paralinguistic Diversity](https://arxiv.org/abs/2510.23541)
*Hanke Xie,Haopeng Lin,Wenxiao Cao,Dake Guo,Wenjie Tian,Jun Wu,Hanlin Wen,Ruixuan Shang,Hongmei Liu,Zhiqi Jiang,Yuepeng Jiang,Wenxi Chen,Ruiqi Yan,Jiale Qian,Yichao Yan,Shunshun Yin,Ming Tao,Xie Chen,Lei Xie,Xinsheng Wang*

Main category: eess.AS

TL;DR: SoulX-Podcast是一个用于播客风格多轮多说话人对话语音生成的系统，在传统TTS任务中达到最先进性能，支持中英文和多种中文方言。


<details>
  <summary>Details</summary>
Motivation: 现有TTS系统主要针对单说话人合成，在多说话人对话语音生成方面存在不足，无法产生连贯的多说话人对话语音。

Method: 集成了一系列副语言控制，支持普通话、英语以及四川话、河南话、粤语等多种中文方言，实现更个性化的播客风格语音生成。

Result: 能够连续生成超过90分钟的对话，保持稳定的说话人音色和平滑的说话人转换，说话人表现出上下文适应的韵律，反映对话进展中的自然节奏和语调变化。

Conclusion: 在多个评估指标上，SoulX-Podcast在独白TTS和多轮对话语音合成中都达到了最先进的性能。

Abstract: Recent advances in text-to-speech (TTS) synthesis have significantly improved
speech expressiveness and naturalness. However, most existing systems are
tailored for single-speaker synthesis and fall short in generating coherent
multi-speaker conversational speech. This technical report presents
SoulX-Podcast, a system designed for podcast-style multi-turn, multi-speaker
dialogic speech generation, while also achieving state-of-the-art performance
in conventional TTS tasks.
  To meet the higher naturalness demands of multi-turn spoken dialogue,
SoulX-Podcast integrates a range of paralinguistic controls and supports both
Mandarin and English, as well as several Chinese dialects, including
Sichuanese, Henanese, and Cantonese, enabling more personalized podcast-style
speech generation. Experimental results demonstrate that SoulX-Podcast can
continuously produce over 90 minutes of conversation with stable speaker timbre
and smooth speaker transitions. Moreover, speakers exhibit contextually
adaptive prosody, reflecting natural rhythm and intonation changes as dialogues
progress. Across multiple evaluation metrics, SoulX-Podcast achieves
state-of-the-art performance in both monologue TTS and multi-turn
conversational speech synthesis.

</details>


### [37] [Matching Reverberant Speech Through Learned Acoustic Embeddings and Feedback Delay Networks](https://arxiv.org/abs/2510.23158)
*Philipp Götz,Gloria Dal Santo,Sebastian J. Schlecht,Vesa Välimäki,Emanuël A. P. Habets*

Main category: eess.AS

TL;DR: 提出了一种基于学习先验的盲估计人工混响参数方法，使用反馈延迟网络结构来匹配目标空间的频率相关衰减时间和直达声与混响声比例，在AAR应用中实现高效、感知一致的混响渲染。


<details>
  <summary>Details</summary>
Motivation: 混响传递了关于环境的关键声学线索，支持空间感知和沉浸感。对于听觉增强现实系统，在没有明确声学测量的情况下实时生成感知上合理的人工混响仍然是一个关键挑战。

Method: 将盲估计人工混响参数制定为混响信号匹配任务，利用学习的房间声学先验。提出反馈延迟网络结构，复现目标空间的频率相关衰减时间和直达声与混响声比例。

Result: 与领先的自动FDN调谐方法相比，在估计的房间声学参数和人工混响语音的感知合理性方面都有改进。

Conclusion: 该方法在AAR应用中具有高效、感知一致的混响渲染潜力。

Abstract: Reverberation conveys critical acoustic cues about the environment,
supporting spatial awareness and immersion. For auditory augmented reality
(AAR) systems, generating perceptually plausible reverberation in real time
remains a key challenge, especially when explicit acoustic measurements are
unavailable. We address this by formulating blind estimation of artificial
reverberation parameters as a reverberant signal matching task, leveraging a
learned room-acoustic prior. Furthermore, we propose a feedback delay network
(FDN) structure that reproduces both frequency-dependent decay times and the
direct-to-reverberation ratio of a target space. Experimental evaluation
against a leading automatic FDN tuning method demonstrates improvements in
estimated room-acoustic parameters and perceptual plausibility of artificial
reverberant speech. These results highlight the potential of our approach for
efficient, perceptually consistent reverberation rendering in AAR applications.

</details>


### [38] [Evaluation of Spherical Wavelet Framework in Comparsion with Ambisonics](https://arxiv.org/abs/2510.23403)
*Ş. Ekmen,H. Lee*

Main category: eess.AS

TL;DR: 球面小波框架(SWF)结合了Ambisonics和基于对象的音频的优点，使用高度局部化的基函数来增强甜点区域、减少定位模糊，同时保持稀疏表示。本研究详细比较了SWF与Ambisonics，发现SWF在空间和音色保真度方面显著优于Ambisonics，但依赖于球面细分且不能原生表示连续方向波。


<details>
  <summary>Details</summary>
Motivation: SWF结合了Ambisonics和基于对象音频的优点，但先前研究仅限于特定条件且缺乏感知指标。本研究旨在更详细地研究SWF，并与Ambisonics进行全面比较。

Method: 使用IACC、ITD和ILD估计以及生态有效声源的听力测试，评估了规则多面体、t-design和Lebedev网格等不同重放布局及其对应的Ambisonics阶数和通道数。

Result: SWF在整体空间和音色保真度方面被评定为比Ambisonics更接近参考，但显著依赖于球面细分，且不能原生表示连续方向波。

Conclusion: SWF在空间音频再现方面优于Ambisonics，但存在球面细分依赖性和连续方向表示限制，需要进一步解决方案。

Abstract: Recently, the Spherical Wavelet Framework (SWF) was proposed to combine the
benefits of Ambisonics and Object-Based Audio (OBA) by utilising highly
localised basis functions. SWF can enhance the sweet-spot area and reduce
localisation blur while still enabling a sparse representation of the complete
sound field, making storage and transmission more efficient. Initial vector
analysis and listening test of SWF have shown promising results; however, these
findings are limited to very specific conditions and do not include perceptual
metrics. The present study investigates SWF in greater detail, comparing it
with Ambisonics. The comparison was carried out using IACC, ITD, and ILD
estimations, as well as listening tests with ecologically valid sound sources.
Various reproduction layouts: regular polyhedron, t-design, and Lebedev grid
with their corresponding Ambisonics orders and channel counts were evaluated.
Results indicate that SWF is rated significantly more similar to the reference
than Ambisonics is, in terms of overall spatial and timbral fidelity; however,
it is considerably dependent on the subdivison of the sphere. Moreover, it
cannot natively represent a wave arriving at a continuous direction. Possible
solutions are proposed.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [39] [GuitarFlow: Realistic Electric Guitar Synthesis From Tablatures via Flow Matching and Style Transfer](https://arxiv.org/abs/2510.21872)
*Jackson Loth,Pedro Sarmento,Mark Sandler,Mathieu Barthet*

Main category: cs.SD

TL;DR: GuitarFlow是一个专门用于电吉他合成的AI模型，使用吉他谱作为引导，通过样本虚拟乐器渲染和流匹配风格转换，生成更逼真的吉他音频。


<details>
  <summary>Details</summary>
Motivation: 当前AI音乐生成在吉他等特定乐器的可控合成方面表达能力有限，吉他特有的演奏技巧（如弯音、闷音、连奏）在其他音乐符号格式（如MIDI）中难以准确表示。

Method: 使用吉他谱作为引导，先通过样本虚拟乐器将吉他谱渲染为音频，然后使用流匹配进行风格转换，将虚拟乐器音频转换为更逼真的声音。

Result: 模型训练和推理速度快，仅需不到6小时的训练数据。客观评估指标和听力测试显示，生成的吉他音频在真实感方面有显著提升。

Conclusion: GuitarFlow提供了一种高效且表达力强的电吉他合成方法，通过吉他谱引导和流匹配技术，能够生成逼真的吉他音频。

Abstract: Music generation in the audio domain using artificial intelligence (AI) has
witnessed steady progress in recent years. However for some instruments,
particularly the guitar, controllable instrument synthesis remains limited in
expressivity. We introduce GuitarFlow, a model designed specifically for
electric guitar synthesis. The generative process is guided using tablatures,
an ubiquitous and intuitive guitar-specific symbolic format. The tablature
format easily represents guitar-specific playing techniques (e.g. bends, muted
strings and legatos), which are more difficult to represent in other common
music notation formats such as MIDI. Our model relies on an intermediary step
of first rendering the tablature to audio using a simple sample-based virtual
instrument, then performing style transfer using Flow Matching in order to
transform the virtual instrument audio into more realistic sounding examples.
This results in a model that is quick to train and to perform inference,
requiring less than 6 hours of training data. We present the results of
objective evaluation metrics, together with a listening test, in which we show
significant improvement in the realism of the generated guitar audio from
tablatures.

</details>


### [40] [Streaming Generation for Music Accompaniment](https://arxiv.org/abs/2510.22105)
*Yusong Wu,Mason Wang,Heidi Lei,Stephen Brade,Lancelot Blanchard,Shih-Lun Wu,Aaron Courville,Anna Huang*

Main category: cs.SD

TL;DR: 该论文研究了实时音频到音频伴奏生成，提出了考虑系统延迟的模型设计，分析了未来可见度和输出块持续时间两个关键参数的权衡关系。


<details>
  <summary>Details</summary>
Motivation: 现有音乐生成模型只能处理完整音频输入，限制了实时编辑和循环工作流程。需要研究实时音频伴奏生成，让模型在听到输入音频流的同时实时生成连贯的伴奏。

Method: 使用Transformer解码器，通过两个设计变量：未来可见度tf（输出播放时间与最新输入时间之间的偏移）和输出块持续时间k（每次调用生成的帧数），在不同(tf,k)组合下训练模型。

Result: 发现两个一致的权衡关系：增加有效tf通过减小近期差距提高连贯性，但需要更快的推理速度；增加k提高吞吐量但降低更新率导致伴奏质量下降。

Conclusion: 朴素的最大似然流式训练对于缺乏未来上下文的连贯伴奏生成不足，需要开发具有前瞻性和代理性的目标函数来支持实时即兴演奏。

Abstract: Music generation models can produce high-fidelity coherent accompaniment
given complete audio input, but are limited to editing and loop-based
workflows. We study real-time audio-to-audio accompaniment: as a model hears an
input audio stream (e.g., a singer singing), it has to also simultaneously
generate in real-time a coherent accompanying stream (e.g., a guitar
accompaniment). In this work, we propose a model design considering inevitable
system delays in practical deployment with two design variables: future
visibility $t_f$, the offset between the output playback time and the latest
input time used for conditioning, and output chunk duration $k$, the number of
frames emitted per call. We train Transformer decoders across a grid of
$(t_f,k)$ and show two consistent trade-offs: increasing effective $t_f$
improves coherence by reducing the recency gap, but requires faster inference
to stay within the latency budget; increasing $k$ improves throughput but
results in degraded accompaniment due to a reduced update rate. Finally, we
observe that naive maximum-likelihood streaming training is insufficient for
coherent accompaniment where future context is not available, motivating
advanced anticipatory and agentic objectives for live jamming.

</details>


### [41] [M-CIF: Multi-Scale Alignment For CIF-Based Non-Autoregressive ASR](https://arxiv.org/abs/2510.22172)
*Ruixiang Mao,Xiangnan Ma,Qing Yang,Ziming Zhu,Yucheng Qiao,Yuan Ge,Tong Xiao,Shengxiang Gao,Zhengtao Yu,Jingbo Zhu*

Main category: cs.SD

TL;DR: 提出了多尺度CIF（M-CIF）方法，通过整合字符和音素级别的监督来增强非自回归语音识别的声学-文本对齐稳定性，在多种语言上显著降低了词错误率。


<details>
  <summary>Details</summary>
Motivation: 传统CIF机制在某些语言（如英语和法语）中由于缺乏细粒度指导而导致稳定性下降，需要改进对齐方法。

Method: M-CIF通过逐步蒸馏字符和音素级别的监督到子词表示中，实现多级对齐，增强声学-文本对齐的鲁棒性。

Result: 实验显示M-CIF相比Paraformer基线显著降低了WER，在CommonVoice上德语降低4.21%，法语降低3.05%。新定义的语音混淆错误和空格相关分割错误指标验证了方法的有效性。

Conclusion: 音素和字符层对于增强渐进式CIF对齐至关重要，M-CIF通过多级对齐机制有效提升了非自回归语音识别的性能。

Abstract: The Continuous Integrate-and-Fire (CIF) mechanism provides effective
alignment for non-autoregressive (NAR) speech recognition. This mechanism
creates a smooth and monotonic mapping from acoustic features to target tokens,
achieving performance on Mandarin competitive with other NAR approaches.
However, without finer-grained guidance, its stability degrades in some
languages such as English and French. In this paper, we propose Multi-scale CIF
(M-CIF), which performs multi-level alignment by integrating character and
phoneme level supervision progressively distilled into subword representations,
thereby enhancing robust acoustic-text alignment. Experiments show that M-CIF
reduces WER compared to the Paraformer baseline, especially on CommonVoice by
4.21% in German and 3.05% in French. To further investigate these gains, we
define phonetic confusion errors (PE) and space-related segmentation errors
(SE) as evaluation metrics. Analysis of these metrics across different M-CIF
settings reveals that the phoneme and character layers are essential for
enhancing progressive CIF alignment.

</details>


### [42] [FOA Tokenizer: Low-bitrate Neural Codec for First Order Ambisonics with Spatial Consistency Loss](https://arxiv.org/abs/2510.22241)
*Parthasaarathy Sudarsanam,Sebastian Braun,Hannes Gamper*

Main category: cs.SD

TL;DR: 提出了首个针对一阶环绕声(FOA)的离散神经空间音频编解码器，将4通道FOA音频压缩至0.9kbps，同时保持空间方向线索，并在下游任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 神经音频编解码器在单声道和立体声方面已有广泛研究，但空间音频领域仍待探索，特别是需要开发能够有效压缩多通道空间音频同时保持方向信息的编解码器。

Method: 基于WavTokenizer架构扩展支持4通道FOA信号，引入新颖的空间一致性损失函数来在高度压缩表示中保持重建信号的方向线索，压缩率为75个离散标记/秒。

Result: 在模拟混响混合、非混响干净语音和真实房间脉冲响应的FOA混合三种条件下，平均角度误差分别为13.76°、3.96°和25.83°，在STARSS23真实录音上的声音事件定位和检测任务中表现出有用特征。

Conclusion: 该编解码器成功实现了FOA空间音频的高效压缩，同时保持了重要的空间信息，其离散潜在表示对下游空间音频任务具有实用价值。

Abstract: Neural audio codecs have been widely studied for mono and stereo signals, but
spatial audio remains largely unexplored. We present the first discrete neural
spatial audio codec for first-order ambisonics (FOA). Building on the
WavTokenizer architecture, we extend it to support four-channel FOA signals and
introduce a novel spatial consistency loss to preserve directional cues in the
reconstructed signals under a highly compressed representation. Our codec
compresses 4-channel FOA audio at 24 kHz into 75 discrete tokens per second,
corresponding to a bit rate of 0.9 kbps. Evaluations on simulated reverberant
mixtures, non-reverberant clean speech, and FOA mixtures with real room impulse
responses show accurate reconstruction, with mean angular errors of
13.76{\deg}, 3.96{\deg}, and 25.83{\deg}, respectively, across the three
conditions. In addition, discrete latent representations derived from our codec
provide useful features for downstream spatial audio tasks, as demonstrated on
sound event localization and detection with STARSS23 real recordings.

</details>


### [43] [PromptReverb: Multimodal Room Impulse Response Generation Through Latent Rectified Flow Matching](https://arxiv.org/abs/2510.22439)
*Ali Vosoughi,Yongyi Zang,Qihui Yang,Nathan Peak,Randal Leistikow,Chenliang Xu*

Main category: cs.SD

TL;DR: PromptReverb是一个两阶段生成框架，通过变分自编码器将带限RIR上采样至全频带质量，并使用基于整流流匹配的条件扩散变换器模型从自然语言描述生成RIR，实现了优越的感知质量和声学准确性。


<details>
  <summary>Details</summary>
Motivation: 解决当前RIR生成方法面临的两个基本限制：全频带RIR数据集的稀缺性，以及现有模型无法从多样化输入模态生成声学准确响应的能力。

Method: 采用两阶段生成框架：第一阶段使用变分自编码器将带限RIR上采样至48kHz全频带质量；第二阶段使用基于整流流匹配的条件扩散变换器模型，从自然语言描述生成RIR。

Result: PromptReverb生成的RIR在感知质量和声学准确性方面优于现有方法，平均RT60误差为8.8%，而广泛使用的基线方法为-37%，并产生更真实的房间声学参数。

Conclusion: 该方法在虚拟现实、建筑声学和音频制作等需要灵活、高质量RIR合成的实际应用中具有重要价值。

Abstract: Room impulse response (RIR) generation remains a critical challenge for
creating immersive virtual acoustic environments. Current methods suffer from
two fundamental limitations: the scarcity of full-band RIR datasets and the
inability of existing models to generate acoustically accurate responses from
diverse input modalities. We present PromptReverb, a two-stage generative
framework that addresses these challenges. Our approach combines a variational
autoencoder that upsamples band-limited RIRs to full-band quality (48 kHz), and
a conditional diffusion transformer model based on rectified flow matching that
generates RIRs from descriptions in natural language. Empirical evaluation
demonstrates that PromptReverb produces RIRs with superior perceptual quality
and acoustic accuracy compared to existing methods, achieving 8.8% mean RT60
error compared to -37% for widely used baselines and yielding more realistic
room-acoustic parameters. Our method enables practical applications in virtual
reality, architectural acoustics, and audio production where flexible,
high-quality RIR synthesis is essential.

</details>


### [44] [Evaluating Multimodal Large Language Models on Core Music Perception Tasks](https://arxiv.org/abs/2510.22455)
*Brandon James Carone,Iran R. Roman,Pablo Ripollés*

Main category: cs.SD

TL;DR: 该论文评估了多模态大语言模型在音乐理解方面的表现，揭示了模型在MIDI符号处理上表现优异但在音频感知上存在明显差距，表明当前系统擅长符号推理但尚未实现可靠的音频聆听能力。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型声称具有"音乐理解"能力，但其评估往往混淆了听觉感知和乐谱阅读。本研究旨在明确区分这两种能力，揭示模型在音乐理解方面的真实局限性。

Method: 研究评估了三个最先进的大语言模型在三个核心音乐技能上的表现：切分音评分、移调检测和和弦质量识别。通过分离三个变异性来源：感知限制（音频vs MIDI输入）、示例暴露（零样本vs少样本）和推理策略（独立推理、思维链、LogicLM），并特别将LogicLM框架适配到音乐领域。

Result: 结果显示了明显的感知差距：模型在MIDI上表现接近天花板水平，但在音频上准确率显著下降。推理策略和少样本提示带来的改进有限。尽管LogicLM在MIDI上达到近乎完美的准确率，但在音频上仍然表现脆弱。在所有模型中，Gemini Pro在大多数条件下表现最佳。

Conclusion: 当前系统擅长基于符号（MIDI）的推理，但尚未实现可靠的音频聆听能力。研究方法和数据集明确了感知与推理的边界，为构建稳健的音频优先音乐系统提供了可操作的指导。

Abstract: Multimodal Large Language Models (LLMs) claim "musical understanding" via
evaluations that conflate listening with score reading. We benchmark three SOTA
LLMs (Gemini 2.5 Pro, Gemini 2.5 Flash, and Qwen2.5-Omni) across three core
music skills: Syncopation Scoring, Transposition Detection, and Chord Quality
Identification. Moreover, we separate three sources of variability: (i)
perceptual limitations (audio vs. MIDI inputs), (ii) exposure to examples
(zero- vs. few-shot manipulations), and (iii) reasoning strategies (Standalone,
CoT, LogicLM). For the latter we adapt LogicLM, a framework combining LLMs with
symbolic solvers to perform structured reasoning, to music. Results reveal a
clear perceptual gap: models perform near ceiling on MIDI but show accuracy
drops on audio. Reasoning and few-shot prompting offer minimal gains. This is
expected for MIDI, where performance reaches saturation, but more surprising
for audio, where LogicLM, despite near-perfect MIDI accuracy, remains notably
brittle. Among models, Gemini Pro achieves the highest performance across most
conditions. Overall, current systems reason well over symbols (MIDI) but do not
yet "listen" reliably from audio. Our method and dataset make the
perception-reasoning boundary explicit and offer actionable guidance for
building robust, audio-first music systems.

</details>


### [45] [SAO-Instruct: Free-form Audio Editing using Natural Language Instructions](https://arxiv.org/abs/2510.22795)
*Michael Ungersböck,Florian Grötschla,Luca A. Lanzendörfer,June Young Yi,Changho Choi,Roger Wattenhofer*

Main category: cs.SD

TL;DR: SAO-Instruct是一个基于Stable Audio Open的音频编辑模型，能够使用任意自由形式的自然语言指令编辑音频片段，在客观指标上表现优异，在主观听力研究中优于其他音频编辑方法。


<details>
  <summary>Details</summary>
Motivation: 生成模型在从简短文本描述合成高保真音频方面取得了显著进展，但使用自然语言编辑现有音频仍然很大程度上未被探索。现有方法要么需要完整描述编辑后的音频，要么局限于缺乏灵活性的预定义编辑指令。

Method: 基于Stable Audio Open构建SAO-Instruct模型，使用Prompt-to-Prompt、DDPM反演和手动编辑流程创建音频编辑三元组（输入音频、编辑指令、输出音频）数据集进行训练。

Result: 尽管部分训练数据是合成的，但模型能够很好地泛化到真实环境中的音频片段和未见过的编辑指令，在客观指标上表现优异，在主观听力研究中优于其他音频编辑方法。

Conclusion: SAO-Instruct实现了使用自由形式自然语言指令编辑音频的能力，为未来研究提供了代码和模型权重。

Abstract: Generative models have made significant progress in synthesizing
high-fidelity audio from short textual descriptions. However, editing existing
audio using natural language has remained largely underexplored. Current
approaches either require the complete description of the edited audio or are
constrained to predefined edit instructions that lack flexibility. In this
work, we introduce SAO-Instruct, a model based on Stable Audio Open capable of
editing audio clips using any free-form natural language instruction. To train
our model, we create a dataset of audio editing triplets (input audio, edit
instruction, output audio) using Prompt-to-Prompt, DDPM inversion, and a manual
editing pipeline. Although partially trained on synthetic data, our model
generalizes well to real in-the-wild audio clips and unseen edit instructions.
We demonstrate that SAO-Instruct achieves competitive performance on objective
metrics and outperforms other audio editing approaches in a subjective
listening study. To encourage future research, we release our code and model
weights.

</details>


### [46] [TwinShift: Benchmarking Audio Deepfake Detection across Synthesizer and Speaker Shifts](https://arxiv.org/abs/2510.23096)
*Jiyoung Hong,Yoonseo Chung,Seungyeon Oh,Juntae Kim,Jiyoung Lee,Sookyung Kim,Hyunsoo Cho*

Main category: cs.SD

TL;DR: TWINSHIFT是一个专门评估音频深度伪造检测器在严格未见条件下鲁棒性的基准测试，通过六个不同的合成系统和不相交的说话人集合来测试检测器的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 音频深度伪造技术快速发展，已被用于欺诈和虚假信息传播。现有检测器虽然在基准测试中表现良好，但在面对新的合成方法和不同说话人时泛化能力不足，限制了实际应用的可靠性。

Method: 构建TWINSHIFT基准测试，包含六个不同的合成系统，每个系统配有不重叠的说话人集合，用于严格评估检测器在生成模型和说话人身份同时变化时的泛化能力。

Result: 通过广泛实验，TWINSHIFT揭示了重要的鲁棒性差距，发现了被忽视的局限性，并为开发音频深度伪造检测系统提供了原则性指导。

Conclusion: TWINSHIFT基准测试能够有效评估音频深度伪造检测器的鲁棒性，为开发更可靠的检测系统提供了重要工具和指导。

Abstract: Audio deepfakes pose a growing threat, already exploited in fraud and
misinformation. A key challenge is ensuring detectors remain robust to unseen
synthesis methods and diverse speakers, since generation techniques evolve
quickly. Despite strong benchmark results, current systems struggle to
generalize to new conditions limiting real-world reliability. To address this,
we introduce TWINSHIFT, a benchmark explicitly designed to evaluate detection
robustness under strictly unseen conditions. Our benchmark is constructed from
six different synthesis systems, each paired with disjoint sets of speakers,
allowing for a rigorous assessment of how well detectors generalize when both
the generative model and the speaker identity change. Through extensive
experiments, we show that TWINSHIFT reveals important robustness gaps, uncover
overlooked limitations, and provide principled guidance for developing ADD
systems. The TWINSHIFT benchmark can be accessed at
https://github.com/intheMeantime/TWINSHIFT.

</details>


### [47] [Low-Resource Audio Codec (LRAC): 2025 Challenge Description](https://arxiv.org/abs/2510.23312)
*Kamil Wojcicki,Yusuf Ziya Isik,Laura Lechler,Mansur Yesilbursa,Ivana Balić,Wolfgang Mack,Rafał Łaganowski,Guoqing Zhang,Yossi Adi,Minje Kim,Shinji Watanabe*

Main category: cs.SD

TL;DR: 2025年低资源音频编解码挑战赛旨在推动神经和混合编解码器在资源受限应用中的发展，解决现有神经音频编解码器在低资源运行和对声学失真鲁棒性方面的障碍。


<details>
  <summary>Details</summary>
Motivation: 现有神经音频编解码器虽然在超低比特率下提供优越的语音质量，但在低资源运行和对背景噪声、混响等声学失真的鲁棒性方面存在障碍，限制了其在实际边缘部署场景中的应用。

Method: 通过举办2025年低资源音频编解码挑战赛，提供标准化训练数据集、两个基线系统和全面评估框架，支持参与者开发适用于资源受限应用的神经和混合编解码器。

Result: 挑战赛预期将为编解码器设计和相关下游音频任务提供有价值的见解。

Conclusion: 该挑战赛旨在推动神经音频编解码器在边缘部署场景中的实际应用，解决其在计算约束、低延迟、低比特率以及对声学失真鲁棒性方面的关键问题。

Abstract: While recent neural audio codecs deliver superior speech quality at ultralow
bitrates over traditional methods, their practical adoption is hindered by
obstacles related to low-resource operation and robustness to acoustic
distortions. Edge deployment scenarios demand codecs that operate under
stringent compute constraints while maintaining low latency and bitrate. The
presence of background noise and reverberation further necessitates designs
that are resilient to such degradations. The performance of neural codecs under
these constraints and their integration with speech enhancement remain largely
unaddressed. To catalyze progress in this area, we introduce the 2025
Low-Resource Audio Codec Challenge, which targets the development of neural and
hybrid codecs for resource-constrained applications. Participants are supported
with a standardized training dataset, two baseline systems, and a comprehensive
evaluation framework. The challenge is expected to yield valuable insights
applicable to both codec design and related downstream audio tasks.

</details>


### [48] [Learning Linearity in Audio Consistency Autoencoders via Implicit Regularization](https://arxiv.org/abs/2510.23530)
*Bernardo Torres,Manuel Moussallam,Gabriel Meseguer-Brocal*

Main category: cs.SD

TL;DR: 提出一种训练方法，使一致性自编码器在高压缩率下具有线性特性，支持直观的代数操作如混合和缩放，而不改变模型架构或损失函数。


<details>
  <summary>Details</summary>
Motivation: 音频自编码器学习有用的压缩音频表示，但其非线性潜在空间阻碍了直观的代数操作，如混合或缩放。

Method: 通过数据增强训练方法，在不改变模型架构或损失函数的情况下，诱导高压缩一致性自编码器具有线性特性，包括同质性（对标量增益的等变性）和可加性（解码器保持加法）。

Result: 使用该方法训练的CAE在编码器和解码器中都表现出线性行为，同时保持了重建保真度。在音乐源合成和分离任务中通过简单的潜在算术验证了实用性。

Conclusion: 这项工作提供了一种构建结构化潜在空间的简单技术，实现了更直观和高效的音频处理。

Abstract: Audio autoencoders learn useful, compressed audio representations, but their
non-linear latent spaces prevent intuitive algebraic manipulation such as
mixing or scaling. We introduce a simple training methodology to induce
linearity in a high-compression Consistency Autoencoder (CAE) by using data
augmentation, thereby inducing homogeneity (equivariance to scalar gain) and
additivity (the decoder preserves addition) without altering the model's
architecture or loss function. When trained with our method, the CAE exhibits
linear behavior in both the encoder and decoder while preserving reconstruction
fidelity. We test the practical utility of our learned space on music source
composition and separation via simple latent arithmetic. This work presents a
straightforward technique for constructing structured latent spaces, enabling
more intuitive and efficient audio processing.

</details>


### [49] [ISA-Bench: Benchmarking Instruction Sensitivity for Large Audio Language Models](https://arxiv.org/abs/2510.23558)
*Bohan Li,Wenbin Huang,Yuhang Qiu,Yiwei Guo,Hankun Wang,Zhihan Li,Jing Peng,Ziyang Ma,Xie Chen,Kai Yu*

Main category: cs.SD

TL;DR: ISA-Bench是一个评估大型音频语言模型指令敏感性的动态基准，从指令描述、输出格式和任务组合三个维度系统评估模型性能。研究发现现有LALMs存在显著的指令敏感性，通过微调可改善指令遵循能力但会导致灾难性遗忘。


<details>
  <summary>Details</summary>
Motivation: 现有大型音频语言模型对指令表述高度敏感，影响指令遵循率和任务性能，但缺乏系统评估这种敏感性的基准。

Method: 构建ISA-Bench基准，从三个维度评估指令敏感性：指令描述、输出格式和任务组合。评估开源和专有LALMs，并通过在复杂指令变体数据集上微调Qwen2-Audio来缓解指令敏感性问题。

Result: 实验显示即使最先进的LALMs也存在显著的指令敏感性，导致基础音频理解任务性能下降。微调后指令遵循性能显著提升，但会引发灾难性遗忘问题。

Conclusion: ISA-Bench为评估和改进LALMs的指令敏感性提供了标准化基础，强调了实际应用中指令鲁棒音频理解的重要性。

Abstract: Large Audio Language Models (LALMs), which couple acoustic perception with
large language models (LLMs) to extract and understand diverse information from
audio, have attracted intense interest from both academic and industrial
communities. However, existing LALMs are highly sensitive to how instructions
are phrased, affecting both (i) instruction-following rates and (ii) task
performance. Yet, no existing benchmarks offer a systematic and comprehensive
evaluation of this sensitivity. We introduce ISA-Bench, a dynamic benchmark
evaluating instruction sensitivity for LALMs along three axes: instruction
description, output format, and task composition. We assess recent open-source
and proprietary LALMs using ISA-Bench, profiling both compliance and accuracy
under controlled instruction variations. Experimental results reveal that even
state-of-the-art LALMs suffer significant instruction sensitivity, leading to
degraded performance on fundamental audio understanding tasks. To mitigate this
issue, we fine-tune Qwen2-Audio on a specifically constructed complex
instruction-variant dataset, achieving a marked improvement in
instruction-following performance. However, this also induces nontrivial
catastrophic forgetting: the model loses some previously mastered task
capabilities when exposed to new instruction styles. Our benchmark provides a
standardized basis for assessing and improving instruction sensitivity in
LALMs, underscoring the need for instruction-robust audio understanding in
real-world pipelines.

</details>

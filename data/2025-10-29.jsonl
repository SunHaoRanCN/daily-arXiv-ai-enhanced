{"id": "2510.23937", "categories": ["cs.SD", "eess.AS", "eess.SP", "math.OC"], "pdf": "https://arxiv.org/pdf/2510.23937", "abs": "https://arxiv.org/abs/2510.23937", "authors": ["Yuancheng Luo"], "title": "Optimized Loudspeaker Panning for Adaptive Sound-Field Correction and Non-stationary Listening Areas", "comment": null, "summary": "Surround sound systems commonly distribute loudspeakers along standardized\nlayouts for multichannel audio reproduction. However in less controlled\nenvironments, practical layouts vary in loudspeaker quantity, placement, and\nlistening locations / areas. Deviations from standard layouts introduce\nsound-field errors that degrade acoustic timbre, imaging, and clarity of audio\ncontent reproduction. This work introduces both Bayesian loudspeaker\nnormalization and content panning optimization methods for sound-field\ncorrection. Conjugate prior distributions over loudspeaker-listener directions\nupdate estimated layouts for non-stationary listening locations; digital\nfilters adapt loudspeaker acoustic responses to a common reference target at\nthe estimated listening area without acoustic measurements. Frequency-domain\npanning coefficients are then optimized via sensitivity / efficiency objectives\nsubject to spatial, electrical, and acoustic domain constraints; normalized and\npanned loudspeakers form virtual loudspeakers in standardized layouts for\naccurate multichannel reproduction. Experiments investigate robustness of\nBayesian adaptation, and panning optimizations in practical applications."}
{"id": "2510.23969", "categories": ["cs.SD", "cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.23969", "abs": "https://arxiv.org/abs/2510.23969", "authors": ["Harshavardhana T. Gowda", "Lee M. Miller"], "title": "emg2speech: synthesizing speech from electromyography using self-supervised speech models", "comment": null, "summary": "We present a neuromuscular speech interface that translates electromyographic\n(EMG) signals collected from orofacial muscles during speech articulation\ndirectly into audio. We show that self-supervised speech (SS) representations\nexhibit a strong linear relationship with the electrical power of muscle action\npotentials: SS features can be linearly mapped to EMG power with a correlation\nof $r = 0.85$. Moreover, EMG power vectors corresponding to different\narticulatory gestures form structured and separable clusters in feature space.\nThis relationship: $\\text{SS features}$ $\\xrightarrow{\\texttt{linear mapping}}$\n$\\text{EMG power}$ $\\xrightarrow{\\texttt{gesture-specific clustering}}$\n$\\text{articulatory movements}$, highlights that SS models implicitly encode\narticulatory mechanisms. Leveraging this property, we directly map EMG signals\nto SS feature space and synthesize speech, enabling end-to-end EMG-to-speech\ngeneration without explicit articulatory models and vocoder training."}
{"id": "2510.24103", "categories": ["cs.SD", "cs.AI", "cs.MM", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.24103", "abs": "https://arxiv.org/abs/2510.24103", "authors": ["Kang Zhang", "Trung X. Pham", "Suyeon Lee", "Axi Niu", "Arda Senocak", "Joon Son Chung"], "title": "Model-Guided Dual-Role Alignment for High-Fidelity Open-Domain Video-to-Audio Generation", "comment": "accepted by NeurIPS 2025", "summary": "We present MGAudio, a novel flow-based framework for open-domain\nvideo-to-audio generation, which introduces model-guided dual-role alignment as\na central design principle. Unlike prior approaches that rely on\nclassifier-based or classifier-free guidance, MGAudio enables the generative\nmodel to guide itself through a dedicated training objective designed for\nvideo-conditioned audio generation. The framework integrates three main\ncomponents: (1) a scalable flow-based Transformer model, (2) a dual-role\nalignment mechanism where the audio-visual encoder serves both as a\nconditioning module and as a feature aligner to improve generation quality, and\n(3) a model-guided objective that enhances cross-modal coherence and audio\nrealism. MGAudio achieves state-of-the-art performance on VGGSound, reducing\nFAD to 0.40, substantially surpassing the best classifier-free guidance\nbaselines, and consistently outperforms existing methods across FD, IS, and\nalignment metrics. It also generalizes well to the challenging UnAV-100\nbenchmark. These results highlight model-guided dual-role alignment as a\npowerful and scalable paradigm for conditional video-to-audio generation. Code\nis available at: https://github.com/pantheon5100/mgaudio"}
{"id": "2510.24279", "categories": ["cs.SD", "cs.CE", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.24279", "abs": "https://arxiv.org/abs/2510.24279", "authors": ["Matteo Calafà", "Yuanxin Xia", "Cheol-Ho Jeong"], "title": "HergNet: a Fast Neural Surrogate Model for Sound Field Predictions via Superposition of Plane Waves", "comment": null, "summary": "We present a novel neural network architecture for the efficient prediction\nof sound fields in two and three dimensions. The network is designed to\nautomatically satisfy the Helmholtz equation, ensuring that the outputs are\nphysically valid. Therefore, the method can effectively learn solutions to\nboundary-value problems in various wave phenomena, such as acoustics, optics,\nand electromagnetism. Numerical experiments show that the proposed strategy can\npotentially outperform state-of-the-art methods in room acoustics simulation,\nin particular in the range of mid to high frequencies."}
{"id": "2510.23849", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2510.23849", "abs": "https://arxiv.org/abs/2510.23849", "authors": ["Wanting Huang", "Weiran Wang"], "title": "A Neural Model for Contextual Biasing Score Learning and Filtering", "comment": "Accepted to IEEE ASRU 2025", "summary": "Contextual biasing improves automatic speech recognition (ASR) by integrating\nexternal knowledge, such as user-specific phrases or entities, during decoding.\nIn this work, we use an attention-based biasing decoder to produce scores for\ncandidate phrases based on acoustic information extracted by an ASR encoder,\nwhich can be used to filter out unlikely phrases and to calculate bonus for\nshallow-fusion biasing. We introduce a per-token discriminative objective that\nencourages higher scores for ground-truth phrases while suppressing\ndistractors. Experiments on the Librispeech biasing benchmark show that our\nmethod effectively filters out majority of the candidate phrases, and\nsignificantly improves recognition accuracy under different biasing conditions\nwhen the scores are used in shallow fusion biasing. Our approach is modular and\ncan be used with any ASR system, and the filtering mechanism can potentially\nboost performance of other biasing methods."}
{"id": "2510.23780", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.23780", "abs": "https://arxiv.org/abs/2510.23780", "authors": ["Omran Abbas", "Abdullah Zayat", "Loıc Markley", "Anas Chaaban"], "title": "Nonlinear Stacked Intelligent Surfaces for Wireless Systems", "comment": "9 pages, 4 figures", "summary": "Stacked intelligent surfaces (SIS) are a promising technology for\nnext-generation wireless systems, offering an opportunity to enhance\ncommunication performance with low power consumption. Typically, an SIS is\nmodelled as a surface that imparts phase shifts on impinging electromagnetic\nsignals to achieve desired communication objectives. However, this mode of\noperation results in a linear SIS, which limits its applicability to linear\noperations. To unlock further SIS potential, we propose a nonlinear SIS that\ncan mimic the behaviour of nonlinear neural networks. We discuss the\nfeasibility and potential of this idea and propose a nonlinear SIS unit cell\nwith a step-like response. To evaluate the system-level performance of\nnonlinear SIS, we present a case study where SIS structures are optimized to\nminimize the symbol error rate (SER) in an MIMO system with SIS deployed at\nboth the transmitter and receiver sides using only statistical channel\ninformation. We demonstrate that a nonlinear SIS can improve communication\nreliability compared to a linear SIS by forming complex signal patterns across\nthe SIS surface, which provide higher diversity against noise disturbances,\nwhile still allowing the receiver to discern these patterns. Finally, we\noutline several potential applications of nonlinear SIS in wireless\ncommunication scenarios."}
{"id": "2510.24282", "categories": ["cs.SD", "cs.AR", "eess.AS", "B.7; C.3; I.2"], "pdf": "https://arxiv.org/pdf/2510.24282", "abs": "https://arxiv.org/abs/2510.24282", "authors": ["Baizhou Lin", "Yuetong Fang", "Renjing Xu", "Rishad Shafik", "Jagmohan Chauhan"], "title": "TsetlinKWS: A 65nm 16.58uW, 0.63mm2 State-Driven Convolutional Tsetlin Machine-Based Accelerator For Keyword Spotting", "comment": "12 pages, 17 figures. This work has been submitted to the IEEE for\n  possible publication", "summary": "The Tsetlin Machine (TM) has recently attracted attention as a low-power\nalternative to neural networks due to its simple and interpretable inference\nmechanisms. However, its performance on speech-related tasks remains limited.\nThis paper proposes TsetlinKWS, the first algorithm-hardware co-design\nframework for the Convolutional Tsetlin Machine (CTM) on the 12-keyword\nspotting task. Firstly, we introduce a novel Mel-Frequency Spectral Coefficient\nand Spectral Flux (MFSC-SF) feature extraction scheme together with spectral\nconvolution, enabling the CTM to reach its first-ever competitive accuracy of\n87.35% on the 12-keyword spotting task. Secondly, we develop an Optimized\nGrouped Block-Compressed Sparse Row (OG-BCSR) algorithm that achieves a\nremarkable 9.84$\\times$ reduction in model size, significantly improving the\nstorage efficiency on CTMs. Finally, we propose a state-driven architecture\ntailored for the CTM, which simultaneously exploits data reuse and sparsity to\nachieve high energy efficiency. The full system is evaluated in 65 nm process\ntechnology, consuming 16.58 $\\mu$W at 0.7 V with a compact 0.63 mm$^2$ core\narea. TsetlinKWS requires only 907k logic operations per inference,\nrepresenting a 10$\\times$ reduction compared to the state-of-the-art KWS\naccelerators, positioning the CTM as a highly-efficient candidate for\nultra-low-power speech applications."}
{"id": "2510.24024", "categories": ["eess.AS", "cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2510.24024", "abs": "https://arxiv.org/abs/2510.24024", "authors": ["Yuchi Ishikawa", "Toranosuke Manabe", "Tatsuya Komatsu", "Yoshimitsu Aoki"], "title": "Listening without Looking: Modality Bias in Audio-Visual Captioning", "comment": "under review", "summary": "Audio-visual captioning aims to generate holistic scene descriptions by\njointly modeling sound and vision. While recent methods have improved\nperformance through sophisticated modality fusion, it remains unclear to what\nextent the two modalities are complementary in current audio-visual captioning\nmodels and how robust these models are when one modality is degraded. We\naddress these questions by conducting systematic modality robustness tests on\nLAVCap, a state-of-the-art audio-visual captioning model, in which we\nselectively suppress or corrupt the audio or visual streams to quantify\nsensitivity and complementarity. The analysis reveals a pronounced bias toward\nthe audio stream in LAVCap. To evaluate how balanced audio-visual captioning\nmodels are in their use of both modalities, we augment AudioCaps with textual\nannotations that jointly describe the audio and visual streams, yielding the\nAudioVisualCaps dataset. In our experiments, we report LAVCap baseline results\non AudioVisualCaps. We also evaluate the model under modality robustness tests\non AudioVisualCaps and the results indicate that LAVCap trained on\nAudioVisualCaps exhibits less modality bias than when trained on AudioCaps."}
{"id": "2510.23832", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.23832", "abs": "https://arxiv.org/abs/2510.23832", "authors": ["Evan Allen", "Karim Said", "Robert Calderbank", "Lingjia Liu"], "title": "Communication in a Fractional World: MIMO MC-OTFS Precoder Prediction", "comment": null, "summary": "As 6G technologies advance, international bodies and regulatory agencies are\nintensifying efforts to extend seamless connectivity especially for\nhigh-mobility scenarios such as Mobile Ad-Hoc Networks (\\textit{MANETs}) types\nsuch as Vehicular Ad-Hoc Networks (\\textit{VANETs}) and Flying Ad-Hoc Networks\n(\\textit{FANETs}). For these environments to be considered for long term\nadoption and use they must support Multiple-Input-Multiple- (MIMO) technology,\nrapidly fluctuating channel conditions in these environments place a heavy\nburden on traditional time-frequency CSI feedback schemes required for MIMO\nprecoding. This motivates a shift toward delay-Doppler representations like\nthose employed by Orthogonal Time-Frequency Space(OTFS) modulation, which\noffers greater stability under mobility. We derive an expression for the\nvariation over time in the OTFS I/O relationship. We then use this to create a\nphysics informed complex exponential basis expansion model prediction framework\nthat maximizes the usefulness of outdated Channel State Information (CSI) in\nthe presence of integer and fractional delay-Doppler channels and facilitates\nhigh mobility MIMO communication."}
{"id": "2510.24332", "categories": ["cs.SD", "cs.CV", "eess.AS", "eess.IV"], "pdf": "https://arxiv.org/pdf/2510.24332", "abs": "https://arxiv.org/abs/2510.24332", "authors": ["Jonas Hein", "Lazaros Vlachopoulos", "Maurits Geert Laurent Olthof", "Bastian Sigrist", "Philipp Fürnstahl", "Matthias Seibold"], "title": "Sound Source Localization for Spatial Mapping of Surgical Actions in Dynamic Scenes", "comment": null, "summary": "Purpose: Surgical scene understanding is key to advancing computer-aided and\nintelligent surgical systems. Current approaches predominantly rely on visual\ndata or end-to-end learning, which limits fine-grained contextual modeling.\nThis work aims to enhance surgical scene representations by integrating 3D\nacoustic information, enabling temporally and spatially aware multimodal\nunderstanding of surgical environments.\n  Methods: We propose a novel framework for generating 4D audio-visual\nrepresentations of surgical scenes by projecting acoustic localization\ninformation from a phased microphone array onto dynamic point clouds from an\nRGB-D camera. A transformer-based acoustic event detection module identifies\nrelevant temporal segments containing tool-tissue interactions which are\nspatially localized in the audio-visual scene representation. The system was\nexperimentally evaluated in a realistic operating room setup during simulated\nsurgical procedures performed by experts.\n  Results: The proposed method successfully localizes surgical acoustic events\nin 3D space and associates them with visual scene elements. Experimental\nevaluation demonstrates accurate spatial sound localization and robust fusion\nof multimodal data, providing a comprehensive, dynamic representation of\nsurgical activity.\n  Conclusion: This work introduces the first approach for spatial sound\nlocalization in dynamic surgical scenes, marking a significant advancement\ntoward multimodal surgical scene representations. By integrating acoustic and\nvisual data, the proposed framework enables richer contextual understanding and\nprovides a foundation for future intelligent and autonomous surgical systems."}
{"id": "2510.24471", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2510.24471", "abs": "https://arxiv.org/abs/2510.24471", "authors": ["Yujie Zhu", "Jilu Jin", "Xueqin Luo", "Wenxing Yang", "Zhong-Qiu Wang", "Gongping Huang", "Jingdong Chen", "Jacob Benesty"], "title": "Forward Convolutive Prediction for Frame Online Monaural Speech Dereverberation Based on Kronecker Product Decomposition", "comment": null, "summary": "Dereverberation has long been a crucial research topic in speech processing,\naiming to alleviate the adverse effects of reverberation in voice communication\nand speech interaction systems. Among existing approaches, forward\nconvolutional prediction (FCP) has recently attracted attention. It typically\nemploys a deep neural network to predict the direct-path signal and\nsubsequently estimates a linear prediction filter to suppress residual\nreverberation. However, a major drawback of this approach is that the required\nlinear prediction filter is often excessively long, leading to considerable\ncomputational complexity. To address this, our work proposes a novel FCP method\nbased on Kronecker product (KP) decomposition, in which the long prediction\nfilter is modeled as the KP of two much shorter filters. This decomposition\nsignificantly reduces the computational cost. An adaptive algorithm is then\nprovided to iteratively update these shorter filters online. Experimental\nresults show that, compared to conventional methods, our approach achieves\ncompetitive dereverberation performance while substantially reducing\ncomputational cost."}
{"id": "2510.23837", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.23837", "abs": "https://arxiv.org/abs/2510.23837", "authors": ["Ali Amhaz", "Shreya Khisa", "Mohamed Elhattab", "Chadi Assi", "Sanaa Sharafeddine"], "title": "Coordinated Multipoint Transmission in Pinching Antenna Systems", "comment": null, "summary": "We study a coordinated multi-point (CoMP) transmission where two base\nstations (BSs), each supported by a pinching antenna system (PASS), are\ndeployed to jointly serve communication users under spatial division multiple\naccess (SDMA) technology. Pinching Antenna technology was introduced as a\npromising solution to overcome the large-scale fading that has been shown to be\nan impediment in multiple-input multiple-output (MIMO) systems. To realize the\nadvantages of this technology in CoMP systems, which suffer from an upperbound\nrate limitation when traditional uniform linear arrays (ULAs) are adopted, we\nformulate an optimization problem with the aim of maximizing the achievable sum\nrate by jointly determining the transmit beamforming vectors and pinching\nlocations on the waveguides while respecting the quality of service (QoS)\nrequirements of users. This problem is inherently non-convex due to the strong\ncoupling among its decision parameters, making it challenging to solve using\ntraditional optimization methods. Thus, we utilize a gradient-based\nmeta-learning (GML) strategy specifically designed for large-scale optimization\ntasks. Finally, numerical analysis demonstrates the effectiveness of the\nproposed GML approach, achieving 92 percent of the optimal solution, and the\nsuperiority of the solution presented compared to other benchmarks. In\naddition, it achieves a higher upper bound on the achievable rate compared to\nconventional CoMP systems."}
{"id": "2510.24372", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.24372", "abs": "https://arxiv.org/abs/2510.24372", "authors": ["Ziyang Zhang", "Yifan Gao", "Xuenan Xu", "Baoxiangli", "Wen Wu", "Chao Zhang"], "title": "Bayesian Speech synthesizers Can Learn from Multiple Teachers", "comment": null, "summary": "Codec-based text-to-speech (TTS) models have recently gained traction for\ntheir efficiency and strong performance in voice cloning. However, codec-based\nTTS faces limitations due to the challenges of pretraining robust speech codecs\nand the quality degradation introduced by quantization errors. Emerging\nevidence suggests that continuous-valued generative models can alleviate these\nissues and serve as a promising alternative. Yet, effectively modelling diverse\nspeech patterns and developing reliable sampling strategies for\ncontinuous-valued autoregressive (AR) TTS remains underexplored. In this work,\nwe propose BELLE, Bayesian evidential learning with language modelling for TTS,\na novel continuous-valued AR framework that directly predicts mel-spectrograms\nfrom textual input. BELLE treats each mel-spectrogram frame as a Gaussian\ndistribution sampled from a learned hyper distribution, enabling principled\nuncertainty estimation, particularly in scenarios with parallel data (i.e., one\ntext-audio prompt paired with multiple speech samples). To obtain such data,\ndiverse speech samples are synthesized using multiple pre-trained TTS models\ngiven the same text-audio prompts, which are distilled into BELLE via Bayesian\nevidential learning. Experimental results indicate that BELLE demonstrates\nhighly competitive performance compared with the current best open-source TTS\nmodels, even though BELLE is trained on a large amount of synthetic data and\nuses only approximately one-tenth of their training data. Audio samples\ngenerated by BELLE are available at https://belletts.github.io/Belle/. The\ncode, checkpoints, and synthetic data will be released after the paper is\naccepted."}
{"id": "2510.23937", "categories": ["cs.SD", "eess.AS", "eess.SP", "math.OC"], "pdf": "https://arxiv.org/pdf/2510.23937", "abs": "https://arxiv.org/abs/2510.23937", "authors": ["Yuancheng Luo"], "title": "Optimized Loudspeaker Panning for Adaptive Sound-Field Correction and Non-stationary Listening Areas", "comment": null, "summary": "Surround sound systems commonly distribute loudspeakers along standardized\nlayouts for multichannel audio reproduction. However in less controlled\nenvironments, practical layouts vary in loudspeaker quantity, placement, and\nlistening locations / areas. Deviations from standard layouts introduce\nsound-field errors that degrade acoustic timbre, imaging, and clarity of audio\ncontent reproduction. This work introduces both Bayesian loudspeaker\nnormalization and content panning optimization methods for sound-field\ncorrection. Conjugate prior distributions over loudspeaker-listener directions\nupdate estimated layouts for non-stationary listening locations; digital\nfilters adapt loudspeaker acoustic responses to a common reference target at\nthe estimated listening area without acoustic measurements. Frequency-domain\npanning coefficients are then optimized via sensitivity / efficiency objectives\nsubject to spatial, electrical, and acoustic domain constraints; normalized and\npanned loudspeakers form virtual loudspeakers in standardized layouts for\naccurate multichannel reproduction. Experiments investigate robustness of\nBayesian adaptation, and panning optimizations in practical applications."}
{"id": "2510.23844", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.23844", "abs": "https://arxiv.org/abs/2510.23844", "authors": ["Cameron M. Pike", "Brad Oney", "Gabriel Hepner", "Animesh Yadav"], "title": "Accurate Prediction of Nonlinear Distortion of Multi-Carrier Signals", "comment": "7 Pages, 7 figures, 6 pages, 6 figures, accepted for publication in\n  IEEE TCAS-II", "summary": "Nonlinearities in power amplifiers adversely affect multi-carrier modulation\ntechniques. Accurate prediction of nonlinear distortion is essential for making\ndesign trade-offs between output power and network throughput. We use the\nseries form of the characteristic function (ch.f.) method to predict distortion\nspectra for sparse multi-carrier transmissions. This method results in\nefficient calculations of individual signal and distortion components. The\nmethod is validated both theoretically and practically. Theoretical validation\nis performed by modeling the signal as a bandpass Gaussian process that is hard\nlimited, and it is shown that the series ch.f. method produces results that are\nidentical with the classical Price's theorem. Practical validation is shown by\nconsidering an orthogonal frequency division multiplexing (OFDM) signal with a\nfragmented spectrum which is then applied to an amplifier driven into\ncompression for which application of Price's theorem is difficult, and the\npredicted output spectrum corroborates laboratory measurements. Part of the\ncomputational efficiency is realized in that the nonlinearity can be expressed\nas the fast Fourier transform (FFT) of samples of its forward scattering\nparameter (i.e., S21) or transconductance function (including AM-PM effects),\nand distortion contributions of the signal can be expressed as numerical\nautoconvolutions of the clean spectrum. Signal-to-distortion ratio (SDR) can be\neasily computed and parameterized across variables of interest, such as\noverdrive level."}
{"id": "2510.24497", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.24497", "abs": "https://arxiv.org/abs/2510.24497", "authors": ["Yuanhang Qian", "Kunlong Zhao", "Jilu Jin", "Xueqin Luo", "Gongping Huang", "Jingdong Chen", "Jacob Benesty"], "title": "Online neural fusion of distortionless differential beamformers for robust speech enhancement", "comment": null, "summary": "Fixed beamforming is widely used in practice since it does not depend on the\nestimation of noise statistics and provides relatively stable performance.\nHowever, a single beamformer cannot adapt to varying acoustic conditions, which\nlimits its interference suppression capability. To address this, adaptive\nconvex combination (ACC) algorithms have been introduced, where the outputs of\nmultiple fixed beamformers are linearly combined to improve robustness.\nNevertheless, ACC often fails in highly non-stationary scenarios, such as\nrapidly moving interference, since its adaptive updates cannot reliably track\nrapid changes. To overcome this limitation, we propose a frame-online neural\nfusion framework for multiple distortionless differential beamformers, which\nestimates the combination weights through a neural network. Compared with\nconventional ACC, the proposed method adapts more effectively to dynamic\nacoustic environments, achieving stronger interference suppression while\nmaintaining the distortionless constraint."}
{"id": "2510.23969", "categories": ["cs.SD", "cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.23969", "abs": "https://arxiv.org/abs/2510.23969", "authors": ["Harshavardhana T. Gowda", "Lee M. Miller"], "title": "emg2speech: synthesizing speech from electromyography using self-supervised speech models", "comment": null, "summary": "We present a neuromuscular speech interface that translates electromyographic\n(EMG) signals collected from orofacial muscles during speech articulation\ndirectly into audio. We show that self-supervised speech (SS) representations\nexhibit a strong linear relationship with the electrical power of muscle action\npotentials: SS features can be linearly mapped to EMG power with a correlation\nof $r = 0.85$. Moreover, EMG power vectors corresponding to different\narticulatory gestures form structured and separable clusters in feature space.\nThis relationship: $\\text{SS features}$ $\\xrightarrow{\\texttt{linear mapping}}$\n$\\text{EMG power}$ $\\xrightarrow{\\texttt{gesture-specific clustering}}$\n$\\text{articulatory movements}$, highlights that SS models implicitly encode\narticulatory mechanisms. Leveraging this property, we directly map EMG signals\nto SS feature space and synthesize speech, enabling end-to-end EMG-to-speech\ngeneration without explicit articulatory models and vocoder training."}
{"id": "2510.23892", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.23892", "abs": "https://arxiv.org/abs/2510.23892", "authors": ["Kebin Contreras", "Emmanuel Martinez", "Brayan Monroy", "Sebastian Ardila", "Cristian Ramirez", "Mariana Caicedo", "Hans Garcia", "Tatiana Gelvez-Barrera", "Juan Poveda-Jaramillo", "Henry Arguello", "Jorge Bacca"], "title": "Learning-based Spectral Regression for Cocoa Bean Physicochemical Property Prediction", "comment": null, "summary": "Cocoa bean quality assessment is essential for ensuring compliance with\ncommercial standards, protecting consumer health, and increasing the market\nvalue of the cocoa product. The quality assessment estimates key\nphysicochemical properties, such as fermentation level, moisture content,\npolyphenol concentration, and cadmium content, among others. This assessment\nhas traditionally relied on the accurate estimation of these properties via\nvisual or sensory evaluation, jointly with laboratory-based physicochemical\nanalyses, which are often time-consuming, destructive, and difficult to scale.\nThis creates the need for rapid, reliable, and noninvasive alternatives.\nSpectroscopy, particularly in the visible and near-infrared ranges, offers a\nnon-invasive alternative by capturing the molecular signatures associated with\nthese properties. Therefore, this work introduces a scalable methodology for\nevaluating the quality of cocoa beans by predicting key physicochemical\nproperties from the spectral signatures of cocoa beans. This approach utilizes\na conveyor belt system integrated with a VIS-NIR spectrometer, coupled with\nlearning-based regression models. Furthermore, a dataset is built using cocoa\nbean batches from Santander, Colombia. Ground-truth reference values were\nobtained through standardized laboratory analyses and following commercial\ncocoa quality regulations. To further evaluate the proposed methodology's\ngeneralization, performance is tested on samples collected from other Colombian\nregions and from Cusco, Peru. Experimental results show that the proposed\nmodels achieved R2 scores exceeding 0.98 across all physicochemical properties,\nand reached 0.96 accuracy on geographically independent samples. This\nnon-destructive approach represents a suitable and scalable alternative to\nconventional laboratory methods for quality assessment across the cocoa\nproduction chain."}
{"id": "2510.24519", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.24519", "abs": "https://arxiv.org/abs/2510.24519", "authors": ["Rinku Sebastian", "Simon O'Keefe", "Martin Trefzer"], "title": "Audio Signal Processing Using Time Domain Mel-Frequency Wavelet Coefficient", "comment": null, "summary": "Extracting features from the speech is the most critical process in speech\nsignal processing. Mel Frequency Cepstral Coefficients (MFCC) are the most\nwidely used features in the majority of the speaker and speech recognition\napplications, as the filtering in this feature is similar to the filtering\ntaking place in the human ear. But the main drawback of this feature is that it\nprovides only the frequency information of the signal but does not provide the\ninformation about at what time which frequency is present. The wavelet\ntransform, with its flexible time-frequency window, provides time and frequency\ninformation of the signal and is an appropriate tool for the analysis of\nnon-stationary signals like speech. On the other hand, because of its uniform\nfrequency scaling, a typical wavelet transform may be less effective in\nanalysing speech signals, have poorer frequency resolution in low frequencies,\nand be less in line with human auditory perception. Hence, it is necessary to\ndevelop a feature that incorporates the merits of both MFCC and wavelet\ntransform. A great deal of studies are trying to combine both these features.\nThe present Wavelet Transform based Mel-scaled feature extraction methods\nrequire more computation when a wavelet transform is applied on top of\nMel-scale filtering, since it adds extra processing steps. Here we are\nproposing a method to extract Mel scale features in time domain combining the\nconcept of wavelet transform, thus reducing the computational burden of\ntime-frequency conversion and the complexity of wavelet extraction. Combining\nour proposed Time domain Mel frequency Wavelet Coefficient(TMFWC) technique\nwith the reservoir computing methodology has significantly improved the\nefficiency of audio signal processing."}
{"id": "2510.24103", "categories": ["cs.SD", "cs.AI", "cs.MM", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.24103", "abs": "https://arxiv.org/abs/2510.24103", "authors": ["Kang Zhang", "Trung X. Pham", "Suyeon Lee", "Axi Niu", "Arda Senocak", "Joon Son Chung"], "title": "Model-Guided Dual-Role Alignment for High-Fidelity Open-Domain Video-to-Audio Generation", "comment": "accepted by NeurIPS 2025", "summary": "We present MGAudio, a novel flow-based framework for open-domain\nvideo-to-audio generation, which introduces model-guided dual-role alignment as\na central design principle. Unlike prior approaches that rely on\nclassifier-based or classifier-free guidance, MGAudio enables the generative\nmodel to guide itself through a dedicated training objective designed for\nvideo-conditioned audio generation. The framework integrates three main\ncomponents: (1) a scalable flow-based Transformer model, (2) a dual-role\nalignment mechanism where the audio-visual encoder serves both as a\nconditioning module and as a feature aligner to improve generation quality, and\n(3) a model-guided objective that enhances cross-modal coherence and audio\nrealism. MGAudio achieves state-of-the-art performance on VGGSound, reducing\nFAD to 0.40, substantially surpassing the best classifier-free guidance\nbaselines, and consistently outperforms existing methods across FD, IS, and\nalignment metrics. It also generalizes well to the challenging UnAV-100\nbenchmark. These results highlight model-guided dual-role alignment as a\npowerful and scalable paradigm for conditional video-to-audio generation. Code\nis available at: https://github.com/pantheon5100/mgaudio"}
{"id": "2510.23900", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.23900", "abs": "https://arxiv.org/abs/2510.23900", "authors": ["Kuan-Po Chiu", "Sumit Roy"], "title": "LEO Downlink Channel Model Revisited: Scattering Geometry-Inspired Derivation", "comment": "Accepted to Globecom 2025", "summary": "This paper presents a new derivation of LEO-to-ground receiver channel model\nto address a clear gap in the prior art: the lack of an appropriate geometry\naware characterization of non LOS (NLOS) link model represented by the power\nspectral density (PSD). Specifically, the main contribution is a coherent\nderivation of the PSD from 1st principles that is able to reproduce results in\nprior art and explain the causal relationship of main PSD features to the\npropagation geometry parameters."}
{"id": "2510.24693", "categories": ["cs.SD", "cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.24693", "abs": "https://arxiv.org/abs/2510.24693", "authors": ["Zihan Liu", "Zhikang Niu", "Qiuyang Xiao", "Zhisheng Zheng", "Ruoqi Yuan", "Yuhang Zang", "Yuhang Cao", "Xiaoyi Dong", "Jianze Liang", "Xie Chen", "Leilei Sun", "Dahua Lin", "Jiaqi Wang"], "title": "STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence", "comment": "Homepage: https://internlm.github.io/StarBench/", "summary": "Despite rapid progress in Multi-modal Large Language Models and Large\nAudio-Language Models, existing audio benchmarks largely test semantics that\ncan be recovered from text captions, masking deficits in fine-grained\nperceptual reasoning. We formalize audio 4D intelligence that is defined as\nreasoning over sound dynamics in time and 3D space, and introduce STAR-Bench to\nmeasure it. STAR-Bench combines a Foundational Acoustic Perception setting (six\nattributes under absolute and relative regimes) with a Holistic Spatio-Temporal\nReasoning setting that includes segment reordering for continuous and discrete\nprocesses and spatial tasks spanning static localization, multi-source\nrelations, and dynamic trajectories. Our data curation pipeline uses two\nmethods to ensure high-quality samples. For foundational tasks, we use\nprocedurally synthesized and physics-simulated audio. For holistic data, we\nfollow a four-stage process that includes human annotation and final selection\nbased on human performance. Unlike prior benchmarks where caption-only\nanswering reduces accuracy slightly, STAR-Bench induces far larger drops\n(-31.5\\% temporal, -35.2\\% spatial), evidencing its focus on linguistically\nhard-to-describe cues. Evaluating 19 models reveals substantial gaps compared\nwith humans and a capability hierarchy: closed-source models are bottlenecked\nby fine-grained perception, while open-source models lag across perception,\nknowledge, and reasoning. Our STAR-Bench provides critical insights and a clear\npath forward for developing future models with a more robust understanding of\nthe physical world."}
{"id": "2510.24279", "categories": ["cs.SD", "cs.CE", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.24279", "abs": "https://arxiv.org/abs/2510.24279", "authors": ["Matteo Calafà", "Yuanxin Xia", "Cheol-Ho Jeong"], "title": "HergNet: a Fast Neural Surrogate Model for Sound Field Predictions via Superposition of Plane Waves", "comment": null, "summary": "We present a novel neural network architecture for the efficient prediction\nof sound fields in two and three dimensions. The network is designed to\nautomatically satisfy the Helmholtz equation, ensuring that the outputs are\nphysically valid. Therefore, the method can effectively learn solutions to\nboundary-value problems in various wave phenomena, such as acoustics, optics,\nand electromagnetism. Numerical experiments show that the proposed strategy can\npotentially outperform state-of-the-art methods in room acoustics simulation,\nin particular in the range of mid to high frequencies."}
{"id": "2510.23905", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.23905", "abs": "https://arxiv.org/abs/2510.23905", "authors": ["Yiming Zhang", "Vikram Krishnamurthy", "Shashwat Jain"], "title": "Inferring Group Intent as a Cooperative Game. An NLP-based Framework for Trajectory Analysis using Graph Transformer Neural Network", "comment": null, "summary": "This paper studies group target trajectory intent as the outcome of a\ncooperative game where the complex-spatio trajectories are modeled using an\nNLP-based generative model. In our framework, the group intent is specified by\nthe characteristic function of a cooperative game, and allocations for players\nin the cooperative game are specified by either the core, the Shapley value, or\nthe nucleolus. The resulting allocations induce probability distributions that\ngovern the coordinated spatio-temporal trajectories of the targets that reflect\nthe group's underlying intent. We address two key questions: (1) How can the\nintent of a group trajectory be optimally formalized as the characteristic\nfunction of a cooperative game? (2) How can such intent be inferred from noisy\nobservations of the targets? To answer the first question, we introduce a\nFisher-information-based characteristic function of the cooperative game, which\nyields probability distributions that generate coordinated spatio-temporal\npatterns. As a generative model for these patterns, we develop an NLP-based\ngenerative model built on formal grammar, enabling the creation of realistic\nmulti-target trajectory data. To answer the second question, we train a Graph\nTransformer Neural Network (GTNN) to infer group trajectory intent-expressed as\nthe characteristic function of the cooperative game-from observational data\nwith high accuracy. The self-attention function of the GTNN depends on the\ntrack estimates. Thus, the formulation and algorithms provide a multi-layer\napproach that spans target tracking (Bayesian signal processing) and the GTNN\n(for group intent inference)."}
{"id": "2510.23849", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2510.23849", "abs": "https://arxiv.org/abs/2510.23849", "authors": ["Wanting Huang", "Weiran Wang"], "title": "A Neural Model for Contextual Biasing Score Learning and Filtering", "comment": "Accepted to IEEE ASRU 2025", "summary": "Contextual biasing improves automatic speech recognition (ASR) by integrating\nexternal knowledge, such as user-specific phrases or entities, during decoding.\nIn this work, we use an attention-based biasing decoder to produce scores for\ncandidate phrases based on acoustic information extracted by an ASR encoder,\nwhich can be used to filter out unlikely phrases and to calculate bonus for\nshallow-fusion biasing. We introduce a per-token discriminative objective that\nencourages higher scores for ground-truth phrases while suppressing\ndistractors. Experiments on the Librispeech biasing benchmark show that our\nmethod effectively filters out majority of the candidate phrases, and\nsignificantly improves recognition accuracy under different biasing conditions\nwhen the scores are used in shallow fusion biasing. Our approach is modular and\ncan be used with any ASR system, and the filtering mechanism can potentially\nboost performance of other biasing methods."}
{"id": "2510.24282", "categories": ["cs.SD", "cs.AR", "eess.AS", "B.7; C.3; I.2"], "pdf": "https://arxiv.org/pdf/2510.24282", "abs": "https://arxiv.org/abs/2510.24282", "authors": ["Baizhou Lin", "Yuetong Fang", "Renjing Xu", "Rishad Shafik", "Jagmohan Chauhan"], "title": "TsetlinKWS: A 65nm 16.58uW, 0.63mm2 State-Driven Convolutional Tsetlin Machine-Based Accelerator For Keyword Spotting", "comment": "12 pages, 17 figures. This work has been submitted to the IEEE for\n  possible publication", "summary": "The Tsetlin Machine (TM) has recently attracted attention as a low-power\nalternative to neural networks due to its simple and interpretable inference\nmechanisms. However, its performance on speech-related tasks remains limited.\nThis paper proposes TsetlinKWS, the first algorithm-hardware co-design\nframework for the Convolutional Tsetlin Machine (CTM) on the 12-keyword\nspotting task. Firstly, we introduce a novel Mel-Frequency Spectral Coefficient\nand Spectral Flux (MFSC-SF) feature extraction scheme together with spectral\nconvolution, enabling the CTM to reach its first-ever competitive accuracy of\n87.35% on the 12-keyword spotting task. Secondly, we develop an Optimized\nGrouped Block-Compressed Sparse Row (OG-BCSR) algorithm that achieves a\nremarkable 9.84$\\times$ reduction in model size, significantly improving the\nstorage efficiency on CTMs. Finally, we propose a state-driven architecture\ntailored for the CTM, which simultaneously exploits data reuse and sparsity to\nachieve high energy efficiency. The full system is evaluated in 65 nm process\ntechnology, consuming 16.58 $\\mu$W at 0.7 V with a compact 0.63 mm$^2$ core\narea. TsetlinKWS requires only 907k logic operations per inference,\nrepresenting a 10$\\times$ reduction compared to the state-of-the-art KWS\naccelerators, positioning the CTM as a highly-efficient candidate for\nultra-low-power speech applications."}
{"id": "2510.23908", "categories": ["eess.SP", "cs.ET"], "pdf": "https://arxiv.org/pdf/2510.23908", "abs": "https://arxiv.org/abs/2510.23908", "authors": ["M. T. Hassan", "D. Zelenchuk", "M. A. B. Abbasi"], "title": "Machine Learning-Driven User Localization in RIS-Assisted Wireless Systems", "comment": null, "summary": "The sixth generation (6G) targets ultra reliable, low latency (URLLC) gigabit\nconnectivity in mmWave bands, where directional channels require precise beam\nalignment. Reconfigurable intelligent surfaces (RIS) reshape wave propagation\nand extend coverage, but they enlarge the beam search space at the base\nstation, making exhaustive sweeps inefficient due to control overhead and\nlatency. We propose an ML based user localization framework for RIS assisted\ncommunication at 27 GHz. A 20x20 RIS reflects signals from a core network\nconnected base station and sweeps beams across the 0-90 degree elevation plane,\ndivided into four angular sectors. We build a dataset by recording received\nsignal power (Pr in dBm) across user locations and train multiple regressors,\nincluding decision tree (DT), support vector regressor (SVR), k nearest\nneighbor (KNN), XGBoost, gradient boosting, and random forest. In operation, an\nunknown user in the same plane measures four received power values (one per\nsector) and reports them to the pretrained RIS controller, which predicts the\nuser's angular position in real time. Evaluation using mean absolute error\n(MAE), root mean squared error (RMSE), and R squared (R2) shows high accuracy.\nThe DT model achieves an MAE of 4.8 degrees with R2 = 0.96, while other models\nreach 70 to 86 percent. Predicted radiation patterns, including main lobe\nalignment between 52 and 55 degrees, closely track ground truth. The framework\nreduces beam probing, enables faster alignment, and lowers latency for RIS\nassisted 6G networks."}
{"id": "2510.24332", "categories": ["cs.SD", "cs.CV", "eess.AS", "eess.IV"], "pdf": "https://arxiv.org/pdf/2510.24332", "abs": "https://arxiv.org/abs/2510.24332", "authors": ["Jonas Hein", "Lazaros Vlachopoulos", "Maurits Geert Laurent Olthof", "Bastian Sigrist", "Philipp Fürnstahl", "Matthias Seibold"], "title": "Sound Source Localization for Spatial Mapping of Surgical Actions in Dynamic Scenes", "comment": null, "summary": "Purpose: Surgical scene understanding is key to advancing computer-aided and\nintelligent surgical systems. Current approaches predominantly rely on visual\ndata or end-to-end learning, which limits fine-grained contextual modeling.\nThis work aims to enhance surgical scene representations by integrating 3D\nacoustic information, enabling temporally and spatially aware multimodal\nunderstanding of surgical environments.\n  Methods: We propose a novel framework for generating 4D audio-visual\nrepresentations of surgical scenes by projecting acoustic localization\ninformation from a phased microphone array onto dynamic point clouds from an\nRGB-D camera. A transformer-based acoustic event detection module identifies\nrelevant temporal segments containing tool-tissue interactions which are\nspatially localized in the audio-visual scene representation. The system was\nexperimentally evaluated in a realistic operating room setup during simulated\nsurgical procedures performed by experts.\n  Results: The proposed method successfully localizes surgical acoustic events\nin 3D space and associates them with visual scene elements. Experimental\nevaluation demonstrates accurate spatial sound localization and robust fusion\nof multimodal data, providing a comprehensive, dynamic representation of\nsurgical activity.\n  Conclusion: This work introduces the first approach for spatial sound\nlocalization in dynamic surgical scenes, marking a significant advancement\ntoward multimodal surgical scene representations. By integrating acoustic and\nvisual data, the proposed framework enables richer contextual understanding and\nprovides a foundation for future intelligent and autonomous surgical systems."}
{"id": "2510.24017", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.24017", "abs": "https://arxiv.org/abs/2510.24017", "authors": ["Brian Skoglind", "Travis Roberts", "Sourabh Karmakar", "Cameron Turner", "Laine Mears"], "title": "Localized Acoustic-Event Measurement Probe: Connector Confirmation Utilizing Acoustic Signatures", "comment": null, "summary": "Modern consumer products are full of interconnected electrical and electronic\nmodules to fulfill direct and indirect needs. In an automated assembly line\nstill, most of these interconnections are required to be done manually due to\nthe large variety of connector types, connector positions, and the soft,\nflexible nature of their structures. The manual connection points are the\nsource of partial or completely loose connections. Sometimes connections are\nmissed due to the application of unequal mating forces and natural human\nfatigue. Subsequently, these defects can lead to unexpected downtime and\nexpensive rework. For successful connection detection, past approaches such as\nvision verification, Augmented Reality, or circuit parameter-based measurements\nhave shown limited ability to detect the correct connection state. Though most\nconnections emit a specific noise for successful mating, the acoustic-based\nverification system for electrical connection confirmation has not been\nextensively researched. The main discouraging reason for such research is the\ntypically low signal-to-noise ratio (SNR) between the sound of a pair of\nelectrical connector mating and the diverse soundscape of the plant. In this\nstudy, the authors investigated increasing the SNR between the electrical\nconnector mating sound and the plant soundscape to improve connection success\ndetection by employing a physical system for background noise mitigation and\nthe successful met noise signature amplification algorithm. The solution is\nover 75% effective at detecting and classifying connection state. The solution\nhas been constructed without any modification to the existing manual\ninterconnection process."}
{"id": "2510.24372", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.24372", "abs": "https://arxiv.org/abs/2510.24372", "authors": ["Ziyang Zhang", "Yifan Gao", "Xuenan Xu", "Baoxiangli", "Wen Wu", "Chao Zhang"], "title": "Bayesian Speech synthesizers Can Learn from Multiple Teachers", "comment": null, "summary": "Codec-based text-to-speech (TTS) models have recently gained traction for\ntheir efficiency and strong performance in voice cloning. However, codec-based\nTTS faces limitations due to the challenges of pretraining robust speech codecs\nand the quality degradation introduced by quantization errors. Emerging\nevidence suggests that continuous-valued generative models can alleviate these\nissues and serve as a promising alternative. Yet, effectively modelling diverse\nspeech patterns and developing reliable sampling strategies for\ncontinuous-valued autoregressive (AR) TTS remains underexplored. In this work,\nwe propose BELLE, Bayesian evidential learning with language modelling for TTS,\na novel continuous-valued AR framework that directly predicts mel-spectrograms\nfrom textual input. BELLE treats each mel-spectrogram frame as a Gaussian\ndistribution sampled from a learned hyper distribution, enabling principled\nuncertainty estimation, particularly in scenarios with parallel data (i.e., one\ntext-audio prompt paired with multiple speech samples). To obtain such data,\ndiverse speech samples are synthesized using multiple pre-trained TTS models\ngiven the same text-audio prompts, which are distilled into BELLE via Bayesian\nevidential learning. Experimental results indicate that BELLE demonstrates\nhighly competitive performance compared with the current best open-source TTS\nmodels, even though BELLE is trained on a large amount of synthetic data and\nuses only approximately one-tenth of their training data. Audio samples\ngenerated by BELLE are available at https://belletts.github.io/Belle/. The\ncode, checkpoints, and synthetic data will be released after the paper is\naccepted."}
{"id": "2510.24058", "categories": ["eess.SP", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24058", "abs": "https://arxiv.org/abs/2510.24058", "authors": ["Zihan Zhao", "Masood Mortazavi", "Ning Yan"], "title": "PULSE: Privileged Knowledge Transfer from Electrodermal Activity to Low-Cost Sensors for Stress Monitoring", "comment": "Accepted as a finders paper at ML4H 2025", "summary": "Electrodermal activity (EDA), the primary signal for stress detection,\nrequires costly hardware often unavailable in real-world wearables. In this\npaper, we propose PULSE, a framework that utilizes EDA exclusively during\nself-supervised pretraining, while enabling inference without EDA but with more\nreadily available modalities such as ECG, BVP, ACC, and TEMP. Our approach\nseparates encoder outputs into shared and private embeddings. We align shared\nembeddings across modalities and fuse them into a modality-invariant\nrepresentation. The private embeddings carry modality-specific information to\nsupport the reconstruction objective. Pretraining is followed by knowledge\ntransfer where a frozen EDA teacher transfers sympathetic-arousal\nrepresentations into student encoders. On WESAD, our method achieves strong\nstress-detection performance, showing that representations of privileged EDA\ncan be transferred to low-cost sensors to improve accuracy while reducing\nhardware cost."}
{"id": "2510.24497", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.24497", "abs": "https://arxiv.org/abs/2510.24497", "authors": ["Yuanhang Qian", "Kunlong Zhao", "Jilu Jin", "Xueqin Luo", "Gongping Huang", "Jingdong Chen", "Jacob Benesty"], "title": "Online neural fusion of distortionless differential beamformers for robust speech enhancement", "comment": null, "summary": "Fixed beamforming is widely used in practice since it does not depend on the\nestimation of noise statistics and provides relatively stable performance.\nHowever, a single beamformer cannot adapt to varying acoustic conditions, which\nlimits its interference suppression capability. To address this, adaptive\nconvex combination (ACC) algorithms have been introduced, where the outputs of\nmultiple fixed beamformers are linearly combined to improve robustness.\nNevertheless, ACC often fails in highly non-stationary scenarios, such as\nrapidly moving interference, since its adaptive updates cannot reliably track\nrapid changes. To overcome this limitation, we propose a frame-online neural\nfusion framework for multiple distortionless differential beamformers, which\nestimates the combination weights through a neural network. Compared with\nconventional ACC, the proposed method adapts more effectively to dynamic\nacoustic environments, achieving stronger interference suppression while\nmaintaining the distortionless constraint."}
{"id": "2510.24185", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.24185", "abs": "https://arxiv.org/abs/2510.24185", "authors": ["Kwadwo Mensah Obeng Afrane", "Yang Miao", "André B. J. Kokkeler"], "title": "Performance Analysis of Sub-band Full-duplex Cell-free Massive MIMO JCAS Systems", "comment": null, "summary": "In-band Full-duplex joint communication and sensing systems require self\ninterference cancellation as well as decoupling of the mutual interference\nbetween UL communication signals and radar echoes. We present sub-band\nfull-duplex as an alternative duplexing scheme to achieve simultaneous uplink\ncommunication and target parameter estimation in a cell-free massive MIMO\nsystem. Sub-band full-duplex allows uplink and downlink transmissions\nsimultaneously on non-overlapping frequency resources via explicitly defined\nuplink and downlink sub-bands in each timeslot. Thus, we propose a sub-band\nfull-duplex cell-free massive MIMO system with active downlink sensing on\ndownlink sub-bands and uplink communication on uplink sub-band. In the proposed\nsystem, the target illumination signal is transmitted on the downlink (radar)\nsub-band whereas uplink users transmit on the uplink (communication) sub-band.\nBy assuming efficient suppression of inter-sub-band interference between radar\nand communication sub-bands, uplink communication and radar signals can be\nefficiently processed without mutual interference. We show that each AP can\nestimate sensing parameters with high accuracy in SBFD cell-free massive MIMO\nJCAS systems."}
{"id": "2510.24519", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.24519", "abs": "https://arxiv.org/abs/2510.24519", "authors": ["Rinku Sebastian", "Simon O'Keefe", "Martin Trefzer"], "title": "Audio Signal Processing Using Time Domain Mel-Frequency Wavelet Coefficient", "comment": null, "summary": "Extracting features from the speech is the most critical process in speech\nsignal processing. Mel Frequency Cepstral Coefficients (MFCC) are the most\nwidely used features in the majority of the speaker and speech recognition\napplications, as the filtering in this feature is similar to the filtering\ntaking place in the human ear. But the main drawback of this feature is that it\nprovides only the frequency information of the signal but does not provide the\ninformation about at what time which frequency is present. The wavelet\ntransform, with its flexible time-frequency window, provides time and frequency\ninformation of the signal and is an appropriate tool for the analysis of\nnon-stationary signals like speech. On the other hand, because of its uniform\nfrequency scaling, a typical wavelet transform may be less effective in\nanalysing speech signals, have poorer frequency resolution in low frequencies,\nand be less in line with human auditory perception. Hence, it is necessary to\ndevelop a feature that incorporates the merits of both MFCC and wavelet\ntransform. A great deal of studies are trying to combine both these features.\nThe present Wavelet Transform based Mel-scaled feature extraction methods\nrequire more computation when a wavelet transform is applied on top of\nMel-scale filtering, since it adds extra processing steps. Here we are\nproposing a method to extract Mel scale features in time domain combining the\nconcept of wavelet transform, thus reducing the computational burden of\ntime-frequency conversion and the complexity of wavelet extraction. Combining\nour proposed Time domain Mel frequency Wavelet Coefficient(TMFWC) technique\nwith the reservoir computing methodology has significantly improved the\nefficiency of audio signal processing."}
{"id": "2510.24193", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.24193", "abs": "https://arxiv.org/abs/2510.24193", "authors": ["Tailai Wen", "Da Ke", "Xiang Wang", "Zhitao Huang"], "title": "Dual-Domain Constraints: Designing Covert and Efficient Adversarial Examples for Secure Communication", "comment": null, "summary": "The advancements in Automatic Modulation Classification (AMC) have propelled\nthe development of signal sensing and identification technologies in\nnon-cooperative communication scenarios but also enable eavesdroppers to\neffectively intercept user signals in wireless communication environments. To\nprotect user privacy in communication links, we have optimized the adversarial\nexample generation model and introduced a novel framework for generating\nadversarial perturbations for transmitted signals. This framework implements\ndual-domain constraints in both the time and frequency domains, ensuring that\nthe adversarial perturbation cannot be filtered out. Comparative experiments\nconfirm the superiority of the proposed method and the concealment of the\nadversarial examples it generates."}
{"id": "2510.24693", "categories": ["cs.SD", "cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.24693", "abs": "https://arxiv.org/abs/2510.24693", "authors": ["Zihan Liu", "Zhikang Niu", "Qiuyang Xiao", "Zhisheng Zheng", "Ruoqi Yuan", "Yuhang Zang", "Yuhang Cao", "Xiaoyi Dong", "Jianze Liang", "Xie Chen", "Leilei Sun", "Dahua Lin", "Jiaqi Wang"], "title": "STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence", "comment": "Homepage: https://internlm.github.io/StarBench/", "summary": "Despite rapid progress in Multi-modal Large Language Models and Large\nAudio-Language Models, existing audio benchmarks largely test semantics that\ncan be recovered from text captions, masking deficits in fine-grained\nperceptual reasoning. We formalize audio 4D intelligence that is defined as\nreasoning over sound dynamics in time and 3D space, and introduce STAR-Bench to\nmeasure it. STAR-Bench combines a Foundational Acoustic Perception setting (six\nattributes under absolute and relative regimes) with a Holistic Spatio-Temporal\nReasoning setting that includes segment reordering for continuous and discrete\nprocesses and spatial tasks spanning static localization, multi-source\nrelations, and dynamic trajectories. Our data curation pipeline uses two\nmethods to ensure high-quality samples. For foundational tasks, we use\nprocedurally synthesized and physics-simulated audio. For holistic data, we\nfollow a four-stage process that includes human annotation and final selection\nbased on human performance. Unlike prior benchmarks where caption-only\nanswering reduces accuracy slightly, STAR-Bench induces far larger drops\n(-31.5\\% temporal, -35.2\\% spatial), evidencing its focus on linguistically\nhard-to-describe cues. Evaluating 19 models reveals substantial gaps compared\nwith humans and a capability hierarchy: closed-source models are bottlenecked\nby fine-grained perception, while open-source models lag across perception,\nknowledge, and reasoning. Our STAR-Bench provides critical insights and a clear\npath forward for developing future models with a more robust understanding of\nthe physical world."}
{"id": "2510.24223", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.24223", "abs": "https://arxiv.org/abs/2510.24223", "authors": ["Mahmut Kemal Ercan", "Alireza Pourafzal", "Musa Furkan Keskin", "Sinan Gezici", "Henk Wymeersch"], "title": "Pilot Distortion Design for ToA Obfuscation in Uplink OFDM Communication", "comment": "The document consists of 6 pages and features 4 figures. Submitted to\n  IEEE WCNC 2026", "summary": "We study uplink orthogonal frequency-division multiplexing (OFDM) pilot\ndistortion to deliberately obfuscate time-of-arrival (ToA) estimation at a\nsingle base station while preserving communication performance. We design a\ncomplex per-subcarrier distortion vector that increases sidelobes of the\nmismatched ambiguity function (MAF) relative to its mainlobe, using two\nobjectives: the sidelobe-to-peak level ratio and the integrated sidelobe level.\nThe design is subject to a transmit-power budget and a proximity\n(dissimilarity) constraint around the communication-optimal pilot.\nCommunication impact is quantied by a capacity-motivated lower bound obtained\nfrom the linear minimum mean-squared error error covariance with a mismatched\nchannel estimate. The resulting generalized fractional program is solved with\nDinkelbach's transform and a difference-of-convex update that yields a\nclosed-form Karush-Kuhn-Tucker step. Simulations on a single-input\nsingle-output OFDM link show that the optimized distortions raise MAF sidelobes\nand degrade delay estimation, as validated by a mismatched maximum-likelihood\nToA estimator, while incurring only marginal capacity loss over a broad\nsignal-to-noise ratio range. The method requires no protocol changes or\nartificial path injection and provides a signal-level mechanism to control ToA\nobservability under communication constraints."}
{"id": "2510.24243", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.24243", "abs": "https://arxiv.org/abs/2510.24243", "authors": ["Duc Nguyen Dao", "Haibin Zhang", "Andre B. J. Kokkeler", "Yang Miao"], "title": "Joint Beamforming for Multi-user Multi-target FD ISAC System: A Hybrid GRQ-GA Approach", "comment": "6 pages, 4 figures", "summary": "In this paper, we consider a full-duplex (FD) Integrated Sensing and\nCommunication (ISAC) system, in which the base station (BS) performs downlink\nand uplink communications with multiple users while simultaneously sensing\nmultiple targets. In the scope of this work, we assume a narrowband and static\nscenario, aiming to focus on the beamforming and power allocation strategies.\nWe propose a joint beamforming strategy for designing transmit and receive\nbeamformer vectors at the BS. The optimization problem aims to maximize the\ncommunication sum-rate, which is critical for ensuring high-quality service to\nusers, while also maintaining accurate sensing performance for detection tasks\nand adhering to maximum power constraints for efficient resource usage. The\noptimal receive beamformers are first derived using a closed-form Generalized\nRayleigh Quotient (GRQ) solution, reducing the variables to be optimized. Then,\nthe remaining problem is solved using floating-point Genetic Algorithms (GA).\nThe numerical results show that the proposed GA-based solution demonstrates up\nto a 98% enhancement in sum-rate compared to a baseline half-duplex ISAC system\nand provides better performance than a benchmark algorithm from the literature.\nAdditionally, it offers insights into sensing performance effects on beam\npatterns as well as communicationsensing trade-offs in multi-target scenarios."}
{"id": "2510.24255", "categories": ["eess.SP", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24255", "abs": "https://arxiv.org/abs/2510.24255", "authors": ["Jihao Luo", "Zesong Fei", "Xinyi Wang", "Le Zhao", "Yuanhao Cui", "Guangxu Zhu", "Dusit Niyato"], "title": "Trajectory Design for UAV-Based Low-Altitude Wireless Networks in Unknown Environments: A Digital Twin-Assisted TD3 Approach", "comment": "13 pages, 11 figures", "summary": "Unmanned aerial vehicles (UAVs) are emerging as key enablers for low-altitude\nwireless network (LAWN), particularly when terrestrial networks are\nunavailable. In such scenarios, the environmental topology is typically\nunknown; hence, designing efficient and safe UAV trajectories is essential yet\nchallenging. To address this, we propose a digital twin (DT)-assisted training\nand deployment framework. In this framework, the UAV transmits integrated\nsensing and communication signals to provide communication services to ground\nusers, while simultaneously collecting echoes that are uploaded to the DT\nserver to progressively construct virtual environments (VEs). These VEs\naccelerate model training and are continuously updated with real-time UAV\nsensing data during deployment, supporting decision-making and enhancing flight\nsafety. Based on this framework, we further develop a trajectory design scheme\nthat integrates simulated annealing for efficient user scheduling with the\ntwin-delayed deep deterministic policy gradient algorithm for continuous\ntrajectory design, aiming to minimize mission completion time while ensuring\nobstacle avoidance. Simulation results demonstrate that the proposed approach\nachieves faster convergence, higher flight safety, and shorter mission\ncompletion time compared with baseline methods, providing a robust and\nefficient solution for LAWN deployment in unknown environments."}
{"id": "2510.24287", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24287", "abs": "https://arxiv.org/abs/2510.24287", "authors": ["Richard Koebe", "Noah Saibel", "Juan Miguel Lopez Alcaraz", "Simon Schäfer", "Nils Strodthoff"], "title": "Towards actionable hypotension prediction -- predicting catecholamine therapy initiation in the intensive care unit", "comment": "27 pages, 8 figures, source code under\n  https://github.com/AI4HealthUOL/actionable-hypotension", "summary": "Hypotension in critically ill ICU patients is common and life-threatening.\nEscalation to catecholamine therapy marks a key management step, with both\nundertreatment and overtreatment posing risks. Most machine learning (ML)\nmodels predict hypotension using fixed MAP thresholds or MAP forecasting,\noverlooking the clinical decision behind treatment escalation. Predicting\ncatecholamine initiation, the start of vasoactive or inotropic agent\nadministration offers a more clinically actionable target reflecting real\ndecision-making. Using the MIMIC-III database, we modeled catecholamine\ninitiation as a binary event within a 15-minute prediction window. Input\nfeatures included statistical descriptors from a two-hour sliding MAP context\nwindow, along with demographics, biometrics, comorbidities, and ongoing\ntreatments. An Extreme Gradient Boosting (XGBoost) model was trained and\ninterpreted via SHapley Additive exPlanations (SHAP). The model achieved an\nAUROC of 0.822 (0.813-0.830), outperforming the hypotension baseline (MAP < 65,\nAUROC 0.686 [0.675-0.699]). SHAP analysis highlighted recent MAP values, MAP\ntrends, and ongoing treatments (e.g., sedatives, electrolytes) as dominant\npredictors. Subgroup analysis showed higher performance in males, younger\npatients (<53 years), those with higher BMI (>32), and patients without\ncomorbidities or concurrent medications. Predicting catecholamine initiation\nbased on MAP dynamics, treatment context, and patient characteristics supports\nthe critical decision of when to escalate therapy, shifting focus from\nthreshold-based alarms to actionable decision support. This approach is\nfeasible across a broad ICU cohort under natural event imbalance. Future work\nshould enrich temporal and physiological context, extend label definitions to\ninclude therapy escalation, and benchmark against existing hypotension\nprediction systems."}
{"id": "2510.24350", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.24350", "abs": "https://arxiv.org/abs/2510.24350", "authors": ["Yiming Zhu", "Zhuhong Zhu", "Xiaodong Xu", "Hongwei Hou", "Wenjin Wang", "Rui Ding"], "title": "Achieving Constant-Envelope Waveform in CP-OFDMA Framework", "comment": "This work will be submitted to the IEEE for possible publication", "summary": "OFDM is widely adopted in modern wireless communication systems, but its\npower efficiency is limited by high envelope fluctuations. Although various\nhigh power-efficiency waveforms have been proposed, most are incompatible with\nthe CP-OFDMA framework and remain ineffective in multi-user downlink\ntransmissions. To address this issue, we propose a constant-envelope (CE)\nwaveform design, which enables low-complexity transceiver architectures while\nmaintaining full compatibility with the prevailing CP-OFDMA framework.\nSpecifically, we start from a general CE FDMA signal model and develop a\nCP-OFDMA-compatible waveform implementation structure, followed by the design\nof an optimized CE-constrained pulse-shaping filter to suppress out-of-band\nemissions. To tackle channel estimation challenge under non-flat\nfrequency-domain pilots induced by CE modulation, we optimize the time-domain\nbinary pilot sequence to achieve frequency-domain CE properties, and then\npropose a multi-stage method combining delay-domain denoising with power delay\nprofile estimation to facilitate reduced-dimension LMMSE estimation.\nSubsequently, we design a low-complexity maximum ratio combining-aided LMMSE\nequalizer by exploiting the periodicity and conjugate symmetry of the CE\nreceived signals. To mitigate the downlink peak-to-average power ratio increase\ncaused by FDMA, we further develop a multi-user downlink CE transmission scheme\nincluding multiple access mechanism, downlink control information design, and\ncorresponding system-level implementation, which ensures compatibility with the\nNew Radio standard. Numerical results demonstrate that the proposed scheme\nachieves bit error rate performance close to the ideal case while significantly\nreducing transceiver complexity compared to existing CE waveform solutions."}
{"id": "2510.24400", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.24400", "abs": "https://arxiv.org/abs/2510.24400", "authors": ["Francisco Díaz-Ruiz", "Francisco J. Martín-Vega", "José Antonio Cortés", "Gerardo Gómez", "Mari Carmen Aguayo-Torres"], "title": "Deep Learning-Based CSI Prediction Framework for Channel Aging Mitigation in TDD 5G Systems", "comment": null, "summary": "Time division duplexing (TDD) has become the dominant duplexing mode in 5G\nand beyond due to its ability to exploit channel reciprocity for efficient\ndownlink channel state information (CSI) acquisition. However, channel aging\ncaused by user mobility and processing delays degrades the accuracy of CSI,\nleading to suboptimal link adaptation and loss of performance. To address this\nissue, we propose a learning-based CSI prediction framework that leverages\ntemporal correlations in wireless channels to forecast future signal to\ninterference plus noise ratio (SINR) values. The prediction operates in the\neffective SINR domain, obtained via exponential effective SINR mapping (EESM),\nensuring full compatibility with existing 5G standards without requiring\ncontinuous pilot signaling. Two models are considered: a fully connected deep\nneural network (DNN) and an long short-term memory (LSTM)-based network. The\nsimulation results show that the LSTM predictor achieves an improvement of up\nto 2 dB in normalized mean squared error (NMSE) and a gain of up to 1.2 Mbps\nthroughput over a baseline without prediction under moderate Doppler\nconditions. These results confirm the potential of lightweight AI-based CSI\nprediction to effectively mitigate channel aging and enhance link adaptation in\nTDD 5G systems."}
{"id": "2510.24495", "categories": ["eess.SP", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24495", "abs": "https://arxiv.org/abs/2510.24495", "authors": ["Yuzhi Yang", "Sen Yan", "Weijie Zhou", "Brahim Mefgouda", "Ridong Li", "Zhaoyang Zhang", "Mérouane Debbah"], "title": "Diffusion Models for Wireless Transceivers: From Pilot-Efficient Channel Estimation to AI-Native 6G Receivers", "comment": "Submitted for potential publication in IEEE Wireless Communications", "summary": "With the development of artificial intelligence (AI) techniques, implementing\nAI-based techniques to improve wireless transceivers becomes an emerging\nresearch topic. Within this context, AI-based channel characterization and\nestimation become the focus since these methods have not been solved by\ntraditional methods very well and have become the bottleneck of transceiver\nefficiency in large-scale orthogonal frequency division multiplexing (OFDM)\nsystems. Specifically, by formulating channel estimation as a generative AI\nproblem, generative AI methods such as diffusion models (DMs) can efficiently\ndeal with rough initial estimations and have great potential to cooperate with\ntraditional signal processing methods. This paper focuses on the transceiver\ndesign of OFDM systems based on DMs, provides an illustration of the potential\nof DMs in wireless transceivers, and points out the related research directions\nbrought by DMs. We also provide a proof-of-concept case study of further\nadapting DMs for better wireless receiver performance."}
{"id": "2510.24512", "categories": ["eess.SP", "physics.geo-ph", "stat.AP"], "pdf": "https://arxiv.org/pdf/2510.24512", "abs": "https://arxiv.org/abs/2510.24512", "authors": ["Magnus Heimpel", "Irena Hajnsek", "Othmar Frey"], "title": "Quality Coefficients for Interferometric Phase Linking", "comment": null, "summary": "In multi-temporal InSAR, phase linking refers to the estimation of a\nsingle-reference interferometric phase history from the information contained\nin the coherence matrix of a distributed scatterer. Since the phase information\nin the coherence matrix is typically inconsistent, the extent to which the\nestimated phase history captures it must be assessed to exclude unreliable\npixels from further processing. We introduce three quality criteria in the form\nof coefficients, for threshold-based pixel selection: a coefficient based on\nclosure phase that quantifies the internal consistency of the phase information\nin the coherence matrix; a goodness-of-fit coefficient that quantifies how well\na resulting phase history estimate approximates the phase information according\nto the characteristic optimization model of a given phase linking method; and\nan ambiguity coefficient that compares the goodness of fit of the original\nestimate with that of an orthogonal alternative. We formulate the phase linking\nmethods and these criteria within a unified mathematical framework and discuss\ncomputational and algorithmic aspects. Unlike existing goodness-of-fit\nindicators, the proposed coefficients are normalized to the unit interval with\nexplicit noise-floor correction, improving interpretability across stacks of\ndifferent size. Experiments on TerraSAR-X data over Visp, Switzerland, indicate\nthat the closure phase coefficient effectively pre-screens stable areas, the\ngoodness-of-fit coefficient aligns with and systematically generalizes\nestablished quality indicators, and the ambiguity coefficient flags solutions\nthat fit well but are unstable. Together, the coefficients enable systematic\npixel selection and quality control in the interferometric processing of\ndistributed scatterers."}
{"id": "2510.24569", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.24569", "abs": "https://arxiv.org/abs/2510.24569", "authors": ["Ashkan Jafari Fesharaki", "Yasser Mestrah", "Yi Ma", "Rahim Tafazolli", "Ibrahim Hemadeh", "Mohammad Heggo", "Arman Shojaeifard", "Javier Lorca Hernando", "Alain Mourad"], "title": "Advanced Closed-Loop Method with Limited Feedback for ISAC", "comment": null, "summary": "6G wireless networks are poised to seamlessly integrate communication,\ncomputing, localization, and sensing functionalities, ensuring high reliability\nand trustworthiness. This paper introduces Smart Sensing Feedback (SSF), a\nlimited-feedback framework designed to enhance sensing capabilities while\nmaintaining communication performance. SSF adapts the concept of retransmission\nfrom communication to sensing. Specifically, we focus on downlink (DL) bistatic\nsensing, where the User Equipment (UE) performs measurements from reflected\nsensing signals and provides feedback to the network (NW). In sensing services,\nUE reporting can vary significantly due to dynamic factors such as target\ncharacteristics, environmental conditions, and UE status. Our results\ndemonstrate that SSF significantly improves sensing quality while preserving\ncommunication efficiency. Additionally, it enhances key performance metrics\nsuch as probability of detection, latency, and power consumption. These\nimprovements underscore SSF's ability to deliver robust, low-overhead feedback\nand adaptability to support a wide range of ISAC applications."}
{"id": "2510.24597", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.24597", "abs": "https://arxiv.org/abs/2510.24597", "authors": ["Longpan Wang", "Zhuoran Zhang", "Zhenyuan Li", "Xuetao Gan", "Xudong Bai", "Wen Chen", "Qingqing Wu"], "title": "Multifunctional Wideband Digital Metasurface for Secure Electromagnetic Manipulation in S-Band", "comment": null, "summary": "Digital metasurfaces have attracted significant attention in recent years due\nto their ability to manipulate electromagnetic (EM) waves for secure sensing\nand communication. However, most reported metasurfaces operate at relatively\nhigh frequencies, primarily due to the constraints imposed by the physical\nscale of the dielectric substrate, thus limiting their full-wave system\napplications. In this work, a wideband digital reflective metasurface is\npresented for capable of dynamically controlling EM waves, with multifunctional\napplications in the lower-frequency S-band. The metasurface is composed of\nelectronically reconfigurable meta-atoms with wideband characteristics, and\ndesigned by using trapezoidal and M-shaped patches connected by a pin diode.\nSimulation results show that the proposed digital metasurface could achieve\nwideband 1-bit phase quantization with a stable phase difference within 180\ndegree +/- 25 degree and small reflection loss below 0.6 dB from 2.72 to 3.25\nGHz. To validate the proposed design, a 20x20-unit metasurface array was\ndesigned, simulated and fabricated. By dynamically adjusting the coding\nsequence, the metasurface could enable multi-mode orbital angular momentum\n(OAM) beam generation, dynamic beam scanning, and precise direction finding.\nThese capabilities support secure sensing and secure communications through\nhigh-resolution target detection and anti-jamming beam steering, as well as\nphysical-layer security. The proposed wideband metasurface may serve as an\neffective candidate for enhancing spectral efficiency and security performance\nin radar and wireless systems."}
{"id": "2510.23937", "categories": ["cs.SD", "eess.AS", "eess.SP", "math.OC"], "pdf": "https://arxiv.org/pdf/2510.23937", "abs": "https://arxiv.org/abs/2510.23937", "authors": ["Yuancheng Luo"], "title": "Optimized Loudspeaker Panning for Adaptive Sound-Field Correction and Non-stationary Listening Areas", "comment": null, "summary": "Surround sound systems commonly distribute loudspeakers along standardized\nlayouts for multichannel audio reproduction. However in less controlled\nenvironments, practical layouts vary in loudspeaker quantity, placement, and\nlistening locations / areas. Deviations from standard layouts introduce\nsound-field errors that degrade acoustic timbre, imaging, and clarity of audio\ncontent reproduction. This work introduces both Bayesian loudspeaker\nnormalization and content panning optimization methods for sound-field\ncorrection. Conjugate prior distributions over loudspeaker-listener directions\nupdate estimated layouts for non-stationary listening locations; digital\nfilters adapt loudspeaker acoustic responses to a common reference target at\nthe estimated listening area without acoustic measurements. Frequency-domain\npanning coefficients are then optimized via sensitivity / efficiency objectives\nsubject to spatial, electrical, and acoustic domain constraints; normalized and\npanned loudspeakers form virtual loudspeakers in standardized layouts for\naccurate multichannel reproduction. Experiments investigate robustness of\nBayesian adaptation, and panning optimizations in practical applications."}

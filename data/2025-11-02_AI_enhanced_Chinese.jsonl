{"id": "2510.25936", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.25936", "abs": "https://arxiv.org/abs/2510.25936", "authors": ["Sen Yan", "Tianyu Hu", "Brahim Mefgouda", "Samson Lasaulce", "Merouane Debbah"], "title": "Reading Radio from Camera: Visually-Grounded, Lightweight, and Interpretable RSSI Prediction", "comment": "submitted to an IEEE conference", "summary": "Accurate, real-time wireless signal prediction is essential for\nnext-generation networks. However, existing vision-based frameworks often rely\non computationally intensive models and are also sensitive to environmental\ninterference. To overcome these limitations, we propose a novel, physics-guided\nand light-weighted framework that predicts the received signal strength\nindicator (RSSI) from camera images. By decomposing RSSI into its physically\ninterpretable components, path loss and shadow fading, we significantly reduce\nthe model's learning difficulty and exhibit interpretability. Our approach\nestablishes a new state-of-the-art by demonstrating exceptional robustness to\nenvironmental interference, a critical flaw in prior work. Quantitatively, our\nmodel reduces the prediction root mean squared error (RMSE) by 50.3% under\nconventional conditions and still achieves an 11.5% lower RMSE than the\nprevious benchmark's interference-eliminated results. This superior performance\nis achieved with a remarkably lightweight framework, utilizing a\nMobileNet-based model up to 19 times smaller than competing solutions. The\ncombination of high accuracy, robustness to interference, and computational\nefficiency makes our framework highly suitable for real-time, on-device\ndeployment in edge devices, paving the way for more intelligent and reliable\nwireless communication systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u5f15\u5bfc\u7684\u8f7b\u91cf\u7ea7\u6846\u67b6\uff0c\u901a\u8fc7\u6444\u50cf\u5934\u56fe\u50cf\u9884\u6d4b\u65e0\u7ebf\u4fe1\u53f7\u5f3a\u5ea6\uff0c\u5c06RSSI\u5206\u89e3\u4e3a\u8def\u5f84\u635f\u8017\u548c\u9634\u5f71\u8870\u843d\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u9884\u6d4b\u6846\u67b6\u8ba1\u7b97\u91cf\u5927\u4e14\u5bf9\u73af\u5883\u5e72\u6270\u654f\u611f\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u9c81\u68d2\u3001\u8f7b\u91cf\u7684\u5b9e\u65f6\u65e0\u7ebf\u4fe1\u53f7\u9884\u6d4b\u65b9\u6cd5\u3002", "method": "\u5c06RSSI\u5206\u89e3\u4e3a\u8def\u5f84\u635f\u8017\u548c\u9634\u5f71\u8870\u843d\u4e24\u4e2a\u7269\u7406\u53ef\u89e3\u91ca\u5206\u91cf\uff0c\u4f7f\u7528\u57fa\u4e8eMobileNet\u7684\u8f7b\u91cf\u6a21\u578b\u8fdb\u884c\u9884\u6d4b\u3002", "result": "\u5728\u5e38\u89c4\u6761\u4ef6\u4e0b\u5c06\u9884\u6d4bRMSE\u964d\u4f4e50.3%\uff0c\u5728\u5e72\u6270\u6d88\u9664\u6761\u4ef6\u4e0b\u4ecd\u6bd4\u5148\u524d\u57fa\u51c6\u4f4e11.5%\u7684RMSE\uff0c\u6a21\u578b\u5927\u5c0f\u6bd4\u7ade\u54c1\u5c0f19\u500d\u3002", "conclusion": "\u8be5\u6846\u67b6\u5177\u6709\u9ad8\u7cbe\u5ea6\u3001\u6297\u5e72\u6270\u6027\u5f3a\u548c\u8ba1\u7b97\u6548\u7387\u9ad8\u7684\u7279\u70b9\uff0c\u975e\u5e38\u9002\u5408\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u8fdb\u884c\u5b9e\u65f6\u90e8\u7f72\uff0c\u4e3a\u667a\u80fd\u53ef\u9760\u65e0\u7ebf\u901a\u4fe1\u7cfb\u7edf\u94fa\u5e73\u9053\u8def\u3002"}}
{"id": "2510.26093", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.26093", "abs": "https://arxiv.org/abs/2510.26093", "authors": ["Qianchao Wang", "Chuanzhen Jia", "Yuxuan Ding", "Zhe Li", "Yaping Du"], "title": "Lightweight Ac Arc Fault Diagnosis via Fourier Transform Inspired Multi-frequency Neural Network", "comment": null, "summary": "Lightweight online detection of series arc faults is critically needed in\nresidential and industrial power systems to prevent electrical fires. Existing\ndiagnostic methods struggle to achieve both rapid response and robust accuracy\nunder resource-constrained conditions. To overcome the challenge, this work\nsuggests leveraging a multi-frequency neural network named MFNN, embedding\nprior physical knowledge into the network. Inspired by arcing current curve and\nthe Fourier decomposition analysis, we create an adaptive activation function\nwith super-expressiveness, termed EAS, and a novel network architecture with\nbranch networks to help MFNN extract features with multiple frequencies. In our\nexperiments, eight advanced arc fault diagnosis models across an experimental\ndataset with multiple sampling times and multi-level noise are used to\ndemonstrate the superiority of MFNN. The corresponding experiments show: 1) The\nMFNN outperforms other models in arc fault location, befitting from signal\ndecomposition of branch networks. 2) The noise immunity of MFNN is much better\nthan that of other models, achieving 14.51% over LCNN and 16.3% over BLS in\ntest accuracy when SNR=-9. 3) EAS and the network architecture contribute to\nthe excellent performance of MFNN.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMFNN\u7684\u591a\u9891\u795e\u7ecf\u7f51\u7edc\uff0c\u901a\u8fc7\u5d4c\u5165\u7269\u7406\u77e5\u8bc6\u548c\u521b\u65b0\u7684\u7f51\u7edc\u67b6\u6784\uff0c\u5b9e\u73b0\u4e86\u5728\u8d44\u6e90\u53d7\u9650\u6761\u4ef6\u4e0b\u5bf9\u4e32\u8054\u7535\u5f27\u6545\u969c\u7684\u8f7b\u91cf\u7ea7\u5728\u7ebf\u68c0\u6d4b\uff0c\u5728\u7cbe\u5ea6\u548c\u6297\u566a\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7535\u5f27\u6545\u969c\u8bca\u65ad\u65b9\u6cd5\u5728\u8d44\u6e90\u53d7\u9650\u6761\u4ef6\u4e0b\u96be\u4ee5\u540c\u65f6\u5b9e\u73b0\u5feb\u901f\u54cd\u5e94\u548c\u9c81\u68d2\u7cbe\u5ea6\uff0c\u9700\u8981\u5f00\u53d1\u8f7b\u91cf\u7ea7\u5728\u7ebf\u68c0\u6d4b\u65b9\u6848\u6765\u9884\u9632\u7535\u6c14\u706b\u707e\u3002", "method": "\u57fa\u4e8e\u7535\u5f27\u7535\u6d41\u66f2\u7ebf\u548c\u5085\u91cc\u53f6\u5206\u89e3\u5206\u6790\uff0c\u8bbe\u8ba1\u4e86\u5177\u6709\u8d85\u5f3a\u8868\u8fbe\u80fd\u529b\u7684\u81ea\u9002\u5e94\u6fc0\u6d3b\u51fd\u6570EAS\uff0c\u4ee5\u53ca\u5e26\u6709\u5206\u652f\u7f51\u7edc\u7684\u65b0\u578b\u7f51\u7edc\u67b6\u6784\uff0c\u4f7fMFNN\u80fd\u591f\u63d0\u53d6\u591a\u9891\u7279\u5f81\u3002", "result": "1) MFNN\u5728\u7535\u5f27\u6545\u969c\u5b9a\u4f4d\u65b9\u9762\u4f18\u4e8e\u5176\u4ed6\u6a21\u578b\uff1b2) \u5728SNR=-9\u65f6\uff0cMFNN\u7684\u6297\u566a\u6027\u6bd4LCNN\u548cBLS\u5206\u522b\u9ad8\u51fa14.51%\u548c16.3%\uff1b3) EAS\u548c\u7f51\u7edc\u67b6\u6784\u5bf9MFNN\u7684\u4f18\u5f02\u6027\u80fd\u6709\u91cd\u8981\u8d21\u732e\u3002", "conclusion": "MFNN\u901a\u8fc7\u7ed3\u5408\u7269\u7406\u77e5\u8bc6\u548c\u521b\u65b0\u7684\u7f51\u7edc\u8bbe\u8ba1\uff0c\u5728\u7535\u5f27\u6545\u969c\u68c0\u6d4b\u4e2d\u5b9e\u73b0\u4e86\u4f18\u8d8a\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u6297\u566a\u6027\u548c\u5b9a\u4f4d\u7cbe\u5ea6\u65b9\u9762\u8868\u73b0\u7a81\u51fa\u3002"}}
{"id": "2510.26097", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.26097", "abs": "https://arxiv.org/abs/2510.26097", "authors": ["Usman Akram", "Fan Zhang", "Yang Li", "Haris Vikalo"], "title": "Robust Super-Capacity SRS Channel Inpainting via Diffusion Models", "comment": null, "summary": "Accurate channel state information (CSI) is essential for reliable multiuser\nMIMO operation. In 5G NR, reciprocity-based beamforming via uplink Sounding\nReference Signals (SRS) face resource and coverage constraints, motivating\nsparse non-uniform SRS allocation. Prior masked-autoencoder (MAE) approaches\nimprove coverage but overfit to training masks and degrade under unseen\ndistortions (e.g., additional masking, interference, clipping, non-Gaussian\nnoise). We propose a diffusion-based channel inpainting framework that\nintegrates system-model knowledge at inference via a likelihood-gradient term,\nenabling a single trained model to adapt across mismatched conditions. On\nstandardized CDL channels, the score-based diffusion variant consistently\noutperforms a UNet score-model baseline and the one-step MAE under distribution\nshift, with improvements up to 14 dB NMSE in challenging settings (e.g.,\nLaplace noise, user interference), while retaining competitive accuracy under\nmatched conditions. These results demonstrate that diffusion-guided inpainting\nis a robust and generalizable approach for super-capacity SRS design in 5G NR\nsystems.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u4fe1\u9053\u4fee\u590d\u6846\u67b6\uff0c\u901a\u8fc7\u4f3c\u7136\u68af\u5ea6\u9879\u5728\u63a8\u7406\u65f6\u6574\u5408\u7cfb\u7edf\u6a21\u578b\u77e5\u8bc6\uff0c\u4f7f\u5355\u4e00\u8bad\u7ec3\u6a21\u578b\u80fd\u9002\u5e94\u4e0d\u5339\u914d\u6761\u4ef6\uff0c\u57285G NR\u7cfb\u7edf\u4e2d\u5b9e\u73b0\u9c81\u68d2\u4e14\u53ef\u6cdb\u5316\u7684\u8d85\u5bb9\u91cfSRS\u8bbe\u8ba1\u3002", "motivation": "5G NR\u4e2d\u57fa\u4e8e\u4e92\u6613\u6027\u7684\u6ce2\u675f\u6210\u5f62\u9762\u4e34\u8d44\u6e90\u548c\u8986\u76d6\u9650\u5236\uff0c\u9700\u8981\u7a00\u758f\u975e\u5747\u5300SRS\u5206\u914d\u3002\u73b0\u6709MAE\u65b9\u6cd5\u5b58\u5728\u8fc7\u62df\u5408\u8bad\u7ec3\u63a9\u7801\u95ee\u9898\uff0c\u5728\u672a\u89c1\u5931\u771f\u6761\u4ef6\u4e0b\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u6269\u6563\u7684\u4fe1\u9053\u4fee\u590d\u6846\u67b6\uff0c\u5728\u63a8\u7406\u65f6\u901a\u8fc7\u4f3c\u7136\u68af\u5ea6\u9879\u6574\u5408\u7cfb\u7edf\u6a21\u578b\u77e5\u8bc6\uff0c\u4f7f\u7528\u5206\u6570\u6269\u6563\u53d8\u4f53\u8fdb\u884c\u4fe1\u9053\u4fee\u590d\u3002", "result": "\u5728\u6807\u51c6\u5316CDL\u4fe1\u9053\u4e0a\uff0c\u5206\u6570\u6269\u6563\u53d8\u4f53\u5728\u5206\u5e03\u504f\u79fb\u4e0b\u59cb\u7ec8\u4f18\u4e8eUNet\u5206\u6570\u6a21\u578b\u57fa\u7ebf\u548c\u4e00\u6b65MAE\uff0c\u5728\u6311\u6218\u6027\u8bbe\u7f6e\u4e0bNMSE\u63d0\u5347\u8fbe14 dB\uff0c\u540c\u65f6\u5728\u5339\u914d\u6761\u4ef6\u4e0b\u4fdd\u6301\u7ade\u4e89\u529b\u3002", "conclusion": "\u6269\u6563\u5f15\u5bfc\u7684\u4fee\u590d\u662f5G NR\u7cfb\u7edf\u4e2d\u8d85\u5bb9\u91cfSRS\u8bbe\u8ba1\u7684\u9c81\u68d2\u4e14\u53ef\u6cdb\u5316\u65b9\u6cd5\u3002"}}
{"id": "2510.26150", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.26150", "abs": "https://arxiv.org/abs/2510.26150", "authors": ["Jiaying Di", "Kunlun Wang", "Jing Xu", "Wen Chen", "Dusit Niyato"], "title": "Virtual-Real Collaborated Split Learning via Model Partitioning in IRS-Assisted IoT Networks", "comment": null, "summary": "This paper investigates a novel computation and communication co-design\nframework for large-scale split learning in intelligent reflecting surface\n(IRS)-assisted internet of things (IoT) networks integrated with digital twin\n(DT) technique. The considered system consists of a multi-antenna access point\n(AP), multiple heterogeneous user devices (UDs), and an deployed IRS to enhance\nboth uplink and downlink transmission. The training process of a deep neural\nnetwork is partitioned between devices and the AP, where a DT replica is\nactivated to replace UDs with insufficient local computation capabilities. We\nformulate a delay-optimal split learning problem, which optimizes five key\nvariables: layer partitioning points, DT assignment decisions, IRS phase shift\nmatrix, AP downlink power allocation, and DT frequency adjustment, aiming to\nminimize the overall end-to-end delay under communication and computation. The\nproposed optimization problem is a highly coupled non-convex mixed-integer\nproblem. Therefore, we solve using an alternating optimization approach\ncombining closed-form updates, semidefinite relaxation (SDR), and\nlow-complexity heuristics. Extensive simulations demonstrate that the proposed\nscheme significantly reduces training delay compared to conventional baselines\nand achieves up to 35\\% delay improvement, especially under high UD density and\nstringent power constraints.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cdIRS\u8f85\u52a9IoT\u7f51\u7edc\u4e2d\u7ed3\u5408\u6570\u5b57\u5b6a\u751f\u6280\u672f\u7684\u5927\u89c4\u6a21\u5206\u5272\u5b66\u4e60\u8ba1\u7b97\u901a\u4fe1\u534f\u540c\u8bbe\u8ba1\u6846\u67b6\uff0c\u901a\u8fc7\u4f18\u5316\u5c42\u5206\u5272\u70b9\u3001DT\u5206\u914d\u3001IRS\u76f8\u4f4d\u3001\u529f\u7387\u5206\u914d\u548c\u9891\u7387\u8c03\u6574\u6765\u6700\u5c0f\u5316\u7aef\u5230\u7aef\u5ef6\u8fdf\u3002", "motivation": "\u5728IRS\u8f85\u52a9\u7684IoT\u7f51\u7edc\u4e2d\uff0c\u7528\u6237\u8bbe\u5907\u8ba1\u7b97\u80fd\u529b\u5f02\u6784\u4e14\u6709\u9650\uff0c\u9700\u8981\u89e3\u51b3\u5927\u89c4\u6a21\u5206\u5272\u5b66\u4e60\u4e2d\u7684\u5ef6\u8fdf\u4f18\u5316\u95ee\u9898\uff0c\u6570\u5b57\u5b6a\u751f\u6280\u672f\u53ef\u4ee5\u66ff\u4ee3\u8ba1\u7b97\u80fd\u529b\u4e0d\u8db3\u7684\u8bbe\u5907\u53c2\u4e0e\u8bad\u7ec3\u3002", "method": "\u91c7\u7528\u4ea4\u66ff\u4f18\u5316\u65b9\u6cd5\uff0c\u7ed3\u5408\u95ed\u5f0f\u66f4\u65b0\u3001\u534a\u5b9a\u677e\u5f1b\u548c\u4f4e\u590d\u6742\u5ea6\u542f\u53d1\u5f0f\u7b97\u6cd5\uff0c\u4f18\u5316\u5c42\u5206\u5272\u70b9\u3001DT\u5206\u914d\u51b3\u7b56\u3001IRS\u76f8\u4f4d\u77e9\u9635\u3001AP\u4e0b\u884c\u529f\u7387\u5206\u914d\u548cDT\u9891\u7387\u8c03\u6574\u4e94\u4e2a\u5173\u952e\u53d8\u91cf\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6848\u76f8\u6bd4\u4f20\u7edf\u57fa\u7ebf\u663e\u8457\u964d\u4f4e\u8bad\u7ec3\u5ef6\u8fdf\uff0c\u5728\u9ad8\u7528\u6237\u8bbe\u5907\u5bc6\u5ea6\u548c\u4e25\u683c\u529f\u7387\u7ea6\u675f\u4e0b\u53ef\u5b9e\u73b0\u9ad8\u8fbe35%\u7684\u5ef6\u8fdf\u6539\u8fdb\u3002", "conclusion": "\u8be5\u8ba1\u7b97\u901a\u4fe1\u534f\u540c\u8bbe\u8ba1\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86IRS\u8f85\u52a9IoT\u7f51\u7edc\u4e2d\u5927\u89c4\u6a21\u5206\u5272\u5b66\u4e60\u7684\u5ef6\u8fdf\u4f18\u5316\u95ee\u9898\uff0c\u4e3a\u5f02\u6784\u8bbe\u5907\u73af\u5883\u4e0b\u7684\u5206\u5e03\u5f0f\u5b66\u4e60\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.26096", "categories": ["cs.SD", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.26096", "abs": "https://arxiv.org/abs/2510.26096", "authors": ["Weifei Jin", "Yuxin Cao", "Junjie Su", "Minhui Xue", "Jie Hao", "Ke Xu", "Jin Song Dong", "Derui Wang"], "title": "ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio-Language Models", "comment": "Accepted to NeurIPS 2025", "summary": "Recent advances in Audio-Language Models (ALMs) have significantly improved\nmultimodal understanding capabilities. However, the introduction of the audio\nmodality also brings new and unique vulnerability vectors. Previous studies\nhave proposed jailbreak attacks that specifically target ALMs, revealing that\ndefenses directly transferred from traditional audio adversarial attacks or\ntext-based Large Language Model (LLM) jailbreaks are largely ineffective\nagainst these ALM-specific threats. To address this issue, we propose ALMGuard,\nthe first defense framework tailored to ALMs. Based on the assumption that\nsafety-aligned shortcuts naturally exist in ALMs, we design a method to\nidentify universal Shortcut Activation Perturbations (SAPs) that serve as\ntriggers that activate the safety shortcuts to safeguard ALMs at inference\ntime. To better sift out effective triggers while preserving the model's\nutility on benign tasks, we further propose Mel-Gradient Sparse Mask (M-GSM),\nwhich restricts perturbations to Mel-frequency bins that are sensitive to\njailbreaks but insensitive to speech understanding. Both theoretical analyses\nand empirical results demonstrate the robustness of our method against both\nseen and unseen attacks. Overall, \\MethodName reduces the average success rate\nof advanced ALM-specific jailbreak attacks to 4.6% across four models, while\nmaintaining comparable utility on benign benchmarks, establishing it as the new\nstate of the art. Our code and data are available at\nhttps://github.com/WeifeiJin/ALMGuard.", "AI": {"tldr": "\u63d0\u51fa\u4e86ALMGuard\uff0c\u9996\u4e2a\u9488\u5bf9\u97f3\u9891\u8bed\u8a00\u6a21\u578b\u7684\u9632\u5fa1\u6846\u67b6\uff0c\u901a\u8fc7\u8bc6\u522b\u901a\u7528\u6377\u5f84\u6fc0\u6d3b\u6270\u52a8\u6765\u6fc0\u6d3b\u5b89\u5168\u6377\u5f84\uff0c\u4fdd\u62a4\u6a21\u578b\u514d\u53d7\u8d8a\u72f1\u653b\u51fb\u3002", "motivation": "\u97f3\u9891\u8bed\u8a00\u6a21\u578b\u5f15\u5165\u4e86\u65b0\u7684\u6f0f\u6d1e\u5411\u91cf\uff0c\u73b0\u6709\u4ece\u4f20\u7edf\u97f3\u9891\u5bf9\u6297\u653b\u51fb\u6216\u6587\u672cLLM\u8d8a\u72f1\u8f6c\u79fb\u7684\u9632\u5fa1\u65b9\u6cd5\u5bf9ALM\u7279\u5b9a\u5a01\u80c1\u65e0\u6548\u3002", "method": "\u57fa\u4e8e\u5b89\u5168\u5bf9\u9f50\u6377\u5f84\u5b58\u5728\u7684\u5047\u8bbe\uff0c\u8bbe\u8ba1\u6377\u5f84\u6fc0\u6d3b\u6270\u52a8\u4f5c\u4e3a\u89e6\u53d1\u5668\uff1b\u63d0\u51faMel\u68af\u5ea6\u7a00\u758f\u63a9\u7801\uff0c\u9650\u5236\u6270\u52a8\u5230\u5bf9\u8d8a\u72f1\u654f\u611f\u4f46\u5bf9\u8bed\u97f3\u7406\u89e3\u4e0d\u654f\u611f\u7684Mel\u9891\u6bb5\u3002", "result": "\u5c06\u5148\u8fdbALM\u7279\u5b9a\u8d8a\u72f1\u653b\u51fb\u7684\u5e73\u5747\u6210\u529f\u7387\u964d\u81f34.6%\uff0c\u540c\u65f6\u5728\u826f\u6027\u57fa\u51c6\u4e0a\u4fdd\u6301\u53ef\u6bd4\u6548\u7528\u3002", "conclusion": "ALMGuard\u6210\u4e3a\u65b0\u7684\u6700\u5148\u8fdb\u9632\u5fa1\u65b9\u6cd5\uff0c\u80fd\u6709\u6548\u5bf9\u6297\u5df2\u77e5\u548c\u672a\u77e5\u653b\u51fb\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u5b9e\u7528\u6027\u3002"}}
{"id": "2510.25955", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2510.25955", "abs": "https://arxiv.org/abs/2510.25955", "authors": ["Xiaoyu Yang", "Yifan Yang", "Zengrui Jin", "Ziyun Cui", "Wen Wu", "Baoxiang Li", "Chao Zhang", "Phil Woodland"], "title": "SPEAR: A Unified SSL Framework for Learning Speech and Audio Representations", "comment": null, "summary": "Self-Supervised Learning (SSL) excels at learning generic representations of\nacoustic signals, yet prevailing methods remain domain-specific, tailored to\neither speech or general audio, hindering the development of a unified\nrepresentation model with a comprehensive capability over both domains. To\naddress this, we present SPEAR (SPEech and Audio Representations), the first\nSSL framework to successfully learn unified speech and audio representations\nfrom a mixture of speech and audio data. SPEAR proposes a unified pre-training\nobjective based on masked prediction of fine-grained discrete tokens for both\nspeech and general audio. These tokens are derived from continuous speech and\naudio representations using a Multi-codebook Vector Quantisation (MVQ) method,\nretaining rich acoustic detail essential for modelling both speech and complex\naudio events. SPEAR is applied to pre-train both single-domain and unified\nspeech-and-audio SSL models. Our speech-domain model establishes a new\nstate-of-the-art on the SUPERB benchmark, a speech processing benchmark for SSL\nmodels, matching or surpassing the highly competitive WavLM Large on 12 out of\n15 tasks with the same pre-training corpora and a similar model size.\nCrucially, our unified model learns complementary features and demonstrates\ncomprehensive capabilities across two major benchmarks, SUPERB and HEAR, for\nevaluating audio representations. By further scaling up the model size and\npre-training data, we present a unified model with 600M parameters that excels\nin both domains, establishing it as one of the most powerful and versatile\nopen-source SSL models for auditory understanding. The inference code and\npre-trained models will be made publicly available.", "AI": {"tldr": "SPEAR\u662f\u9996\u4e2a\u6210\u529f\u4ece\u8bed\u97f3\u548c\u97f3\u9891\u6df7\u5408\u6570\u636e\u4e2d\u5b66\u4e60\u7edf\u4e00\u8868\u793a\u7684SSL\u6846\u67b6\uff0c\u901a\u8fc7\u63a9\u7801\u9884\u6d4b\u7ec6\u7c92\u5ea6\u79bb\u6563token\u7684\u7edf\u4e00\u9884\u8bad\u7ec3\u76ee\u6807\uff0c\u5728\u8bed\u97f3\u548c\u97f3\u9891\u9886\u57df\u90fd\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u73b0\u6709\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u90fd\u662f\u9886\u57df\u7279\u5b9a\u7684\uff0c\u5206\u522b\u9488\u5bf9\u8bed\u97f3\u6216\u901a\u7528\u97f3\u9891\uff0c\u963b\u788d\u4e86\u5f00\u53d1\u5177\u6709\u7efc\u5408\u80fd\u529b\u7684\u7edf\u4e00\u8868\u793a\u6a21\u578b\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u63a9\u7801\u9884\u6d4b\u7ec6\u7c92\u5ea6\u79bb\u6563token\u7684\u7edf\u4e00\u9884\u8bad\u7ec3\u76ee\u6807\uff0c\u4f7f\u7528\u591a\u7801\u672c\u5411\u91cf\u91cf\u5316\u65b9\u6cd5\u4ece\u8fde\u7eed\u8868\u793a\u4e2d\u63d0\u53d6token\uff0c\u4fdd\u7559\u4e30\u5bcc\u7684\u58f0\u5b66\u7ec6\u8282\u3002", "result": "\u8bed\u97f3\u9886\u57df\u6a21\u578b\u5728SUPERB\u57fa\u51c6\u4e0a\u8fbe\u5230\u65b0SOTA\uff0c\u572815\u4e2a\u4efb\u52a1\u4e2d12\u4e2a\u5339\u914d\u6216\u8d85\u8d8aWavLM Large\uff1b\u7edf\u4e00\u6a21\u578b\u5728SUPERB\u548cHEAR\u57fa\u51c6\u4e0a\u5c55\u793a\u5168\u9762\u80fd\u529b\uff1b600M\u53c2\u6570\u6a21\u578b\u5728\u4e24\u4e2a\u9886\u57df\u90fd\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "SPEAR\u8bc1\u660e\u4e86\u5b66\u4e60\u7edf\u4e00\u8bed\u97f3\u548c\u97f3\u9891\u8868\u793a\u7684\u53ef\u884c\u6027\uff0c\u5efa\u7acb\u4e86\u6700\u5f3a\u5927\u548c\u901a\u7528\u7684\u5f00\u6e90SSL\u542c\u89c9\u7406\u89e3\u6a21\u578b\u4e4b\u4e00\u3002"}}
{"id": "2510.26166", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.26166", "abs": "https://arxiv.org/abs/2510.26166", "authors": ["Juncong Zhou", "Chao Hu", "Guanlin Wu", "Zixiang Ren", "Han Hu", "Juyong Zhang", "Rui Zhang", "Jie Xu"], "title": "6D Channel Knowledge Map Construction via Bidirectional Wireless Gaussian Splatting", "comment": null, "summary": "This paper investigates the construction of channel knowledge map (CKM) from\nsparse channel measurements. Dif ferent from conventional\ntwo-/three-dimensional (2D/3D) CKM approaches assuming fixed base station\nconfigurations, we present a six-dimensional (6D) CKM framework named\nbidirectional wireless Gaussian splatting (BiWGS), which is capable of mod\neling wireless channels across dynamic transmitter (Tx) and receiver (Rx)\npositions in 3D space. BiWGS uses Gaussian el lipsoids to represent virtual\nscatterer clusters and environmental obstacles in the wireless environment. By\nproperly learning the bidirectional scattering patterns and complex attenuation\nprofiles based on channel measurements, these ellipsoids inherently cap ture\nthe electromagnetic transmission characteristics of wireless environments,\nthereby accurately modeling signal transmission under varying transceiver\nconfigurations. Experiment results show that BiWGS significantly outperforms\nclassic multi-layer perception (MLP) for the construction of 6D channel power\ngain map with varying Tx-Rx positions, and achieves spatial spectrum prediction\naccuracy comparable to the state-of-the art wireless radiation field Gaussian\nsplatting (WRF-GS) for 3D CKM construction. This validates the capability of\nthe proposed BiWGS in accomplishing dimensional expansion of 6D CKM\nconstruction, without compromising fidelity.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3a\u53cc\u5411\u65e0\u7ebf\u9ad8\u65af\u6cfc\u6e85(BiWGS)\u7684\u516d\u7ef4\u4fe1\u9053\u77e5\u8bc6\u56fe\u6846\u67b6\uff0c\u80fd\u591f\u5efa\u6a21\u52a8\u6001\u53d1\u5c04\u5668\u548c\u63a5\u6536\u5668\u57283D\u7a7a\u95f4\u4e2d\u7684\u65e0\u7ebf\u4fe1\u9053\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u7684\u591a\u5c42\u611f\u77e5\u673a\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf2D/3D\u4fe1\u9053\u77e5\u8bc6\u56fe\u65b9\u6cd5\u5047\u8bbe\u57fa\u7ad9\u914d\u7f6e\u56fa\u5b9a\uff0c\u65e0\u6cd5\u9002\u5e94\u52a8\u6001\u53d8\u5316\u7684\u53d1\u5c04\u5668\u548c\u63a5\u6536\u5668\u4f4d\u7f6e\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u5efa\u6a216D\u7a7a\u95f4\u4fe1\u9053\u7279\u6027\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u9ad8\u65af\u692d\u7403\u4f53\u8868\u793a\u65e0\u7ebf\u73af\u5883\u4e2d\u7684\u865a\u62df\u6563\u5c04\u7c07\u548c\u73af\u5883\u969c\u788d\u7269\uff0c\u901a\u8fc7\u5b66\u4e60\u53cc\u5411\u6563\u5c04\u6a21\u5f0f\u548c\u590d\u6742\u8870\u51cf\u7279\u6027\u6765\u6355\u83b7\u7535\u78c1\u4f20\u8f93\u7279\u5f81\u3002", "result": "BiWGS\u57286D\u4fe1\u9053\u529f\u7387\u589e\u76ca\u56fe\u6784\u5efa\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u7ecf\u5178\u591a\u5c42\u611f\u77e5\u673a\uff0c\u57283D\u4fe1\u9053\u77e5\u8bc6\u56fe\u6784\u5efa\u65b9\u9762\u8fbe\u5230\u4e0e\u6700\u5148\u8fdb\u7684\u65e0\u7ebf\u8f90\u5c04\u573a\u9ad8\u65af\u6cfc\u6e85\u65b9\u6cd5\u76f8\u5f53\u7684\u7cbe\u5ea6\u3002", "conclusion": "BiWGS\u80fd\u591f\u5728\u4e0d\u727a\u7272\u4fdd\u771f\u5ea6\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b06D\u4fe1\u9053\u77e5\u8bc6\u56fe\u7684\u7ef4\u5ea6\u6269\u5c55\uff0c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u5728\u52a8\u6001\u6536\u53d1\u5668\u914d\u7f6e\u4e0b\u51c6\u786e\u5efa\u6a21\u4fe1\u53f7\u4f20\u8f93\u7684\u80fd\u529b\u3002"}}
{"id": "2510.26190", "categories": ["cs.SD", "cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.26190", "abs": "https://arxiv.org/abs/2510.26190", "authors": ["Hitomi Jin Ling Tee", "Chaoren Wang", "Zijie Zhang", "Zhizheng Wu"], "title": "SP-MCQA: Evaluating Intelligibility of TTS Beyond the Word Level", "comment": null, "summary": "The evaluation of intelligibility for TTS has reached a bottleneck, as\nexisting assessments heavily rely on word-by-word accuracy metrics such as WER,\nwhich fail to capture the complexity of real-world speech or reflect human\ncomprehension needs. To address this, we propose Spoken-Passage Multiple-Choice\nQuestion Answering, a novel subjective approach evaluating the accuracy of key\ninformation in synthesized speech, and release SP-MCQA-Eval, an 8.76-hour\nnews-style benchmark dataset for SP-MCQA evaluation. Our experiments reveal\nthat low WER does not necessarily guarantee high key-information accuracy,\nexposing a gap between traditional metrics and practical intelligibility.\nSP-MCQA shows that even state-of-the-art (SOTA) models still lack robust text\nnormalization and phonetic accuracy. This work underscores the urgent need for\nhigh-level, more life-like evaluation criteria now that many systems already\nexcel at WER yet may fall short on real-world intelligibility.", "AI": {"tldr": "\u63d0\u51faSP-MCQA\u8bc4\u4f30\u65b9\u6cd5\uff0c\u901a\u8fc7\u53e3\u8bed\u6bb5\u843d\u591a\u9009\u9898\u95ee\u7b54\u6765\u8bc4\u4f30\u5408\u6210\u8bed\u97f3\u4e2d\u5173\u952e\u4fe1\u606f\u7684\u51c6\u786e\u6027\uff0c\u53d1\u73b0\u4f4eWER\u5e76\u4e0d\u4fdd\u8bc1\u9ad8\u4fe1\u606f\u51c6\u786e\u5ea6\uff0c\u66b4\u9732\u4f20\u7edf\u6307\u6807\u4e0e\u5b9e\u9645\u53ef\u61c2\u5ea6\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "motivation": "\u73b0\u6709TTS\u53ef\u61c2\u5ea6\u8bc4\u4f30\u8fc7\u5ea6\u4f9d\u8d56\u8bcd\u7ea7\u51c6\u786e\u5ea6\u6307\u6807\u5982WER\uff0c\u65e0\u6cd5\u6355\u6349\u771f\u5b9e\u8bed\u97f3\u7684\u590d\u6742\u6027\u6216\u53cd\u6620\u4eba\u7c7b\u7406\u89e3\u9700\u6c42\uff0c\u5b58\u5728\u8bc4\u4f30\u74f6\u9888\u3002", "method": "\u63d0\u51fa\u53e3\u8bed\u6bb5\u843d\u591a\u9009\u9898\u95ee\u7b54(SP-MCQA)\u4e3b\u89c2\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5e76\u53d1\u5e038.76\u5c0f\u65f6\u65b0\u95fb\u98ce\u683c\u7684SP-MCQA-Eval\u57fa\u51c6\u6570\u636e\u96c6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u4f4eWER\u4e0d\u4e00\u5b9a\u4fdd\u8bc1\u9ad8\u5173\u952e\u4fe1\u606f\u51c6\u786e\u5ea6\uff0c\u5373\u4f7fSOTA\u6a21\u578b\u5728\u6587\u672c\u5f52\u4e00\u5316\u548c\u8bed\u97f3\u51c6\u786e\u6027\u65b9\u9762\u4ecd\u6709\u4e0d\u8db3\u3002", "conclusion": "\u5f53\u524d\u8bb8\u591a\u7cfb\u7edf\u5728WER\u4e0a\u8868\u73b0\u51fa\u8272\u4f46\u5728\u5b9e\u9645\u53ef\u61c2\u5ea6\u4e0a\u4ecd\u6709\u6b20\u7f3a\uff0c\u8feb\u5207\u9700\u8981\u66f4\u8d34\u8fd1\u73b0\u5b9e\u751f\u6d3b\u7684\u9ad8\u5c42\u6b21\u8bc4\u4f30\u6807\u51c6\u3002"}}
{"id": "2510.26245", "categories": ["eess.SP", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.26245", "abs": "https://arxiv.org/abs/2510.26245", "authors": ["Juyeop Kim", "Hyejin Shin", "Sohee Kim", "Ilmu Byun"], "title": "Design of Orthogonal Phase of Arrival Positioning Scheme Based on 5G PRS and Optimization of TOA Performance", "comment": null, "summary": "This study analyzes the performance of positioning techniques based on\nconfiguration changes of 5G New Radio signals. In 5G networks, a terminal\nposition is determined from the Time of Arrival of Positioning Reference\nSignals transmitted by base stations. We propose an algorithm that improves TOA\naccuracy under low sampling rate constraints and implement 5G PRS for\npositioning in a software defined modem. We also examine how flexible time\nfrequency resource allocation of PRS affects TOA estimation accuracy and\ndiscuss optimal PRS configurations for a given signal environment.", "AI": {"tldr": "\u5206\u6790\u57fa\u4e8e5G\u65b0\u65e0\u7ebf\u7535\u4fe1\u53f7\u914d\u7f6e\u53d8\u5316\u7684\u5b9a\u4f4d\u6280\u672f\u6027\u80fd\uff0c\u63d0\u51fa\u5728\u4f4e\u91c7\u6837\u7387\u7ea6\u675f\u4e0b\u63d0\u9ad8\u5230\u8fbe\u65f6\u95f4\u7cbe\u5ea6\u7684\u7b97\u6cd5\uff0c\u5e76\u7814\u7a76\u7075\u6d3b\u65f6\u9891\u8d44\u6e90\u5206\u914d\u5bf9TOA\u4f30\u8ba1\u7cbe\u5ea6\u7684\u5f71\u54cd\u3002", "motivation": "\u57285G\u7f51\u7edc\u4e2d\uff0c\u7ec8\u7aef\u4f4d\u7f6e\u901a\u8fc7\u57fa\u7ad9\u53d1\u9001\u7684\u5b9a\u4f4d\u53c2\u8003\u4fe1\u53f7\u7684\u5230\u8fbe\u65f6\u95f4\u6765\u786e\u5b9a\uff0c\u9700\u8981\u63d0\u9ad8TOA\u7cbe\u5ea6\u5e76\u4f18\u5316PRS\u914d\u7f6e\u4ee5\u9002\u5e94\u4e0d\u540c\u4fe1\u53f7\u73af\u5883\u3002", "method": "\u63d0\u51fa\u6539\u8fdbTOA\u7cbe\u5ea6\u7684\u7b97\u6cd5\uff0c\u5728\u8f6f\u4ef6\u5b9a\u4e49\u8c03\u5236\u89e3\u8c03\u5668\u4e2d\u5b9e\u73b05G PRS\u5b9a\u4f4d\uff0c\u7814\u7a76PRS\u7684\u7075\u6d3b\u65f6\u9891\u8d44\u6e90\u5206\u914d\u5bf9TOA\u4f30\u8ba1\u7cbe\u5ea6\u7684\u5f71\u54cd\u3002", "result": "\u5f00\u53d1\u4e86\u5728\u4f4e\u91c7\u6837\u7387\u7ea6\u675f\u4e0b\u63d0\u9ad8TOA\u7cbe\u5ea6\u7684\u7b97\u6cd5\uff0c\u5b9e\u73b0\u4e865G PRS\u5b9a\u4f4d\u7cfb\u7edf\uff0c\u5206\u6790\u4e86\u4e0d\u540cPRS\u914d\u7f6e\u5bf9\u5b9a\u4f4d\u7cbe\u5ea6\u7684\u5f71\u54cd\u3002", "conclusion": "\u786e\u5b9a\u4e86\u7ed9\u5b9a\u4fe1\u53f7\u73af\u5883\u4e0b\u7684\u6700\u4f18PRS\u914d\u7f6e\uff0c\u4e3a5G\u7f51\u7edc\u4e2d\u7684\u7cbe\u786e\u5b9a\u4f4d\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u6280\u672f\u65b9\u6848\u3002"}}
{"id": "2510.26299", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.26299", "abs": "https://arxiv.org/abs/2510.26299", "authors": ["Sofiene Kammoun", "Xavier Alameda-Pineda", "Simon Leglaive"], "title": "Modeling strategies for speech enhancement in the latent space of a neural audio codec", "comment": null, "summary": "Neural audio codecs (NACs) provide compact latent speech representations in\nthe form of sequences of continuous vectors or discrete tokens. In this work,\nwe investigate how these two types of speech representations compare when used\nas training targets for supervised speech enhancement. We consider both\nautoregressive and non-autoregressive speech enhancement models based on the\nConformer architecture, as well as a simple baseline where the NAC encoder is\nsimply fine-tuned for speech enhancement. Our experiments reveal three key\nfindings: predicting continuous latent representations consistently outperforms\ndiscrete token prediction; autoregressive models achieve higher quality but at\nthe expense of intelligibility and efficiency, making non-autoregressive models\nmore attractive in practice; and encoder fine-tuning yields the strongest\nenhancement metrics overall, though at the cost of degraded codec\nreconstruction. The code and audio samples are available online.", "AI": {"tldr": "\u6bd4\u8f83\u795e\u7ecf\u97f3\u9891\u7f16\u89e3\u7801\u5668\u4e2d\u8fde\u7eed\u5411\u91cf\u548c\u79bb\u6563token\u4f5c\u4e3a\u8bed\u97f3\u589e\u5f3a\u8bad\u7ec3\u76ee\u6807\u7684\u6548\u679c\uff0c\u53d1\u73b0\u8fde\u7eed\u5411\u91cf\u9884\u6d4b\u4f18\u4e8e\u79bb\u6563token\uff0c\u975e\u81ea\u56de\u5f52\u6a21\u578b\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u66f4\u4f18\uff0c\u7f16\u7801\u5668\u5fae\u8c03\u80fd\u83b7\u5f97\u6700\u5f3a\u589e\u5f3a\u6548\u679c\u4f46\u4f1a\u964d\u4f4e\u7f16\u89e3\u7801\u91cd\u5efa\u8d28\u91cf\u3002", "motivation": "\u7814\u7a76\u795e\u7ecf\u97f3\u9891\u7f16\u89e3\u7801\u5668(NACs)\u4e2d\u8fde\u7eed\u5411\u91cf\u548c\u79bb\u6563token\u4e24\u79cd\u8bed\u97f3\u8868\u793a\u5728\u76d1\u7763\u8bed\u97f3\u589e\u5f3a\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u5dee\u5f02\uff0c\u4e3a\u9009\u62e9\u5408\u9002\u7684\u8bad\u7ec3\u76ee\u6807\u63d0\u4f9b\u6307\u5bfc\u3002", "method": "\u4f7f\u7528\u57fa\u4e8eConformer\u67b6\u6784\u7684\u81ea\u56de\u5f52\u548c\u975e\u81ea\u56de\u5f52\u8bed\u97f3\u589e\u5f3a\u6a21\u578b\uff0c\u4ee5\u53ca\u7b80\u5355\u7684\u7f16\u7801\u5668\u5fae\u8c03\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5bf9\u6bd4\u8fde\u7eed\u6f5c\u5728\u8868\u793a\u548c\u79bb\u6563token\u9884\u6d4b\u7684\u6548\u679c\u3002", "result": "\u8fde\u7eed\u6f5c\u5728\u8868\u793a\u9884\u6d4b\u59cb\u7ec8\u4f18\u4e8e\u79bb\u6563token\u9884\u6d4b\uff1b\u81ea\u56de\u5f52\u6a21\u578b\u8d28\u91cf\u66f4\u9ad8\u4f46\u727a\u7272\u4e86\u53ef\u61c2\u5ea6\u548c\u6548\u7387\uff1b\u7f16\u7801\u5668\u5fae\u8c03\u83b7\u5f97\u6700\u5f3a\u7684\u589e\u5f3a\u6307\u6807\u4f46\u964d\u4f4e\u4e86\u7f16\u89e3\u7801\u91cd\u5efa\u8d28\u91cf\u3002", "conclusion": "\u5728\u8bed\u97f3\u589e\u5f3a\u4efb\u52a1\u4e2d\uff0c\u8fde\u7eed\u5411\u91cf\u4f5c\u4e3a\u8bad\u7ec3\u76ee\u6807\u4f18\u4e8e\u79bb\u6563token\uff0c\u975e\u81ea\u56de\u5f52\u6a21\u578b\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u66f4\u53ef\u53d6\uff0c\u7f16\u7801\u5668\u5fae\u8c03\u662f\u6709\u6548\u7684\u589e\u5f3a\u7b56\u7565\u4f46\u9700\u6743\u8861\u7f16\u89e3\u7801\u6027\u80fd\u635f\u5931\u3002"}}
{"id": "2510.26262", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.26262", "abs": "https://arxiv.org/abs/2510.26262", "authors": ["Francesco Verde", "Donatella Darsena", "Marco Di Renzo", "Vincenzo Galdi"], "title": "Optimal transmit field distribution for partially obstructed continuous radiating surfaces in near-field communication systems", "comment": "5 pages, 5 figures, conference", "summary": "This paper deals with the optimal synthesis of aperture fields for\n(radiating) near-field communications in obstructed environments. A physically\nconsistent model based on knife-edge diffraction is used to formulate the\nproblem as a maximization in Hilbert space. The optimal solution is obtained as\na matched filter that ``matches\" the shape of a diffraction-induced kernel,\nthus linking wave propagation with signal processing methods. The framework\nsupports hardware implementation using continuous apertures such as\nmetasurfaces or lens antennas. This approach bridges physically grounded\nmodeling, signal processing, and hardware design for efficient energy focusing\nin near-field obstructed channels.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5200\u8fb9\u884d\u5c04\u6a21\u578b\u7684\u8fd1\u573a\u901a\u4fe1\u5b54\u5f84\u573a\u4f18\u5316\u5408\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u6700\u5927\u5316\u95ee\u9898\u83b7\u5f97\u5339\u914d\u6ee4\u6ce2\u5668\u5f62\u5f0f\u7684\u89e3\uff0c\u8fde\u63a5\u4e86\u6ce2\u4f20\u64ad\u4e0e\u4fe1\u53f7\u5904\u7406\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u5728\u906e\u6321\u73af\u5883\u4e2d\u8fd1\u573a\u901a\u4fe1\u7684\u80fd\u91cf\u805a\u7126\u95ee\u9898\uff0c\u9700\u8981\u7269\u7406\u4e00\u81f4\u7684\u5efa\u6a21\u65b9\u6cd5\u6765\u4f18\u5316\u5b54\u5f84\u573a\u5408\u6210\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u5200\u8fb9\u884d\u5c04\u7684\u7269\u7406\u6a21\u578b\uff0c\u5c06\u95ee\u9898\u8868\u8ff0\u4e3a\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u4e2d\u7684\u6700\u5927\u5316\u95ee\u9898\uff0c\u83b7\u5f97\u5339\u914d\u884d\u5c04\u6838\u7684\u6ee4\u6ce2\u5668\u89e3\u3002", "result": "\u5f97\u5230\u4e86\u80fd\u591f\u5339\u914d\u884d\u5c04\u6838\u5f62\u72b6\u7684\u6700\u4f18\u89e3\uff0c\u652f\u6301\u4f7f\u7528\u8d85\u8868\u9762\u6216\u900f\u955c\u5929\u7ebf\u7b49\u8fde\u7eed\u5b54\u5f84\u7684\u786c\u4ef6\u5b9e\u73b0\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5c06\u7269\u7406\u5efa\u6a21\u3001\u4fe1\u53f7\u5904\u7406\u548c\u786c\u4ef6\u8bbe\u8ba1\u76f8\u7ed3\u5408\uff0c\u4e3a\u906e\u6321\u8fd1\u573a\u4fe1\u9053\u4e2d\u7684\u9ad8\u6548\u80fd\u91cf\u805a\u7126\u63d0\u4f9b\u4e86\u7edf\u4e00\u6846\u67b6\u3002"}}
{"id": "2510.26372", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2510.26372", "abs": "https://arxiv.org/abs/2510.26372", "authors": ["Chengwei Liu", "Haoyin Yan", "Shaofei Xue", "Xiaotao Liang", "Yinghao Liu", "Zheng Xue", "Gang Song", "Boyang Zhou"], "title": "UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens", "comment": "21 pages, 3 figures", "summary": "Generative modeling has recently achieved remarkable success across text,\nimage, and audio domains, demonstrating powerful capabilities for unified\nrepresentation learning. However, audio generation models still face challenges\nin terms of audio quality and generalization ability across tasks. This\nfragmentation results in redundant development efforts, inconsistent\nperformance, and limited extensibility. To address these issues, we propose\n\\textbf{UniTok-Audio}, a scalable and extensible framework for unified audio\ngeneration tasks. Specifically, 1) UniTok-Audio extracts continuous feature of\nconditions to generates discrete tokens of target audio in an autoregressive\nmanner; 2) a special task identifier token unifies different learning patterns\nof multiple tasks in a single framework; 3) a dual-stream audio codec involving\nacoustic and semantic branch is developed for high-fidelity waveform\nreconstruction. Experimental results demonstrate that UniTok-Audio achieves\ncompetitive performance in comparation with state-of-the-art task-specific or\nmulti-task systems across five time-aligned tasks: speech restoration, target\nspeaker extraction, speech separation, voice conversion, and language-queried\naudio source separation. To foster future research, we will open-source our\ncodebase. The demo page of our work can be found here:\nhttps://alibaba.github.io/unified-audio.", "AI": {"tldr": "UniTok-Audio\u662f\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u7edf\u4e00\u97f3\u9891\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u8fde\u7eed\u7279\u5f81\u63d0\u53d6\u3001\u4efb\u52a1\u6807\u8bc6\u7b26\u548c\u53cc\u6d41\u97f3\u9891\u7f16\u89e3\u7801\u5668\uff0c\u5728\u591a\u4e2a\u65f6\u95f4\u5bf9\u9f50\u4efb\u52a1\u4e2d\u5b9e\u73b0\u7ade\u4e89\u6027\u6027\u80fd\u3002", "motivation": "\u97f3\u9891\u751f\u6210\u6a21\u578b\u5728\u97f3\u9891\u8d28\u91cf\u548c\u8de8\u4efb\u52a1\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u4ecd\u9762\u4e34\u6311\u6218\uff0c\u5bfc\u81f4\u5f00\u53d1\u5197\u4f59\u3001\u6027\u80fd\u4e0d\u4e00\u81f4\u548c\u6269\u5c55\u6027\u6709\u9650\u3002", "method": "1) \u63d0\u53d6\u6761\u4ef6\u8fde\u7eed\u7279\u5f81\u4ee5\u81ea\u56de\u5f52\u65b9\u5f0f\u751f\u6210\u76ee\u6807\u97f3\u9891\u79bb\u6563\u4ee4\u724c\uff1b2) \u4f7f\u7528\u7279\u6b8a\u4efb\u52a1\u6807\u8bc6\u7b26\u7edf\u4e00\u4e0d\u540c\u5b66\u4e60\u6a21\u5f0f\uff1b3) \u5f00\u53d1\u5305\u542b\u58f0\u5b66\u548c\u8bed\u4e49\u5206\u652f\u7684\u53cc\u6d41\u97f3\u9891\u7f16\u89e3\u7801\u5668\u3002", "result": "\u5728\u8bed\u97f3\u6062\u590d\u3001\u76ee\u6807\u8bf4\u8bdd\u4eba\u63d0\u53d6\u3001\u8bed\u97f3\u5206\u79bb\u3001\u8bed\u97f3\u8f6c\u6362\u548c\u8bed\u8a00\u67e5\u8be2\u97f3\u9891\u6e90\u5206\u79bb\u4e94\u4e2a\u65f6\u95f4\u5bf9\u9f50\u4efb\u52a1\u4e2d\uff0c\u4e0e\u6700\u5148\u8fdb\u7684\u7279\u5b9a\u4efb\u52a1\u6216\u591a\u4efb\u52a1\u7cfb\u7edf\u76f8\u6bd4\u5177\u6709\u7ade\u4e89\u6027\u6027\u80fd\u3002", "conclusion": "UniTok-Audio\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u97f3\u9891\u751f\u6210\u4e2d\u7684\u788e\u7247\u5316\u95ee\u9898\uff0c\u5e76\u5c06\u5f00\u6e90\u4ee3\u7801\u5e93\u4ee5\u4fc3\u8fdb\u672a\u6765\u7814\u7a76\u3002"}}
{"id": "2510.26340", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.26340", "abs": "https://arxiv.org/abs/2510.26340", "authors": ["Shih-Kai Chou", "Mengran Zhao", "Cheng-Nan Hu", "Kuang-Chung Chou", "Carolina Fortuna", "Jernej Hribar"], "title": "SABER: Symbolic Regression-based Angle of Arrival and Beam Pattern Estimator", "comment": "12 pages, 11 figures", "summary": "Accurate Angle-of-arrival (AoA) estimation is essential for next-generation\nwireless communication systems to enable reliable beamforming, high-precision\nlocalization, and integrated sensing. Unfortunately, classical high-resolution\ntechniques require multi-element arrays and extensive snapshot collection,\nwhile generic Machine Learning (ML) approaches often yield black-box models\nthat lack physical interpretability. To address these limitations, we propose a\nSymbolic Regression (SR)-based ML framework. Namely, Symbolic Regression-based\nAngle of Arrival and Beam Pattern Estimator (SABER), a constrained\nsymbolic-regression framework that automatically discovers closed-form beam\npattern and AoA models from path loss measurements with interpretability. SABER\nachieves high accuracy while bridging the gap between opaque ML methods and\ninterpretable physics-driven estimators. First, we validate our approach in a\ncontrolled free-space anechoic chamber, showing that both direct inversion of\nthe known $\\cos^n$ beam and a low-order polynomial surrogate achieve sub-0.5\ndegree Mean Absolute Error (MAE). A purely unconstrained SR method can further\nreduce the error of the predicted angles, but produces complex formulas that\nlack physical insight. Then, we implement the same SR-learned inversions in a\nreal-world, Reconfigurable Intelligent Surface (RIS)-aided indoor testbed.\nSABER and unconstrained SR models accurately recover the true AoA with\nnear-zero error. Finally, we benchmark SABER against the Cram\\'er-Rao Lower\nBounds (CRLBs). Our results demonstrate that SABER is an interpretable and\naccurate alternative to state-of-the-art and black-box ML-based methods for AoA\nestimation.", "AI": {"tldr": "\u63d0\u51faSABER\u6846\u67b6\uff0c\u4f7f\u7528\u7b26\u53f7\u56de\u5f52\u81ea\u52a8\u53d1\u73b0\u53ef\u89e3\u91ca\u7684\u6ce2\u675f\u6a21\u5f0f\u548cAoA\u95ed\u5f0f\u6a21\u578b\uff0c\u5728\u771f\u5b9eRIS\u8f85\u52a9\u5ba4\u5185\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u8fd1\u96f6\u8bef\u5dee\uff0c\u662f\u53ef\u89e3\u91ca\u4e14\u51c6\u786e\u7684AoA\u4f30\u8ba1\u66ff\u4ee3\u65b9\u6848\u3002", "motivation": "\u4f20\u7edf\u9ad8\u5206\u8fa8\u7387AoA\u4f30\u8ba1\u6280\u672f\u9700\u8981\u591a\u5929\u7ebf\u9635\u5217\u548c\u5927\u91cf\u5feb\u7167\u91c7\u96c6\uff0c\u800c\u901a\u7528\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u7f3a\u4e4f\u7269\u7406\u53ef\u89e3\u91ca\u6027\uff0c\u9700\u8981\u89e3\u51b3\u8fd9\u4e9b\u5c40\u9650\u6027\u3002", "method": "\u91c7\u7528\u7ea6\u675f\u7b26\u53f7\u56de\u5f52\u6846\u67b6\uff0c\u4ece\u8def\u5f84\u635f\u8017\u6d4b\u91cf\u4e2d\u81ea\u52a8\u53d1\u73b0\u95ed\u5f0f\u6ce2\u675f\u6a21\u5f0f\u548cAoA\u6a21\u578b\uff0c\u4fdd\u6301\u7269\u7406\u53ef\u89e3\u91ca\u6027\u3002", "result": "\u5728\u81ea\u7531\u7a7a\u95f4\u6d88\u58f0\u5ba4\u4e2d\u5b9e\u73b0\u4e9a0.5\u5ea6MAE\uff0c\u5728RIS\u8f85\u52a9\u5ba4\u5185\u6d4b\u8bd5\u4e2d\u51c6\u786e\u6062\u590d\u771f\u5b9eAoA\u4e14\u8fd1\u96f6\u8bef\u5dee\uff0c\u6027\u80fd\u63a5\u8fd1\u514b\u62c9\u7f8e\u7f57\u4e0b\u754c\u3002", "conclusion": "SABER\u662f\u9ed1\u76d2\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u7684\u53ef\u89e3\u91ca\u4e14\u51c6\u786e\u66ff\u4ee3\u65b9\u6848\uff0c\u5728AoA\u4f30\u8ba1\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2510.26532", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.26532", "abs": "https://arxiv.org/abs/2510.26532", "authors": ["Margarita Cabrera-Bean", "Josep Vidal", "Sergio Fernandez-Bertolin", "Albert Roso-Llorach", "Concepcion Violan"], "title": "HMM for short independent sequences: Multiple sequence Baum-Welch application", "comment": "18 pages Affiliation (1) Universitat Politecnica de Catalunya (UPC);\n  (2) IDIAP Jordi Gol", "summary": "In the classical setting, the training of a Hidden Markov Model (HMM)\ntypically relies on a single, sufficiently long observation sequence that can\nbe regarded as representative of the underlying stochastic process. In this\ncontext, the Expectation Maximization (EM) algorithm is applied in its\nspecialized form for HMMs, namely the Baum Welch algorithm, which has been\nextensively employed in applications such as speech recognition. The objective\nof this work is to present pseudocode formulations for both the training and\ndecoding procedures of HMMs in a different scenario, where the available data\nconsist of multiple independent temporal sequences generated by the same model,\neach of relatively short duration, i.e., containing only a limited number of\nsamples. Special emphasis is placed on the relevance of this formulation to\nlongitudinal studies in population health, where datasets are naturally\nstructured as collections of short trajectories across individuals with point\ndata at follow up.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9002\u7528\u4e8e\u591a\u72ec\u7acb\u77ed\u5e8f\u5217\u573a\u666f\u7684\u9690\u9a6c\u5c14\u53ef\u592b\u6a21\u578b\u8bad\u7ec3\u548c\u89e3\u7801\u4f2a\u4ee3\u7801\uff0c\u7279\u522b\u5173\u6ce8\u4eba\u53e3\u5065\u5eb7\u7eb5\u5411\u7814\u7a76\u4e2d\u7684\u5e94\u7528\u3002", "motivation": "\u4f20\u7edfHMM\u8bad\u7ec3\u4f9d\u8d56\u5355\u4e2a\u957f\u89c2\u6d4b\u5e8f\u5217\uff0c\u4f46\u5728\u4eba\u53e3\u5065\u5eb7\u7b49\u7eb5\u5411\u7814\u7a76\u4e2d\uff0c\u6570\u636e\u901a\u5e38\u7531\u591a\u4e2a\u4e2a\u4f53\u7684\u77ed\u8f68\u8ff9\u7ec4\u6210\uff0c\u9700\u8981\u4e13\u95e8\u7684\u5904\u7406\u65b9\u6cd5\u3002", "method": "\u5f00\u53d1\u4e86\u9488\u5bf9\u591a\u72ec\u7acb\u77ed\u5e8f\u5217\u7684HMM\u8bad\u7ec3\u548c\u89e3\u7801\u4f2a\u4ee3\u7801\uff0c\u6269\u5c55\u4e86Baum-Welch\u7b97\u6cd5\u4ee5\u9002\u5e94\u8fd9\u79cd\u6570\u636e\u7ed3\u6784\u3002", "result": "\u63d0\u4f9b\u4e86\u9002\u7528\u4e8e\u591a\u77ed\u5e8f\u5217\u573a\u666f\u7684HMM\u8bad\u7ec3\u548c\u89e3\u7801\u7b97\u6cd5\u6846\u67b6\uff0c\u80fd\u591f\u5904\u7406\u4eba\u53e3\u5065\u5eb7\u7eb5\u5411\u7814\u7a76\u4e2d\u7684\u5178\u578b\u6570\u636e\u7ed3\u6784\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5904\u7406\u591a\u72ec\u7acb\u77ed\u5e8f\u5217\u7684HMM\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u9002\u7528\u4e8e\u4eba\u53e3\u5065\u5eb7\u7b49\u7eb5\u5411\u7814\u7a76\u9886\u57df\u3002"}}
{"id": "2510.26604", "categories": ["eess.SP", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.26604", "abs": "https://arxiv.org/abs/2510.26604", "authors": ["Shahab Moradi Torkashvand", "Arina Kharazi", "Emad Sadeghi", "Seyed Hossein Hesamedin Sadeghi", "Adel Nasiri"], "title": "Statistically Adaptive Differential Protection for AC Microgrids Based on Kullback-Leibler Divergence", "comment": null, "summary": "The proliferation of inverter-based resources challenges traditional\nmicrogrid protection by introducing variable fault currents and complex\ntransients. This paper presents a statistically adaptive differential\nprotection scheme based on Kullback-Leibler divergence, implemented via a\nBartlett-corrected G-statistic computed on logarithm-transformed current\nmagnitudes. The method is a multivariate fault detection engine that employs\nthe Mahalanobis distance to distinguish healthy and faulty states, enabling\nrobust detection even in noisy environments. Detection thresholds are\nstatistically derived from a chi-squared distribution for precise control over\nthe false alarm rate. Upon detection, a lightweight classifier identifies the\nfault type by assessing per-phase G-statistics against dedicated thresholds,\nenhanced by a temporal persistence filter for security. Extensive simulations\non a modified CIGRE 14-bus microgrid show high efficacy: sub-cycle average\ndetection delays, high detection and classification accuracy across operating\nmodes, resilience to high-impedance faults up to 250 Ohms, tolerance to 10 ms\ncommunication delay, and noise levels down to a 20 dB signal-to-noise ratio.\nThese findings demonstrate a reproducible and computationally efficient\nsolution for next-generation AC microgrid protection.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eKullback-Leibler\u6563\u5ea6\u7684\u7edf\u8ba1\u81ea\u9002\u5e94\u5dee\u52a8\u4fdd\u62a4\u65b9\u6848\uff0c\u4f7f\u7528Bartlett\u6821\u6b63G\u7edf\u8ba1\u91cf\u5bf9\u5bf9\u6570\u53d8\u6362\u7535\u6d41\u5e45\u503c\u8fdb\u884c\u8ba1\u7b97\uff0c\u5b9e\u73b0\u5fae\u7535\u7f51\u6545\u969c\u68c0\u6d4b\u548c\u5206\u7c7b\u3002", "motivation": "\u9006\u53d8\u5668\u8d44\u6e90\u7684\u666e\u53ca\u7ed9\u4f20\u7edf\u5fae\u7535\u7f51\u4fdd\u62a4\u5e26\u6765\u6311\u6218\uff0c\u56e0\u4e3a\u5f15\u5165\u4e86\u53ef\u53d8\u7684\u6545\u969c\u7535\u6d41\u548c\u590d\u6742\u77ac\u6001\u8fc7\u7a0b\u3002", "method": "\u91c7\u7528\u591a\u5143\u6545\u969c\u68c0\u6d4b\u5f15\u64ce\uff0c\u4f7f\u7528\u9a6c\u6c0f\u8ddd\u79bb\u533a\u5206\u5065\u5eb7\u4e0e\u6545\u969c\u72b6\u6001\uff0c\u68c0\u6d4b\u9608\u503c\u57fa\u4e8e\u5361\u65b9\u5206\u5e03\u7edf\u8ba1\u63a8\u5bfc\uff0c\u6545\u969c\u7c7b\u578b\u8bc6\u522b\u901a\u8fc7\u8f7b\u91cf\u7ea7\u5206\u7c7b\u5668\u8bc4\u4f30\u6bcf\u76f8G\u7edf\u8ba1\u91cf\uff0c\u5e76\u4f7f\u7528\u65f6\u5e8f\u6301\u4e45\u6027\u6ee4\u6ce2\u5668\u589e\u5f3a\u5b89\u5168\u6027\u3002", "result": "\u5728\u6539\u8fdb\u7684CIGRE 14\u603b\u7ebf\u5fae\u7535\u7f51\u4e0a\u8fdb\u884c\u5e7f\u6cdb\u4eff\u771f\uff0c\u7ed3\u679c\u663e\u793a\uff1a\u4e9a\u5468\u671f\u5e73\u5747\u68c0\u6d4b\u5ef6\u8fdf\uff0c\u9ad8\u68c0\u6d4b\u548c\u5206\u7c7b\u7cbe\u5ea6\uff0c\u5bf9\u9ad8\u8fbe250\u6b27\u59c6\u7684\u9ad8\u963b\u6297\u6545\u969c\u5177\u6709\u5f39\u6027\uff0c\u5bb9\u5fcd10ms\u901a\u4fe1\u5ef6\u8fdf\uff0c\u566a\u58f0\u6c34\u5e73\u4f4e\u81f320dB\u4fe1\u566a\u6bd4\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u4e0b\u4e00\u4ee3\u4ea4\u6d41\u5fae\u7535\u7f51\u4fdd\u62a4\u63d0\u4f9b\u4e86\u53ef\u91cd\u73b0\u4e14\u8ba1\u7b97\u6548\u7387\u9ad8\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.26756", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.26756", "abs": "https://arxiv.org/abs/2510.26756", "authors": ["Soujanya Hazra", "Sanjay Ghosh"], "title": "Graph Guided Modulo Recovery of EEG Signals", "comment": "5 pages, 1 figure, and 2 tables", "summary": "Electroencephalography (EEG) often shows significant variability among\npeople. This fluctuation disrupts reliable acquisition and may result in\ndistortion or clipping. Modulo sampling is now a promising solution to this\nproblem, by folding signals instead of saturating them. Recovery of the\noriginal waveform from folded observations is a highly ill-posed problem. In\nthis work, we propose a method based on a graph neural network, referred to as\nGraphUnwrapNet, for the modulo recovery of EEG signals. Our core idea is to\nrepresent an EEG signal as an organized graph whose channels and temporal\nconnections establish underlying interdependence. One of our key contributions\nis in introducing a pre-estimation guided feature injection module to provide\ncoarse folding indicators that enhance stability during recovery at wrap\nboundaries. This design integrates structural information with folding priors\ninto an integrated framework. We performed comprehensive experiments on the\nSimultaneous Task EEG Workload (STEW) dataset. The results demonstrate\nconsistent enhancements over traditional optimization techniques and\ncompetitive accuracy relative to current deep learning models. Our findings\nemphasize the potential of graph-based methodology for robust modulo EEG\nrecovery.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u795e\u7ecf\u7f51\u7edc\u7684EEG\u4fe1\u53f7\u6a21\u6570\u6062\u590d\u65b9\u6cd5GraphUnwrapNet\uff0c\u901a\u8fc7\u5c06EEG\u4fe1\u53f7\u8868\u793a\u4e3a\u6709\u7ec4\u7ec7\u7684\u56fe\u7ed3\u6784\uff0c\u7ed3\u5408\u9884\u4f30\u8ba1\u5f15\u5bfc\u7684\u7279\u5f81\u6ce8\u5165\u6a21\u5757\uff0c\u5728STEW\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4f18\u4e8e\u4f20\u7edf\u4f18\u5316\u65b9\u6cd5\u548c\u4e0e\u5f53\u524d\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u76f8\u7ade\u4e89\u7684\u7cbe\u5ea6\u3002", "motivation": "EEG\u4fe1\u53f7\u5b58\u5728\u663e\u8457\u7684\u4e2a\u4f53\u95f4\u53d8\u5f02\u6027\uff0c\u8fd9\u79cd\u6ce2\u52a8\u4f1a\u5e72\u6270\u53ef\u9760\u91c7\u96c6\u5e76\u53ef\u80fd\u5bfc\u81f4\u5931\u771f\u6216\u524a\u6ce2\u3002\u6a21\u6570\u91c7\u6837\u901a\u8fc7\u6298\u53e0\u4fe1\u53f7\u800c\u4e0d\u662f\u9971\u548c\u4fe1\u53f7\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u4f46\u4ece\u6298\u53e0\u89c2\u6d4b\u4e2d\u6062\u590d\u539f\u59cb\u6ce2\u5f62\u662f\u4e00\u4e2a\u9ad8\u5ea6\u4e0d\u9002\u5b9a\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u56fe\u795e\u7ecf\u7f51\u7edcGraphUnwrapNet\uff0c\u5c06EEG\u4fe1\u53f7\u8868\u793a\u4e3a\u6709\u7ec4\u7ec7\u7684\u56fe\u7ed3\u6784\uff0c\u5176\u4e2d\u901a\u9053\u548c\u65f6\u95f4\u8fde\u63a5\u5efa\u7acb\u5e95\u5c42\u76f8\u4e92\u4f9d\u8d56\u5173\u7cfb\u3002\u5173\u952e\u8d21\u732e\u662f\u5f15\u5165\u4e86\u9884\u4f30\u8ba1\u5f15\u5bfc\u7684\u7279\u5f81\u6ce8\u5165\u6a21\u5757\uff0c\u63d0\u4f9b\u7c97\u7565\u7684\u6298\u53e0\u6307\u793a\u5668\uff0c\u589e\u5f3a\u5728\u6298\u53e0\u8fb9\u754c\u5904\u6062\u590d\u7684\u7a33\u5b9a\u6027\u3002", "result": "\u5728STEW\u6570\u636e\u96c6\u4e0a\u7684\u7efc\u5408\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u76f8\u6bd4\u4f20\u7edf\u4f18\u5316\u6280\u672f\u6709\u6301\u7eed\u6539\u8fdb\uff0c\u4e0e\u5f53\u524d\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u76f8\u6bd4\u5177\u6709\u7ade\u4e89\u6027\u7684\u51c6\u786e\u5ea6\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86\u57fa\u4e8e\u56fe\u7684\u65b9\u6cd5\u5728\u9c81\u68d2\u6a21\u6570EEG\u6062\u590d\u65b9\u9762\u7684\u6f5c\u529b\u3002"}}

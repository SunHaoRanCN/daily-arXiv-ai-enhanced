{"id": "2510.26817", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.26817", "abs": "https://arxiv.org/abs/2510.26817", "authors": ["Jianbing Xiahou", "Weixi Zhai", "Xu Cui"], "title": "Oral Tradition-Encoded NanyinHGNN: Integrating Nanyin Music Preservation and Generation through a Pipa-Centric Dataset", "comment": "10 pages, 2 figures", "summary": "We propose NanyinHGNN, a heterogeneous graph network model for generating\nNanyin instrumental music. As a UNESCO-recognized intangible cultural heritage,\nNanyin follows a heterophonic tradition centered around the pipa, where core\nmelodies are notated in traditional notation while ornamentations are passed\ndown orally, presenting challenges for both preservation and contemporary\ninnovation. To address this, we construct a Pipa-Centric MIDI dataset, develop\nNanyinTok as a specialized tokenization method, and convert symbolic sequences\ninto graph structures using a Graph Converter to ensure that key musical\nfeatures are preserved. Our key innovation reformulates ornamentation\ngeneration as the creation of ornamentation nodes within a heterogeneous graph.\nFirst, a graph neural network generates melodic outlines optimized for\nornamentations. Then, a rule-guided system informed by Nanyin performance\npractices refines these outlines into complete ornamentations without requiring\nexplicit ornamentation annotations during training. Experimental results\ndemonstrate that our model successfully generates authentic heterophonic\nensembles featuring four traditional instruments. These findings validate that\nintegrating domain-specific knowledge into model architecture can effectively\nmitigate data scarcity challenges in computational ethnomusicology.", "AI": {"tldr": "\u63d0\u51faNanyinHGNN\u5f02\u6784\u56fe\u7f51\u7edc\u6a21\u578b\uff0c\u7528\u4e8e\u751f\u6210\u5357\u97f3\u5668\u4e50\u97f3\u4e50\uff0c\u901a\u8fc7\u5c06\u88c5\u9970\u97f3\u751f\u6210\u91cd\u65b0\u5b9a\u4e49\u4e3a\u5728\u5f02\u6784\u56fe\u4e2d\u521b\u5efa\u88c5\u9970\u97f3\u8282\u70b9\u6765\u89e3\u51b3\u5357\u97f3\u4f20\u627f\u6311\u6218\u3002", "motivation": "\u5357\u97f3\u4f5c\u4e3a\u8054\u5408\u56fd\u6559\u79d1\u6587\u7ec4\u7ec7\u8ba4\u53ef\u7684\u975e\u7269\u8d28\u6587\u5316\u9057\u4ea7\uff0c\u91c7\u7528\u4ee5\u7435\u7436\u4e3a\u4e2d\u5fc3\u7684\u652f\u58f0\u590d\u8c03\u4f20\u7edf\uff0c\u6838\u5fc3\u65cb\u5f8b\u6709\u4f20\u7edf\u8bb0\u8c31\u800c\u88c5\u9970\u97f3\u9760\u53e3\u4f20\u5fc3\u6388\uff0c\u7ed9\u4fdd\u62a4\u548c\u5f53\u4ee3\u521b\u65b0\u5e26\u6765\u6311\u6218\u3002", "method": "\u6784\u5efa\u7435\u7436\u4e2d\u5fc3MIDI\u6570\u636e\u96c6\uff0c\u5f00\u53d1\u4e13\u7528\u5206\u8bcd\u65b9\u6cd5NanyinTok\uff0c\u4f7f\u7528\u56fe\u8f6c\u6362\u5668\u5c06\u7b26\u53f7\u5e8f\u5217\u8f6c\u4e3a\u56fe\u7ed3\u6784\uff0c\u901a\u8fc7\u56fe\u795e\u7ecf\u7f51\u7edc\u751f\u6210\u65cb\u5f8b\u8f6e\u5ed3\uff0c\u518d\u57fa\u4e8e\u5357\u97f3\u8868\u6f14\u5b9e\u8df5\u7684\u89c4\u5219\u5f15\u5bfc\u7cfb\u7edf\u5b8c\u5584\u4e3a\u5b8c\u6574\u88c5\u9970\u97f3\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u6a21\u578b\u6210\u529f\u751f\u6210\u4e86\u5305\u542b\u56db\u79cd\u4f20\u7edf\u4e50\u5668\u7684\u771f\u5b9e\u652f\u58f0\u590d\u8c03\u5408\u594f\uff0c\u9a8c\u8bc1\u4e86\u5c06\u9886\u57df\u7279\u5b9a\u77e5\u8bc6\u878d\u5165\u6a21\u578b\u67b6\u6784\u80fd\u6709\u6548\u7f13\u89e3\u8ba1\u7b97\u6c11\u65cf\u97f3\u4e50\u5b66\u4e2d\u7684\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u3002", "conclusion": "\u5c06\u9886\u57df\u7279\u5b9a\u77e5\u8bc6\u6574\u5408\u5230\u6a21\u578b\u67b6\u6784\u4e2d\u53ef\u4ee5\u6709\u6548\u7f13\u89e3\u8ba1\u7b97\u6c11\u65cf\u97f3\u4e50\u5b66\u4e2d\u7684\u6570\u636e\u7a00\u7f3a\u6311\u6218\uff0c\u4e3a\u975e\u7269\u8d28\u6587\u5316\u9057\u4ea7\u7684\u4fdd\u62a4\u548c\u521b\u65b0\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2510.26818", "categories": ["cs.SD", "cs.AI", "cs.MM", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.26818", "abs": "https://arxiv.org/abs/2510.26818", "authors": ["Jinting Wang", "Chenxing Li", "Li Liu"], "title": "GACA-DiT: Diffusion-based Dance-to-Music Generation with Genre-Adaptive Rhythm and Context-Aware Alignment", "comment": "5 pages, 3 figures, submitted to ICASSP 2026", "summary": "Dance-to-music (D2M) generation aims to automatically compose music that is\nrhythmically and temporally aligned with dance movements. Existing methods\ntypically rely on coarse rhythm embeddings, such as global motion features or\nbinarized joint-based rhythm values, which discard fine-grained motion cues and\nresult in weak rhythmic alignment. Moreover, temporal mismatches introduced by\nfeature downsampling further hinder precise synchronization between dance and\nmusic. To address these problems, we propose \\textbf{GACA-DiT}, a diffusion\ntransformer-based framework with two novel modules for rhythmically consistent\nand temporally aligned music generation. First, a \\textbf{genre-adaptive rhythm\nextraction} module combines multi-scale temporal wavelet analysis and spatial\nphase histograms with adaptive joint weighting to capture fine-grained,\ngenre-specific rhythm patterns. Second, a \\textbf{context-aware temporal\nalignment} module resolves temporal mismatches using learnable context queries\nto align music latents with relevant dance rhythm features. Extensive\nexperiments on the AIST++ and TikTok datasets demonstrate that GACA-DiT\noutperforms state-of-the-art methods in both objective metrics and human\nevaluation. Project page: https://beria-moon.github.io/GACA-DiT/.", "AI": {"tldr": "\u63d0\u51fa\u4e86GACA-DiT\u6846\u67b6\uff0c\u901a\u8fc7\u6d41\u6d3e\u81ea\u9002\u5e94\u8282\u594f\u63d0\u53d6\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u65f6\u95f4\u5bf9\u9f50\u6a21\u5757\uff0c\u89e3\u51b3\u821e\u8e48\u5230\u97f3\u4e50\u751f\u6210\u4e2d\u7684\u8282\u594f\u5bf9\u9f50\u548c\u65f6\u95f4\u540c\u6b65\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u7c97\u7cd9\u7684\u8282\u594f\u5d4c\u5165\uff0c\u4e22\u5f03\u4e86\u7ec6\u7c92\u5ea6\u8fd0\u52a8\u7ebf\u7d22\uff0c\u5bfc\u81f4\u8282\u594f\u5bf9\u9f50\u6548\u679c\u5dee\uff0c\u4e14\u7279\u5f81\u4e0b\u91c7\u6837\u5f15\u5165\u7684\u65f6\u95f4\u4e0d\u5339\u914d\u8fdb\u4e00\u6b65\u963b\u788d\u4e86\u821e\u8e48\u4e0e\u97f3\u4e50\u7684\u7cbe\u786e\u540c\u6b65\u3002", "method": "\u57fa\u4e8e\u6269\u6563\u53d8\u6362\u5668\u7684\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u65b0\u6a21\u5757\uff1a1\uff09\u6d41\u6d3e\u81ea\u9002\u5e94\u8282\u594f\u63d0\u53d6\u6a21\u5757\uff0c\u7ed3\u5408\u591a\u5c3a\u5ea6\u65f6\u95f4\u5c0f\u6ce2\u5206\u6790\u548c\u7a7a\u95f4\u76f8\u4f4d\u76f4\u65b9\u56fe\uff1b2\uff09\u4e0a\u4e0b\u6587\u611f\u77e5\u65f6\u95f4\u5bf9\u9f50\u6a21\u5757\uff0c\u4f7f\u7528\u53ef\u5b66\u4e60\u4e0a\u4e0b\u6587\u67e5\u8be2\u6765\u5bf9\u9f50\u97f3\u4e50\u6f5c\u5728\u8868\u793a\u4e0e\u821e\u8e48\u8282\u594f\u7279\u5f81\u3002", "result": "\u5728AIST++\u548cTikTok\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cGACA-DiT\u5728\u5ba2\u89c2\u6307\u6807\u548c\u4eba\u7c7b\u8bc4\u4f30\u4e2d\u90fd\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "GACA-DiT\u901a\u8fc7\u7ec6\u7c92\u5ea6\u8282\u594f\u63d0\u53d6\u548c\u7cbe\u786e\u65f6\u95f4\u5bf9\u9f50\uff0c\u663e\u8457\u63d0\u5347\u4e86\u821e\u8e48\u5230\u97f3\u4e50\u751f\u6210\u7684\u8282\u594f\u4e00\u81f4\u6027\u548c\u65f6\u95f4\u540c\u6b65\u8d28\u91cf\u3002"}}
{"id": "2510.26823", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.26823", "abs": "https://arxiv.org/abs/2510.26823", "authors": ["Unzela Talpur", "Zafi Sherhan Syed", "Muhammad Shehram Shah Syed", "Abbas Shah Syed"], "title": "Cross-Corpus Validation of Speech Emotion Recognition in Urdu using Domain-Knowledge Acoustic Features", "comment": "Conference paper, 4 pages, including 3 figures and 3 tables", "summary": "Speech Emotion Recognition (SER) is a key affective computing technology that\nenables emotionally intelligent artificial intelligence. While SER is\nchallenging in general, it is particularly difficult for low-resource languages\nsuch as Urdu. This study investigates Urdu SER in a cross-corpus setting, an\narea that has remained largely unexplored. We employ a cross-corpus evaluation\nframework across three different Urdu emotional speech datasets to test model\ngeneralization. Two standard domain-knowledge based acoustic feature sets,\neGeMAPS and ComParE, are used to represent speech signals as feature vectors\nwhich are then passed to Logistic Regression and Multilayer Perceptron\nclassifiers. Classification performance is assessed using unweighted average\nrecall (UAR) whilst considering class-label imbalance. Results show that\nSelf-corpus validation often overestimates performance, with UAR exceeding\ncross-corpus evaluation by up to 13%, underscoring that cross-corpus evaluation\noffers a more realistic measure of model robustness. Overall, this work\nemphasizes the importance of cross-corpus validation for Urdu SER and its\nimplications contribute to advancing affective computing research for\nunderrepresented language communities.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e4c\u5c14\u90fd\u8bed\u8bed\u97f3\u60c5\u611f\u8bc6\u522b\u5728\u8de8\u8bed\u6599\u5e93\u8bbe\u7f6e\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u81ea\u8bed\u6599\u5e93\u9a8c\u8bc1\u4f1a\u9ad8\u4f30\u6a21\u578b\u6027\u80fd\uff0c\u8de8\u8bed\u6599\u5e93\u8bc4\u4f30\u80fd\u66f4\u771f\u5b9e\u53cd\u6620\u6a21\u578b\u9c81\u68d2\u6027\u3002", "motivation": "\u4e4c\u5c14\u90fd\u8bed\u4f5c\u4e3a\u4f4e\u8d44\u6e90\u8bed\u8a00\uff0c\u5176\u8bed\u97f3\u60c5\u611f\u8bc6\u522b\u7814\u7a76\u8f83\u5c11\uff0c\u7279\u522b\u662f\u5728\u8de8\u8bed\u6599\u5e93\u8bbe\u7f6e\u65b9\u9762\u51e0\u4e4e\u672a\u88ab\u63a2\u7d22\uff0c\u9700\u8981\u8bc4\u4f30\u6a21\u578b\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u4f7f\u7528\u4e09\u4e2a\u4e4c\u5c14\u90fd\u8bed\u60c5\u611f\u8bed\u97f3\u6570\u636e\u96c6\u8fdb\u884c\u8de8\u8bed\u6599\u5e93\u8bc4\u4f30\uff0c\u91c7\u7528eGeMAPS\u548cComParE\u58f0\u5b66\u7279\u5f81\u96c6\uff0c\u901a\u8fc7\u903b\u8f91\u56de\u5f52\u548c\u591a\u5c42\u611f\u77e5\u673a\u5206\u7c7b\u5668\u8fdb\u884c\u60c5\u611f\u8bc6\u522b\u3002", "result": "\u81ea\u8bed\u6599\u5e93\u9a8c\u8bc1\u7684UAR\u6bd4\u8de8\u8bed\u6599\u5e93\u8bc4\u4f30\u9ad8\u51fa\u6700\u591a13%\uff0c\u8868\u660e\u81ea\u8bed\u6599\u5e93\u9a8c\u8bc1\u4f1a\u9ad8\u4f30\u6027\u80fd\uff0c\u8de8\u8bed\u6599\u5e93\u8bc4\u4f30\u80fd\u66f4\u51c6\u786e\u8861\u91cf\u6a21\u578b\u9c81\u68d2\u6027\u3002", "conclusion": "\u8de8\u8bed\u6599\u5e93\u9a8c\u8bc1\u5bf9\u4e4c\u5c14\u90fd\u8bed\u8bed\u97f3\u60c5\u611f\u8bc6\u522b\u81f3\u5173\u91cd\u8981\uff0c\u8fd9\u9879\u7814\u7a76\u6709\u52a9\u4e8e\u63a8\u8fdb\u4ee3\u8868\u6027\u4e0d\u8db3\u8bed\u8a00\u7fa4\u4f53\u7684\u60c5\u611f\u8ba1\u7b97\u7814\u7a76\u3002"}}
{"id": "2510.26825", "categories": ["cs.SD", "cs.CV", "cs.MM", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.26825", "abs": "https://arxiv.org/abs/2510.26825", "authors": ["Jiarong Du", "Zhan Jin", "Peijun Yang", "Juan Liu", "Zhuo Li", "Xin Liu", "Ming Li"], "title": "Audio-Visual Speech Enhancement In Complex Scenarios With Separation And Dereverberation Joint Modeling", "comment": null, "summary": "Audio-visual speech enhancement (AVSE) is a task that uses visual auxiliary\ninformation to extract a target speaker's speech from mixed audio. In\nreal-world scenarios, there often exist complex acoustic environments,\naccompanied by various interfering sounds and reverberation. Most previous\nmethods struggle to cope with such complex conditions, resulting in poor\nperceptual quality of the extracted speech. In this paper, we propose an\neffective AVSE system that performs well in complex acoustic environments.\nSpecifically, we design a \"separation before dereverberation\" pipeline that can\nbe extended to other AVSE networks. The 4th COGMHEAR Audio-Visual Speech\nEnhancement Challenge (AVSEC) aims to explore new approaches to speech\nprocessing in multimodal complex environments. We validated the performance of\nour system in AVSEC-4: we achieved excellent results in the three objective\nmetrics on the competition leaderboard, and ultimately secured first place in\nthe human subjective listening test.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u590d\u6742\u58f0\u5b66\u73af\u5883\u4e2d\u8868\u73b0\u4f18\u5f02\u7684\u89c6\u542c\u8bed\u97f3\u589e\u5f3a\u7cfb\u7edf\uff0c\u91c7\u7528\"\u5148\u5206\u79bb\u540e\u53bb\u6df7\u54cd\"\u7684\u6d41\u7a0b\uff0c\u5728AVSEC-4\u7ade\u8d5b\u4e2d\u53d6\u5f97\u4e86\u5ba2\u89c2\u6307\u6807\u548c\u4e3b\u89c2\u542c\u6d4b\u53cc\u7b2c\u4e00\u7684\u4f18\u5f02\u6210\u7ee9\u3002", "motivation": "\u73b0\u5b9e\u573a\u666f\u4e2d\u5e38\u5b58\u5728\u590d\u6742\u58f0\u5b66\u73af\u5883\uff0c\u5305\u542b\u5404\u79cd\u5e72\u6270\u58f0\u548c\u6df7\u54cd\uff0c\u800c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9\u8fd9\u4e9b\u590d\u6742\u6761\u4ef6\uff0c\u5bfc\u81f4\u63d0\u53d6\u8bed\u97f3\u7684\u611f\u77e5\u8d28\u91cf\u8f83\u5dee\u3002", "method": "\u8bbe\u8ba1\u4e86\"\u5148\u5206\u79bb\u540e\u53bb\u6df7\u54cd\"\u7684\u6d41\u7a0b\uff0c\u8be5\u6d41\u7a0b\u53ef\u6269\u5c55\u5230\u5176\u4ed6\u89c6\u542c\u8bed\u97f3\u589e\u5f3a\u7f51\u7edc\u3002", "result": "\u5728AVSEC-4\u7ade\u8d5b\u4e2d\uff0c\u5728\u4e09\u4e2a\u5ba2\u89c2\u6307\u6807\u4e0a\u53d6\u5f97\u4f18\u5f02\u6210\u7ee9\uff0c\u5e76\u5728\u4eba\u7c7b\u4e3b\u89c2\u542c\u6d4b\u4e2d\u6700\u7ec8\u83b7\u5f97\u7b2c\u4e00\u540d\u3002", "conclusion": "\u63d0\u51fa\u7684\u89c6\u542c\u8bed\u97f3\u589e\u5f3a\u7cfb\u7edf\u5728\u590d\u6742\u58f0\u5b66\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u9a8c\u8bc1\u4e86\"\u5148\u5206\u79bb\u540e\u53bb\u6df7\u54cd\"\u6d41\u7a0b\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2510.26819", "categories": ["eess.AS", "cs.AI", "cs.CV", "cs.SD"], "pdf": "https://arxiv.org/pdf/2510.26819", "abs": "https://arxiv.org/abs/2510.26819", "authors": ["Jinting Wang", "Jun Wang", "Hei Victor Cheng", "Li Liu"], "title": "See the Speaker: Crafting High-Resolution Talking Faces from Speech with Prior Guidance and Region Refinement", "comment": "16 pages,15 figures, accepted by TASLP", "summary": "Unlike existing methods that rely on source images as appearance references\nand use source speech to generate motion, this work proposes a novel approach\nthat directly extracts information from the speech, addressing key challenges\nin speech-to-talking face. Specifically, we first employ a speech-to-face\nportrait generation stage, utilizing a speech-conditioned diffusion model\ncombined with statistical facial prior and a sample-adaptive weighting module\nto achieve high-quality portrait generation. In the subsequent speech-driven\ntalking face generation stage, we embed expressive dynamics such as lip\nmovement, facial expressions, and eye movements into the latent space of the\ndiffusion model and further optimize lip synchronization using a\nregion-enhancement module. To generate high-resolution outputs, we integrate a\npre-trained Transformer-based discrete codebook with an image rendering\nnetwork, enhancing video frame details in an end-to-end manner. Experimental\nresults demonstrate that our method outperforms existing approaches on the\nHDTF, VoxCeleb, and AVSpeech datasets. Notably, this is the first method\ncapable of generating high-resolution, high-quality talking face videos\nexclusively from a single speech input.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u76f4\u63a5\u4ece\u8bed\u97f3\u751f\u6210\u9ad8\u8d28\u91cf\u8bf4\u8bdd\u4eba\u8138\u89c6\u9891\u7684\u65b0\u65b9\u6cd5\uff0c\u65e0\u9700\u4f9d\u8d56\u6e90\u56fe\u50cf\u4f5c\u4e3a\u5916\u89c2\u53c2\u8003\uff0c\u901a\u8fc7\u8bed\u97f3\u5230\u9762\u90e8\u8096\u50cf\u751f\u6210\u548c\u8bed\u97f3\u9a71\u52a8\u7684\u8bf4\u8bdd\u4eba\u8138\u751f\u6210\u4e24\u4e2a\u9636\u6bb5\u5b9e\u73b0\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u6e90\u56fe\u50cf\u4f5c\u4e3a\u5916\u89c2\u53c2\u8003\u548c\u4f7f\u7528\u6e90\u8bed\u97f3\u751f\u6210\u52a8\u4f5c\u7684\u5c40\u9650\u6027\uff0c\u63a2\u7d22\u76f4\u63a5\u4ece\u8bed\u97f3\u4fe1\u606f\u4e2d\u63d0\u53d6\u4fe1\u606f\u6765\u751f\u6210\u8bf4\u8bdd\u4eba\u8138\u89c6\u9891\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a1\uff09\u8bed\u97f3\u5230\u9762\u90e8\u8096\u50cf\u751f\u6210\u9636\u6bb5\uff0c\u4f7f\u7528\u8bed\u97f3\u6761\u4ef6\u6269\u6563\u6a21\u578b\u7ed3\u5408\u7edf\u8ba1\u9762\u90e8\u5148\u9a8c\u548c\u6837\u672c\u81ea\u9002\u5e94\u52a0\u6743\u6a21\u5757\uff1b2\uff09\u8bed\u97f3\u9a71\u52a8\u7684\u8bf4\u8bdd\u4eba\u8138\u751f\u6210\u9636\u6bb5\uff0c\u5728\u6269\u6563\u6a21\u578b\u6f5c\u5728\u7a7a\u95f4\u4e2d\u5d4c\u5165\u8868\u60c5\u52a8\u6001\uff0c\u5e76\u4f7f\u7528\u533a\u57df\u589e\u5f3a\u6a21\u5757\u4f18\u5316\u5507\u90e8\u540c\u6b65\uff0c\u96c6\u6210Transformer\u79bb\u6563\u7801\u672c\u548c\u56fe\u50cf\u6e32\u67d3\u7f51\u7edc\u751f\u6210\u9ad8\u5206\u8fa8\u7387\u8f93\u51fa\u3002", "result": "\u5728HDTF\u3001VoxCeleb\u548cAVSpeech\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u80fd\u591f\u4ece\u5355\u4e00\u8bed\u97f3\u8f93\u5165\u751f\u6210\u9ad8\u5206\u8fa8\u7387\u3001\u9ad8\u8d28\u91cf\u7684\u8bf4\u8bdd\u4eba\u8138\u89c6\u9891\u3002", "conclusion": "\u8fd9\u662f\u9996\u4e2a\u80fd\u591f\u4ec5\u4ece\u5355\u4e00\u8bed\u97f3\u8f93\u5165\u751f\u6210\u9ad8\u5206\u8fa8\u7387\u3001\u9ad8\u8d28\u91cf\u8bf4\u8bdd\u4eba\u8138\u89c6\u9891\u7684\u65b9\u6cd5\uff0c\u4e3a\u8bed\u97f3\u5230\u8bf4\u8bdd\u4eba\u8138\u751f\u6210\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.26803", "categories": ["eess.SP", "cs.ET", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.26803", "abs": "https://arxiv.org/abs/2510.26803", "authors": ["Hang Lin", "Liuxun Xue", "Shu Sun", "Ruifeng Gao", "Jue Wang", "Tengjiao Wang"], "title": "Investigation of Superdirectivity in Planar Holographic Arrays", "comment": "in Chinese language", "summary": "This paper studies the superdirectivity characteristics of uniform\nrectangular arrays (URAs) for holographic multiple-input multiple-output\nsystems. By establishing a mathematical directivity model for the URA, an\nanalytical expression for the maximum directivity is derived. Accordingly,\nsystematic analysis is performed in conjunction with numerical simulations.\nResults show that the directivity can be significantly enhanced via rational\nutilization of coupling effects. However, this enhancement yields diminishing\nreturns when antenna spacings transition to deep sub-wavelength scales. This\nstudy provides a theoretical basis for the design of superdirective URAs and\noffers valuable insights for holographic array optimization in 5G/6G\ncommunication systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5747\u5300\u77e9\u5f62\u9635\u5217\u5728holographic MIMO\u7cfb\u7edf\u4e2d\u7684\u8d85\u6307\u5411\u6027\u7279\u6027\uff0c\u63a8\u5bfc\u4e86\u6700\u5927\u6307\u5411\u6027\u7684\u89e3\u6790\u8868\u8fbe\u5f0f\uff0c\u5e76\u901a\u8fc7\u6570\u503c\u6a21\u62df\u9a8c\u8bc1\u4e86\u8026\u5408\u6548\u5e94\u80fd\u663e\u8457\u589e\u5f3a\u6307\u5411\u6027\uff0c\u4f46\u5728\u6df1\u4e9a\u6ce2\u957f\u5c3a\u5ea6\u4e0b\u589e\u76ca\u9012\u51cf\u3002", "motivation": "\u7814\u7a76\u5747\u5300\u77e9\u5f62\u9635\u5217\u5728holographic MIMO\u7cfb\u7edf\u4e2d\u7684\u8d85\u6307\u5411\u6027\u7279\u6027\uff0c\u4e3a5G/6G\u901a\u4fe1\u7cfb\u7edf\u4e2d\u7684holographic\u9635\u5217\u4f18\u5316\u63d0\u4f9b\u7406\u8bba\u57fa\u7840\u3002", "method": "\u5efa\u7acb\u4e86\u5747\u5300\u77e9\u5f62\u9635\u5217\u7684\u6570\u5b66\u6307\u5411\u6027\u6a21\u578b\uff0c\u63a8\u5bfc\u4e86\u6700\u5927\u6307\u5411\u6027\u7684\u89e3\u6790\u8868\u8fbe\u5f0f\uff0c\u5e76\u7ed3\u5408\u6570\u503c\u6a21\u62df\u8fdb\u884c\u7cfb\u7edf\u5206\u6790\u3002", "result": "\u7ed3\u679c\u8868\u660e\u901a\u8fc7\u5408\u7406\u5229\u7528\u8026\u5408\u6548\u5e94\u53ef\u4ee5\u663e\u8457\u589e\u5f3a\u6307\u5411\u6027\uff0c\u4f46\u5f53\u5929\u7ebf\u95f4\u8ddd\u8fc7\u6e21\u5230\u6df1\u4e9a\u6ce2\u957f\u5c3a\u5ea6\u65f6\uff0c\u8fd9\u79cd\u589e\u5f3a\u6548\u679c\u4f1a\u9012\u51cf\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u8d85\u6307\u5411\u6027\u5747\u5300\u77e9\u5f62\u9635\u5217\u7684\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u5e76\u4e3a5G/6G\u901a\u4fe1\u7cfb\u7edf\u4e2d\u7684holographic\u9635\u5217\u4f18\u5316\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3\u3002"}}
{"id": "2510.27102", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.27102", "abs": "https://arxiv.org/abs/2510.27102", "authors": ["Jonathan Morse", "Azadeh Naderi", "Swen Gaudl", "Mark Cartwright", "Amy K. Hoover", "Mark J. Nelson"], "title": "Expressive Range Characterization of Open Text-to-Audio Models", "comment": "Accepted at the AAAI Conference on Artificial Intelligence and\n  Interactive Digital Entertainment (AIIDE 2025)", "summary": "Text-to-audio models are a type of generative model that produces audio\noutput in response to a given textual prompt. Although level generators and the\nproperties of the functional content that they create (e.g., playability)\ndominate most discourse in procedurally generated content (PCG), games that\nemotionally resonate with players tend to weave together a range of creative\nand multimodal content (e.g., music, sounds, visuals, narrative tone), and\nmultimodal models have begun seeing at least experimental use for this purpose.\nHowever, it remains unclear what exactly such models generate, and with what\ndegree of variability and fidelity: audio is an extremely broad class of output\nfor a generative system to target.\n  Within the PCG community, expressive range analysis (ERA) has been used as a\nquantitative way to characterize generators' output space, especially for level\ngenerators. This paper adapts ERA to text-to-audio models, making the analysis\ntractable by looking at the expressive range of outputs for specific, fixed\nprompts. Experiments are conducted by prompting the models with several\nstandardized prompts derived from the Environmental Sound Classification\n(ESC-50) dataset. The resulting audio is analyzed along key acoustic dimensions\n(e.g., pitch, loudness, and timbre). More broadly, this paper offers a\nframework for ERA-based exploratory evaluation of generative audio models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8868\u8fbe\u8303\u56f4\u5206\u6790(ERA)\u7684\u65b9\u6cd5\u6765\u8bc4\u4f30\u6587\u672c\u5230\u97f3\u9891\u751f\u6210\u6a21\u578b\uff0c\u901a\u8fc7\u56fa\u5b9a\u63d0\u793a\u8bcd\u5206\u6790\u751f\u6210\u97f3\u9891\u5728\u97f3\u9ad8\u3001\u54cd\u5ea6\u3001\u97f3\u8272\u7b49\u58f0\u5b66\u7ef4\u5ea6\u4e0a\u7684\u53d8\u5316\u8303\u56f4\u3002", "motivation": "\u867d\u7136\u591a\u6a21\u6001\u6a21\u578b\u5728\u6e38\u620f\u5185\u5bb9\u751f\u6210\u4e2d\u5f00\u59cb\u5e94\u7528\uff0c\u4f46\u6587\u672c\u5230\u97f3\u9891\u6a21\u578b\u751f\u6210\u5185\u5bb9\u7684\u5177\u4f53\u7279\u6027\u548c\u53d8\u5f02\u6027\u4ecd\u4e0d\u6e05\u695a\uff0c\u9700\u8981\u7cfb\u7edf\u5316\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u5c06\u8868\u8fbe\u8303\u56f4\u5206\u6790(ERA)\u65b9\u6cd5\u4ece\u5173\u5361\u751f\u6210\u5668\u6269\u5c55\u5230\u6587\u672c\u5230\u97f3\u9891\u6a21\u578b\uff0c\u4f7f\u7528ESC-50\u6570\u636e\u96c6\u7684\u6807\u51c6\u63d0\u793a\u8bcd\uff0c\u5206\u6790\u751f\u6210\u97f3\u9891\u5728\u5173\u952e\u58f0\u5b66\u7ef4\u5ea6\u4e0a\u7684\u5206\u5e03\u3002", "result": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8eERA\u7684\u63a2\u7d22\u6027\u8bc4\u4f30\u6846\u67b6\uff0c\u80fd\u591f\u91cf\u5316\u5206\u6790\u6587\u672c\u5230\u97f3\u9891\u6a21\u578b\u5bf9\u7279\u5b9a\u63d0\u793a\u8bcd\u751f\u6210\u97f3\u9891\u7684\u8868\u8fbe\u8303\u56f4\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u751f\u6210\u97f3\u9891\u6a21\u578b\u7684\u7cfb\u7edf\u8bc4\u4f30\u63d0\u4f9b\u4e86\u53ef\u884c\u6846\u67b6\uff0c\u6709\u52a9\u4e8e\u7406\u89e3\u6a21\u578b\u751f\u6210\u5185\u5bb9\u7684\u7279\u6027\u548c\u5c40\u9650\u6027\u3002"}}
{"id": "2510.26838", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.AP", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.26838", "abs": "https://arxiv.org/abs/2510.26838", "authors": ["Amine Razig", "Youssef Soulaymani", "Loubna Benabbou", "Pierre Cauchy"], "title": "Multi-Representation Attention Framework for Underwater Bioacoustic Denoising and Recognition", "comment": null, "summary": "Automated monitoring of marine mammals in the St. Lawrence Estuary faces\nextreme challenges: calls span low-frequency moans to ultrasonic clicks, often\noverlap, and are embedded in variable anthropogenic and environmental noise. We\nintroduce a multi-step, attention-guided framework that first segments\nspectrograms to generate soft masks of biologically relevant energy and then\nfuses these masks with the raw inputs for multi-band, denoised classification.\nImage and mask embeddings are integrated via mid-level fusion, enabling the\nmodel to focus on salient spectrogram regions while preserving global context.\nUsing real-world recordings from the Saguenay St. Lawrence Marine Park Research\nStation in Canada, we demonstrate that segmentation-driven attention and\nmid-level fusion improve signal discrimination, reduce false positive\ndetections, and produce reliable representations for operational marine mammal\nmonitoring across diverse environmental conditions and signal-to-noise ratios.\nBeyond in-distribution evaluation, we further assess the generalization of\nMask-Guided Classification (MGC) under distributional shifts by testing on\nspectrograms generated with alternative acoustic transformations. While\nhigh-capacity baseline models lose accuracy in this Out-of-distribution (OOD)\nsetting, MGC maintains stable performance, with even simple fusion mechanisms\n(gated, concat) achieving comparable results across distributions. This\nrobustness highlights the capacity of MGC to learn transferable representations\nrather than overfitting to a specific transformation, thereby reinforcing its\nsuitability for large-scale, real-world biodiversity monitoring. We show that\nin all experimental settings, the MGC framework consistently outperforms\nbaseline architectures, yielding substantial gains in accuracy on both\nin-distribution and OOD data.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u591a\u6b65\u9aa4\u3001\u6ce8\u610f\u529b\u5f15\u5bfc\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u6781\u7aef\u6311\u6218\u6027\u73af\u5883\u4e0b\u81ea\u52a8\u76d1\u6d4b\u6d77\u6d0b\u54fa\u4e73\u52a8\u7269\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u5206\u5272\u58f0\u8c31\u56fe\u751f\u6210\u8f6f\u63a9\u7801\uff0c\u7136\u540e\u5c06\u63a9\u7801\u4e0e\u539f\u59cb\u8f93\u5165\u878d\u5408\uff0c\u5b9e\u73b0\u591a\u9891\u5e26\u3001\u53bb\u566a\u5206\u7c7b\uff0c\u5728\u5206\u5e03\u5185\u548c\u5206\u5e03\u5916\u6570\u636e\u4e0a\u90fd\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "\u5723\u52b3\u4f26\u65af\u6cb3\u53e3\u6d77\u6d0b\u54fa\u4e73\u52a8\u7269\u81ea\u52a8\u76d1\u6d4b\u9762\u4e34\u6781\u7aef\u6311\u6218\uff1a\u53eb\u58f0\u8303\u56f4\u4ece\u4f4e\u9891\u547b\u541f\u5230\u8d85\u58f0\u6ce2\u70b9\u51fb\uff0c\u7ecf\u5e38\u91cd\u53e0\uff0c\u5e76\u4e14\u5d4c\u5165\u5728\u591a\u53d8\u7684\u4eba\u4e3a\u548c\u73af\u5883\u566a\u58f0\u4e2d\u3002", "method": "\u5f15\u5165\u591a\u6b65\u9aa4\u6ce8\u610f\u529b\u5f15\u5bfc\u6846\u67b6\uff1a\u9996\u5148\u5206\u5272\u58f0\u8c31\u56fe\u751f\u6210\u751f\u7269\u76f8\u5173\u80fd\u91cf\u7684\u8f6f\u63a9\u7801\uff0c\u7136\u540e\u5c06\u8fd9\u4e9b\u63a9\u7801\u4e0e\u539f\u59cb\u8f93\u5165\u878d\u5408\u8fdb\u884c\u591a\u9891\u5e26\u53bb\u566a\u5206\u7c7b\u3002\u901a\u8fc7\u4e2d\u5c42\u878d\u5408\u6574\u5408\u56fe\u50cf\u548c\u63a9\u7801\u5d4c\u5165\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u5173\u6ce8\u663e\u8457\u7684\u58f0\u8c31\u56fe\u533a\u57df\u540c\u65f6\u4fdd\u7559\u5168\u5c40\u4e0a\u4e0b\u6587\u3002", "result": "\u5728\u52a0\u62ff\u5927\u8428\u683c\u5948\u5723\u52b3\u4f26\u65af\u6d77\u6d0b\u516c\u56ed\u7814\u7a76\u7ad9\u7684\u771f\u5b9e\u5f55\u97f3\u4e0a\uff0c\u5206\u5272\u9a71\u52a8\u7684\u6ce8\u610f\u529b\u548c\u4e2d\u5c42\u878d\u5408\u63d0\u9ad8\u4e86\u4fe1\u53f7\u8fa8\u522b\u80fd\u529b\uff0c\u51cf\u5c11\u4e86\u8bef\u62a5\u68c0\u6d4b\uff0c\u5e76\u4e3a\u4e0d\u540c\u73af\u5883\u6761\u4ef6\u548c\u4fe1\u566a\u6bd4\u4e0b\u7684\u64cd\u4f5c\u76d1\u6d4b\u4ea7\u751f\u4e86\u53ef\u9760\u7684\u8868\u793a\u3002\u5728\u5206\u5e03\u5916\u8bc4\u4f30\u4e2d\uff0cMGC\u4fdd\u6301\u7a33\u5b9a\u6027\u80fd\uff0c\u800c\u9ad8\u5bb9\u91cf\u57fa\u7ebf\u6a21\u578b\u5931\u53bb\u51c6\u786e\u6027\u3002", "conclusion": "MGC\u6846\u67b6\u5728\u6240\u6709\u5b9e\u9a8c\u8bbe\u7f6e\u4e2d\u90fd\u4f18\u4e8e\u57fa\u7ebf\u67b6\u6784\uff0c\u5728\u5206\u5e03\u5185\u548c\u5206\u5e03\u5916\u6570\u636e\u4e0a\u5747\u83b7\u5f97\u663e\u8457\u7684\u51c6\u786e\u7387\u63d0\u5347\u3002\u5176\u9c81\u68d2\u6027\u7a81\u51fa\u4e86MGC\u5b66\u4e60\u53ef\u8fc1\u79fb\u8868\u793a\u800c\u975e\u8fc7\u5ea6\u62df\u5408\u7279\u5b9a\u53d8\u6362\u7684\u80fd\u529b\uff0c\u4ece\u800c\u589e\u5f3a\u4e86\u5176\u5728\u5927\u89c4\u6a21\u3001\u771f\u5b9e\u4e16\u754c\u751f\u7269\u591a\u6837\u6027\u76d1\u6d4b\u4e2d\u7684\u9002\u7528\u6027\u3002"}}
{"id": "2510.26822", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.26822", "abs": "https://arxiv.org/abs/2510.26822", "authors": ["Yuanhang Qian", "Xueqin Luo", "Jilu Jin", "Gongping Huang", "Jingdong Chen", "Jacob Benesty"], "title": "Joint optimization of microphone array geometry, sensor directivity pattern, and beamforming parameters for linear superarrays", "comment": null, "summary": "Linear superarrays (LSAs) have been proposed to address the limited steering\ncapability of conventional linear differential microphone arrays (LDMAs) by\nintegrating omnidirectional and directional microphones, enabling more flexible\nbeamformer designs. However, existing approaches remain limited because array\ngeometry and element directivity, both critical to beamforming performance, are\nnot jointly optimized. This paper presents a generalized LSA optimization\nframework that simultaneously optimizes array geometry, element directivity,\nand the beamforming filter to minimize the approximation error between the\ndesigned beampattern and an ideal directivity pattern (IDP) over the full\nfrequency band and all steering directions within the region of interest. The\nbeamformer is derived by approximating the IDP using a Jacobi-Anger series\nexpansion, while the array geometry and element directivity are optimized via a\ngenetic algorithm. Simulation results show that the proposed optimized array\nachieves lower approximation error than conventional LSAs across the target\nfrequency band and steering range. Additionally, its directivity factor and\nwhite noise gain demonstrate more stable and improved performance across\nfrequencies and steering angles.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5e7f\u4e49\u7ebf\u6027\u8d85\u9635\u5217\u4f18\u5316\u6846\u67b6\uff0c\u540c\u65f6\u4f18\u5316\u9635\u5217\u51e0\u4f55\u3001\u5143\u4ef6\u6307\u5411\u6027\u548c\u6ce2\u675f\u5f62\u6210\u6ee4\u6ce2\u5668\uff0c\u4ee5\u5728\u5168\u9891\u5e26\u548c\u6240\u6709\u8f6c\u5411\u65b9\u5411\u4e0a\u6700\u5c0f\u5316\u8bbe\u8ba1\u6ce2\u675f\u6a21\u5f0f\u4e0e\u7406\u60f3\u6307\u5411\u6027\u6a21\u5f0f\u4e4b\u95f4\u7684\u903c\u8fd1\u8bef\u5dee\u3002", "motivation": "\u4f20\u7edf\u7ebf\u6027\u5dee\u5206\u9ea6\u514b\u98ce\u9635\u5217\u7684\u8f6c\u5411\u80fd\u529b\u6709\u9650\uff0c\u73b0\u6709\u7ebf\u6027\u8d85\u9635\u5217\u65b9\u6cd5\u7531\u4e8e\u672a\u8054\u5408\u4f18\u5316\u9635\u5217\u51e0\u4f55\u548c\u5143\u4ef6\u6307\u5411\u6027\u8fd9\u4e24\u4e2a\u5bf9\u6ce2\u675f\u5f62\u6210\u6027\u80fd\u81f3\u5173\u91cd\u8981\u7684\u56e0\u7d20\uff0c\u4ecd\u7136\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "\u4f7f\u7528\u96c5\u53ef\u6bd4-\u5b89\u683c\u5c14\u7ea7\u6570\u5c55\u5f00\u903c\u8fd1\u7406\u60f3\u6307\u5411\u6027\u6a21\u5f0f\u6765\u63a8\u5bfc\u6ce2\u675f\u5f62\u6210\u5668\uff0c\u540c\u65f6\u901a\u8fc7\u9057\u4f20\u7b97\u6cd5\u4f18\u5316\u9635\u5217\u51e0\u4f55\u548c\u5143\u4ef6\u6307\u5411\u6027\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u4f18\u5316\u9635\u5217\u5728\u76ee\u6807\u9891\u5e26\u548c\u8f6c\u5411\u8303\u56f4\u5185\u6bd4\u4f20\u7edf\u7ebf\u6027\u8d85\u9635\u5217\u5b9e\u73b0\u4e86\u66f4\u4f4e\u7684\u903c\u8fd1\u8bef\u5dee\uff0c\u5176\u6307\u5411\u6027\u56e0\u5b50\u548c\u767d\u566a\u58f0\u589e\u76ca\u5728\u9891\u7387\u548c\u8f6c\u5411\u89d2\u5ea6\u4e0a\u8868\u73b0\u51fa\u66f4\u7a33\u5b9a\u548c\u6539\u5584\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u4f18\u5316\u6846\u67b6\u901a\u8fc7\u8054\u5408\u4f18\u5316\u9635\u5217\u51e0\u4f55\u3001\u5143\u4ef6\u6307\u5411\u6027\u548c\u6ce2\u675f\u5f62\u6210\u6ee4\u6ce2\u5668\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7ebf\u6027\u8d85\u9635\u5217\u7684\u6ce2\u675f\u5f62\u6210\u6027\u80fd\u3002"}}
{"id": "2510.27530", "categories": ["cs.SD", "cs.LG", "cs.SI", "H.5.5; G.2.2; I.5.4"], "pdf": "https://arxiv.org/pdf/2510.27530", "abs": "https://arxiv.org/abs/2510.27530", "authors": ["A. V. Bomediano", "R. J. Conanan", "L. D. Santuyo", "A. Coronel"], "title": "Representing Classical Compositions through Implication-Realization Temporal-Gestalt Graphs", "comment": "8 pages, 11 figures", "summary": "Understanding the structural and cognitive underpinnings of musical\ncompositions remains a key challenge in music theory and computational\nmusicology. While traditional methods focus on harmony and rhythm, cognitive\nmodels such as the Implication-Realization (I-R) model and Temporal Gestalt\ntheory offer insight into how listeners perceive and anticipate musical\nstructure. This study presents a graph-based computational approach that\noperationalizes these models by segmenting melodies into perceptual units and\nannotating them with I-R patterns. These segments are compared using Dynamic\nTime Warping and organized into k-nearest neighbors graphs to model intra- and\ninter-segment relationships.\n  Each segment is represented as a node in the graph, and nodes are further\nlabeled with melodic expectancy values derived from Schellenberg's two-factor\nI-R model-quantifying pitch proximity and pitch reversal at the segment level.\nThis labeling enables the graphs to encode both structural and cognitive\ninformation, reflecting how listeners experience musical tension and\nresolution.\n  To evaluate the expressiveness of these graphs, we apply the\nWeisfeiler-Lehman graph kernel to measure similarity between and within\ncompositions. Results reveal statistically significant distinctions between\nintra- and inter-graph structures. Segment-level analysis via multidimensional\nscaling confirms that structural similarity at the graph level reflects\nperceptual similarity at the segment level. Graph2vec embeddings and clustering\ndemonstrate that these representations capture stylistic and structural\nfeatures that extend beyond composer identity.\n  These findings highlight the potential of graph-based methods as a\nstructured, cognitively informed framework for computational music analysis,\nenabling a more nuanced understanding of musical structure and style through\nthe lens of listener perception.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u7684\u8ba1\u7b97\u65b9\u6cd5\uff0c\u5c06\u97f3\u4e50\u8ba4\u77e5\u6a21\u578b\uff08\u5982I-R\u6a21\u578b\uff09\u5e94\u7528\u4e8e\u65cb\u5f8b\u5206\u6790\uff0c\u901a\u8fc7\u56fe\u7ed3\u6784\u7f16\u7801\u97f3\u4e50\u7684\u7ed3\u6784\u548c\u8ba4\u77e5\u4fe1\u606f\uff0c\u5e76\u4f7f\u7528\u56fe\u6838\u65b9\u6cd5\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u5728\u6355\u6349\u97f3\u4e50\u98ce\u683c\u548c\u7ed3\u6784\u7279\u5f81\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "motivation": "\u7406\u89e3\u97f3\u4e50\u4f5c\u54c1\u7684\u7ed3\u6784\u548c\u8ba4\u77e5\u57fa\u7840\u662f\u97f3\u4e50\u7406\u8bba\u548c\u8ba1\u7b97\u97f3\u4e50\u5b66\u7684\u5173\u952e\u6311\u6218\u3002\u4f20\u7edf\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u548c\u58f0\u548c\u8282\u594f\uff0c\u800c\u8ba4\u77e5\u6a21\u578b\u5982I-R\u6a21\u578b\u548cTemporal Gestalt\u7406\u8bba\u80fd\u591f\u63ed\u793a\u542c\u4f17\u5982\u4f55\u611f\u77e5\u548c\u9884\u671f\u97f3\u4e50\u7ed3\u6784\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u56fe\u7684\u8ba1\u7b97\u65b9\u6cd5\uff1a\u5c06\u65cb\u5f8b\u5206\u5272\u4e3a\u611f\u77e5\u5355\u5143\u5e76\u6807\u6ce8I-R\u6a21\u5f0f\uff1b\u4f7f\u7528\u52a8\u6001\u65f6\u95f4\u89c4\u6574\u6bd4\u8f83\u7247\u6bb5\uff1b\u6784\u5efak\u8fd1\u90bb\u56fe\u5efa\u6a21\u7247\u6bb5\u95f4\u5173\u7cfb\uff1b\u7528Schellenberg\u7684I-R\u6a21\u578b\u8ba1\u7b97\u65cb\u5f8b\u671f\u671b\u503c\uff1b\u5e94\u7528Weisfeiler-Lehman\u56fe\u6838\u6d4b\u91cf\u76f8\u4f3c\u6027\u3002", "result": "\u7ed3\u679c\u663e\u793a\u56fe\u5185\u548c\u56fe\u95f4\u7ed3\u6784\u5b58\u5728\u663e\u8457\u7edf\u8ba1\u5dee\u5f02\uff1b\u591a\u7ef4\u7f29\u653e\u5206\u6790\u8bc1\u5b9e\u56fe\u7ea7\u7ed3\u6784\u76f8\u4f3c\u6027\u53cd\u6620\u4e86\u7247\u6bb5\u7ea7\u611f\u77e5\u76f8\u4f3c\u6027\uff1bGraph2vec\u5d4c\u5165\u548c\u805a\u7c7b\u8868\u660e\u8fd9\u4e9b\u8868\u793a\u80fd\u591f\u6355\u6349\u8d85\u8d8a\u4f5c\u66f2\u5bb6\u8eab\u4efd\u7684\u98ce\u683c\u548c\u7ed3\u6784\u7279\u5f81\u3002", "conclusion": "\u57fa\u4e8e\u56fe\u7684\u65b9\u6cd5\u4e3a\u8ba1\u7b97\u97f3\u4e50\u5206\u6790\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7ed3\u6784\u5316\u3001\u8ba4\u77e5\u77e5\u60c5\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u542c\u4f17\u611f\u77e5\u7684\u89c6\u89d2\u5b9e\u73b0\u4e86\u5bf9\u97f3\u4e50\u7ed3\u6784\u548c\u98ce\u683c\u7684\u66f4\u7ec6\u81f4\u7406\u89e3\u3002"}}
{"id": "2510.27143", "categories": ["eess.AS", "cs.SD", "eess.SP"], "pdf": "https://arxiv.org/pdf/2510.27143", "abs": "https://arxiv.org/abs/2510.27143", "authors": ["Takahiro Iwami", "Naohisa Inoue", "Akira Omoto"], "title": "Beamforming in the Reproducing Kernel Domain Based on Spatial Differentiation", "comment": null, "summary": "This paper proposes a novel beamforming framework in the reproducing kernel\ndomain, derived from a unified interpretation of directional response as\nspatial differentiation of the sound field. By representing directional\nresponse using polynomial differential operators, the proposed method enables\nthe formulation of arbitrary beam patterns including non-axisymmetric. The\nderivation of the reproducing kernel associated with the interior fields is\nmathematically supported by Hobson's theorem, which allows concise analytical\nexpressions. Furthermore, the proposed framework generalizes conventional\nspherical harmonic domain beamformers by reinterpreting them as spatial\ndifferential operators, thereby clarifying their theoretical structure and\nextensibility. Three numerical simulations conducted in two-dimensional space\nconfirm the validity of the method.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u518d\u751f\u6838\u57df\u4e2d\u7684\u65b0\u578b\u6ce2\u675f\u6210\u5f62\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u65b9\u5411\u6027\u54cd\u5e94\u89e3\u91ca\u4e3a\u58f0\u573a\u7684\u7a7a\u95f4\u5fae\u5206\uff0c\u80fd\u591f\u6784\u5efa\u4efb\u610f\u6ce2\u675f\u6a21\u5f0f\uff08\u5305\u62ec\u975e\u8f74\u5bf9\u79f0\u6a21\u5f0f\uff09\u3002", "motivation": "\u4f20\u7edf\u6ce2\u675f\u6210\u5f62\u65b9\u6cd5\u5728\u6784\u5efa\u590d\u6742\u6ce2\u675f\u6a21\u5f0f\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u4e00\u79cd\u7edf\u4e00\u7684\u7406\u8bba\u6846\u67b6\u6765\u751f\u6210\u4efb\u610f\u65b9\u5411\u6027\u54cd\u5e94\uff0c\u5305\u62ec\u975e\u8f74\u5bf9\u79f0\u6a21\u5f0f\u3002", "method": "\u4f7f\u7528\u591a\u9879\u5f0f\u5fae\u5206\u7b97\u5b50\u8868\u793a\u65b9\u5411\u6027\u54cd\u5e94\uff0c\u57fa\u4e8eHobson\u5b9a\u7406\u63a8\u5bfc\u518d\u751f\u6838\uff0c\u5c06\u4f20\u7edf\u7403\u8c10\u57df\u6ce2\u675f\u6210\u5f62\u5668\u91cd\u65b0\u89e3\u91ca\u4e3a\u7a7a\u95f4\u5fae\u5206\u7b97\u5b50\u3002", "result": "\u5728\u4e8c\u7ef4\u7a7a\u95f4\u8fdb\u884c\u7684\u4e09\u4e2a\u6570\u503c\u6a21\u62df\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u80fd\u591f\u6210\u529f\u751f\u6210\u5404\u79cd\u6ce2\u675f\u6a21\u5f0f\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u6ce2\u675f\u6210\u5f62\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u7406\u8bba\u57fa\u7840\uff0c\u4e0d\u4ec5\u63a8\u5e7f\u4e86\u4f20\u7edf\u65b9\u6cd5\uff0c\u8fd8\u63ed\u793a\u4e86\u5176\u7406\u8bba\u7ed3\u6784\u548c\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2510.27040", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.27040", "abs": "https://arxiv.org/abs/2510.27040", "authors": ["Dian Chen", "Yunkai Chen", "Tong Lin", "Sijie Chen", "Xiaolin Cheng"], "title": "GeoPep: A geometry-aware masked language model for protein-peptide binding site prediction", "comment": "11 pages, 5 figures", "summary": "Multimodal approaches that integrate protein structure and sequence have\nachieved remarkable success in protein-protein interface prediction. However,\nextending these methods to protein-peptide interactions remains challenging due\nto the inherent conformational flexibility of peptides and the limited\navailability of structural data that hinder direct training of structure-aware\nmodels. To address these limitations, we introduce GeoPep, a novel framework\nfor peptide binding site prediction that leverages transfer learning from ESM3,\na multimodal protein foundation model. GeoPep fine-tunes ESM3's rich\npre-learned representations from protein-protein binding to address the limited\navailability of protein-peptide binding data. The fine-tuned model is further\nintegrated with a parameter-efficient neural network architecture capable of\nlearning complex patterns from sparse data. Furthermore, the model is trained\nusing distance-based loss functions that exploit 3D structural information to\nenhance binding site prediction. Comprehensive evaluations demonstrate that\nGeoPep significantly outperforms existing methods in protein-peptide binding\nsite prediction by effectively capturing sparse and heterogeneous binding\npatterns.", "AI": {"tldr": "GeoPep\u662f\u4e00\u4e2a\u57fa\u4e8eESM3\u591a\u6a21\u6001\u86cb\u767d\u8d28\u57fa\u7840\u6a21\u578b\u7684\u80bd\u7ed3\u5408\u4f4d\u70b9\u9884\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u8fc1\u79fb\u5b66\u4e60\u548c\u53c2\u6570\u9ad8\u6548\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u86cb\u767d\u8d28-\u80bd\u76f8\u4e92\u4f5c\u7528\u9884\u6d4b\u4e2d\u80bd\u6784\u8c61\u7075\u6d3b\u6027\u548c\u7ed3\u6784\u6570\u636e\u6709\u9650\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u6a21\u6001\u65b9\u6cd5\u5728\u86cb\u767d\u8d28-\u86cb\u767d\u8d28\u754c\u9762\u9884\u6d4b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u6269\u5c55\u5230\u86cb\u767d\u8d28-\u80bd\u76f8\u4e92\u4f5c\u7528\u9762\u4e34\u6311\u6218\uff0c\u4e3b\u8981\u7531\u4e8e\u80bd\u7684\u6784\u8c61\u7075\u6d3b\u6027\u548c\u7ed3\u6784\u6570\u636e\u6709\u9650\uff0c\u963b\u788d\u4e86\u7ed3\u6784\u611f\u77e5\u6a21\u578b\u7684\u76f4\u63a5\u8bad\u7ec3\u3002", "method": "\u901a\u8fc7\u4eceESM3\u591a\u6a21\u6001\u86cb\u767d\u8d28\u57fa\u7840\u6a21\u578b\u8fdb\u884c\u8fc1\u79fb\u5b66\u4e60\uff0c\u5fae\u8c03\u5176\u4e30\u5bcc\u7684\u9884\u5b66\u4e60\u8868\u793a\uff1b\u96c6\u6210\u53c2\u6570\u9ad8\u6548\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u5b66\u4e60\u7a00\u758f\u6570\u636e\u4e2d\u7684\u590d\u6742\u6a21\u5f0f\uff1b\u4f7f\u7528\u57fa\u4e8e\u8ddd\u79bb\u7684\u635f\u5931\u51fd\u6570\u5229\u75283D\u7ed3\u6784\u4fe1\u606f\u589e\u5f3a\u7ed3\u5408\u4f4d\u70b9\u9884\u6d4b\u3002", "result": "\u7efc\u5408\u8bc4\u4f30\u8868\u660e\uff0cGeoPep\u5728\u86cb\u767d\u8d28-\u80bd\u7ed3\u5408\u4f4d\u70b9\u9884\u6d4b\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u6355\u6349\u7a00\u758f\u548c\u5f02\u8d28\u7684\u7ed3\u5408\u6a21\u5f0f\u3002", "conclusion": "GeoPep\u901a\u8fc7\u8fc1\u79fb\u5b66\u4e60\u548c\u7ed3\u6784\u4fe1\u606f\u5229\u7528\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u86cb\u767d\u8d28-\u80bd\u76f8\u4e92\u4f5c\u7528\u9884\u6d4b\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u4e3a\u8fd9\u4e00\u9886\u57df\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.27198", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2510.27198", "abs": "https://arxiv.org/abs/2510.27198", "authors": ["Anselm Lohmann", "Tomohiro Nakatani", "Rintaro Ikeshita", "Marc Delcroix", "Shoko Araki", "Simon Doclo"], "title": "Reference Microphone Selection for Guided Source Separation based on the Normalized L-p Norm", "comment": null, "summary": "Guided Source Separation (GSS) is a popular front-end for distant automatic\nspeech recognition (ASR) systems using spatially distributed microphones. When\nconsidering spatially distributed microphones, the choice of reference\nmicrophone may have a large influence on the quality of the output signal and\nthe downstream ASR performance. In GSS-based speech enhancement, reference\nmicrophone selection is typically performed using the signal-to-noise ratio\n(SNR), which is optimal for noise reduction but may neglect differences in\nearly-to-late-reverberant ratio (ELR) across microphones. In this paper, we\npropose two reference microphone selection methods for GSS-based speech\nenhancement that are based on the normalized $\\ell_p$-norm, either using only\nthe normalized $\\ell_p$-norm or combining the normalized $\\ell_p$-norm and the\nSNR to account for both differences in SNR and ELR across microphones.\nExperimental evaluation using a CHiME-8 distant ASR system shows that the\nproposed $\\ell_p$-norm-based methods outperform the baseline method, reducing\nthe macro-average word error rate.", "AI": {"tldr": "\u63d0\u51fa\u4e24\u79cd\u57fa\u4e8e\u5f52\u4e00\u5316\u2113p\u8303\u6570\u7684\u53c2\u8003\u9ea6\u514b\u98ce\u9009\u62e9\u65b9\u6cd5\uff0c\u7528\u4e8eGSS\u8bed\u97f3\u589e\u5f3a\uff0c\u901a\u8fc7\u8003\u8651SNR\u548c\u65e9\u671f\u5230\u665a\u671f\u6df7\u54cd\u6bd4\u5dee\u5f02\u6765\u63d0\u5347ASR\u6027\u80fd", "motivation": "\u4f20\u7edfGSS\u4e2d\u57fa\u4e8eSNR\u7684\u53c2\u8003\u9ea6\u514b\u98ce\u9009\u62e9\u65b9\u6cd5\u867d\u7136\u5bf9\u964d\u566a\u6709\u6548\uff0c\u4f46\u53ef\u80fd\u5ffd\u7565\u4e86\u4e0d\u540c\u9ea6\u514b\u98ce\u95f4\u65e9\u671f\u5230\u665a\u671f\u6df7\u54cd\u6bd4\u7684\u5dee\u5f02\uff0c\u5f71\u54cd\u4e0b\u6e38ASR\u6027\u80fd", "method": "\u63d0\u51fa\u4e24\u79cd\u65b9\u6cd5\uff1a\u4ec5\u4f7f\u7528\u5f52\u4e00\u5316\u2113p\u8303\u6570\uff0c\u6216\u7ed3\u5408\u5f52\u4e00\u5316\u2113p\u8303\u6570\u548cSNR\uff0c\u4ee5\u540c\u65f6\u8003\u8651SNR\u548cELR\u5dee\u5f02", "result": "\u5728CHiME-8\u8fdc\u573aASR\u7cfb\u7edf\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u63d0\u51fa\u7684\u2113p\u8303\u6570\u65b9\u6cd5\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u964d\u4f4e\u4e86\u5e73\u5747\u8bcd\u9519\u8bef\u7387", "conclusion": "\u57fa\u4e8e\u2113p\u8303\u6570\u7684\u53c2\u8003\u9ea6\u514b\u98ce\u9009\u62e9\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u5347GSS\u8bed\u97f3\u589e\u5f3a\u6027\u80fd\uff0c\u6539\u5584\u4e0b\u6e38ASR\u7cfb\u7edf\u8868\u73b0"}}
{"id": "2510.27043", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.27043", "abs": "https://arxiv.org/abs/2510.27043", "authors": ["Hao Jiang", "Xiaojun Yuan", "Yinuo Huang", "Qinghua Guo"], "title": "Blind MIMO Semantic Communication via Parallel Variational Diffusion: A Completely Pilot-Free Approach", "comment": null, "summary": "In this paper, we propose a novel blind multi-input multi-output (MIMO)\nsemantic communication (SC) framework named Blind-MIMOSC that consists of a\ndeep joint source-channel coding (DJSCC) transmitter and a diffusion-based\nblind receiver. The DJSCC transmitter aims to compress and map the source data\ninto the transmitted signal by exploiting the structural characteristics of the\nsource data, while the diffusion-based blind receiver employs a parallel\nvariational diffusion (PVD) model to simultaneously recover the channel and the\nsource data from the received signal without using any pilots. The PVD model\nleverages two pre-trained score networks to characterize the prior information\nof the channel and the source data, operating in a plug-and-play manner during\ninference. This design allows only the affected network to be retrained when\nchannel conditions or source datasets change, avoiding the complicated\nfull-network retraining required by end-to-end methods. This work presents the\nfirst fully pilot-free solution for joint channel estimation and source\nrecovery in block-fading MIMO systems. Extensive experiments show that\nBlind-MIMOSC with PVD achieves superior channel and source recovery accuracy\ncompared to state-of-the-art approaches, with drastically reduced channel\nbandwidth ratio.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aBlind-MIMOSC\u7684\u76f2\u591a\u8f93\u5165\u591a\u8f93\u51fa\u8bed\u4e49\u901a\u4fe1\u6846\u67b6\uff0c\u5305\u542bDJSCC\u53d1\u5c04\u5668\u548c\u57fa\u4e8e\u6269\u6563\u7684\u76f2\u63a5\u6536\u5668\uff0c\u65e0\u9700\u5bfc\u9891\u5373\u53ef\u540c\u65f6\u6062\u590d\u4fe1\u9053\u548c\u6e90\u6570\u636e\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u901a\u4fe1\u7cfb\u7edf\u4e2d\u9700\u8981\u5bfc\u9891\u8fdb\u884c\u4fe1\u9053\u4f30\u8ba1\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u5b8c\u5168\u65e0\u5bfc\u9891\u7684\u8054\u5408\u4fe1\u9053\u4f30\u8ba1\u548c\u6e90\u6570\u636e\u6062\u590d\uff0c\u63d0\u9ad8\u9891\u8c31\u6548\u7387\u3002", "method": "\u4f7f\u7528\u6df1\u5ea6\u8054\u5408\u6e90\u4fe1\u9053\u7f16\u7801(DJSCC)\u53d1\u5c04\u5668\u538b\u7f29\u6e90\u6570\u636e\uff0c\u63a5\u6536\u5668\u91c7\u7528\u5e76\u884c\u53d8\u5206\u6269\u6563(PVD)\u6a21\u578b\uff0c\u5229\u7528\u4e24\u4e2a\u9884\u8bad\u7ec3\u8bc4\u5206\u7f51\u7edc\u5206\u522b\u8868\u5f81\u4fe1\u9053\u548c\u6e90\u6570\u636e\u7684\u5148\u9a8c\u4fe1\u606f\u3002", "result": "\u5728\u5757\u8870\u843dMIMO\u7cfb\u7edf\u4e2d\uff0cBlind-MIMOSC\u5b9e\u73b0\u4e86\u6bd4\u73b0\u6709\u65b9\u6cd5\u66f4\u4f18\u7684\u4fe1\u9053\u548c\u6e90\u6062\u590d\u7cbe\u5ea6\uff0c\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u4e86\u4fe1\u9053\u5e26\u5bbd\u6bd4\u3002", "conclusion": "\u8fd9\u662f\u9996\u4e2a\u5b8c\u5168\u65e0\u5bfc\u9891\u7684\u5757\u8870\u843dMIMO\u7cfb\u7edf\u8054\u5408\u4fe1\u9053\u4f30\u8ba1\u548c\u6e90\u6062\u590d\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u63d2\u62d4\u5f0f\u8bbe\u8ba1\u4f18\u52bf\uff0c\u53ea\u9700\u91cd\u65b0\u8bad\u7ec3\u53d7\u5f71\u54cd\u7f51\u7edc\u5373\u53ef\u9002\u5e94\u4fe1\u9053\u6216\u6e90\u6570\u636e\u53d8\u5316\u3002"}}
{"id": "2510.27069", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.27069", "abs": "https://arxiv.org/abs/2510.27069", "authors": ["Mohammad Hossein Shokouhi", "Vincent W. S. Wong"], "title": "Distributed Precoding for Cell-free Massive MIMO in O-RAN: A Multi-agent Deep Reinforcement Learning Framework", "comment": null, "summary": "Cell-free massive multiple-input multiple-output (MIMO) is a key technology\nfor next-generation wireless systems. The integration of cell-free massive MIMO\nwithin the open radio access network (O-RAN) architecture addresses the growing\nneed for decentralized, scalable, and high-capacity networks that can support\ndifferent use cases. Precoding is a crucial step in the operation of cell-free\nmassive MIMO, where O-RUs steer their beams towards the intended users while\nmitigating interference to other users. Current precoding schemes for cell-free\nmassive MIMO are either fully centralized or fully distributed. Centralized\nschemes are not scalable, whereas distributed schemes may lead to a high\ninter-O-RU interference. In this paper, we propose a distributed and scalable\nprecoding framework for cell-free massive MIMO that uses limited information\nexchange among precoding agents to mitigate interference. We formulate an\noptimization problem for precoding that maximizes the aggregate throughput\nwhile guaranteeing the minimum data rate requirements of users. The formulated\nproblem is nonconvex. We propose a multi-timescale framework that combines\nmulti-agent deep reinforcement learning (DRL) with expert insights from an\niterative algorithm to determine the precoding matrices efficiently. We conduct\nsimulations and compare the proposed framework with the centralized precoding\nand distributed precoding methods for different numbers of O-RUs, users, and\ntransmit antennas. The results show that the proposed framework achieves a\nhigher aggregate throughput than the distributed regularized zero-forcing\n(D-RZF) scheme and the weighted minimum mean square error (WMMSE) algorithm.\nWhen compared with the centralized regularized zero-forcing (C-RZF) scheme, the\nproposed framework achieves similar aggregate throughput performance but with a\nlower signaling overhead.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u65e0\u8702\u7a9d\u5927\u89c4\u6a21MIMO\u7cfb\u7edf\u7684\u5206\u5e03\u5f0f\u53ef\u6269\u5c55\u9884\u7f16\u7801\u6846\u67b6\uff0c\u901a\u8fc7\u6709\u9650\u4fe1\u606f\u4ea4\u6362\u548c\u591a\u667a\u80fd\u4f53\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u6765\u964d\u4f4e\u5e72\u6270\u5e76\u63d0\u9ad8\u805a\u5408\u541e\u5410\u91cf\u3002", "motivation": "\u89e3\u51b3\u5f53\u524d\u65e0\u8702\u7a9d\u5927\u89c4\u6a21MIMO\u9884\u7f16\u7801\u65b9\u6848\u4e2d\u96c6\u4e2d\u5f0f\u4e0d\u53ef\u6269\u5c55\u548c\u5206\u5e03\u5f0f\u5e72\u6270\u4e25\u91cd\u7684\u95ee\u9898\uff0c\u6ee1\u8db3\u4e0b\u4e00\u4ee3\u65e0\u7ebf\u7cfb\u7edf\u5bf9\u53bb\u4e2d\u5fc3\u5316\u3001\u53ef\u6269\u5c55\u548c\u9ad8\u5bb9\u91cf\u7f51\u7edc\u7684\u9700\u6c42\u3002", "method": "\u91c7\u7528\u591a\u65f6\u95f4\u5c3a\u5ea6\u6846\u67b6\uff0c\u7ed3\u5408\u591a\u667a\u80fd\u4f53\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u548c\u8fed\u4ee3\u7b97\u6cd5\u7684\u4e13\u5bb6\u6d1e\u5bdf\uff0c\u901a\u8fc7\u6709\u9650\u4fe1\u606f\u4ea4\u6362\u6765\u9ad8\u6548\u786e\u5b9a\u9884\u7f16\u7801\u77e9\u9635\u3002", "result": "\u4e0e\u5206\u5e03\u5f0f\u6b63\u5219\u5316\u8feb\u96f6\u548c\u52a0\u6743\u6700\u5c0f\u5747\u65b9\u8bef\u5dee\u7b97\u6cd5\u76f8\u6bd4\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u805a\u5408\u541e\u5410\u91cf\uff1b\u4e0e\u96c6\u4e2d\u5f0f\u6b63\u5219\u5316\u8feb\u96f6\u76f8\u6bd4\uff0c\u5728\u76f8\u4f3c\u541e\u5410\u91cf\u6027\u80fd\u4e0b\u5177\u6709\u66f4\u4f4e\u7684\u4fe1\u4ee4\u5f00\u9500\u3002", "conclusion": "\u63d0\u51fa\u7684\u5206\u5e03\u5f0f\u9884\u7f16\u7801\u6846\u67b6\u5728\u65e0\u8702\u7a9d\u5927\u89c4\u6a21MIMO\u7cfb\u7edf\u4e2d\u5b9e\u73b0\u4e86\u826f\u597d\u7684\u541e\u5410\u91cf\u6027\u80fd\u548c\u53ef\u6269\u5c55\u6027\uff0c\u662f\u96c6\u4e2d\u5f0f\u548c\u5206\u5e03\u5f0f\u65b9\u6848\u7684\u6709\u6548\u6298\u8877\u3002"}}
{"id": "2510.27078", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.27078", "abs": "https://arxiv.org/abs/2510.27078", "authors": ["Meles Weldegebriel", "Zihan Li", "Greg Hellbourg", "Ning Zhang", "Neal Patwari"], "title": "RFI Detection and Identification at OVRO Using Pseudonymetry", "comment": null, "summary": "Protecting radio astronomy observatories from unintended interference is\ncritical as wireless transmissions increases near protected bands. While\ndatabase-driven coordination frameworks and radio quiet zones exist, they\ncannot rapidly identify or suppress specific interfering transmitters,\nespecially at low signal-to-noise ratio (SNR) levels. This paper presents the\nfirst over-the-air field demonstration of Pseudonymetry at the Owens Valley\nRadio Observatory (OVRO), illustrating cooperative spectrum sharing between\nheterogeneous wireless systems. In our experiment, a narrow-band secondary\ntransmitter embeds a pseudonym watermark into its signal, while the wide-band\nradio telescope passively extracts the watermark from spectrogram data. Results\nshow that interference can be reliably detected and the interfering device\nuniquely identified even at low SNR where conventional demodulation is\ninfeasible. These findings validate that passive scientific receivers can\nparticipate in a lightweight feedback loop to trigger shutdown of harmful\ntransmissions, demonstrating the potential of Pseudonymetry as a complementary\nenforcement tool for protecting radio astronomy environments.", "AI": {"tldr": "\u5728Owens Valley\u5c04\u7535\u5929\u6587\u53f0\u8fdb\u884c\u7684Pseudonymetry\u73b0\u573a\u6f14\u793a\uff0c\u5c55\u793a\u4e86\u5f02\u6784\u65e0\u7ebf\u7cfb\u7edf\u95f4\u7684\u534f\u4f5c\u9891\u8c31\u5171\u4eab\uff0c\u901a\u8fc7\u6c34\u5370\u6280\u672f\u8bc6\u522b\u4f4e\u4fe1\u566a\u6bd4\u4e0b\u7684\u5e72\u6270\u6e90\u3002", "motivation": "\u968f\u7740\u65e0\u7ebf\u4f20\u8f93\u5728\u53d7\u4fdd\u62a4\u9891\u6bb5\u9644\u8fd1\u7684\u589e\u52a0\uff0c\u9700\u8981\u5feb\u901f\u8bc6\u522b\u548c\u6291\u5236\u7279\u5b9a\u5e72\u6270\u6e90\uff0c\u5c24\u5176\u662f\u5728\u4f4e\u4fe1\u566a\u6bd4\u6761\u4ef6\u4e0b\u3002", "method": "\u7a84\u5e26\u6b21\u7ea7\u53d1\u5c04\u5668\u5728\u4fe1\u53f7\u4e2d\u5d4c\u5165\u4f2a\u540d\u6c34\u5370\uff0c\u5bbd\u5e26\u5c04\u7535\u671b\u8fdc\u955c\u4ece\u9891\u8c31\u56fe\u4e2d\u88ab\u52a8\u63d0\u53d6\u6c34\u5370\u3002", "result": "\u5373\u4f7f\u5728\u4f20\u7edf\u89e3\u8c03\u4e0d\u53ef\u884c\u7684\u4f4e\u4fe1\u566a\u6bd4\u6761\u4ef6\u4e0b\uff0c\u4e5f\u80fd\u53ef\u9760\u68c0\u6d4b\u5e72\u6270\u5e76\u552f\u4e00\u8bc6\u522b\u5e72\u6270\u8bbe\u5907\u3002", "conclusion": "Pseudonymetry\u53ef\u4f5c\u4e3a\u4fdd\u62a4\u5c04\u7535\u5929\u6587\u73af\u5883\u7684\u8865\u5145\u6267\u6cd5\u5de5\u5177\uff0c\u4f7f\u88ab\u52a8\u79d1\u5b66\u63a5\u6536\u5668\u53c2\u4e0e\u8f7b\u91cf\u7ea7\u53cd\u9988\u5faa\u73af\u4ee5\u89e6\u53d1\u6709\u5bb3\u4f20\u8f93\u7684\u5173\u95ed\u3002"}}
{"id": "2510.27110", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.27110", "abs": "https://arxiv.org/abs/2510.27110", "authors": ["Gal Shtendel", "Ayush Bhandari"], "title": "Unlimited Sampling of Multiband Signals: Single-Channel Acquisition and Recovery", "comment": "IEEE Signal Processing Letters (in press)", "summary": "In this paper, we address the problem of reconstructing multiband signals\nfrom modulo-folded, pointwise samples within the Unlimited Sensing Framework\n(USF). Focusing on a low-complexity, single-channel acquisition setup, we\nestablish recovery guarantees demonstrating that sub-Nyquist sampling is\nachievable under the USF paradigm. In doing so, we also tighten the previous\nsampling theorem for bandpass signals. Our recovery algorithm demonstrates up\nto a 13x dynamic range improvement in hardware experiments with up to 6\nspectral bands. These results enable practical high-dynamic-range multiband\nacquisition in scenarios previously limited by dynamic range and excessive\noversampling.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5728\u65e0\u9650\u4f20\u611f\u6846\u67b6\u4e0b\u4ece\u6a21\u6298\u53e0\u70b9\u91c7\u6837\u4e2d\u91cd\u5efa\u591a\u9891\u5e26\u4fe1\u53f7\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4f4e\u590d\u6742\u5ea6\u5355\u901a\u9053\u91c7\u96c6\u65b9\u6848\uff0c\u5b9e\u73b0\u4e86\u4e9a\u5948\u594e\u65af\u7279\u91c7\u6837\u5e76\u6539\u8fdb\u4e86\u5e26\u901a\u4fe1\u53f7\u7684\u91c7\u6837\u5b9a\u7406\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u91c7\u6837\u4e2d\u52a8\u6001\u8303\u56f4\u53d7\u9650\u548c\u8fc7\u5ea6\u91c7\u6837\u7684\u95ee\u9898\uff0c\u5728\u65e0\u9650\u4f20\u611f\u6846\u67b6\u4e0b\u5b9e\u73b0\u9ad8\u52a8\u6001\u8303\u56f4\u7684\u591a\u9891\u5e26\u4fe1\u53f7\u91c7\u96c6\u3002", "method": "\u91c7\u7528\u4f4e\u590d\u6742\u5ea6\u5355\u901a\u9053\u91c7\u96c6\u8bbe\u7f6e\uff0c\u57fa\u4e8e\u6a21\u6298\u53e0\u70b9\u91c7\u6837\u6280\u672f\uff0c\u5efa\u7acb\u6062\u590d\u4fdd\u8bc1\u5e76\u5f00\u53d1\u6062\u590d\u7b97\u6cd5\u3002", "result": "\u786c\u4ef6\u5b9e\u9a8c\u663e\u793a\u52a8\u6001\u8303\u56f4\u63d0\u5347\u9ad8\u8fbe13\u500d\uff0c\u53ef\u5904\u7406\u591a\u8fbe6\u4e2a\u9891\u8c31\u5e26\uff0c\u5b9e\u73b0\u4e86\u4e9a\u5948\u594e\u65af\u7279\u91c7\u6837\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u5728\u5148\u524d\u53d7\u9650\u4e8e\u52a8\u6001\u8303\u56f4\u548c\u8fc7\u5ea6\u91c7\u6837\u7684\u573a\u666f\u4e2d\u5b9e\u73b0\u5b9e\u7528\u7684\u9ad8\u52a8\u6001\u8303\u56f4\u591a\u9891\u5e26\u91c7\u96c6\u3002"}}
{"id": "2510.27192", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.27192", "abs": "https://arxiv.org/abs/2510.27192", "authors": ["Haoran Yin", "Yanqun Tang", "Jun Xiong", "Fan Liu", "Yuanhan Ni", "Qu Luo", "Roberto Bomfin", "Marwa Chafii", "Marios Kountouris", "Christos Masouros"], "title": "From OFDM to AFDM: Enabling Adaptive Integrated Sensing and Communication in High-Mobility Scenarios", "comment": "Magazine paper submitted to IEEE", "summary": "Integrated sensing and communication (ISAC) is a key feature of\nnext-generation wireless networks, enabling a wide range of emerging\napplications such as vehicle-to-everything (V2X) and unmanned aerial vehicles\n(UAVs), which operate in high-mobility scenarios. Notably, the wireless\nchannels within these applications typically exhibit severe delay and Doppler\nspreads. The latter causes serious communication performance degradation in the\nOrthogonal Frequency-Division Multiplexing (OFDM) waveform that is widely\nadopted in current wireless networks. To address this challenge, the recently\nproposed Doppler-resilient affine frequency division multiplexing (AFDM)\nwaveform, which uses flexible chirp signals as subcarriers, shows great\npotential for achieving adaptive ISAC in high-mobility scenarios. This article\nprovides a comprehensive overview of AFDM-ISAC. We begin by presenting the\nfundamentals of AFDM-ISAC, highlighting its inherent frequency-modulated\ncontinuous-wave (FMCW)-like characteristics. Then, we explore its ISAC\nperformance limits by analyzing its diversity order, ambiguity function (AF),\nand Cramer-Rao Bound (CRB). Finally, we present several effective sensing\nalgorithms and opportunities for AFDM-ISAC, with the aim of sparking new ideas\nin this emerging field.", "AI": {"tldr": "AFDM-ISAC\u662f\u4e00\u79cd\u7528\u4e8e\u9ad8\u79fb\u52a8\u6027\u573a\u666f\u7684\u96c6\u6210\u611f\u77e5\u4e0e\u901a\u4fe1\u6280\u672f\uff0c\u901a\u8fc7\u4f7f\u7528\u5541\u557e\u4fe1\u53f7\u4f5c\u4e3a\u5b50\u8f7d\u6ce2\u6765\u89e3\u51b3OFDM\u5728\u591a\u666e\u52d2\u6269\u5c55\u4e0b\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\u3002", "motivation": "\u4e0b\u4e00\u4ee3\u65e0\u7ebf\u7f51\u7edc\u4e2d\u7684\u9ad8\u79fb\u52a8\u6027\u5e94\u7528\uff08\u5982V2X\u548c\u65e0\u4eba\u673a\uff09\u9762\u4e34\u4e25\u91cd\u7684\u5ef6\u8fdf\u548c\u591a\u666e\u52d2\u6269\u5c55\u95ee\u9898\uff0c\u4f20\u7edfOFDM\u6ce2\u5f62\u5728\u8fd9\u4e9b\u573a\u666f\u4e0b\u901a\u4fe1\u6027\u80fd\u4e25\u91cd\u4e0b\u964d\u3002", "method": "\u91c7\u7528\u591a\u666e\u52d2\u5f39\u6027\u4eff\u5c04\u9891\u5206\u590d\u7528(AFDM)\u6ce2\u5f62\uff0c\u4f7f\u7528\u7075\u6d3b\u7684\u5541\u557e\u4fe1\u53f7\u4f5c\u4e3a\u5b50\u8f7d\u6ce2\uff0c\u5177\u6709\u7c7b\u4f3cFMCW\u7684\u7279\u6027\u3002", "result": "AFDM-ISAC\u80fd\u591f\u5b9e\u73b0\u9ad8\u79fb\u52a8\u6027\u573a\u666f\u4e0b\u7684\u81ea\u9002\u5e94\u96c6\u6210\u611f\u77e5\u4e0e\u901a\u4fe1\uff0c\u901a\u8fc7\u5206\u6790\u5206\u96c6\u9636\u6570\u3001\u6a21\u7cca\u51fd\u6570\u548c\u514b\u62c9\u7f8e-\u7f57\u754c\u6765\u8bc4\u4f30\u5176\u6027\u80fd\u6781\u9650\u3002", "conclusion": "AFDM-ISAC\u4e3a\u9ad8\u79fb\u52a8\u6027\u573a\u666f\u4e0b\u7684\u96c6\u6210\u611f\u77e5\u4e0e\u901a\u4fe1\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u63d0\u51fa\u4e86\u6709\u6548\u7684\u611f\u77e5\u7b97\u6cd5\u548c\u672a\u6765\u53d1\u5c55\u673a\u4f1a\u3002"}}
{"id": "2510.27217", "categories": ["eess.SP", "cs.ET"], "pdf": "https://arxiv.org/pdf/2510.27217", "abs": "https://arxiv.org/abs/2510.27217", "authors": ["Boxuan Xie", "Lauri Mela", "Alexis A. Dowhuszko", "Yu Bai", "Zehui Xiong", "Zhu Han", "Dusit Niyato", "Riku J\u00e4ntti"], "title": "Joint Visible Light and Backscatter Communications for Proximity-Based Indoor Asset Tracking Enabled by Energy-Neutral Devices", "comment": "14 pages, 14 figures, 4 tables", "summary": "In next-generation wireless systems, providing location-based mobile\ncomputing services for energy-neutral devices has become a crucial objective\nfor the provision of sustainable Internet of Things (IoT). Visible light\npositioning (VLP) has gained great research attention as a complementary method\nto radio frequency (RF) solutions since it can leverage ubiquitous lighting\ninfrastructure. However, conventional VLP receivers often rely on\nphotodetectors or cameras that are power-hungry, complex, and expensive. To\naddress this challenge, we propose a hybrid indoor asset tracking system that\nintegrates visible light communication (VLC) and backscatter communication (BC)\nwithin a simultaneous lightwave information and power transfer (SLIPT)\nframework. We design a low-complexity and energy-neutral IoT node, namely\nbackscatter device (BD) which harvests energy from light-emitting diode (LED)\naccess points, and then modulates and reflects ambient RF carriers to indicate\nits location within particular VLC cells. We present a multi-cell VLC\ndeployment with frequency division multiplexing (FDM) method that mitigates\ninterference among LED access points by assigning them distinct frequency pairs\nbased on a four-color map scheduling principle. We develop a lightweight\nparticle filter (PF) tracking algorithm at an edge RF reader, where the fusion\nof proximity reports and the received backscatter signal strength are employed\nto track the BD. Experimental results show that this approach achieves the\npositioning error of 0.318 m at 50th percentile and 0.634 m at 90th percentile,\nwhile avoiding the use of complex photodetectors and active RF synthesizing\ncomponents at the energy-neutral IoT node. By demonstrating robust performance\nin multiple indoor trajectories, the proposed solution enables scalable,\ncost-effective, and energy-neutral indoor tracking for pervasive and\nedge-assisted IoT applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u96c6\u6210\u53ef\u89c1\u5149\u901a\u4fe1\u548c\u53cd\u5411\u6563\u5c04\u901a\u4fe1\u7684\u6df7\u5408\u5ba4\u5185\u8d44\u4ea7\u8ffd\u8e2a\u7cfb\u7edf\uff0c\u901a\u8fc7\u80fd\u91cf\u6536\u96c6\u5b9e\u73b0\u96f6\u529f\u8017\u7269\u8054\u7f51\u8282\u70b9\u5b9a\u4f4d", "motivation": "\u4e3a\u4e0b\u4e00\u4ee3\u65e0\u7ebf\u7cfb\u7edf\u4e2d\u80fd\u91cf\u4e2d\u6027\u7684\u7269\u8054\u7f51\u8bbe\u5907\u63d0\u4f9b\u53ef\u6301\u7eed\u7684\u4f4d\u7f6e\u670d\u52a1\uff0c\u89e3\u51b3\u4f20\u7edf\u53ef\u89c1\u5149\u5b9a\u4f4d\u63a5\u6536\u5668\u529f\u8017\u9ad8\u3001\u590d\u6742\u4e14\u6602\u8d35\u7684\u95ee\u9898", "method": "\u8bbe\u8ba1\u4f4e\u590d\u6742\u5ea6\u80fd\u91cf\u4e2d\u6027\u7269\u8054\u7f51\u8282\u70b9\uff0c\u5229\u7528LED\u63a5\u5165\u70b9\u6536\u96c6\u80fd\u91cf\u5e76\u8c03\u5236\u53cd\u5c04\u73af\u5883RF\u8f7d\u6ce2\uff1b\u91c7\u7528\u591a\u5c0f\u533aVLC\u90e8\u7f72\u548c\u9891\u5206\u590d\u7528\u65b9\u6cd5\u51cf\u5c11\u5e72\u6270\uff1b\u5f00\u53d1\u8f7b\u91cf\u7ea7\u7c92\u5b50\u6ee4\u6ce2\u8ddf\u8e2a\u7b97\u6cd5", "result": "\u5b9a\u4f4d\u8bef\u5dee\u572850%\u767e\u5206\u4f4d\u4e3a0.318\u7c73\uff0c90%\u767e\u5206\u4f4d\u4e3a0.634\u7c73\uff0c\u65e0\u9700\u590d\u6742\u5149\u7535\u63a2\u6d4b\u5668\u548c\u6709\u6e90RF\u5408\u6210\u7ec4\u4ef6", "conclusion": "\u8be5\u89e3\u51b3\u65b9\u6848\u4e3a\u666e\u53ca\u6027\u548c\u8fb9\u7f18\u8f85\u52a9\u7684\u7269\u8054\u7f51\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u3001\u6210\u672c\u6548\u76ca\u9ad8\u4e14\u80fd\u91cf\u4e2d\u6027\u7684\u5ba4\u5185\u8ffd\u8e2a\u80fd\u529b"}}
{"id": "2510.27270", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.27270", "abs": "https://arxiv.org/abs/2510.27270", "authors": ["Yida Zhang", "Qiuyan Liu", "Yuqi Xia", "Guoxu Xia", "Qiang Wang"], "title": "SIM-Assisted End-to-End Co-Frequency Co-Time Full-Duplex System", "comment": null, "summary": "To further suppress the inherent self-interference (SI) in co-frequency and\nco-time full-duplex (CCFD) systems, we propose integrating a stacked\nintelligent metasurface (SIM) into the RF front-end to enhance signal\nprocessing in the wave domain. Furthermore, an end-to-end (E2E) learning-based\nsignal processing method is adopted to control the metasurface. Specifically,\nthe real metasurface is abstracted as hidden layers of a network, thereby\nconstructing an electromagnetic neural network (EMNN) to enable driving control\nof the real communication system. Traditional communication tasks, such as\nchannel coding, modulation, precoding, combining, demodulation, and channel\ndecoding, are synchronously carried out during the electromagnetic (EM) forward\npropagation through the metasurface. Simulation results show that, benefiting\nfrom the additional wave-domain processing capability of the SIM, the\nSIM-assisted CCFD system achieves significantly reduced bit error rate (BER)\ncompared with conventional CCFD systems. Our study fully demonstrates the\npotential applications of EMNN and SIM-assisted E2E CCFD systems in\nnext-generation transceiver design.", "AI": {"tldr": "\u63d0\u51fa\u4f7f\u7528\u5806\u53e0\u667a\u80fd\u8d85\u8868\u9762(SIM)\u96c6\u6210\u5230\u5c04\u9891\u524d\u7aef\uff0c\u901a\u8fc7\u7535\u78c1\u795e\u7ecf\u7f51\u7edc(EMNN)\u63a7\u5236\u8d85\u8868\u9762\uff0c\u5728\u540c\u9891\u540c\u65f6\u5168\u53cc\u5de5\u7cfb\u7edf\u4e2d\u663e\u8457\u964d\u4f4e\u8bef\u7801\u7387\u3002", "motivation": "\u4e3a\u4e86\u8fdb\u4e00\u6b65\u6291\u5236\u540c\u9891\u540c\u65f6\u5168\u53cc\u5de5(CCFD)\u7cfb\u7edf\u4e2d\u7684\u56fa\u6709\u81ea\u5e72\u6270(SI)\uff0c\u9700\u8981\u589e\u5f3a\u6ce2\u57df\u4fe1\u53f7\u5904\u7406\u80fd\u529b\u3002", "method": "\u5c06\u771f\u5b9e\u8d85\u8868\u9762\u62bd\u8c61\u4e3a\u7f51\u7edc\u9690\u85cf\u5c42\u6784\u5efa\u7535\u78c1\u795e\u7ecf\u7f51\u7edc\uff0c\u91c7\u7528\u7aef\u5230\u7aef\u5b66\u4e60\u65b9\u6cd5\u63a7\u5236\u8d85\u8868\u9762\uff0c\u5728\u7535\u78c1\u524d\u5411\u4f20\u64ad\u8fc7\u7a0b\u4e2d\u540c\u6b65\u5b8c\u6210\u4f20\u7edf\u901a\u4fe1\u4efb\u52a1\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u663e\u793a\uff0c\u5f97\u76ca\u4e8eSIM\u7684\u989d\u5916\u6ce2\u57df\u5904\u7406\u80fd\u529b\uff0cSIM\u8f85\u52a9\u7684CCFD\u7cfb\u7edf\u76f8\u6bd4\u4f20\u7edfCCFD\u7cfb\u7edf\u5b9e\u73b0\u4e86\u663e\u8457\u964d\u4f4e\u7684\u8bef\u7801\u7387\u3002", "conclusion": "\u8be5\u7814\u7a76\u5145\u5206\u5c55\u793a\u4e86\u7535\u78c1\u795e\u7ecf\u7f51\u7edc\u548cSIM\u8f85\u52a9\u7684\u7aef\u5230\u7aefCCFD\u7cfb\u7edf\u5728\u4e0b\u4e00\u4ee3\u6536\u53d1\u5668\u8bbe\u8ba1\u4e2d\u7684\u6f5c\u5728\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2510.27345", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.27345", "abs": "https://arxiv.org/abs/2510.27345", "authors": ["Anders Malthe Westerkam", "Am\u00e9lia Struyf", "Dimitri Lederer", "Troels Pedersen", "Fran\u00e7ois Quitin"], "title": "Variational Bayesian Estimation of Low Earth Orbits for Satellite Communication", "comment": null, "summary": "Low-earth-orbit (LEO) satellite communication systems that use\nmillimeter-wave frequencies rely on large antenna arrays with hybrid\nanalog-digital architectures for rapid beam steering. LEO satellites are only\nvisible from the ground for short periods of times (a few tens of minutes) due\nto their high orbital speeds. This paper presents a variational message passing\nalgorithm for joint localization and beam tracking of a LEO satellite from a\nground station equipped with a hybrid transceiver architecture. The algorithm\nrelies on estimating the parameters of the orbit, which is modelled as\ncircular. Angles are then obtained from the orbit in a straightforward manner.\nSimulation results show that the proposed method is highly resilient to missed\ndetections, enables reliable satellite tracking even near the horizon, and\neffectively alleviates the ambiguities inherent in hybrid architectures.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u4f4e\u5730\u7403\u8f68\u9053\u536b\u661f\u8054\u5408\u5b9a\u4f4d\u4e0e\u6ce2\u675f\u8ddf\u8e2a\u7684\u53d8\u5206\u6d88\u606f\u4f20\u9012\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u901a\u8fc7\u4f30\u8ba1\u5706\u5f62\u8f68\u9053\u53c2\u6570\u6765\u83b7\u53d6\u89d2\u5ea6\u4fe1\u606f\uff0c\u5728\u6df7\u5408\u6536\u53d1\u5668\u67b6\u6784\u4e0b\u5177\u6709\u9ad8\u9c81\u68d2\u6027\u3002", "motivation": "\u4f4e\u5730\u7403\u8f68\u9053\u536b\u661f\u7531\u4e8e\u9ad8\u901f\u8fd0\u52a8\u5728\u5730\u9762\u53ef\u89c1\u65f6\u95f4\u77ed\uff0c\u9700\u8981\u5feb\u901f\u6ce2\u675f\u8f6c\u5411\u6280\u672f\u6765\u7ef4\u6301\u901a\u4fe1\uff0c\u800c\u6df7\u5408\u6a21\u62df-\u6570\u5b57\u67b6\u6784\u5b58\u5728\u56fa\u6709\u7684\u6a21\u7cca\u6027\u95ee\u9898\u9700\u8981\u89e3\u51b3\u3002", "method": "\u4f7f\u7528\u53d8\u5206\u6d88\u606f\u4f20\u9012\u7b97\u6cd5\uff0c\u901a\u8fc7\u5efa\u6a21\u536b\u661f\u4e3a\u5706\u5f62\u8f68\u9053\u6765\u4f30\u8ba1\u8f68\u9053\u53c2\u6570\uff0c\u4ece\u800c\u76f4\u63a5\u83b7\u53d6\u89d2\u5ea6\u4fe1\u606f\uff0c\u5b9e\u73b0\u8054\u5408\u5b9a\u4f4d\u548c\u6ce2\u675f\u8ddf\u8e2a\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5bf9\u6f0f\u68c0\u5177\u6709\u9ad8\u5ea6\u9c81\u68d2\u6027\uff0c\u5373\u4f7f\u5728\u63a5\u8fd1\u5730\u5e73\u7ebf\u65f6\u4e5f\u80fd\u53ef\u9760\u8ddf\u8e2a\u536b\u661f\uff0c\u5e76\u80fd\u6709\u6548\u7f13\u89e3\u6df7\u5408\u67b6\u6784\u7684\u6a21\u7cca\u6027\u95ee\u9898\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u53d8\u5206\u6d88\u606f\u4f20\u9012\u7b97\u6cd5\u4e3aLEO\u536b\u661f\u901a\u4fe1\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u8054\u5408\u5b9a\u4f4d\u548c\u6ce2\u675f\u8ddf\u8e2a\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u9002\u7528\u4e8e\u6df7\u5408\u6536\u53d1\u5668\u67b6\u6784\u3002"}}
{"id": "2510.27371", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.27371", "abs": "https://arxiv.org/abs/2510.27371", "authors": ["Sagar Dutta", "Banani Basu", "Fazal Ahmed Talukdar"], "title": "Classification of Lower Limb Activities Based on Discrete Wavelet Transform Using On-Body Creeping Wave Propagation", "comment": null, "summary": "This article investigates how the creeping wave propagation around the human\nthigh could be used to monitor the leg movements. The propagation path around\nthe human thigh gives information regarding leg motions that can be used for\nthe classification of activities. The variation of the transmission coefficient\nis measured between two on-body polyethylene terephthalate (PET) flexible\nantennas for six different leg-based activities that exhibit unique\ntime-varying signatures. A discrete wavelet transform (DWT) along with\ndifferent classifiers, such as support vector machine (SVM), decision trees,\nnaive Bayes, and K-nearest neighbors (KNN), is applied for feature extraction\nand classification to evaluate the efficiency for classifying different\nactivity signals. Additional algorithms, such as dynamic time warping (DTW) and\ndeep convolutional neural network (DCNN), have also been implemented, and in\neach case, SVM with DWT outperforms the others. Simulation to evaluate a\nspecific absorption rate (SAR) is carried out as the antenna is positioned on\nthe human thigh leaving no gap. The results show that the SAR is within the\nthreshold as per the Federal Communications Commission (FCC) standard.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5229\u7528\u4eba\u4f53\u5927\u817f\u5468\u56f4\u7684\u722c\u884c\u6ce2\u4f20\u64ad\u6765\u76d1\u6d4b\u817f\u90e8\u8fd0\u52a8\uff0c\u901a\u8fc7\u67d4\u6027\u5929\u7ebf\u6d4b\u91cf\u4f20\u8f93\u7cfb\u6570\u53d8\u5316\uff0c\u7ed3\u5408\u591a\u79cd\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u8fdb\u884c\u6d3b\u52a8\u5206\u7c7b\uff0c\u5176\u4e2dSVM\u4e0eDWT\u7ec4\u5408\u8868\u73b0\u6700\u4f73\uff0c\u4e14SAR\u503c\u7b26\u5408FCC\u6807\u51c6\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u5229\u7528\u4eba\u4f53\u5927\u817f\u5468\u56f4\u7684\u722c\u884c\u6ce2\u4f20\u64ad\u7279\u6027\u6765\u76d1\u6d4b\u817f\u90e8\u8fd0\u52a8\uff0c\u4e3a\u57fa\u4e8e\u8eab\u4f53\u7684\u6d3b\u52a8\u5206\u7c7b\u63d0\u4f9b\u65b0\u7684\u4f20\u611f\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u4e24\u4e2a\u67d4\u6027PET\u5929\u7ebf\u6d4b\u91cf\u4f20\u8f93\u7cfb\u6570\u53d8\u5316\uff0c\u5bf9\u516d\u79cd\u817f\u90e8\u6d3b\u52a8\u8fdb\u884c\u6d4b\u91cf\uff0c\u5e94\u7528DWT\u8fdb\u884c\u7279\u5f81\u63d0\u53d6\uff0c\u5e76\u6bd4\u8f83SVM\u3001\u51b3\u7b56\u6811\u3001\u6734\u7d20\u8d1d\u53f6\u65af\u3001KNN\u3001DTW\u548cDCNN\u7b49\u591a\u79cd\u5206\u7c7b\u5668\u7684\u6027\u80fd\u3002", "result": "SVM\u4e0eDWT\u7ec4\u5408\u5728\u6240\u6709\u5206\u7c7b\u5668\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u80fd\u591f\u6709\u6548\u533a\u5206\u4e0d\u540c\u7684\u817f\u90e8\u6d3b\u52a8\u3002SAR\u4eff\u771f\u7ed3\u679c\u663e\u793a\u7b26\u5408FCC\u5b89\u5168\u6807\u51c6\u3002", "conclusion": "\u722c\u884c\u6ce2\u4f20\u64ad\u53ef\u7528\u4e8e\u76d1\u6d4b\u817f\u90e8\u8fd0\u52a8\uff0cSVM\u4e0eDWT\u7ed3\u5408\u7684\u5206\u7c7b\u65b9\u6cd5\u6548\u679c\u6700\u597d\uff0c\u4e14\u5929\u7ebf\u8bbe\u8ba1\u7b26\u5408\u5b89\u5168\u6807\u51c6\uff0c\u4e3a\u53ef\u7a7f\u6234\u6d3b\u52a8\u76d1\u6d4b\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2510.27382", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.27382", "abs": "https://arxiv.org/abs/2510.27382", "authors": ["Sagar Dutta", "Banani Basu", "Fazal Ahmed Talukdar"], "title": "Classification of Induction Motor Fault and Imbalance Based on Vibration Signal Using Single Antenna's Reactive Near Field", "comment": null, "summary": "Early fault diagnosis is imperative for the proper functioning of rotating\nmachines. It can reduce economic losses in the industry due to unexpected\nfailures. Existing fault analysis methods are either expensive or demand\nexpertise for the installation of the sensors. This article proposes a novel\nmethod for the detection of bearing faults and imbalance in induction motors\nusing an antenna as the sensor, which is noninvasive and cost-efficient.\nTime-varying S11 is measured using an omnidirectional antenna, and it is seen\nthat the spectrogram of S11 shows unique characteristics for different fault\nconditions. The experimental setup has analytically evaluated the vibration\nfrequencies due to fault and validated the characteristic fault frequency by\napplying FFT analysis on the captured S11 data. This article has evaluated the\naverage power content of the detected signals at normal and different fault\nconditions. A deep learning model is used to classify the faults based on the\nreflection coefficient ( S11). It is found that classification accuracy of\n98.2% is achieved using both magnitude and phase of S11, 96% using the\nmagnitude of S11 and 92.1% using the phase of S11. The classification accuracy\nfor different operating frequencies, antenna location, and time windows are\nalso investigated.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u5929\u7ebf\u4f5c\u4e3a\u4f20\u611f\u5668\u68c0\u6d4b\u611f\u5e94\u7535\u673a\u8f74\u627f\u6545\u969c\u548c\u5931\u8861\u7684\u65b0\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u975e\u4fb5\u5165\u4e14\u6210\u672c\u6548\u76ca\u9ad8\uff0c\u901a\u8fc7\u5206\u6790S11\u53c2\u6570\u7684\u65f6\u53d8\u7279\u6027\u6765\u8bc6\u522b\u6545\u969c\uff0c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5206\u7c7b\u51c6\u786e\u7387\u8fbe\u523098.2%\u3002", "motivation": "\u73b0\u6709\u6545\u969c\u5206\u6790\u65b9\u6cd5\u8981\u4e48\u6602\u8d35\uff0c\u8981\u4e48\u9700\u8981\u4e13\u4e1a\u77e5\u8bc6\u5b89\u88c5\u4f20\u611f\u5668\uff0c\u9700\u8981\u5f00\u53d1\u975e\u4fb5\u5165\u4e14\u6210\u672c\u6548\u76ca\u9ad8\u7684\u65e9\u671f\u6545\u969c\u8bca\u65ad\u65b9\u6cd5\uff0c\u4ee5\u51cf\u5c11\u5de5\u4e1a\u610f\u5916\u6545\u969c\u9020\u6210\u7684\u7ecf\u6d4e\u635f\u5931\u3002", "method": "\u4f7f\u7528\u5168\u5411\u5929\u7ebf\u6d4b\u91cf\u65f6\u53d8S11\u53c2\u6570\uff0c\u901a\u8fc7\u5206\u6790S11\u9891\u8c31\u56fe\u8bc6\u522b\u4e0d\u540c\u6545\u969c\u7279\u5f81\uff0c\u5e94\u7528FFT\u5206\u6790\u9a8c\u8bc1\u7279\u5f81\u6545\u969c\u9891\u7387\uff0c\u5e76\u4f7f\u7528\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u57fa\u4e8e\u53cd\u5c04\u7cfb\u6570S11\u8fdb\u884c\u5206\u7c7b\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u4e0d\u540c\u6545\u969c\u6761\u4ef6\u4e0b\u7684\u7279\u5f81\u9891\u7387\uff0c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u4f7f\u7528S11\u5e45\u503c\u548c\u76f8\u4f4d\u7ec4\u5408\u7684\u5206\u7c7b\u51c6\u786e\u7387\u8fbe\u523098.2%\uff0c\u4ec5\u4f7f\u7528\u5e45\u503c\u65f6\u4e3a96%\uff0c\u4ec5\u4f7f\u7528\u76f8\u4f4d\u65f6\u4e3a92.1%\u3002", "conclusion": "\u5929\u7ebf\u4f20\u611f\u5668\u65b9\u6cd5\u4e3a\u65cb\u8f6c\u673a\u68b0\u7684\u65e9\u671f\u6545\u969c\u8bca\u65ad\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u3001\u975e\u4fb5\u5165\u4e14\u6210\u672c\u6548\u76ca\u9ad8\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u4e0d\u540c\u5de5\u4f5c\u9891\u7387\u3001\u5929\u7ebf\u4f4d\u7f6e\u548c\u65f6\u95f4\u7a97\u53e3\u4e0b\u90fd\u8868\u73b0\u51fa\u826f\u597d\u7684\u5206\u7c7b\u6027\u80fd\u3002"}}
{"id": "2510.27394", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.27394", "abs": "https://arxiv.org/abs/2510.27394", "authors": ["Yuhao Zhang", "Guangjin Pan", "Musa Furkan Keskin", "Ossi Kaltiokallio", "Mikko Valkama", "Henk Wymeersch"], "title": "UNILocPro: Unified Localization Integrating Model-Based Geometry and Channel Charting", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "In this paper, we propose a unified localization framework (called UNILocPro)\nthat integrates model-based localization and channel charting (CC) for mixed\nline-of-sight (LoS)/non-line-of-sight (NLoS) scenarios. Specifically, based on\nLoS/NLoS identification, an adaptive activation between the model-based and\nCC-based methods is conducted. Aiming for unsupervised learning, information\nobtained from the model-based method is utilized to train the CC model, where a\npairwise distance loss (involving a new dissimilarity metric design), a triplet\nloss (if timestamps are available), a LoS-based loss, and an optimal transport\n(OT)-based loss are jointly employed such that the global geometry can be well\npreserved. To reduce the training complexity of UNILocPro, we propose a\nlow-complexity implementation (called UNILoc), where the CC model is trained\nwith self-generated labels produced by a single pre-training OT transformation,\nwhich avoids iterative Sinkhorn updates involved in the OT-based loss\ncomputation. Extensive numerical experiments demonstrate that the proposed\nunified frameworks achieve significantly improved positioning accuracy compared\nto both model-based and CC-based methods. Notably, UNILocPro with timestamps\nattains performance on par with fully-supervised fingerprinting despite\noperating without labelled training data. It is also shown that the\nlow-complexity UNILoc can substantially reduce training complexity with only\nmarginal performance degradation.", "AI": {"tldr": "\u63d0\u51fa\u4e86UNILocPro\u7edf\u4e00\u5b9a\u4f4d\u6846\u67b6\uff0c\u7ed3\u5408\u6a21\u578b\u5b9a\u4f4d\u548c\u4fe1\u9053\u5236\u56fe\u65b9\u6cd5\uff0c\u7528\u4e8e\u6df7\u5408\u89c6\u8ddd/\u975e\u89c6\u8ddd\u573a\u666f\u3002\u901a\u8fc7\u81ea\u9002\u5e94\u6fc0\u6d3b\u673a\u5236\u548c\u65e0\u76d1\u7763\u5b66\u4e60\uff0c\u5728\u4fdd\u6301\u5168\u5c40\u51e0\u4f55\u7ed3\u6784\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8bad\u7ec3\u590d\u6742\u5ea6\u3002", "motivation": "\u89e3\u51b3\u6df7\u5408\u89c6\u8ddd/\u975e\u89c6\u8ddd\u573a\u666f\u4e0b\u7684\u5b9a\u4f4d\u95ee\u9898\uff0c\u7ed3\u5408\u6a21\u578b\u5b9a\u4f4d\u548c\u4fe1\u9053\u5236\u56fe\u7684\u4f18\u52bf\uff0c\u5b9e\u73b0\u65e0\u76d1\u7763\u5b66\u4e60\u4e0b\u7684\u9ad8\u7cbe\u5ea6\u5b9a\u4f4d\u3002", "method": "\u57fa\u4e8e\u89c6\u8ddd/\u975e\u89c6\u8ddd\u8bc6\u522b\u8fdb\u884c\u81ea\u9002\u5e94\u6fc0\u6d3b\uff1b\u4f7f\u7528\u6a21\u578b\u5b9a\u4f4d\u4fe1\u606f\u8bad\u7ec3\u4fe1\u9053\u5236\u56fe\u6a21\u578b\uff0c\u8054\u5408\u4f7f\u7528\u6210\u5bf9\u8ddd\u79bb\u635f\u5931\u3001\u4e09\u5143\u7ec4\u635f\u5931\u3001\u89c6\u8ddd\u635f\u5931\u548c\u6700\u4f18\u4f20\u8f93\u635f\u5931\uff1b\u63d0\u51fa\u4f4e\u590d\u6742\u5ea6\u7248\u672cUNILoc\uff0c\u901a\u8fc7\u5355\u6b21\u9884\u8bad\u7ec3\u6700\u4f18\u4f20\u8f93\u53d8\u6362\u751f\u6210\u81ea\u6807\u7b7e\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u7edf\u4e00\u6846\u67b6\u76f8\u6bd4\u5355\u72ec\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u5b9a\u4f4d\u7cbe\u5ea6\uff1bUNILocPro\u5728\u65e0\u6807\u7b7e\u6570\u636e\u4e0b\u8fbe\u5230\u4e0e\u5168\u76d1\u7763\u6307\u7eb9\u5b9a\u4f4d\u76f8\u5f53\u7684\u6027\u80fd\uff1bUNILoc\u5927\u5e45\u964d\u4f4e\u8bad\u7ec3\u590d\u6742\u5ea6\u4e14\u6027\u80fd\u635f\u5931\u5f88\u5c0f\u3002", "conclusion": "\u63d0\u51fa\u7684\u7edf\u4e00\u6846\u67b6\u5728\u6df7\u5408\u89c6\u8ddd/\u975e\u89c6\u8ddd\u573a\u666f\u4e0b\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u65e0\u76d1\u7763\u5b9a\u4f4d\uff0c\u4f4e\u590d\u6742\u5ea6\u7248\u672c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u5f00\u9500\u3002"}}
{"id": "2510.27408", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.27408", "abs": "https://arxiv.org/abs/2510.27408", "authors": ["Nelson Matti\u00e9", "Arturo Sanchez-Azofeifa", "Pablo Crespo-Peremarch", "Juan-Ygnacio L\u00f3pez-Hern\u00e1ndez"], "title": "Estimation of aboveground biomass in a tropical dry forest: An intercomparison of airborne, unmanned, and space laser scanning", "comment": "32 pages, 17 figures, research paper", "summary": "According to the Paris Climate Change Agreement, all nations are required to\nsubmit reports on their greenhouse gas emissions and absorption every two years\nby 2024. Consequently, forests play a crucial role in reducing carbon\nemissions, which is essential for meeting these obligations. Recognizing the\nsignificance of forest conservation in the global battle against climate\nchange, Article 5 of the Paris Agreement emphasizes the need for high-quality\nforest data. This study focuses on enhancing methods for mapping aboveground\nbiomass in tropical dry forests. Tropical dry forests are considered one of the\nleast understood tropical forest environments; therefore, there is a need for\naccurate approaches to estimate carbon pools. We employ a comparative analysis\nof AGB estimates, utilizing different discrete and full-waveform laser scanning\ndatasets in conjunction with Ordinary Least Squares and Bayesian approaches\nSVM. Airborne Laser Scanning, Unmanned Laser Scanning, and Space Laser Scanning\nwere used as independent variables for extracting forest metrics. Variable\nselection, SVM regression tuning, and cross-validation via a machine-learning\napproach were applied to account for overfitting and underfitting. The results\nindicate that six key variables primarily related to tree height: Elev.minimum,\nElev.L3, lev.MAD.mode, Elev.mode, Elev.MAD.median, and Elev.skewness, are\nimportant for AGB estimation using ALSD and ULSD , while Leaf Area Index,\ncanopy coverage and height, terrain elevation, and full-waveform signal energy\nemerged as the most vital variables. AGB values estimated from ten permanent\ntropical dry forest plots in Costa Rica Guanacaste province ranged from 26.02\nMg/ha to 175.43 Mg/ha . The SVM regressions demonstrated a 17.89 error across\nall laser scanning systems, with SLSF W exhibiting the lowest error 17.07 in\nestimating total biomass per plot.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u6bd4\u8f83\u5206\u6790\u4e0d\u540c\u6fc0\u5149\u626b\u63cf\u6570\u636e\u4e0e\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u6539\u8fdb\u4e86\u70ed\u5e26\u5e72\u65f1\u68ee\u6797\u5730\u4e0a\u751f\u7269\u91cf\u7684\u4f30\u7b97\u65b9\u6cd5\uff0c\u652f\u6301\u5df4\u9ece\u534f\u5b9a\u5bf9\u9ad8\u8d28\u91cf\u68ee\u6797\u6570\u636e\u7684\u9700\u6c42\u3002", "motivation": "\u6839\u636e\u5df4\u9ece\u6c14\u5019\u534f\u5b9a\uff0c\u5404\u56fd\u9700\u8981\u5b9a\u671f\u62a5\u544a\u6e29\u5ba4\u6c14\u4f53\u6392\u653e\u548c\u5438\u6536\u60c5\u51b5\uff0c\u68ee\u6797\u5728\u78b3\u51cf\u6392\u4e2d\u8d77\u5173\u952e\u4f5c\u7528\u3002\u70ed\u5e26\u5e72\u65f1\u68ee\u6797\u662f\u6700\u4e0d\u4e86\u89e3\u7684\u70ed\u5e26\u68ee\u6797\u73af\u5883\u4e4b\u4e00\uff0c\u9700\u8981\u51c6\u786e\u4f30\u7b97\u78b3\u5e93\u7684\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u79bb\u6563\u548c\u5168\u6ce2\u5f62\u6fc0\u5149\u626b\u63cf\u6570\u636e\uff08\u673a\u8f7d\u3001\u65e0\u4eba\u673a\u548c\u7a7a\u95f4\u6fc0\u5149\u626b\u63cf\uff09\uff0c\u7ed3\u5408\u666e\u901a\u6700\u5c0f\u4e8c\u4e58\u6cd5\u548c\u8d1d\u53f6\u65af\u652f\u6301\u5411\u91cf\u673a\u65b9\u6cd5\u3002\u5e94\u7528\u53d8\u91cf\u9009\u62e9\u3001SVM\u56de\u5f52\u8c03\u4f18\u548c\u673a\u5668\u5b66\u4e60\u4ea4\u53c9\u9a8c\u8bc1\u6765\u5904\u7406\u8fc7\u62df\u5408\u548c\u6b20\u62df\u5408\u95ee\u9898\u3002", "result": "\u8bc6\u522b\u51fa6\u4e2a\u4e0e\u6811\u9ad8\u76f8\u5173\u7684\u5173\u952e\u53d8\u91cf\u5bf9AGB\u4f30\u7b97\u5f88\u91cd\u8981\u3002\u5728\u54e5\u65af\u8fbe\u9ece\u52a010\u4e2a\u70ed\u5e26\u5e72\u65f1\u68ee\u6797\u6837\u5730\u7684AGB\u4f30\u7b97\u503c\u4e3a26.02-175.43 Mg/ha\u3002SVM\u56de\u5f52\u5728\u6240\u6709\u6fc0\u5149\u626b\u63cf\u7cfb\u7edf\u4e2d\u8bef\u5dee\u4e3a17.89%\uff0c\u5176\u4e2d\u7a7a\u95f4\u6fc0\u5149\u626b\u63cf\u5168\u6ce2\u5f62\u6570\u636e\u8bef\u5dee\u6700\u4f4e\uff0817.07%\uff09\u3002", "conclusion": "\u6fc0\u5149\u626b\u63cf\u6280\u672f\u7ed3\u5408\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u53ef\u4ee5\u6709\u6548\u4f30\u7b97\u70ed\u5e26\u5e72\u65f1\u68ee\u6797\u7684\u5730\u4e0a\u751f\u7269\u91cf\uff0c\u4e3a\u5c65\u884c\u5df4\u9ece\u534f\u5b9a\u62a5\u544a\u4e49\u52a1\u63d0\u4f9b\u53ef\u9760\u6570\u636e\u652f\u6301\u3002"}}
{"id": "2510.27503", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.27503", "abs": "https://arxiv.org/abs/2510.27503", "authors": ["Anubhab Ghosh", "Yonina C. Eldar", "Saikat Chatterjee"], "title": "pDANSE: Particle-based Data-driven Nonlinear State Estimation from Nonlinear Measurements", "comment": "11 pages, 10 figures, under review at IEEE Transactions on Signal\n  Processing", "summary": "We consider the problem of designing a data-driven nonlinear state estimation\n(DANSE) method that uses (noisy) nonlinear measurements of a process whose\nunderlying state transition model (STM) is unknown. Such a process is referred\nto as a model-free process. A recurrent neural network (RNN) provides\nparameters of a Gaussian prior that characterize the state of the model-free\nprocess, using all previous measurements at a given time point. In the case of\nDANSE, the measurement system was linear, leading to a closed-form solution for\nthe state posterior. However, the presence of a nonlinear measurement system\nrenders a closed-form solution infeasible. Instead, the second-order statistics\nof the state posterior are computed using the nonlinear measurements observed\nat the time point. We address the nonlinear measurements using a\nreparameterization trick-based particle sampling approach, and estimate the\nsecond-order statistics of the state posterior. The proposed method is referred\nto as particle-based DANSE (pDANSE). The RNN of pDANSE uses sequential\nmeasurements efficiently and avoids the use of computationally intensive\nsequential Monte-Carlo (SMC) and/or ancestral sampling. We describe the\nsemi-supervised learning method for pDANSE, which transitions to unsupervised\nlearning in the absence of labeled data. Using a stochastic Lorenz-$63$ system\nas a benchmark process, we experimentally demonstrate the state estimation\nperformance for four nonlinear measurement systems. We explore cubic\nnonlinearity and a camera-model nonlinearity where unsupervised learning is\nused; then we explore half-wave rectification nonlinearity and\nCartesian-to-spherical nonlinearity where semi-supervised learning is used. The\nperformance of state estimation is shown to be competitive vis-\\`a-vis particle\nfilters that have complete knowledge of the STM of the Lorenz-$63$ system.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7c92\u5b50\u7684\u6570\u636e\u9a71\u52a8\u975e\u7ebf\u6027\u72b6\u6001\u4f30\u8ba1\u65b9\u6cd5(pDANSE)\uff0c\u7528\u4e8e\u5904\u7406\u72b6\u6001\u8f6c\u79fb\u6a21\u578b\u672a\u77e5\u4e14\u6d4b\u91cf\u7cfb\u7edf\u975e\u7ebf\u6027\u7684\u8fc7\u7a0b\u3002\u8be5\u65b9\u6cd5\u4f7f\u7528RNN\u751f\u6210\u9ad8\u65af\u5148\u9a8c\uff0c\u5e76\u901a\u8fc7\u91cd\u53c2\u6570\u5316\u6280\u5de7\u8fdb\u884c\u7c92\u5b50\u91c7\u6837\u6765\u4f30\u8ba1\u72b6\u6001\u540e\u9a8c\u7684\u4e8c\u9636\u7edf\u8ba1\u91cf\u3002", "motivation": "\u73b0\u6709DANSE\u65b9\u6cd5\u4ec5\u9002\u7528\u4e8e\u7ebf\u6027\u6d4b\u91cf\u7cfb\u7edf\uff0c\u5f53\u6d4b\u91cf\u7cfb\u7edf\u975e\u7ebf\u6027\u65f6\u65e0\u6cd5\u83b7\u5f97\u95ed\u5f0f\u89e3\u3002\u9700\u8981\u5f00\u53d1\u80fd\u591f\u5904\u7406\u975e\u7ebf\u6027\u6d4b\u91cf\u4e14\u65e0\u9700\u72b6\u6001\u8f6c\u79fb\u6a21\u578b\u77e5\u8bc6\u7684\u72b6\u6001\u4f30\u8ba1\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528RNN\u57fa\u4e8e\u5386\u53f2\u6d4b\u91cf\u751f\u6210\u9ad8\u65af\u5148\u9a8c\uff0c\u901a\u8fc7\u91cd\u53c2\u6570\u5316\u6280\u5de7\u8fdb\u884c\u7c92\u5b50\u91c7\u6837\u6765\u4f30\u8ba1\u975e\u7ebf\u6027\u6d4b\u91cf\u4e0b\u7684\u72b6\u6001\u540e\u9a8c\u7edf\u8ba1\u91cf\uff0c\u907f\u514d\u4e86\u8ba1\u7b97\u5bc6\u96c6\u7684\u5e8f\u5217\u8499\u7279\u5361\u6d1b\u65b9\u6cd5\u3002", "result": "\u5728\u968f\u673aLorenz-63\u7cfb\u7edf\u4e0a\u6d4b\u8bd5\u4e86\u56db\u79cd\u975e\u7ebf\u6027\u6d4b\u91cf\u7cfb\u7edf\uff0c\u5305\u62ec\u7acb\u65b9\u975e\u7ebf\u6027\u3001\u76f8\u673a\u6a21\u578b\u975e\u7ebf\u6027\u3001\u534a\u6ce2\u6574\u6d41\u975e\u7ebf\u6027\u548c\u7b1b\u5361\u5c14\u5230\u7403\u5750\u6807\u975e\u7ebf\u6027\u3002\u72b6\u6001\u4f30\u8ba1\u6027\u80fd\u4e0e\u5b8c\u5168\u77e5\u9053\u72b6\u6001\u8f6c\u79fb\u6a21\u578b\u7684\u7c92\u5b50\u6ee4\u6ce2\u5668\u76f8\u5f53\u3002", "conclusion": "pDANSE\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5904\u7406\u975e\u7ebf\u6027\u6d4b\u91cf\u7cfb\u7edf\u7684\u72b6\u6001\u4f30\u8ba1\u95ee\u9898\uff0c\u5728\u65e0\u9700\u72b6\u6001\u8f6c\u79fb\u6a21\u578b\u77e5\u8bc6\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e0e\u57fa\u4e8e\u5b8c\u6574\u6a21\u578b\u77e5\u8bc6\u7684\u7c92\u5b50\u6ee4\u6ce2\u5668\u76f8\u7ade\u4e89\u7684\u6027\u80fd\u3002"}}
{"id": "2510.27576", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.27576", "abs": "https://arxiv.org/abs/2510.27576", "authors": ["Leatile Marata", "Mariona Jaramillo-Civill", "Tales Imbiriba", "Petri V\u00e4lisuo", "Heidi Kuusniemi", "Elena Simona Lohan", "Pau Closas"], "title": "Trends and Challenges in Next-Generation GNSS Interference Management", "comment": "Submitted to AESM", "summary": "The global navigation satellite system (GNSS) continues to evolve in order to\nmeet the demands of emerging applications such as autonomous driving and smart\nenvironmental monitoring. However, these advancements are accompanied by a rise\nin interference threats, which can significantly compromise the reliability and\nsafety of GNSS. Such interference problems are typically addressed through\nsignal-processing techniques that rely on physics-based mathematical models.\nUnfortunately, solutions of this nature can often fail to fully capture the\ncomplex forms of interference. To address this, artificial intelligence\n(AI)-inspired solutions are expected to play a key role in future interference\nmanagement solutions, thanks to their ability to exploit data in addition to\nphysics-based models. This magazine paper discusses the main challenges and\ntasks required to secure GNSS and present a research vision on how AI can be\nleveraged towards achieving more robust GNSS-based positioning.", "AI": {"tldr": "\u672c\u6587\u8ba8\u8bba\u4e86\u5168\u7403\u5bfc\u822a\u536b\u661f\u7cfb\u7edf\u9762\u4e34\u7684\u5e72\u6270\u5a01\u80c1\uff0c\u63d0\u51fa\u4eba\u5de5\u667a\u80fd\u65b9\u6cd5\u53ef\u4ee5\u589e\u5f3aGNSS\u7684\u6297\u5e72\u6270\u80fd\u529b\uff0c\u5b9e\u73b0\u66f4\u7a33\u5065\u7684\u5b9a\u4f4d\u3002", "motivation": "\u968f\u7740GNSS\u5728\u81ea\u52a8\u9a7e\u9a76\u7b49\u65b0\u5174\u5e94\u7528\u4e2d\u7684\u53d1\u5c55\uff0c\u5e72\u6270\u5a01\u80c1\u65e5\u76ca\u4e25\u91cd\uff0c\u4f20\u7edf\u57fa\u4e8e\u7269\u7406\u6a21\u578b\u7684\u4fe1\u53f7\u5904\u7406\u65b9\u6cd5\u96be\u4ee5\u5b8c\u5168\u5e94\u5bf9\u590d\u6742\u5e72\u6270\u5f62\u5f0f\u3002", "method": "\u91c7\u7528\u4eba\u5de5\u667a\u80fd\u65b9\u6cd5\uff0c\u7ed3\u5408\u6570\u636e\u9a71\u52a8\u548c\u7269\u7406\u6a21\u578b\uff0c\u5f00\u53d1\u66f4\u6709\u6548\u7684\u5e72\u6270\u7ba1\u7406\u89e3\u51b3\u65b9\u6848\u3002", "result": "AI\u65b9\u6cd5\u80fd\u591f\u66f4\u597d\u5730\u6355\u6349\u590d\u6742\u5e72\u6270\u6a21\u5f0f\uff0c\u63d0\u5347GNSS\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u548c\u5b89\u5168\u6027\u3002", "conclusion": "\u4eba\u5de5\u667a\u80fd\u5c06\u5728\u672a\u6765GNSS\u5e72\u6270\u7ba1\u7406\u4e2d\u53d1\u6325\u5173\u952e\u4f5c\u7528\uff0c\u4e3a\u5b9e\u73b0\u66f4\u7a33\u5065\u7684\u5b9a\u4f4d\u63d0\u4f9b\u65b0\u7684\u7814\u7a76\u65b9\u5411\u3002"}}

{"id": "2511.00225", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.00225", "abs": "https://arxiv.org/abs/2511.00225", "authors": ["Pinjun Zheng", "Md. Jahangir Hossain", "Anas Chaaban"], "title": "Model-Free Channel Estimation for Massive MIMO: A Channel Charting-Inspired Approach", "comment": null, "summary": "Channel estimation is fundamental to wireless communications, yet it becomes\nincreasingly challenging in massive multiple-input multiple-output (MIMO)\nsystems where base stations employ hundreds of antennas. Traditional\nleast-squares methods require prohibitive pilot overhead that scales with\nantenna count, while sparse estimation methods depend on precise channel models\nthat may not always be practical. This paper proposes a model-free approach\ncombining deep autoencoders and LSTM networks. The method first learns\nlow-dimensional channel representations preserving temporal correlation through\naugmenting a channel charting-inspired loss function, then tracks these\nfeatures to recover full channel information from limited pilots. Simulation\nresults using ray-tracing datasets show that the proposed approach achieves up\nto 9 dB improvement in normalized mean square error compared to the\nleast-squares methods under ill-conditioned scenarios, while maintaining\nscalability across MIMO configurations."}
{"id": "2511.00482", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.00482", "abs": "https://arxiv.org/abs/2511.00482", "authors": ["Ying Zhang", "Fan Liu", "Yifeng Xiong", "Tao Liu", "Shi Jin"], "title": "Discrete-Periodic Ambiguity Function of Random Communication Signals", "comment": "5 pages, 2 figures, submitted to ICASSP 2026 for possible publication", "summary": "This paper investigates the ambiguity function (AF) of communication signals\ncarrying random data payloads, which is a fundamental metric characterizing\nsensing capability in ISAC systems. We first develop a unified analytical\nframework to evaluate the AF of communication-centric ISAC signals constructed\nfrom arbitrary orthonormal bases and independent identically distributed\n(i.i.d.) constellation symbols. Subsequently, we derive the discrete periodic\nambiguity function (DP-AF) and provide closed-form expressions for its expected\nintegrated sidelobe level (EISL) and average sidelobe level. Notably, we prove\nthat the normalized EISL is invariant across all constellations and modulation\nbases. Finally, the theoretical findings are validated through simulations."}
{"id": "2511.00491", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.00491", "abs": "https://arxiv.org/abs/2511.00491", "authors": ["Leatile Marata", "Juhani Sankari", "Eslam Eldeeb", "Mikko Valkama", "Elena Simona Lohan"], "title": "Meta-Learning Based Radio Frequency Fingerprinting for GNSS Spoofing Detection", "comment": null, "summary": "The rapid development of technology has led to an increase in the number of\ndevices that rely on position, velocity, and time (PVT) information to perform\ntheir functions. As such, the Global Navigation Satellite Systems (GNSS) have\nbeen adopted as one of the most promising solutions to provide PVT.\nConsequently, there are renewed efforts aimed at enhancing GNSS capabilities to\nmeet emerging use cases and their requirements. For example, GNSS is evolving\nto rely on low-earth-orbit satellites, shifting the focus from traditional\nmedium-earth-orbit satellites. Unfortunately, these developments also bring\nforth higher risks of interference signals such as spoofers, which pose serious\nsecurity threats. To address this challenge, artificial intelligence\n(AI)-inspired solutions are being developed to overcome the limitations of\nconventional mathematics-based approaches, which have proven inflexible when\ndealing with diverse forms of interference. In this paper, we advance this\ndirection by proposing a meta-learning framework that enables GNSS receivers to\ndetect various types of spoofers. Specifically, our approach exploits the radio\nfrequency fingerprints present in the signal at both the pre-correlation and\npost-correlation stages of the receiver. The proposed solution has superior\ngeneralization properties compared to the state-of-the-art solutions. Numerical\nresults demonstrate that our proposed solution significantly detects spoofers\nof different forms, with spoofing detection accuracies of more than 95% on\nmultiple datasets from the Texas Spoofing Test Battery (TEXBAT) and the Oak\nRidge Spoofing and Interference Test Battery (OAKBAT) repositories"}
{"id": "2511.00494", "categories": ["eess.SP", "cs.AI", "94-11", "H.4.3; I.6.3"], "pdf": "https://arxiv.org/pdf/2511.00494", "abs": "https://arxiv.org/abs/2511.00494", "authors": ["Ljupcho Milosheski", "Kuon Akiyama", "Blaž Bertalanič", "Jernej Hribar", "Ryoichi Shinkuma"], "title": "A Multimodal Dataset for Indoor Radio Mapping with 3D Point Clouds and RSSI", "comment": "11 pages, 7 figures, 3 tables, under review to Nature Scientific Data", "summary": "The growing number of smart devices supporting bandwidth-intensive and\nlatency-sensitive applications, such as real-time video analytics, smart\nsensing, and Extended Reality (XR), necessitates reliable wireless connectivity\nin indoor environments. Therein, accurate estimation of Radio Environment Maps\n(REMs) enables adaptive wireless network planning and optimization of Access\nPoint (AP) placement. However, generating realistic REMs remains challenging\ndue to the complexity of indoor spaces. To overcome this challenge, this paper\nintroduces a multimodal dataset that integrates high-resolution 3D LiDAR scans\nwith Wi-Fi Received Signal Strength Indicator (RSSI) measurements collected\nunder 20 distinct AP configurations in a multi-room indoor environment. The\ndataset captures two measurement scenarios: the first without human presence in\nthe environment, and the second with human presence. Thus, the presented\ndataset supports the study of dynamic environmental effects on wireless signal\npropagation. This resource is designed to facilitate research in data-driven\nwireless modeling, particularly in the context of emerging high-frequency\nstandards such as IEEE 802.11be (Wi-Fi 7), and aims to advance the development\nof robust, high-capacity indoor communication systems."}
{"id": "2511.00402", "categories": ["cs.SD", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00402", "abs": "https://arxiv.org/abs/2511.00402", "authors": ["Lucky Onyekwelu-Udoka", "Md Shafiqul Islam", "Md Shahedul Hasan"], "title": "Emotion Detection in Speech Using Lightweight and Transformer-Based Models: A Comparative and Ablation Study", "comment": null, "summary": "Emotion recognition from speech plays a vital role in the development of\nempathetic human-computer interaction systems. This paper presents a\ncomparative analysis of lightweight transformer-based models, DistilHuBERT and\nPaSST, by classifying six core emotions from the CREMA-D dataset. We benchmark\ntheir performance against a traditional CNN-LSTM baseline model using MFCC\nfeatures. DistilHuBERT demonstrates superior accuracy (70.64%) and F1 score\n(70.36%) while maintaining an exceptionally small model size (0.02 MB),\noutperforming both PaSST and the baseline. Furthermore, we conducted an\nablation study on three variants of the PaSST, Linear, MLP, and Attentive\nPooling heads, to understand the effect of classification head architecture on\nmodel performance. Our results indicate that PaSST with an MLP head yields the\nbest performance among its variants but still falls short of DistilHuBERT.\nAmong the emotion classes, angry is consistently the most accurately detected,\nwhile disgust remains the most challenging. These findings suggest that\nlightweight transformers like DistilHuBERT offer a compelling solution for\nreal-time speech emotion recognition on edge devices. The code is available at:\nhttps://github.com/luckymaduabuchi/Emotion-detection-."}
{"id": "2511.00256", "categories": ["eess.AS", "cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2511.00256", "abs": "https://arxiv.org/abs/2511.00256", "authors": ["Zongyang Du", "Shreeram Suresh Chandra", "Ismail Rasim Ulgen", "Aurosweta Mahapatra", "Ali N. Salman", "Carlos Busso", "Berrak Sisman"], "title": "NaturalVoices: A Large-Scale, Spontaneous and Emotional Podcast Dataset for Voice Conversion", "comment": "Under review for IEEE Transactions on Affective Computing", "summary": "Everyday speech conveys far more than words, it reflects who we are, how we\nfeel, and the circumstances surrounding our interactions. Yet, most existing\nspeech datasets are acted, limited in scale, and fail to capture the expressive\nrichness of real-life communication. With the rise of large neural networks,\nseveral large-scale speech corpora have emerged and been widely adopted across\nvarious speech processing tasks. However, the field of voice conversion (VC)\nstill lacks large-scale, expressive, and real-life speech resources suitable\nfor modeling natural prosody and emotion. To fill this gap, we release\nNaturalVoices (NV), the first large-scale spontaneous podcast dataset\nspecifically designed for emotion-aware voice conversion. It comprises 5,049\nhours of spontaneous podcast recordings with automatic annotations for emotion\n(categorical and attribute-based), speech quality, transcripts, speaker\nidentity, and sound events. The dataset captures expressive emotional variation\nacross thousands of speakers, diverse topics, and natural speaking styles. We\nalso provide an open-source pipeline with modular annotation tools and flexible\nfiltering, enabling researchers to construct customized subsets for a wide\nrange of VC tasks. Experiments demonstrate that NaturalVoices supports the\ndevelopment of robust and generalizable VC models capable of producing natural,\nexpressive speech, while revealing limitations of current architectures when\napplied to large-scale spontaneous data. These results suggest that\nNaturalVoices is both a valuable resource and a challenging benchmark for\nadvancing the field of voice conversion. Dataset is available at:\nhttps://huggingface.co/JHU-SmileLab"}
{"id": "2511.00607", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.00607", "abs": "https://arxiv.org/abs/2511.00607", "authors": ["Tianyu Jiang", "Yan Yang", "Hongjin Liu", "Runyu Han", "Bo Ai", "Mohsen Guizani"], "title": "Fast Time-Varying mmWave Channel Estimation: A Rank-Aware Matrix Completion Approach", "comment": null, "summary": "We consider the problem of high-dimensional channel estimation in fast\ntime-varying millimeter-wave MIMO systems with a hybrid architecture. By\nexploiting the low-rank and sparsity properties of the channel matrix, we\npropose a two-phase compressed sensing framework consisting of observation\nmatrix completion and channel matrix sparse recovery, respectively. First, we\nformulate the observation matrix completion problem as a low-rank matrix\ncompletion (LRMC) problem and develop a robust rank-one matrix completion\n(R1MC) algorithm that enables the matrix and its rank to iteratively update.\nThis approach achieves high-precision completion of the observation matrix and\nexplicit rank estimation without prior knowledge. Second, we devise a\nrank-aware batch orthogonal matching pursuit (OMP) method for achieving\nlow-latency sparse channel recovery. To handle abrupt rank changes caused by\nuser mobility, we establish a discrete-time autoregressive (AR) model that\nleverages the temporal rank correlation between continuous-time instances to\nobtain a complete observation matrix capable of perceiving rank changes for\nmore accurate channel estimates. Simulation results confirm the effectiveness\nof the proposed channel estimation frame and demonstrate that our algorithms\nachieve state-of-the-art performance in low-rank matrix recovery with\ntheoretical guarantees."}
{"id": "2511.00428", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2511.00428", "abs": "https://arxiv.org/abs/2511.00428", "authors": ["Kazuya Yokota", "Ryosuke Harakawa", "Masaaki Baba", "Masahiro Iwahashi"], "title": "Physics-Informed Neural Networks for Speech Production", "comment": "11 pages, 10 figures", "summary": "The analysis of speech production based on physical models of the vocal folds\nand vocal tract is essential for studies on vocal-fold behavior and linguistic\nresearch. This paper proposes a speech production analysis method using\nphysics-informed neural networks (PINNs). The networks are trained directly on\nthe governing equations of vocal-fold vibration and vocal-tract acoustics.\nVocal-fold collisions introduce nondifferentiability and vanishing gradients,\nchallenging phenomena for PINNs. We demonstrate, however, that introducing a\ndifferentiable approximation function enables the analysis of vocal-fold\nvibrations within the PINN framework. The period of self-excited vocal-fold\nvibration is generally unknown. We show that by treating the period as a\nlearnable network parameter, a periodic solution can be obtained. Furthermore,\nby implementing the coupling between glottal flow and vocal-tract acoustics as\na hard constraint, glottis-tract interaction is achieved without additional\nloss terms. We confirmed the method's validity through forward and inverse\nanalyses, demonstrating that the glottal flow rate, vocal-fold vibratory state,\nand subglottal pressure can be simultaneously estimated from speech signals.\nNotably, the same network architecture can be applied to both forward and\ninverse analyses, highlighting the versatility of this approach. The proposed\nmethod inherits the advantages of PINNs, including mesh-free computation and\nthe natural incorporation of nonlinearities, and thus holds promise for a wide\nrange of applications."}
{"id": "2511.00850", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2511.00850", "abs": "https://arxiv.org/abs/2511.00850", "authors": ["Yayue Deng", "Guoqiang Hu", "Haiyang Sun", "Xiangyu Zhang", "Haoyang Zhang", "Fei Tian", "Xuerui Yang", "Gang Yu", "Eng Siong Chng"], "title": "MULTI-Bench: A Multi-Turn Interactive Benchmark for Assessing Emotional Intelligence ability of Spoken Dialogue Models", "comment": "Submitted to ICASSP 2026", "summary": "Spoken Dialogue Models (SDMs) have advanced rapidly, yet their ability to\nsustain genuinely interactive multi-turn conversations remains underexplored,\nas most benchmarks focus on single-turn exchanges. We introduce Multi-Bench,\nthe first benchmark explicitly designed to evaluate SDMs in multi-turn\ninteractive dialogue with an emphasis on emotional intelligence. Multi-Bench\nemploys a hierarchical structure with a basic track for emotion understanding\nand reasoning and an advanced track for emotion support and application. It\ncomprises five carefully designed tasks and about 3.2K samples, ranging from\nemotion recognition to complex reasoning and interactive dialogue, supported by\na reproducible evaluation framework. We evaluate six representative SDMs on\neight subsets of Multi-Bench. Results show that while current SDMs achieve good\nperformance on basic understanding tasks, they still have room for improvement\nin advanced multi-turn interactive dialogue and reasoning-related tasks,\nparticularly in emotion awareness and application."}
{"id": "2511.00721", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.00721", "abs": "https://arxiv.org/abs/2511.00721", "authors": ["Thanh Nha To", "Hoang Lai Pham", "Quynh Nguyen Thi", "Tuan Anh Pham", "Le Thanh Bang"], "title": "Fairness-Aware Secure Communication in ISAC Systems with STAR-RIS and RSMA", "comment": null, "summary": "In this paper, we investigate the integration of simultaneously transmitting\nand reflecting reconfigurable intelligent surfaces (STAR-RIS) with\nrate-splitting multiple access (RSMA) for improving physical layer security\n(PLS) in integrated sensing and communication (ISAC) systems. Specifically, we\nconsider a multi-user, multi-sensing target scenario, where each sensing target\nis treated as a potential eavesdropper, reflecting realistic deployment\nconditions. To enable fairness-aware secure communication among users while\nmaintaining sensing performance, we formulate a joint optimization problem that\ndesigns the base station beamforming vectors and STAR-RIS coefficients, aiming\nto maximize the minimum secrecy rate under a minimum beampattern gain\nconstraint. To solve the resulting non-convex problem, we propose an efficient\nalgorithm based on alternating optimization (AO) and the\nmajorization-minimization (MM) method. Simulation results verify the fast\nconvergence of the proposed algorithm and demonstrate significant improvements\nin secure communication performance."}
{"id": "2511.00641", "categories": ["cs.SD", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00641", "abs": "https://arxiv.org/abs/2511.00641", "authors": ["Swapnil Bhosale", "Cosmin Frateanu", "Camilla Clark", "Arnoldas Jasonas", "Chris Mitchell", "Xiatian Zhu", "Vamsi Krishna Ithapu", "Giacomo Ferroni", "Cagdas Bilen", "Sanjeel Parekh"], "title": "More Than A Shortcut: A Hyperbolic Approach To Early-Exit Networks", "comment": null, "summary": "Deploying accurate event detection on resource-constrained devices is\nchallenged by the trade-off between performance and computational cost. While\nEarly-Exit (EE) networks offer a solution through adaptive computation, they\noften fail to enforce a coherent hierarchical structure, limiting the\nreliability of their early predictions. To address this, we propose Hyperbolic\nEarly-Exit networks (HypEE), a novel framework that learns EE representations\nin the hyperbolic space. Our core contribution is a hierarchical training\nobjective with a novel entailment loss, which enforces a partial-ordering\nconstraint to ensure that deeper network layers geometrically refine the\nrepresentations of shallower ones. Experiments on multiple audio event\ndetection tasks and backbone architectures show that HypEE significantly\noutperforms standard Euclidean EE baselines, especially at the earliest, most\ncomputationally-critical exits. The learned geometry also provides a principled\nmeasure of uncertainty, enabling a novel triggering mechanism that makes the\noverall system both more efficient and more accurate than a conventional EE and\nstandard backbone models without early-exits."}
{"id": "2511.01056", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2511.01056", "abs": "https://arxiv.org/abs/2511.01056", "authors": ["Dong Liu", "Ming Li"], "title": "WhisperVC: Target Speaker-Controllable Mandarin Whisper-to-Speech Conversion", "comment": null, "summary": "Whispered speech lacks vocal-fold excitation and exhibits reduced energy and\nshifted formant frequencies, making natural and intelligible voice\nreconstruction highly challenging. To address this issue, we propose\n\\emph{WhisperVC}, a three-stage framework for Mandarin whisper-to-speech (W2S)\nconversion. Stage~1 employs a fine-tuned Content Encoder based on the OpenAI\nWhisper-large~V3 model and a Conformer-based variational autoencoder with\nsoft-DTW alignment to learn domain-invariant and temporally consistent\nrepresentations. Stage~2 introduces a deterministic Length--Channel Aligner and\na duration-free FastSpeech~2 model conditioned on speaker embeddings for\ncontrollable timbre and stable prosody. Stage~3 fine-tunes a HiFi-GAN vocoder\non predicted mel-spectrograms to synthesize high-fidelity waveforms.\nExperiments on the AISHELL6-Whisper corpus demonstrate that WhisperVC achieves\nnear ground-truth quality (\\textbf{DNSMOS~3.11}, \\textbf{UTMOS~2.52},\n\\textbf{CER~18.67\\%}), while maintaining speaker similarity\n(\\textbf{cosine~0.76}) and robust performance under whisper-only inference."}
{"id": "2511.00779", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.00779", "abs": "https://arxiv.org/abs/2511.00779", "authors": ["Erfan Khordad", "Peter J. Smith", "Sachitha C. Bandara", "Rajitha Senanayake", "Robert W. Heath Jr"], "title": "Target Detection with Tightly-coupled Antennas: Analysis for Unknown Wideband Signals", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "This paper presents analysis for target detection using tightly-coupled\nantenna (TCA) arrays with high mutual coupling (MC). We show that the wide\noperational bandwidth of TCAs is advantageous for target detection. We assume a\nsensing receiver equipped with a TCA array that collects joint time and\nfrequency samples of the target's echo signals. Echoes are assumed to be\nunknown wideband signals, and noise at the TCA array follows a\nfrequency-varying correlation model due to MC. We also assume that the echo\nsignals are time varying, with no assumption on the temporal variation. We\nconsider three regimes in frequency as constant, slowly or rapidly varying, to\ncapture all possible spectral dynamics of the echoes. We propose a novel\ndetector for the slowly-varying regime, and derive detectors based on maximum\nlikelihood estimation (MLE) for the other regimes. For the rapidly-varying\nregime, we derive an extended energy detector for correlated noise with\nfrequency and time samples. We analyze the performance of all the detectors. We\nalso derive and analyze an ideal detector giving an upper bound on performance.\nWe validate our analysis with simulations and demonstrate that our proposed\ndetector outperforms the MLE-based detectors in terms of robustness to\nfrequency variation. Also, we highlight that TCA arrays offer clear advantages\nover weakly-coupled antenna arrays in target detection."}
{"id": "2511.01091", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2511.01091", "abs": "https://arxiv.org/abs/2511.01091", "authors": ["Junqi Zhao", "Chenxing Li", "Jinzheng Zhao", "Rilin Chen", "Dong Yu", "Mark D. Plumbley", "Wenwu Wang"], "title": "Feedback-driven Retrieval-augmented Audio Generation with Large Audio Language Models", "comment": null, "summary": "We propose a general feedback-driven retrieval-augmented generation (RAG)\napproach that leverages Large Audio Language Models (LALMs) to address the\nmissing or imperfect synthesis of specific sound events in text-to-audio (TTA)\ngeneration. Unlike previous RAG-based TTA methods that typically train\nspecialized models from scratch, we utilize LALMs to analyze audio generation\noutputs, retrieve concepts that pre-trained models struggle to generate from an\nexternal database, and incorporate the retrieved information into the\ngeneration process. Experimental results show that our method not only enhances\nthe ability of LALMs to identify missing sound events but also delivers\nimprovements across different models, outperforming existing RAG-specialized\napproaches."}
{"id": "2511.01299", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2511.01299", "abs": "https://arxiv.org/abs/2511.01299", "authors": ["Siyin Wang", "Zengrui Jin", "Changli Tang", "Qiujia Li", "Bo Li", "Chen Chen", "Yuchen Hu", "Wenyi Yu", "Yixuan Li", "Jimin Zhuang", "Yudong Yang", "Mingqiu Wang", "Michael Han", "Yifan Ding", "Junwen Bai", "Tom Ouyang", "Shuo-yiin Chang", "Xianzhao Chen", "Xiaohai Tian", "Jun Zhang", "Lu Lu", "Guangzhi Sun", "Zhehuai Chen", "Ji Wu", "Bowen Zhou", "Yuxuan Wang", "Tara Sainath", "Yonghui Wu", "Chao Zhang"], "title": "Towards General Auditory Intelligence: Large Multimodal Models for Machine Listening and Speaking", "comment": "22 pages, 11 figures", "summary": "In the era of large language models (LLMs) and artificial general\nintelligence (AGI), computer audition must evolve beyond traditional paradigms\nto fully leverage the capabilities of foundation models, towards more\ncomprehensive understanding, more natural generation and more human-like\ninteraction. Audio, as a modality rich in semantic, emotional, and contextual\ncues, plays a vital role in achieving naturalistic and embodied machine\nintelligence. This survey provides a comprehensive review of recent progress in\nintegrating audio into LLMs, with a focus on four key areas: audio\ncomprehension, audio generation, speech-based interaction, and audio-visual\nunderstanding. We analyze how LLMs are reshaping audio perception and\nreasoning, enabling systems to understand sound at a deeper semantic level,\ngenerate expressive audio outputs, and engage in human-like spoken interaction.\nFurthermore, we explore how the fusion of audio and visual modalities enhances\nsituational awareness and cross-modal reasoning, pushing the boundaries of\nmultimodal intelligence. This survey not only synthesizes existing research but\nalso identifies critical challenges and future directions for building\naudio-native AGI systems capable of perceiving, understanding, and interacting\nthrough sound as naturally as humans do."}
{"id": "2511.00878", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.00878", "abs": "https://arxiv.org/abs/2511.00878", "authors": ["Ahmed Magbool", "Vaibhav Kumar", "Marco Di Renzo", "Mark F. Flanagan"], "title": "Stacked Flexible Intelligent Metasurface Design for Multi-User Wireless Communications", "comment": "Submitted to IEEE for possible publication", "summary": "Stacked intelligent metasurfaces (SIMs) have recently emerged as an effective\nsolution for next-generation wireless networks. A SIM comprises multiple\nmetasurface layers that enable signal processing directly in the wave domain.\nMoreover, recent advances in flexible metamaterials have highlighted the\npotential of flexible intelligent metasurfaces (FIMs), which can be physically\nmorphed to enhance communication performance. In this paper, we propose a\nstacked flexible intelligent metasurface (SFIM)-based communication system for\nthe first time, where each metasurface layer is deformable to improve the\nsystem's performance. We first present the system model, including the transmit\nand receive signal models as well as the channel model, and then formulate an\noptimization problem to maximize the system sum rate under constraints on the\ntransmit power budget, morphing distance, and the unit-modulus condition of the\nmeta-atom responses. To solve this problem, we develop an alternating\noptimization framework based on the gradient projection method. Simulation\nresults demonstrate that the proposed SFIM-based system achieves significant\nperformance gains compared to its rigid SIM counterpart."}
{"id": "2511.01261", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2511.01261", "abs": "https://arxiv.org/abs/2511.01261", "authors": ["Jiatong Shi", "Jionghao Han", "Yichen Lu", "Santiago Pascual", "Pengfei Wu", "Chenye Cui", "Shinji Watanabe", "Chao Weng", "Cong Zhou"], "title": "Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play", "comment": "67 pages", "summary": "Role-play has become a key testbed for generative models, expanding from\ntext-only dialogue to multimodal interaction. Extending role-play to speech\ncaptures prosody, emotion, and delivery, but also poses new evaluation\nchallenges. Current pipelines often use audio large language models (ALLMs) as\nzero-shot judges, which miss paralinguistic cues, collapse multiple aspects\ninto coarse scores, and rely on synthetic speech references that fail to\nreflect real-world roles. We present Speech-DRAME, a unified framework that\ncontributes at three levels: (i) Speech-DRAME-EvalBench, an evaluation\nbenchmark with bilingual human-annotated data and protocols for training and\ntesting speech evaluation models (SEMs), (ii) DRAME-Eval, a fine-tuned\nevaluation model, which substantially outperforms zero-shot and few-shot ALLMs,\nand (iii) Speech-DRAME-RoleBench, a speech role-play benchmark that leverages\nDRAME-Eval as an automatic judge to compare speech foundation models (SFMs).\nSpeech-DRAME distinguishes between two complementary evaluation strategies:\nArchetype Evaluation, a top-down approach measuring adherence to broad role\narchetypes, and Realism Evaluation, a bottom-up approach grounded in real human\nspeech that emphasizes nuanced role quality. Compared to zero-shot ALLM judges,\nDRAME-Eval achieves stronger agreement with human ratings (Pearson correlation\nfrom 0.480 to 0.629 in archetypes, and 0.390 to 0.625 in realism). By\nintegrating transparent benchmark resources, modeling approaches, and\nsystem-level evaluation, Speech-DRAME provides the first comprehensive,\nreproducible foundation for assessing spoken role-play."}
{"id": "2511.01372", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2511.01372", "abs": "https://arxiv.org/abs/2511.01372", "authors": ["Sagar Dutta", "Vipul Arora"], "title": "AudioNet: Supervised Deep Hashing for Retrieval of Similar Audio Events", "comment": null, "summary": "This work presents a supervised deep hashing method for retrieving similar\naudio events. The proposed method, named AudioNet, is a deep-learning-based\nsystem for efficient hashing and retrieval of similar audio events using an\naudio example as a query. AudioNet achieves high retrieval performance on\nmultiple standard datasets by generating binary hash codes for similar audio\nevents, setting new benchmarks in the field, and highlighting its efficacy and\neffectiveness compare to other hashing methods. Through comprehensive\nexperiments on standard datasets, our research represents a pioneering effort\nin evaluating the retrieval performance of similar audio events. A novel loss\nfunction is proposed which incorporates weighted contrastive and weighted\npairwise loss along with hashcode balancing to improve the efficiency of audio\nevent retrieval. The method adopts discrete gradient propagation, which allows\ngradients to be propagated through discrete variables during backpropagation.\nThis enables the network to optimize the discrete hash codes using standard\ngradient-based optimization algorithms, which are typically used for continuous\nvariables. The proposed method showcases promising retrieval performance, as\nevidenced by the experimental results, even when dealing with imbalanced\ndatasets. The systematic analysis conducted in this study further supports the\nsignificant benefits of the proposed method in retrieval performance across\nmultiple datasets. The findings presented in this work establish a baseline for\nfuture studies on the efficient retrieval of similar audio events using deep\naudio embeddings."}
{"id": "2511.00919", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.00919", "abs": "https://arxiv.org/abs/2511.00919", "authors": ["Mahdi Maleki", "Reza Agahzadeh Ayoubi", "Marouan Mizmizi", "Umberto Spagnolini"], "title": "Towards Channel Charting Enhancement with Non-Reconfigurable Intelligent Surfaces", "comment": null, "summary": "We investigate how fully-passive electromagnetic skins (EMSs) can be\nengineered to enhance channel charting (CC) in dense urban environments. We\nemploy two complementary state-of-the-art CC techniques, semi-supervised\nt-distributed stochastic neighbor embedding (t-SNE) and a semi-supervised\nAutoencoder (AE), to verify the consistency of results across nonparametric and\nparametric mappings. We show that the accuracy of CC hinges on a balance\nbetween signal-to-noise ratio (SNR) and spatial dissimilarity: EMS codebooks\nthat only maximize gain, as in conventional Reconfigurable Intelligent Surface\n(RIS) optimization, suppress location fingerprints and degrade CC, while\nrandomized phases increase diversity but reduce SNR. To address this trade-off,\nwe design static EMS phase profiles via a quantile-driven criterion that\ntargets worst-case users and improves both trustworthiness and continuity. In a\n3D ray-traced city at 30 GHz, the proposed EMS reduces the 90th-percentile\nlocalization error from > 50 m to < 25 m for both t-SNE and AE-based CC, and\ndecreases severe trajectory dropouts by over 4x under 15% supervision. The\nimprovements hold consistently across the evaluated configurations,\nestablishing static, pre-configured EMS as a practical enabler of CC without\nreconfiguration overheads."}
{"id": "2511.01663", "categories": ["cs.SD", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2511.01663", "abs": "https://arxiv.org/abs/2511.01663", "authors": ["Louis Bradshaw", "Alexander Spangher", "Stella Biderman", "Simon Colton"], "title": "The Ghost in the Keys: A Disklavier Demo for Human-AI Musical Co-Creativity", "comment": null, "summary": "While generative models for music composition are increasingly capable, their\nadoption by musicians is hindered by text-prompting, an asynchronous workflow\ndisconnected from the embodied, responsive nature of instrumental performance.\nTo address this, we introduce Aria-Duet, an interactive system facilitating a\nreal-time musical duet between a human pianist and Aria, a state-of-the-art\ngenerative model, using a Yamaha Disklavier as a shared physical interface. The\nframework enables a turn-taking collaboration: the user performs, signals a\nhandover, and the model generates a coherent continuation performed\nacoustically on the piano. Beyond describing the technical architecture\nenabling this low-latency interaction, we analyze the system's output from a\nmusicological perspective, finding the model can maintain stylistic semantics\nand develop coherent phrasal ideas, demonstrating that such embodied systems\ncan engage in musically sophisticated dialogue and open a promising new path\nfor human-AI co-creation."}
{"id": "2511.01652", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2511.01652", "abs": "https://arxiv.org/abs/2511.01652", "authors": ["Mehmet Sinan Yıldırım", "Ruijie Tao", "Wupeng Wang", "Junyi Ao", "Haizhou Li"], "title": "Leveraging Language Information for Target Language Extraction", "comment": "Accepted to APSIPA ASC 2025", "summary": "Target Language Extraction aims to extract speech in a specific language from\na mixture waveform that contains multiple speakers speaking different\nlanguages. The human auditory system is adept at performing this task with the\nknowledge of the particular language. However, the performance of the\nconventional extraction systems is limited by the lack of this prior knowledge.\nSpeech pre-trained models, which capture rich linguistic and phonetic\nrepresentations from large-scale in-the-wild corpora, can provide this missing\nlanguage knowledge to these systems. In this work, we propose a novel\nend-to-end framework to leverage language knowledge from speech pre-trained\nmodels. This knowledge is used to guide the extraction model to better capture\nthe target language characteristics, thereby improving extraction quality. To\ndemonstrate the effectiveness of our proposed approach, we construct the first\npublicly available multilingual dataset for Target Language Extraction.\nExperimental results show that our method achieves improvements of 1.22 dB and\n1.12 dB in SI-SNR for English and German extraction, respectively, from\nmixtures containing both languages."}
{"id": "2511.00943", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.00943", "abs": "https://arxiv.org/abs/2511.00943", "authors": ["Yangyang Zhao", "Matti Kaisti", "Olli Lahdenoja", "Jonas Sandelin", "Arman Anzanpour", "Joonas Lehto", "Joel Nuotio", "Jussi Jaakkola", "Arto Relander", "Tuija Vasankari", "Juhani Airaksinen", "Tuomas Kiviniemi", "Tero Koivisto"], "title": "Lightweight ResNet-Based Deep Learning for Photoplethysmography Signal Quality Assessment", "comment": "Accepted for presentation at IEEE Engineering in Medicine and Biology\n  Conference (EMBC 2025). 7 pages, 3 figures. Author's accepted manuscript\n  (AAM). The final version will appear in IEEE Xplore", "summary": "With the growing application of deep learning in wearable devices,\nlightweight and efficient models are critical to address the computational\nconstraints in resource-limited platforms. The performance of these approaches\ncan be potentially improved by using various preprocessing methods. This study\nproposes a lightweight ResNet-based deep learning framework with\nSqueeze-and-Excitation (SE) modules for photoplethysmography (PPG) signal\nquality assessment (SQA) and compares different input configurations, including\nthe PPG signal alone, its first derivative (FDP), its second derivative (SDP),\nthe autocorrelation of PPG (ATC), and various combinations of these channels.\nExperimental evaluations on the Moore4Medical (M4M) and MIMIC-IV datasets\ndemonstrate the model's performance, achieving up to 96.52% AUC on the M4M test\ndataset and up to 84.43% AUC on the MIMIC-IV dataset. The novel M4M dataset was\ncollected to explore PPG-based monitoring for detecting atrial fibrillation\n(AF) and AF burden in high-risk patients. Compared to the five reproduced\nexisting studies, our models achieves over 99% reduction in parameters and more\nthan 60% reduction in floating-point operations (FLOPs)."}
{"id": "2511.01773", "categories": ["cs.SD", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.01773", "abs": "https://arxiv.org/abs/2511.01773", "authors": ["Daniel Jimon", "Mircea Vaida", "Adriana Stan"], "title": "ADNAC: Audio Denoiser using Neural Audio Codec", "comment": "Accepted and presented at the 13th International Conference on Speech\n  Technology and Human-Computer Dialogue (SpeD), Cluj-Napoca, Romania, October\n  19-22, 2025. 4 pages, 1 figure. IEEE Catalog Number: CFP2555H-USB, ISBN:\n  979-8-3315-7485-7", "summary": "Audio denoising is critical in signal processing, enhancing intelligibility\nand fidelity for applications like restoring musical recordings. This paper\npresents a proof-of-concept for adapting a state-of-the-art neural audio codec,\nthe Descript Audio Codec (DAC), for music denoising. This work overcomes the\nlimitations of traditional architectures like U-Nets by training the model on a\nlarge-scale, custom-synthesized dataset built from diverse sources. Training is\nguided by a multi objective loss function that combines time-domain, spectral,\nand signal-level fidelity metrics. Ultimately, this paper aims to present a PoC\nfor high-fidelity, generative audio restoration."}
{"id": "2511.01261", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2511.01261", "abs": "https://arxiv.org/abs/2511.01261", "authors": ["Jiatong Shi", "Jionghao Han", "Yichen Lu", "Santiago Pascual", "Pengfei Wu", "Chenye Cui", "Shinji Watanabe", "Chao Weng", "Cong Zhou"], "title": "Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play", "comment": "67 pages", "summary": "Role-play has become a key testbed for generative models, expanding from\ntext-only dialogue to multimodal interaction. Extending role-play to speech\ncaptures prosody, emotion, and delivery, but also poses new evaluation\nchallenges. Current pipelines often use audio large language models (ALLMs) as\nzero-shot judges, which miss paralinguistic cues, collapse multiple aspects\ninto coarse scores, and rely on synthetic speech references that fail to\nreflect real-world roles. We present Speech-DRAME, a unified framework that\ncontributes at three levels: (i) Speech-DRAME-EvalBench, an evaluation\nbenchmark with bilingual human-annotated data and protocols for training and\ntesting speech evaluation models (SEMs), (ii) DRAME-Eval, a fine-tuned\nevaluation model, which substantially outperforms zero-shot and few-shot ALLMs,\nand (iii) Speech-DRAME-RoleBench, a speech role-play benchmark that leverages\nDRAME-Eval as an automatic judge to compare speech foundation models (SFMs).\nSpeech-DRAME distinguishes between two complementary evaluation strategies:\nArchetype Evaluation, a top-down approach measuring adherence to broad role\narchetypes, and Realism Evaluation, a bottom-up approach grounded in real human\nspeech that emphasizes nuanced role quality. Compared to zero-shot ALLM judges,\nDRAME-Eval achieves stronger agreement with human ratings (Pearson correlation\nfrom 0.480 to 0.629 in archetypes, and 0.390 to 0.625 in realism). By\nintegrating transparent benchmark resources, modeling approaches, and\nsystem-level evaluation, Speech-DRAME provides the first comprehensive,\nreproducible foundation for assessing spoken role-play."}
{"id": "2511.00966", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.00966", "abs": "https://arxiv.org/abs/2511.00966", "authors": ["Andrea De Simone", "Noemi Giordano", "Silvia Seoni", "Kristen M. Meiburger", "Fabrizio Riente"], "title": "Optimizing Uncertainty-Aware Deep Learning for On-the-Edge Murmur Detection in Low-Resource Settings", "comment": null, "summary": "Early and reliable detection of heart murmurs is essential for the timely\ndiagnosis of cardiovascular diseases, yet traditional auscultation remains\nsubjective and dependent on expert interpretation. This work investigates\nartificial intelligence (AI)-based murmur detection using the CirCor Heart\nSound dataset, with a focus on enabling uncertainty-aware, resource-efficient\ndeployment on edge devices. Three convolutional neural network (CNN)\narchitectures of increasing complexity (Light, Baseline, and Heavy) were\ncompared in terms of classification performance, computational cost, and\nsuitability for on-device inference. Additionally, Monte Carlo Dropout was\napplied for uncertainty estimation, providing confidence measures to improve\nprediction sensitivity. Results show that lightweight models can achieve\naccuracy comparable to deeper networks (91%) while requiring two orders of\nmagnitude fewer parameters. Incorporating uncertainty-based selective\nclassification further improved sensitivity by 3%, enhancing robustness and\nclinical reliability. The findings highlight the feasibility of developing\ncomputationally efficient, uncertainty-aware AI systems for heart murmur\nscreening in low-resource and remote healthcare settings."}
{"id": "2511.00256", "categories": ["eess.AS", "cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2511.00256", "abs": "https://arxiv.org/abs/2511.00256", "authors": ["Zongyang Du", "Shreeram Suresh Chandra", "Ismail Rasim Ulgen", "Aurosweta Mahapatra", "Ali N. Salman", "Carlos Busso", "Berrak Sisman"], "title": "NaturalVoices: A Large-Scale, Spontaneous and Emotional Podcast Dataset for Voice Conversion", "comment": "Under review for IEEE Transactions on Affective Computing", "summary": "Everyday speech conveys far more than words, it reflects who we are, how we\nfeel, and the circumstances surrounding our interactions. Yet, most existing\nspeech datasets are acted, limited in scale, and fail to capture the expressive\nrichness of real-life communication. With the rise of large neural networks,\nseveral large-scale speech corpora have emerged and been widely adopted across\nvarious speech processing tasks. However, the field of voice conversion (VC)\nstill lacks large-scale, expressive, and real-life speech resources suitable\nfor modeling natural prosody and emotion. To fill this gap, we release\nNaturalVoices (NV), the first large-scale spontaneous podcast dataset\nspecifically designed for emotion-aware voice conversion. It comprises 5,049\nhours of spontaneous podcast recordings with automatic annotations for emotion\n(categorical and attribute-based), speech quality, transcripts, speaker\nidentity, and sound events. The dataset captures expressive emotional variation\nacross thousands of speakers, diverse topics, and natural speaking styles. We\nalso provide an open-source pipeline with modular annotation tools and flexible\nfiltering, enabling researchers to construct customized subsets for a wide\nrange of VC tasks. Experiments demonstrate that NaturalVoices supports the\ndevelopment of robust and generalizable VC models capable of producing natural,\nexpressive speech, while revealing limitations of current architectures when\napplied to large-scale spontaneous data. These results suggest that\nNaturalVoices is both a valuable resource and a challenging benchmark for\nadvancing the field of voice conversion. Dataset is available at:\nhttps://huggingface.co/JHU-SmileLab"}
{"id": "2511.01023", "categories": ["eess.SP", "cs.AI", "cs.CR", "cs.LG", "68T07, 68P25, 94A60, 68Q32, 68Q87, 94A17", "I.2.6; C.2.0; D.4.6; E.3; I.5.1; K.6.5"], "pdf": "https://arxiv.org/pdf/2511.01023", "abs": "https://arxiv.org/abs/2511.01023", "authors": ["Ayşe Selin Okatan", "Mustafa İlhan Akbaş", "Laxima Niure Kandel", "Berker Peköz"], "title": "Seed-Induced Uniqueness in Transformer Models: Subspace Alignment Governs Subliminal Transfer", "comment": "Cite as A. S. Okatan, M. I. Akba\\c{s}, L. N. Kandel, and B. Pek\\\"oz,\n  \"Seed-Induced Uniqueness in Transformer Models: Subspace Alignment Governs\n  Subliminal Transfer,\" in Proc. 2025 Cyber Awareness and Research Symp. (IEEE\n  CARS 2025), Grand Forks, ND, Oct. 2025, pp. 6", "summary": "We analyze subliminal transfer in Transformer models, where a teacher embeds\nhidden traits that can be linearly decoded by a student without degrading\nmain-task performance. Prior work often attributes transferability to global\nrepresentational similarity, typically quantified with Centered Kernel\nAlignment (CKA). Using synthetic corpora with disentangled public and private\nlabels, we distill students under matched and independent random\ninitializations. We find that transfer strength hinges on alignment within a\ntrait-discriminative subspace: same-seed students inherit this alignment and\nshow higher leakage {\\tau \\approx} 0.24, whereas different-seed\nstudents--despite global CKA > 0.9--exhibit substantially reduced excess\naccuracy {\\tau \\approx} 0.12 - 0.13. We formalize this with subspace-level CKA\ndiagnostic and residualized probes, showing that leakage tracks alignment\nwithin the trait-discriminative subspace rather than global representational\nsimilarity. Security controls (projection penalty, adversarial reversal,\nright-for-the-wrong-reasons regularization) reduce leakage in same-base models\nwithout impairing public-task fidelity. These results establish seed-induced\nuniqueness as a resilience property and argue for subspace-aware diagnostics\nfor secure multi-model deployments."}
{"id": "2511.00850", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2511.00850", "abs": "https://arxiv.org/abs/2511.00850", "authors": ["Yayue Deng", "Guoqiang Hu", "Haiyang Sun", "Xiangyu Zhang", "Haoyang Zhang", "Fei Tian", "Xuerui Yang", "Gang Yu", "Eng Siong Chng"], "title": "MULTI-Bench: A Multi-Turn Interactive Benchmark for Assessing Emotional Intelligence ability of Spoken Dialogue Models", "comment": "Submitted to ICASSP 2026", "summary": "Spoken Dialogue Models (SDMs) have advanced rapidly, yet their ability to\nsustain genuinely interactive multi-turn conversations remains underexplored,\nas most benchmarks focus on single-turn exchanges. We introduce Multi-Bench,\nthe first benchmark explicitly designed to evaluate SDMs in multi-turn\ninteractive dialogue with an emphasis on emotional intelligence. Multi-Bench\nemploys a hierarchical structure with a basic track for emotion understanding\nand reasoning and an advanced track for emotion support and application. It\ncomprises five carefully designed tasks and about 3.2K samples, ranging from\nemotion recognition to complex reasoning and interactive dialogue, supported by\na reproducible evaluation framework. We evaluate six representative SDMs on\neight subsets of Multi-Bench. Results show that while current SDMs achieve good\nperformance on basic understanding tasks, they still have room for improvement\nin advanced multi-turn interactive dialogue and reasoning-related tasks,\nparticularly in emotion awareness and application."}
{"id": "2511.01099", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.01099", "abs": "https://arxiv.org/abs/2511.01099", "authors": ["Zhenqiao Cheng", "Chongjun Ouyang", "Nicola Marchetti"], "title": "On the Performance of Tri-Hybrid Beamforming Using Pinching Antennas", "comment": "6 pages", "summary": "The Pinching-Antenna System (PASS) reconfigures wireless channels through\n\\emph{pinching beamforming}, in which the active positions of pinching antennas\n(PAs) along dielectric waveguides are optimized to shape the radiation pattern.\nThis article investigates the performance of PASS-enabled tri-hybrid\nbeamforming, where pinched waveguides are integrated with a hybrid\ndigital-analog beamformer to mitigate path loss and enhance spectral\nefficiency. The channel capacity of the proposed system is characterized by\nderiving the optimal tri-hybrid beamformer at both the digital and analog\ndomains, as well as the optimal placement of PAs. Closed-form upper and lower\nbounds of the channel capacity are obtained, leading to a capacity scaling law\nwith respect to the number of PAs. Numerical results verify the tightness of\nthe derived bounds and demonstrate that applying PASS to tri-hybrid beamforming\nyields a significant performance gain over conventional hybrid beamforming\nunder the same number of radio-frequency chains."}
{"id": "2511.01652", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2511.01652", "abs": "https://arxiv.org/abs/2511.01652", "authors": ["Mehmet Sinan Yıldırım", "Ruijie Tao", "Wupeng Wang", "Junyi Ao", "Haizhou Li"], "title": "Leveraging Language Information for Target Language Extraction", "comment": "Accepted to APSIPA ASC 2025", "summary": "Target Language Extraction aims to extract speech in a specific language from\na mixture waveform that contains multiple speakers speaking different\nlanguages. The human auditory system is adept at performing this task with the\nknowledge of the particular language. However, the performance of the\nconventional extraction systems is limited by the lack of this prior knowledge.\nSpeech pre-trained models, which capture rich linguistic and phonetic\nrepresentations from large-scale in-the-wild corpora, can provide this missing\nlanguage knowledge to these systems. In this work, we propose a novel\nend-to-end framework to leverage language knowledge from speech pre-trained\nmodels. This knowledge is used to guide the extraction model to better capture\nthe target language characteristics, thereby improving extraction quality. To\ndemonstrate the effectiveness of our proposed approach, we construct the first\npublicly available multilingual dataset for Target Language Extraction.\nExperimental results show that our method achieves improvements of 1.22 dB and\n1.12 dB in SI-SNR for English and German extraction, respectively, from\nmixtures containing both languages."}
{"id": "2511.01254", "categories": ["eess.SP", "cs.NE"], "pdf": "https://arxiv.org/pdf/2511.01254", "abs": "https://arxiv.org/abs/2511.01254", "authors": ["Huseyin Goksu"], "title": "Hi-WaveTST: A Hybrid High-Frequency Wavelet-Transformer for Time-Series Classification", "comment": null, "summary": "Transformers have become state-of-the-art (SOTA) for time-series\nclassification, with models like PatchTST demonstrating exceptional\nperformance. These models rely on patching the time series and learning\nrelationships between raw temporal data blocks. We argue that this approach is\nblind to critical, non-obvious high-frequency information that is complementary\nto the temporal dynamics. In this letter, we propose Hi-WaveTST, a novel Hybrid\narchitecture that augments the original temporal patch with a learnable,\nHigh-Frequency wavelet feature stream. Our wavelet stream uses a deep Wavelet\nPacket Decomposition (WPD) on each patch and extracts features using a\nlearnable Generalized Mean (GeM) pooling layer. On the UCI-HAR benchmark\ndataset, our hybrid model achieves a mean accuracy of 93.38 percent plus-minus\n0.0043, significantly outperforming the SOTA PatchTST baseline (92.59 percent\nplus-minus 0.0039). A comprehensive ablation study proves that every component\nof our design-the hybrid architecture, the deep high-frequency wavelet\ndecomposition, and the learnable GeM pooling-is essential for this\nstate-of-the-art performance."}
{"id": "2511.01371", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.01371", "abs": "https://arxiv.org/abs/2511.01371", "authors": ["Sagar Dutta", "Banani Basu", "Fazal Ahmed Talukdar"], "title": "Classification of motor faults based on transmission coefficient and reflection coefficient of omni-directional antenna using DCNN", "comment": null, "summary": "The most commonly used electrical rotary machines in the field are induction\nmachines. In this paper, we propose an antenna based approach for the\nclassification of motor faults in induction motors using the reflection\ncoefficient S11 and the transmission coefficient S21 of the antenna. The\nspectrograms of S11 and S21 are seen to possess unique signatures for various\nfault conditions that are used for the classification. To learn the required\ncharacteristics and classification boundaries, deep convolution neural network\n(DCNN) is applied to the spectrogram of the S-parameter. DCNN has been found to\nreach classification accuracy 93% using S11, 98.1% using S21 and 100% using\nboth S11 and S21. The effect of antenna operating frequency, its location and\nduration of signal on the classification accuracy is also presented and\ndiscussed."}
{"id": "2511.01398", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.01398", "abs": "https://arxiv.org/abs/2511.01398", "authors": ["Wang Hao", "Kuang Zhang", "Hou Chengyu", "Tan Chenxing", "Cui Weiming", "Fu Weifeng", "Yao Xinran"], "title": "CRMD: Complex Robust Modal Decomposition", "comment": null, "summary": "Compared to real-valued signals, complex-valued signals provide a unique and\nintuitive representation of the phase of real physical systems and processes,\nwhich holds fundamental significance and is widely applied across many fields\nof science and engineering. In this paper, we propose a robust modal\ndecomposition (RMD) in the complex domain as a natural and general extension of\nthe original real-valued RMD. We revisit and derive the mathematical principles\nof RMD in the complex domain, and develop an algorithmic version tailored for\nthis domain. Extensive experiments are conducted on synthetic simulation\ndatasets and real-world datasets from diverse fields, including a\nmillimeter-wave radar physiological signal detection dataset, a faulty bearing\ndataset, a radio-frequency unmanned aerial vehicle identification dataset, and\na WiFi CSI-based respiration detection dataset. The results demonstrate that\nthe proposed complex-domain robust modal decomposition significantly improves\nperformance across these various applications."}
{"id": "2511.01405", "categories": ["eess.SP", "cs.ET"], "pdf": "https://arxiv.org/pdf/2511.01405", "abs": "https://arxiv.org/abs/2511.01405", "authors": ["Vanessa Wirth", "Johanna Bräunig", "Martin Vossiek", "Tim Weyrich", "Marc Stamminger"], "title": "MM-2FSK: Multimodal Frequency Shift Keying for Ultra-Efficient and Robust High-Resolution MIMO Radar Imaging", "comment": "9 pages", "summary": "Accurate reconstruction of static and rapidly moving targets demands\nthree-dimensional imaging solutions with high temporal and spatial resolution.\nRadar sensors are a promising sensing modality because of their fast capture\nrates and their independence from lighting conditions. To achieve high spatial\nresolution, MIMO radars with large apertures are required. Yet, they are\ninfrequently used for dynamic scenarios due to significant limitations in\nsignal processing algorithms. These limitations impose substantial hardware\nconstraints due to their computational intensity and reliance on large signal\nbandwidths, ultimately restricting the sensor's capture rate.\n  One solution of previous work is to use few frequencies only, which enables\nfaster capture and requires less computation; however, this requires coarse\nknowledge of the target's position and works in a limited depth range only. To\naddress these challenges, we extend previous work into the multimodal domain\nwith MM-2FSK, which leverages an assistive optical depth sensing modality to\nobtain a depth prior, enabling high framerate capture with only few\nfrequencies.\n  We evaluate our method using various target objects with known ground truth\ngeometry that is spatially registered to real millimeter-wave MIMO radar\nmeasurements. Our method demonstrates superior performance in terms of depth\nquality, being able to compete with the time- and resource-intensive\nmeasurements with many frequencies."}
{"id": "2511.01406", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.01406", "abs": "https://arxiv.org/abs/2511.01406", "authors": ["Abolfazl Zakeri", "Nhan Thanh Nguyen", "Ahmed Alkhateeb", "Markku Juntti"], "title": "AoI-Aware Machine Learning for Constrained Multimodal Sensing-Aided Communications", "comment": "Submitted to an IEEE conference", "summary": "Using environmental sensory data can enhance communications beam training and\nreduce its overhead compared to conventional methods. However, the availability\nof fresh sensory data during inference may be limited due to sensing\nconstraints or sensor failures, necessitating a realistic model for multimodal\nsensing. This paper proposes a joint multimodal sensing and beam prediction\nframework that operates under a constraint on the average sensing rate, i.e.,\nhow often fresh sensory data should be obtained. The proposed method combines\ndeep reinforcement learning, i.e., a deep Q-network (DQN), with a neural\nnetwork (NN)-based beam predictor. The DQN determines the sensing decisions,\nwhile the NN predicts the best beam from the codebook. To capture the effect of\nlimited fresh data during inference, the age of information (AoI) is\nincorporated into the training of both the DQN and the beam predictor. Lyapunov\noptimization is employed to design a reward function that enforces the average\nsensing constraint. Simulation results on a real-world dataset show that\nAoI-aware training improves top-1 and top-3 inference accuracy by 44.16% and\n52.96%, respectively, under a strict sensing constraint. The performance gain,\nhowever, diminishes as the sensing constraint is relaxed."}
{"id": "2511.01431", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.01431", "abs": "https://arxiv.org/abs/2511.01431", "authors": ["Simin Zhu", "Satish Ravindran", "Lihui Chen", "Alexander Yarovoy", "Francesco Fioranelli"], "title": "Robust Radar Mounting Angle Estimation in Operational Driving Conditions", "comment": "10 pages, 6 figures, under review at IEEE Transactions on Radar\n  Systems", "summary": "The robust estimation of the mounting angle for millimeter-wave automotive\nradars installed on moving vehicles is investigated. We propose a novel signal\nprocessing pipeline that combines radar and inertial measurement unit (IMU)\ndata to achieve accurate and reliable performance in realistic driving\nscenarios. Unlike previous studies, the method employs neural networks to\nprocess sparse and noisy radar measurements, reject detections from moving\nobjects, and estimate radar motion. In addition, a measurement model is\nintroduced to correct IMU bias and scale factor errors. Using vehicle\nkinematics, the radar mounting angle is then computed from the estimated radar\nmotion and the vehicle's yaw rate. To benchmark performance, the proposed\napproach is comprehensively compared with two problem formulations and four\nestimation techniques reported in the literature. Validation is carried out on\nthe challenging RadarScenes dataset, covering over 79 km of real-world driving.\nResults show that the proposed method achieves state-of-the-art accuracy and\nrobustness, with reliable estimates obtained within approximately 25 seconds of\ndriving. To the best of our knowledge, this is the first study to demonstrate\nthat automotive radar mounting angles can be accurately estimated in complex,\nreal traffic conditions, without requiring controlled environments, dedicated\ntargets, or specially designed driving routes."}
{"id": "2511.01575", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2511.01575", "abs": "https://arxiv.org/abs/2511.01575", "authors": ["Marjan Boloori", "Chu Li", "Aydin Sezgin"], "title": "Optimizing Movable Antenna Position and Transmissive RIS Phase for Efficient Base Station Design", "comment": null, "summary": "Movable antennas (MA) and transmissive reconfigurable intelligent surfaces\n(TRIS) represent two innovative technologies that significantly enhance the\nflexibility of wireless communication systems. In this paper, we propose a\nnovel and compact base station architecture that synergistically integrates a\nmovable antenna with a transmissive RIS in the near field, enabling joint\noptimization of antenna positioning and TRIS phase adjustments. The proposed\nmodel compensates for phase quantization loss and significantly enhances signal\nstrength, even with low-resolution (1-2 bit) phase shifters. Leveraging this\nframework, we systematically evaluate system performance as a function of TRIS\nsize and antenna placement. Our results indicate that antenna mobility provides\nan additional degree of freedom to enhance the desired signal and achieve a\nhigher SNR, particularly when combined with TRIS capabilities. These findings\ndemonstrate that MA-TRIS integration offers a cost-effective and\nenergy-efficient pathway toward compact 6G base stations, combining hardware\nsimplicity with strong performance gains."}
{"id": "2511.01599", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.01599", "abs": "https://arxiv.org/abs/2511.01599", "authors": ["M. Ertug Pihtili", "Julia Equi", "Ossi Kaltiokallio", "Jukka Talvitie", "Elena Simona Lohan", "Ertugrul Basar", "Mikko Valkama"], "title": "Clutter Suppression in Bistatic ISAC with Joint Angle and Doppler Estimation", "comment": null, "summary": "The coexistence of radar and communications in wireless systems marks a\nparadigm shift for the sixth-generation (6G) networks. As 6G systems are\nexpected to operate at higher frequencies and employ larger antenna arrays than\nfifth-generation (5G) systems, they can also enable more accurate sensing\ncapabilities. To this end, the integrated sensing and communication (ISAC)\nparadigm aims to unify the physical and radio frequency (RF) domains by\nintroducing the sensing functionality into the communication network. However,\nthe clutter poses a challenge, as it can significantly degrade the sensing\naccuracy in ISAC systems. This paper presents a novel two-dimensional root\nmultiple signal classification (2D-rootMUSIC)-based algorithm for static\nbackground clutter suppression. Computer simulation results indicate that the\nproposed method effectively mitigates the strong background clutter, yields\naccurate parameter estimation performance, and offers a notable improvement in\nthe signal-to-clutter-and-noise ratio (SCNR), while outperforming the prior-art\nbenchmark methods."}
{"id": "2511.01747", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.01747", "abs": "https://arxiv.org/abs/2511.01747", "authors": ["Guangkun Nie", "Gongzheng Tang", "Yujie Xiao", "Jun Li", "Shun Huang", "Deyun Zhang", "Qinghao Zhao", "Shenda Hong"], "title": "AnyPPG: An ECG-Guided PPG Foundation Model Trained on Over 100,000 Hours of Recordings for Holistic Health Profiling", "comment": null, "summary": "Background: Photoplethysmography (PPG) offers a noninvasive and accessible\nmodality for health monitoring beyond clinical settings. However, existing\nstudies are limited by the scale and diversity of labeled data, constraining\nmodel accuracy, generalizability, and the exploration of broader applications.\nThis study investigates the potential of PPG for holistic health profiling\nthrough the integration of foundation model techniques.\n  Methods: We present AnyPPG, a PPG foundation model pretrained on large-scale,\nmulti-source synchronized PPG-ECG data. By aligning PPG and ECG representations\nwithin a shared space, AnyPPG learns physiologically meaningful features from\nunlabeled signals. Its capability was further evaluated across a diverse set of\ndownstream tasks, encompassing both conventional physiological analysis and\ncomprehensive multi-organ disease diagnosis.\n  Results: Across eleven physiological analysis tasks spanning six independent\ndatasets, AnyPPG achieved state-of-the-art performance, with average\nimprovements of 12.8% in regression and 9.1% in classification tasks over the\nnext-best model. In multi-organ disease diagnosis, AnyPPG demonstrated broad\ncross-system diagnostic potential. Among 1,014 ICD-10 three-digit disease\ncategories, 13 achieved an AUC above 0.8 and 137 exceeded 0.7. Beyond strong\nperformance in cardiovascular diseases such as heart failure, valvular\ndisorders, and hypertension, AnyPPG also showed substantial diagnostic value\nfor non-cardiovascular conditions, exemplified by Parkinson's disease (AUC =\n0.78) and chronic kidney disease (AUC = 0.74).\n  Conclusions: AnyPPG demonstrates that a PPG foundation model trained through\nphysiological alignment with ECG can produce accurate and robust signal\nrepresentations. Building on this capability, it underscores the potential of\nPPG as a modality for comprehensive assessment of systemic and multi-organ\nhealth."}
{"id": "2511.01780", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.01780", "abs": "https://arxiv.org/abs/2511.01780", "authors": ["Quan Gao", "Shuai S. A. Yuan", "Zhanwen Wang", "Wanchen Yang", "Chongwen Huang", "Xiaoming Chen", "Wei E. I. Sha"], "title": "On Systematic Performance of 3-D Holographic MIMO: Clarke, Kronecker, and 3GPP Models", "comment": "11 pages, 17 figures, submitted to Electromagnetic Science", "summary": "Holographic multiple-input multiple-output (MIMO) has emerged as a key\nenabler for 6G networks, yet conventional planar implementations suffer from\nspatial correlation and mutual coupling at sub-wavelength spacing, which\nfundamentally limit the effective degrees of freedom (EDOF) and channel\ncapacity. Three-dimensional (3-D) holographic MIMO offers a pathway to overcome\nthese constraints by exploiting volumetric array configurations that enlarge\nthe effective aperture and unlock additional spatial modes. This work presents\nthe first systematic evaluation that jointly incorporates electromagnetic (EM)\ncharacteristics, such as mutual coupling and radiation efficiency, into the\nanalysis of 3-D arrays under Clarke, Kronecker, and standardized 3rd Generation\nPartnership Project (3GPP) channel models. Analytical derivations and full-wave\nsimulations demonstrate that 3-D architectures achieve higher EDOF, narrower\nbeamwidths, and notable capacity improvements compared with planar baselines.\nIn 3GPP urban macro channels with horizontal element spacing of 0.3 lambda, 3-D\nconfigurations yield approximately 20% capacity improvement over conventional\n2-D arrays, confirming the robustness and scalability of volumetric designs\nunder realistic conditions. These findings bridge the gap between theoretical\nfeasibility and practical deployment, offering design guidance for\nnext-generation 6G base station arrays."}
{"id": "2511.01787", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.01787", "abs": "https://arxiv.org/abs/2511.01787", "authors": ["David Nozadze", "Zurab Kiguradze", "Amendra Koul", "Sayed Ashraf Mamun", "Mike Sapozhnikov"], "title": "Practical Approaches to Quantifying Intra-Pair Skew Impact via Insertion Loss Deviation", "comment": null, "summary": "The surge in AI workloads and escalating data center requirements have\ncreated demand for ultra-high-speed interconnects exceeding 200 Gb/s. As unit\nintervals (UI) shrink, even a few picoseconds of intra-pair skew can\nsignificantly degrade serializer-deserializer (SerDes) performance. To quantify\nthe impact of intra-pair skew, conventional time-domain methods are often\nunreliable for coupled interconnects due to skew variations across voltage\nlevels, while frequency-domain approaches frequently fail to address\nreciprocity and symmetry issues. This can result in channels that meet skew\nspecifications in one direction but not the other, despite the inherently\nreciprocal nature of skew impact. To address these limitations, we introduce\ntwo new reciprocal parameters for quantifying intra-pair skew effects:\nSkew-Induced Insertion Loss Deviation (SILD) and its complementary Figure of\nMerit (FOM SILD). Measurements conducted using 224 Gb/s SerDes IP and a variety\nof channels with different intra-pair skews demonstrate a strong correlation\nbetween FOM SILD and bit error rate (BER). Results show that when FOM SILD is\nbelow 0.2-0.3 dB, BER remains stable, indicating minimal signal integrity\ndegradation; however, BER increases noticeably as FOM SILD exceeds 0.3 dB.\nStatistical analysis across more than 3,000 high-speed twinax cables reveals\nthat the majority exhibit FOM SILD values less than 0.1 dB, underscoring the\npractical relevance of the proposed metrics for high-speed interconnect\nassessment."}

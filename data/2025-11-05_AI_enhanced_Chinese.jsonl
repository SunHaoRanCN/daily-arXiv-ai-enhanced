{"id": "2511.02104", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2511.02104", "abs": "https://arxiv.org/abs/2511.02104", "authors": ["Cedric Chan", "Jianjing Kuang"], "title": "Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach", "comment": null, "summary": "Prosody is essential for speech technology, shaping comprehension,\nnaturalness, and expressiveness. However, current text-to-speech (TTS) systems\nstill struggle to accurately capture human-like prosodic variation, in part\nbecause existing evaluation methods for prosody remain limited. Traditional\nmetrics like Mean Opinion Score (MOS) are resource-intensive, inconsistent, and\noffer little insight into why a system sounds unnatural. This study introduces\na linguistically informed, semi-automatic framework for evaluating TTS prosody\nthrough a two-tier architecture that mirrors human prosodic organization. The\nmethod uses quantitative linguistic criteria to evaluate synthesized speech\nagainst human speech corpora across multiple acoustic dimensions. By\nintegrating discrete and continuous prosodic measures, it provides objective\nand interpretable metrics of both event placement and cue realization, while\naccounting for the natural variability observed across speakers and prosodic\ncues. Results show strong correlations with perceptual MOS ratings while\nrevealing model-specific weaknesses that traditional perceptual tests alone\ncannot capture. This approach provides a principled path toward diagnosing,\nbenchmarking, and ultimately improving the prosodic naturalness of\nnext-generation TTS systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bed\u8a00\u5b66\u77e5\u8bc6\u7684\u534a\u81ea\u52a8\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30TTS\u7cfb\u7edf\u7684\u97f5\u5f8b\u8868\u73b0\uff0c\u901a\u8fc7\u4e24\u5c42\u67b6\u6784\u6a21\u62df\u4eba\u7c7b\u97f5\u5f8b\u7ec4\u7ec7\uff0c\u63d0\u4f9b\u5ba2\u89c2\u53ef\u89e3\u91ca\u7684\u97f5\u5f8b\u8bc4\u4f30\u6307\u6807\u3002", "motivation": "\u5f53\u524dTTS\u7cfb\u7edf\u96be\u4ee5\u51c6\u786e\u6355\u6349\u4eba\u7c7b\u97f5\u5f8b\u53d8\u5316\uff0c\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u5982MOS\u8bc4\u5206\u8d44\u6e90\u5bc6\u96c6\u3001\u4e0d\u4e00\u81f4\u4e14\u65e0\u6cd5\u89e3\u91ca\u7cfb\u7edf\u4e3a\u4f55\u542c\u8d77\u6765\u4e0d\u81ea\u7136\u3002", "method": "\u4f7f\u7528\u5b9a\u91cf\u8bed\u8a00\u5b66\u6807\u51c6\uff0c\u5728\u591a\u4e2a\u58f0\u5b66\u7ef4\u5ea6\u4e0a\u8bc4\u4f30\u5408\u6210\u8bed\u97f3\u4e0e\u4eba\u7c7b\u8bed\u97f3\u8bed\u6599\u5e93\u7684\u5dee\u5f02\uff0c\u6574\u5408\u79bb\u6563\u548c\u8fde\u7eed\u97f5\u5f8b\u6d4b\u91cf\uff0c\u8003\u8651\u8bf4\u8bdd\u4eba\u548c\u97f5\u5f8b\u7ebf\u7d22\u7684\u81ea\u7136\u53d8\u5f02\u6027\u3002", "result": "\u7ed3\u679c\u663e\u793a\u4e0e\u611f\u77e5MOS\u8bc4\u5206\u6709\u5f3a\u76f8\u5173\u6027\uff0c\u540c\u65f6\u63ed\u793a\u4e86\u4f20\u7edf\u611f\u77e5\u6d4b\u8bd5\u65e0\u6cd5\u6355\u6349\u7684\u6a21\u578b\u7279\u5b9a\u5f31\u70b9\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u8bca\u65ad\u3001\u57fa\u51c6\u6d4b\u8bd5\u548c\u6700\u7ec8\u6539\u8fdb\u4e0b\u4e00\u4ee3TTS\u7cfb\u7edf\u7684\u97f5\u5f8b\u81ea\u7136\u5ea6\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u8def\u5f84\u3002"}}
{"id": "2511.02252", "categories": ["eess.AS", "cs.SD", "eess.SP"], "pdf": "https://arxiv.org/pdf/2511.02252", "abs": "https://arxiv.org/abs/2511.02252", "authors": ["Junyi Fan", "Donald S. Williamson"], "title": "From the perspective of perceptual speech quality: The robustness of frequency bands to noise", "comment": "Accepted to J. Acoust. Soc. Am. (JASA) 155, 1916-1927 (2024)", "summary": "Speech quality is one of the main foci of speech-related research, where it\nis frequently studied with speech intelligibility, another essential\nmeasurement. Band-level perceptual speech intelligibility, however, has been\nstudied frequently, whereas speech quality has not been thoroughly analyzed. In\nthis paper, a Multiple Stimuli With Hidden Reference and Anchor (MUSHRA)\ninspired approach was proposed to study the individual robustness of frequency\nbands to noise with perceptual speech quality as the measure. Speech signals\nwere filtered into thirty-two frequency bands with compromising real-world\nnoise employed at different signal-to-noise ratios. Robustness to noise indices\nof individual frequency bands was calculated based on the human-rated\nperceptual quality scores assigned to the reconstructed noisy speech signals.\nTrends in the results suggest the mid-frequency region appeared less robust to\nnoise in terms of perceptual speech quality. These findings suggest future\nresearch aiming at improving speech quality should pay more attention to the\nmid-frequency region of the speech signals accordingly.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eMUSHRA\u7684\u65b9\u6cd5\u6765\u7814\u7a76\u4e0d\u540c\u9891\u6bb5\u5bf9\u566a\u58f0\u7684\u9c81\u68d2\u6027\uff0c\u53d1\u73b0\u4e2d\u9891\u533a\u57df\u5728\u611f\u77e5\u8bed\u97f3\u8d28\u91cf\u65b9\u9762\u5bf9\u566a\u58f0\u7684\u9c81\u68d2\u6027\u8f83\u5dee\u3002", "motivation": "\u8bed\u97f3\u8d28\u91cf\u662f\u8bed\u97f3\u7814\u7a76\u7684\u91cd\u8981\u6307\u6807\uff0c\u4f46\u4ee5\u5f80\u7814\u7a76\u591a\u5173\u6ce8\u9891\u6bb5\u7ea7\u7684\u8bed\u97f3\u53ef\u61c2\u5ea6\uff0c\u800c\u5bf9\u8bed\u97f3\u8d28\u91cf\u7684\u9891\u6bb5\u5206\u6790\u4e0d\u591f\u6df1\u5165\u3002", "method": "\u91c7\u7528MUSHRA\u542f\u53d1\u7684\u65b9\u6cd5\uff0c\u5c06\u8bed\u97f3\u4fe1\u53f7\u5206\u4e3a32\u4e2a\u9891\u6bb5\uff0c\u5728\u4e0d\u540c\u4fe1\u566a\u6bd4\u4e0b\u52a0\u5165\u771f\u5b9e\u566a\u58f0\uff0c\u57fa\u4e8e\u4eba\u7c7b\u8bc4\u5206\u7684\u611f\u77e5\u8d28\u91cf\u8ba1\u7b97\u5404\u9891\u6bb5\u7684\u566a\u58f0\u9c81\u68d2\u6027\u6307\u6570\u3002", "result": "\u7ed3\u679c\u663e\u793a\u4e2d\u9891\u533a\u57df\u5728\u611f\u77e5\u8bed\u97f3\u8d28\u91cf\u65b9\u9762\u5bf9\u566a\u58f0\u7684\u9c81\u68d2\u6027\u8f83\u5dee\u3002", "conclusion": "\u672a\u6765\u6539\u5584\u8bed\u97f3\u8d28\u91cf\u7684\u7814\u7a76\u5e94\u66f4\u591a\u5173\u6ce8\u8bed\u97f3\u4fe1\u53f7\u7684\u4e2d\u9891\u533a\u57df\u3002"}}
{"id": "2511.02270", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2511.02270", "abs": "https://arxiv.org/abs/2511.02270", "authors": ["Kaimeng Jia", "Minzhu Tu", "Zengrui Jin", "Siyin Wang", "Chao Zhang"], "title": "Augmenting Open-Vocabulary Dysarthric Speech Assessment with Human Perceptual Supervision", "comment": "Submission of IEEE ICASSP 2026", "summary": "Dysarthria is a speech disorder characterized by impaired intelligibility and\nreduced communicative effectiveness. Automatic dysarthria assessment provides a\nscalable, cost-effective approach for supporting the diagnosis and treatment of\nneurological conditions such as Parkinson's disease, Alzheimer's disease, and\nstroke. This study investigates leveraging human perceptual annotations from\nspeech synthesis assessment as reliable out-of-domain knowledge for dysarthric\nspeech assessment. Experimental results suggest that such supervision can yield\nconsistent and substantial performance improvements in self-supervised learning\npre-trained models. These findings suggest that perceptual ratings aligned with\nhuman judgments from speech synthesis evaluations represent valuable resources\nfor dysarthric speech modeling, enabling effective cross-domain knowledge\ntransfer.", "AI": {"tldr": "\u5229\u7528\u8bed\u97f3\u5408\u6210\u8bc4\u4f30\u4e2d\u7684\u4eba\u7c7b\u611f\u77e5\u6807\u6ce8\u4f5c\u4e3a\u8de8\u9886\u57df\u77e5\u8bc6\uff0c\u63d0\u5347\u6784\u97f3\u969c\u788d\u8bed\u97f3\u8bc4\u4f30\u6a21\u578b\u7684\u6027\u80fd", "motivation": "\u6784\u97f3\u969c\u788d\u662f\u4e00\u79cd\u5f71\u54cd\u8bed\u97f3\u6e05\u6670\u5ea6\u548c\u4ea4\u6d41\u6548\u679c\u7684\u8a00\u8bed\u969c\u788d\uff0c\u81ea\u52a8\u8bc4\u4f30\u53ef\u4e3a\u5e15\u91d1\u68ee\u75c5\u3001\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u548c\u4e2d\u98ce\u7b49\u795e\u7ecf\u7cfb\u7edf\u75be\u75c5\u7684\u8bca\u65ad\u548c\u6cbb\u7597\u63d0\u4f9b\u53ef\u6269\u5c55\u3001\u7ecf\u6d4e\u6709\u6548\u7684\u652f\u6301", "method": "\u7814\u7a76\u63a2\u7d22\u5c06\u8bed\u97f3\u5408\u6210\u8bc4\u4f30\u4e2d\u7684\u4eba\u7c7b\u611f\u77e5\u6807\u6ce8\u4f5c\u4e3a\u53ef\u9760\u7684\u8de8\u9886\u57df\u77e5\u8bc6\uff0c\u7528\u4e8e\u6784\u97f3\u969c\u788d\u8bed\u97f3\u8bc4\u4f30\uff0c\u5e76\u5728\u81ea\u76d1\u7763\u5b66\u4e60\u9884\u8bad\u7ec3\u6a21\u578b\u4e2d\u5e94\u7528\u8fd9\u79cd\u76d1\u7763\u65b9\u6cd5", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8fd9\u79cd\u76d1\u7763\u65b9\u6cd5\u80fd\u5728\u81ea\u76d1\u7763\u5b66\u4e60\u9884\u8bad\u7ec3\u6a21\u578b\u4e2d\u4ea7\u751f\u4e00\u81f4\u4e14\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347", "conclusion": "\u4e0e\u8bed\u97f3\u5408\u6210\u8bc4\u4f30\u4e2d\u4eba\u7c7b\u5224\u65ad\u4e00\u81f4\u7684\u611f\u77e5\u8bc4\u5206\u662f\u6784\u97f3\u969c\u788d\u8bed\u97f3\u5efa\u6a21\u7684\u5b9d\u8d35\u8d44\u6e90\uff0c\u80fd\u591f\u5b9e\u73b0\u6709\u6548\u7684\u8de8\u9886\u57df\u77e5\u8bc6\u8fc1\u79fb"}}
{"id": "2511.02278", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2511.02278", "abs": "https://arxiv.org/abs/2511.02278", "authors": ["Zheqi Yuan", "Yucheng Huang", "Guangzhi Sun", "Zengrui Jin", "Chao Zhang"], "title": "Multiplexing Neural Audio Watermarks", "comment": "Submission of IEEE ICASSP 2026", "summary": "Audio watermarking is a promising tool to ensure authenticity of speech\ncontent. However, existing watermarking methods remain vulnerable to more\nadvanced dilution attacks such as lossy compression and neural reconstruction.\nIn this paper, we propose to multiplex neural audio watermarking techniques to\nleverage their complementarity under different types of attacks. Specifically,\nfive different multiplexing designs are investigated, including parallel,\nsequential, frequency-division, time-division and perceptual adaptive\ntime-frequency multiplexing (PA-TFM). We evaluate our multiplexing technique on\nLibriSpeech data with 11 different attack methods, including 2 new neural\nreconstruction attacks featuring recent advancements in speech processing. As a\nresult, the proposed PA-TFM as a training-free multiplexing method achieves\nbetter performance than single watermarking baselines by clear margins,\nshowcasing a more robust way of using watermarks for audio.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u8def\u590d\u7528\u795e\u7ecf\u97f3\u9891\u6c34\u5370\u6280\u672f\uff0c\u901a\u8fc7\u4e94\u79cd\u4e0d\u540c\u7684\u591a\u8def\u590d\u7528\u8bbe\u8ba1\u6765\u589e\u5f3a\u6c34\u5370\u5728\u5404\u7c7b\u653b\u51fb\u4e0b\u7684\u9c81\u68d2\u6027\uff0c\u5176\u4e2d\u611f\u77e5\u81ea\u9002\u5e94\u65f6\u9891\u591a\u8def\u590d\u7528(PA-TFM)\u5728\u65e0\u9700\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u663e\u8457\u4f18\u4e8e\u5355\u6c34\u5370\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u97f3\u9891\u6c34\u5370\u65b9\u6cd5\u5728\u9762\u5bf9\u6709\u635f\u538b\u7f29\u548c\u795e\u7ecf\u91cd\u5efa\u7b49\u66f4\u5148\u8fdb\u7684\u7a00\u91ca\u653b\u51fb\u65f6\u4ecd\u7136\u8106\u5f31\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u9c81\u68d2\u7684\u8ba4\u8bc1\u6280\u672f\u6765\u786e\u4fdd\u8bed\u97f3\u5185\u5bb9\u7684\u771f\u5b9e\u6027\u3002", "method": "\u7814\u7a76\u4e86\u4e94\u79cd\u591a\u8def\u590d\u7528\u8bbe\u8ba1\uff1a\u5e76\u884c\u3001\u987a\u5e8f\u3001\u9891\u5206\u3001\u65f6\u5206\u548c\u611f\u77e5\u81ea\u9002\u5e94\u65f6\u9891\u591a\u8def\u590d\u7528(PA-TFM)\uff0c\u5728LibriSpeech\u6570\u636e\u4e0a\u4f7f\u752811\u79cd\u4e0d\u540c\u653b\u51fb\u65b9\u6cd5\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u63d0\u51fa\u7684PA-TFM\u4f5c\u4e3a\u65e0\u9700\u8bad\u7ec3\u7684\u591a\u8def\u590d\u7528\u65b9\u6cd5\uff0c\u5728\u6027\u80fd\u4e0a\u660e\u663e\u4f18\u4e8e\u5355\u6c34\u5370\u57fa\u7ebf\uff0c\u5c55\u793a\u4e86\u66f4\u9c81\u68d2\u7684\u97f3\u9891\u6c34\u5370\u4f7f\u7528\u65b9\u5f0f\u3002", "conclusion": "\u591a\u8def\u590d\u7528\u795e\u7ecf\u97f3\u9891\u6c34\u5370\u6280\u672f\u80fd\u591f\u5229\u7528\u4e0d\u540c\u6c34\u5370\u65b9\u6cd5\u5728\u5404\u79cd\u653b\u51fb\u4e0b\u7684\u4e92\u8865\u6027\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6c34\u5370\u7684\u9c81\u68d2\u6027\uff0c\u7279\u522b\u662fPA-TFM\u65b9\u6cd5\u8868\u73b0\u6700\u4f73\u3002"}}
{"id": "2511.02454", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2511.02454", "abs": "https://arxiv.org/abs/2511.02454", "authors": ["Shogo Seki", "Shaoxiang Dang", "Li Li"], "title": "Improving DF-Conformer Using Hydra For High-Fidelity Generative Speech Enhancement on Discrete Codec Token", "comment": "Submitted to ICASSP 2026. Audio samples available at\n  https://s-seki.github.io/dc_hydra/", "summary": "The Dilated FAVOR Conformer (DF-Conformer) is an efficient variant of the\nConformer architecture designed for speech enhancement (SE). It employs fast\nattention through positive orthogonal random features (FAVOR+) to mitigate the\nquadratic complexity associated with self-attention, while utilizing dilated\nconvolution to expand the receptive field. This combination results in\nimpressive performance across various SE models. In this paper, we propose\nreplacing FAVOR+ with bidirectional selective structured state-space sequence\nmodels to achieve two main objectives:(1) enhancing global sequential modeling\nby eliminating the approximations inherent in FAVOR+, and (2) maintaining\nlinear complexity relative to the sequence length. Specifically, we utilize\nHydra, a bidirectional extension of Mamba, framed within the structured matrix\nmixer framework. Experiments conducted using a generative SE model on discrete\ncodec tokens, known as Genhancer, demonstrate that the proposed method\nsurpasses the performance of the DF-Conformer.", "AI": {"tldr": "\u63d0\u51fa\u7528\u53cc\u5411\u9009\u62e9\u6027\u7ed3\u6784\u5316\u72b6\u6001\u7a7a\u95f4\u5e8f\u5217\u6a21\u578b\u66ff\u4ee3DF-Conformer\u4e2d\u7684FAVOR+\u6ce8\u610f\u529b\u673a\u5236\uff0c\u4ee5\u589e\u5f3a\u5168\u5c40\u5e8f\u5217\u5efa\u6a21\u80fd\u529b\u5e76\u4fdd\u6301\u7ebf\u6027\u590d\u6742\u5ea6\u3002", "motivation": "\u6539\u8fdbDF-Conformer\u67b6\u6784\uff0c\u6d88\u9664FAVOR+\u4e2d\u7684\u8fd1\u4f3c\u8bef\u5dee\uff0c\u540c\u65f6\u4fdd\u6301\u7ebf\u6027\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u4ee5\u63d0\u5347\u8bed\u97f3\u589e\u5f3a\u6027\u80fd\u3002", "method": "\u4f7f\u7528Hydra\uff08Mamba\u7684\u53cc\u5411\u6269\u5c55\uff09\u4f5c\u4e3a\u7ed3\u6784\u5316\u77e9\u9635\u6df7\u5408\u5668\uff0c\u66ff\u4ee3\u539f\u6709\u7684FAVOR+\u6ce8\u610f\u529b\u673a\u5236\u3002", "result": "\u5728\u57fa\u4e8e\u79bb\u6563\u7f16\u89e3\u7801\u5668token\u7684\u751f\u6210\u5f0f\u8bed\u97f3\u589e\u5f3a\u6a21\u578bGenhancer\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6027\u80fd\u4f18\u4e8eDF-Conformer\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u6210\u529f\u63d0\u5347\u4e86\u8bed\u97f3\u589e\u5f3a\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u53cc\u5411\u9009\u62e9\u6027\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u5728\u66ff\u4ee3\u8fd1\u4f3c\u6ce8\u610f\u529b\u673a\u5236\u65b9\u9762\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2511.01867", "categories": ["eess.SP", "cs.AI", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2511.01867", "abs": "https://arxiv.org/abs/2511.01867", "authors": ["Zhengdong Hu", "Chong Han", "Wolfgang Gerstacker", "Robert Schober"], "title": "DiffPace: Diffusion-based Plug-and-play Augmented Channel Estimation in mmWave and Terahertz Ultra-Massive MIMO Systems", "comment": null, "summary": "Millimeter-wave (mmWave) and Terahertz (THz)-band communications hold great\npromise in meeting the growing data-rate demands of next-generation wireless\nnetworks, offering abundant bandwidth. To mitigate the severe path loss\ninherent to these high frequencies and reduce hardware costs, ultra-massive\nmultiple-input multiple-output (UM-MIMO) systems with hybrid beamforming\narchitectures can deliver substantial beamforming gains and enhanced spectral\nefficiency. However, accurate channel estimation (CE) in mmWave and THz UM-MIMO\nsystems is challenging due to high channel dimensionality and compressed\nobservations from a limited number of RF chains, while the hybrid near- and\nfar-field radiation patterns, arising from large array apertures and high\ncarrier frequencies, further complicate CE. Conventional compressive sensing\nbased frameworks rely on predefined sparsifying matrices, which cannot\nfaithfully capture the hybrid near-field and far-field channel structures,\nleading to degraded estimation performance. This paper introduces DiffPace, a\ndiffusion-based plug-and-play method for channel estimation. DiffPace uses a\ndiffusion model (DM) to capture the channel distribution based on the hybrid\nspherical and planar-wave (HPSM) model. By applying the plug-and-play approach,\nit leverages the DM as prior knowledge, improving CE accuracy. Moreover, DM\nperforms inference by solving an ordinary differential equation, minimizing the\nnumber of required inference steps compared with stochastic sampling method.\nExperimental results show that DiffPace achieves competitive CE performance,\nattaining -15 dB normalized mean square error (NMSE) at a signal-to-noise ratio\n(SNR) of 10 dB, with 90\\% fewer inference steps compared to state-of-the-art\nschemes, simultaneously providing high estimation precision and enhanced\ncomputational efficiency.", "AI": {"tldr": "DiffPace\u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u5373\u63d2\u5373\u7528\u4fe1\u9053\u4f30\u8ba1\u65b9\u6cd5\uff0c\u7528\u4e8e\u6beb\u7c73\u6ce2\u548c\u592a\u8d6b\u5179\u8d85\u5927\u89c4\u6a21MIMO\u7cfb\u7edf\uff0c\u901a\u8fc7\u6df7\u5408\u7403\u9762\u6ce2\u548c\u5e73\u9762\u6ce2\u6a21\u578b\u6355\u83b7\u4fe1\u9053\u5206\u5e03\uff0c\u5728\u51cf\u5c1190%\u63a8\u7406\u6b65\u9aa4\u7684\u540c\u65f6\u5b9e\u73b0-15 dB NMSE\u7684\u9ad8\u7cbe\u5ea6\u4f30\u8ba1\u3002", "motivation": "\u6beb\u7c73\u6ce2\u548c\u592a\u8d6b\u5179\u901a\u4fe1\u9762\u4e34\u4e25\u91cd\u7684\u8def\u5f84\u635f\u8017\uff0c\u4f20\u7edf\u538b\u7f29\u611f\u77e5\u65b9\u6cd5\u65e0\u6cd5\u51c6\u786e\u6355\u6349\u6df7\u5408\u8fd1\u573a\u548c\u8fdc\u573a\u4fe1\u9053\u7ed3\u6784\uff0c\u5bfc\u81f4\u4fe1\u9053\u4f30\u8ba1\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u4f7f\u7528\u6269\u6563\u6a21\u578b\u4f5c\u4e3a\u5148\u9a8c\u77e5\u8bc6\uff0c\u901a\u8fc7\u5373\u63d2\u5373\u7528\u65b9\u6cd5\u7ed3\u5408\u6df7\u5408\u7403\u9762\u6ce2\u548c\u5e73\u9762\u6ce2\u6a21\u578b\uff0c\u5229\u7528\u5e38\u5fae\u5206\u65b9\u7a0b\u8fdb\u884c\u63a8\u7406\u4ee5\u51cf\u5c11\u63a8\u7406\u6b65\u9aa4\u3002", "result": "\u5728\u4fe1\u566a\u6bd410 dB\u65f6\u8fbe\u5230-15 dB\u5f52\u4e00\u5316\u5747\u65b9\u8bef\u5dee\uff0c\u6bd4\u6700\u5148\u8fdb\u65b9\u6848\u51cf\u5c1190%\u63a8\u7406\u6b65\u9aa4\uff0c\u540c\u65f6\u63d0\u4f9b\u9ad8\u4f30\u8ba1\u7cbe\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "DiffPace\u5728\u6beb\u7c73\u6ce2\u548c\u592a\u8d6b\u5179\u8d85\u5927\u89c4\u6a21MIMO\u7cfb\u7edf\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u4fe1\u9053\u4f30\u8ba1\u548c\u663e\u8457\u7684\u8ba1\u7b97\u6548\u7387\u63d0\u5347\uff0c\u4e3a\u4e0b\u4e00\u4ee3\u65e0\u7ebf\u7f51\u7edc\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.02717", "categories": ["eess.SP", "cs.AI", "cs.CV", "cs.SY", "eess.AS", "eess.SY", "68T05 (Learning and adaptive systems)", "I.2.6; I.2.8"], "pdf": "https://arxiv.org/pdf/2511.02717", "abs": "https://arxiv.org/abs/2511.02717", "authors": ["Marios Impraimakis", "Andrew W. Smyth"], "title": "An unscented Kalman filter method for real time input-parameter-state estimation", "comment": "author-accepted manuscript (AAM) published in Mechanical Systems and\n  Signal Processing", "summary": "The input-parameter-state estimation capabilities of a novel unscented Kalman\nfilter is examined herein on both linear and nonlinear systems. The unknown\ninput is estimated in two stages within each time step. Firstly, the predicted\ndynamic states and the system parameters provide an estimation of the input.\nSecondly, the corrected with measurements states and parameters provide a final\nestimation. Importantly, it is demonstrated using the perturbation analysis\nthat, a system with at least a zero or a non-zero known input can potentially\nbe uniquely identified. This output-only methodology allows for a better\nunderstanding of the system compared to classical output-only parameter\nidentification strategies, given that all the dynamic states, the parameters,\nand the input are estimated jointly and in real-time.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u4e00\u79cd\u65b0\u578b\u65e0\u8ff9\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\u5728\u7ebf\u6027\u548c\u975e\u7ebf\u6027\u7cfb\u7edf\u4e2d\u7684\u8f93\u5165-\u53c2\u6570-\u72b6\u6001\u4f30\u8ba1\u80fd\u529b\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8f93\u5165\u4f30\u8ba1\u548c\u6270\u52a8\u5206\u6790\u8bc1\u660e\u4e86\u7cfb\u7edf\u53ef\u8bc6\u522b\u6027\u3002", "motivation": "\u4f20\u7edf\u8f93\u51fa\u53c2\u6570\u8bc6\u522b\u7b56\u7565\u53ea\u80fd\u4f30\u8ba1\u53c2\u6570\uff0c\u800c\u8be5\u65b9\u6cd5\u80fd\u591f\u8054\u5408\u5b9e\u65f6\u4f30\u8ba1\u6240\u6709\u52a8\u6001\u72b6\u6001\u3001\u53c2\u6570\u548c\u8f93\u5165\uff0c\u4ece\u800c\u66f4\u597d\u5730\u7406\u89e3\u7cfb\u7edf\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u8f93\u5165\u4f30\u8ba1\u65b9\u6cd5\uff1a\u9996\u5148\u7528\u9884\u6d4b\u7684\u52a8\u6001\u72b6\u6001\u548c\u7cfb\u7edf\u53c2\u6570\u4f30\u8ba1\u8f93\u5165\uff0c\u7136\u540e\u7528\u6d4b\u91cf\u6821\u6b63\u540e\u7684\u72b6\u6001\u548c\u53c2\u6570\u63d0\u4f9b\u6700\u7ec8\u4f30\u8ba1\u3002", "result": "\u901a\u8fc7\u6270\u52a8\u5206\u6790\u8bc1\u660e\uff0c\u81f3\u5c11\u5177\u6709\u96f6\u6216\u975e\u96f6\u5df2\u77e5\u8f93\u5165\u7684\u7cfb\u7edf\u53ef\u80fd\u88ab\u552f\u4e00\u8bc6\u522b\uff0c\u5b9e\u73b0\u4e86\u6bd4\u4f20\u7edf\u8f93\u51fa\u53c2\u6570\u8bc6\u522b\u7b56\u7565\u66f4\u597d\u7684\u7cfb\u7edf\u7406\u89e3\u3002", "conclusion": "\u8be5\u8f93\u51fa\u65b9\u6cd5\u80fd\u591f\u8054\u5408\u5b9e\u65f6\u4f30\u8ba1\u52a8\u6001\u72b6\u6001\u3001\u53c2\u6570\u548c\u8f93\u5165\uff0c\u4e3a\u7cfb\u7edf\u8bc6\u522b\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.02726", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2511.02726", "abs": "https://arxiv.org/abs/2511.02726", "authors": ["Yuexuan Kong", "Viet-Anh Tran", "Romain Hennequin"], "title": "Perceived Femininity in Singing Voice: Analysis and Prediction", "comment": null, "summary": "This paper focuses on the often-overlooked aspect of perceived voice\nfemininity in singing voices. While existing research has examined perceived\nvoice femininity in speech, the same concept has not yet been studied in\nsinging voice. The analysis of gender bias in music content could benefit from\nsuch study. To address this gap, we design a stimuli-based survey to measure\nperceived singing voice femininity (PSVF), and collect responses from 128\nparticipants. Our analysis reveals intriguing insights into how PSVF varies\nacross different demographic groups. Furthermore, we propose an automatic PSVF\nprediction model by fine-tuning an x-vector model, offering a novel tool for\nexploring gender stereotypes related to voices in music content analysis beyond\nbinary sex classification. This study contributes to a deeper understanding of\nthe complexities surrounding perceived femininity in singing voices by\nanalyzing survey and proposes an automatic tool for future research.", "AI": {"tldr": "\u672c\u7814\u7a76\u9996\u6b21\u63a2\u8ba8\u6b4c\u5531\u58f0\u97f3\u4e2d\u7684\u611f\u77e5\u5973\u6027\u6c14\u8d28\uff0c\u901a\u8fc7\u8c03\u67e5\u548c\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5206\u6790\u8fd9\u4e00\u88ab\u5ffd\u89c6\u7684\u58f0\u5b66\u7279\u5f81\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u8bed\u97f3\u4e2d\u7684\u611f\u77e5\u5973\u6027\u6c14\u8d28\uff0c\u4f46\u6b4c\u5531\u58f0\u97f3\u4e2d\u7684\u8fd9\u4e00\u6982\u5ff5\u5c1a\u672a\u88ab\u7814\u7a76\uff0c\u800c\u5206\u6790\u97f3\u4e50\u5185\u5bb9\u4e2d\u7684\u6027\u522b\u504f\u89c1\u9700\u8981\u6b64\u7c7b\u7814\u7a76\u3002", "method": "\u8bbe\u8ba1\u4e86\u57fa\u4e8e\u523a\u6fc0\u7684\u95ee\u5377\u8c03\u67e5\u6765\u6d4b\u91cf\u611f\u77e5\u6b4c\u5531\u58f0\u97f3\u5973\u6027\u6c14\u8d28\uff0c\u6536\u96c6\u4e86128\u540d\u53c2\u4e0e\u8005\u7684\u53cd\u9988\uff0c\u5e76\u901a\u8fc7\u5fae\u8c03x-vector\u6a21\u578b\u6784\u5efa\u4e86\u81ea\u52a8\u9884\u6d4b\u6a21\u578b\u3002", "result": "\u5206\u6790\u63ed\u793a\u4e86\u4e0d\u540c\u4eba\u53e3\u7edf\u8ba1\u7fa4\u4f53\u4e2d\u611f\u77e5\u6b4c\u5531\u58f0\u97f3\u5973\u6027\u6c14\u8d28\u7684\u5dee\u5f02\uff0c\u5e76\u6210\u529f\u5f00\u53d1\u4e86\u81ea\u52a8\u9884\u6d4b\u5de5\u5177\u3002", "conclusion": "\u672c\u7814\u7a76\u6df1\u5316\u4e86\u5bf9\u6b4c\u5531\u58f0\u97f3\u4e2d\u611f\u77e5\u5973\u6027\u6c14\u8d28\u590d\u6742\u6027\u7684\u7406\u89e3\uff0c\u5e76\u4e3a\u672a\u6765\u97f3\u4e50\u5185\u5bb9\u5206\u6790\u4e2d\u8d85\u8d8a\u4e8c\u5143\u6027\u522b\u5206\u7c7b\u7684\u6027\u522b\u523b\u677f\u5370\u8c61\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u3002"}}
{"id": "2511.01879", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.01879", "abs": "https://arxiv.org/abs/2511.01879", "authors": ["HM Shadman Tabib", "Md. Hasnaen Adil", "Ayesha Rahman", "Ahmmad Nur Swapnil", "Maoyejatun Hasana", "Ahmed Hossain Chowdhury", "A. B. M. Alim Al Islam"], "title": "Affordable EEG, Actionable Insights: An Open Dataset and Evaluation Framework for Epilepsy Patient Stratification", "comment": null, "summary": "Access to clinical multi-channel EEG remains limited in many regions\nworldwide. We present NEUROSKY-EPI, the first open dataset of single-channel,\nconsumer-grade EEG for epilepsy, collected in a South Asian clinical setting\nalong with rich contextual metadata. To explore its utility, we introduce\nEmbedCluster, a patient-stratification pipeline that transfers representations\nfrom EEGNet models trained on clinical data and enriches them with contextual\nautoencoder embeddings, followed by unsupervised clustering of patients based\non EEG patterns. Results show that low-cost, single-channel data can support\nmeaningful stratification. Beyond algorithmic performance, we emphasize\nhuman-centered concerns such as deployability in resource-constrained\nenvironments, interpretability for non-specialists, and safeguards for privacy,\ninclusivity, and bias. By releasing the dataset and code, we aim to catalyze\ninterdisciplinary research across health technology, human-computer\ninteraction, and machine learning, advancing the goal of affordable and\nactionable EEG-based epilepsy care.", "AI": {"tldr": "NEUROSKY-EPI\u662f\u9996\u4e2a\u7528\u4e8e\u766b\u75eb\u7814\u7a76\u7684\u5f00\u6e90\u5355\u901a\u9053\u6d88\u8d39\u7ea7EEG\u6570\u636e\u96c6\uff0c\u7ed3\u5408EmbedCluster\u60a3\u8005\u5206\u5c42\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u4f4e\u6210\u672cEEG\u6570\u636e\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u7684\u4e34\u5e8a\u5e94\u7528\u6f5c\u529b\u3002", "motivation": "\u5168\u7403\u8bb8\u591a\u5730\u533a\u7f3a\u4e4f\u4e34\u5e8a\u591a\u901a\u9053EEG\u6570\u636e\uff0c\u9700\u8981\u5f00\u53d1\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u53ef\u90e8\u7f72\u7684\u4f4e\u6210\u672c\u766b\u75eb\u8bca\u65ad\u5de5\u5177\u3002", "method": "\u63d0\u51faEmbedCluster\u60a3\u8005\u5206\u5c42\u6d41\u7a0b\uff1a\u4ece\u4e34\u5e8a\u6570\u636e\u8bad\u7ec3\u7684EEGNet\u6a21\u578b\u8fc1\u79fb\u8868\u5f81\uff0c\u7ed3\u5408\u4e0a\u4e0b\u6587\u81ea\u7f16\u7801\u5668\u5d4c\u5165\uff0c\u901a\u8fc7\u65e0\u76d1\u7763\u805a\u7c7b\u5bf9\u60a3\u8005\u8fdb\u884cEEG\u6a21\u5f0f\u5206\u5c42\u3002", "result": "\u7814\u7a76\u8868\u660e\u4f4e\u6210\u672c\u5355\u901a\u9053EEG\u6570\u636e\u80fd\u591f\u652f\u6301\u6709\u610f\u4e49\u7684\u60a3\u8005\u5206\u5c42\uff0c\u5728\u7b97\u6cd5\u6027\u80fd\u4e4b\u5916\u5f3a\u8c03\u53ef\u90e8\u7f72\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u9690\u79c1\u4fdd\u62a4\u3002", "conclusion": "\u901a\u8fc7\u53d1\u5e03\u6570\u636e\u96c6\u548c\u4ee3\u7801\uff0c\u4fc3\u8fdb\u8de8\u5b66\u79d1\u7814\u7a76\uff0c\u63a8\u52a8\u7ecf\u6d4e\u5b9e\u60e0\u4e14\u53ef\u64cd\u4f5c\u7684EEG\u766b\u75eb\u62a4\u7406\u53d1\u5c55\u3002"}}
{"id": "2511.01882", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2511.01882", "abs": "https://arxiv.org/abs/2511.01882", "authors": ["Tingting Huang", "Jundong Chen", "Huanqiang Zeng", "Guofa Cai", "Haoyu Zhou"], "title": "Design of an M-ary Chaos Shift Keying System Using Combined Chaotic Systems", "comment": null, "summary": "In traditional chaos shift keying (CSK) communication systems, implementing\nchaotic synchronization techniques is costly but practically unattainable in a\nnoisy environment. This paper proposes a combined chaotic sequences-based\n$M$-ary CSK (CCS-$M$-CSK) system that eliminates the need for chaotic\nsynchronization. At the transmitter, the chaotic sequence is constructed by\ncombining two chaotic segments of different lengths, where each is generated\nfrom distinct chaotic systems and only one kind of chaotic segment modulates\nthe information signal. At the receiver, a deep learning unit with binary\nclassification is meticulously designed to recover information symbols. The\nsymbol error rate (SER) performance of the proposed system is evaluated over\nadditive white Gaussian noise (AWGN) and multipath Rayleigh fading channels.\nSpecifically, the impact of varying misalignment lengths on the SER performance\nof the system is analyzed when the received sequence is misaligned.\nFurthermore, the proposed system demonstrates significant performance\nadvantages over existing CSK-based systems in multipath Rayleigh fading\nchannels. These features establish CCS-$M$-CSK as a promising candidate for\nvarious applications, including Vehicle-to-Everything (V2X).", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7ec4\u5408\u6df7\u6c8c\u5e8f\u5217\u7684M\u5143CSK\u7cfb\u7edf\uff0c\u65e0\u9700\u6df7\u6c8c\u540c\u6b65\uff0c\u4f7f\u7528\u6df1\u5ea6\u5b66\u4e60\u8fdb\u884c\u7b26\u53f7\u6062\u590d\uff0c\u5728AWGN\u548c\u591a\u5f84\u745e\u5229\u8870\u843d\u4fe1\u9053\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709CSK\u7cfb\u7edf\u3002", "motivation": "\u4f20\u7edfCSK\u901a\u4fe1\u7cfb\u7edf\u5b9e\u73b0\u6df7\u6c8c\u540c\u6b65\u6210\u672c\u9ad8\u4e14\u5728\u566a\u58f0\u73af\u5883\u4e2d\u96be\u4ee5\u5b9e\u73b0\uff0c\u9700\u8981\u4e00\u79cd\u65e0\u9700\u540c\u6b65\u7684\u6df7\u6c8c\u901a\u4fe1\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u7ec4\u5408\u4e24\u4e2a\u4e0d\u540c\u6df7\u6c8c\u7cfb\u7edf\u751f\u6210\u7684\u4e0d\u540c\u957f\u5ea6\u6df7\u6c8c\u6bb5\u6784\u5efa\u6df7\u6c8c\u5e8f\u5217\uff0c\u4ec5\u7528\u4e00\u79cd\u6df7\u6c8c\u6bb5\u8c03\u5236\u4fe1\u606f\u4fe1\u53f7\uff1b\u63a5\u6536\u7aef\u4f7f\u7528\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u4e8c\u5143\u5206\u7c7b\u6df1\u5ea6\u5b66\u4e60\u5355\u5143\u6062\u590d\u4fe1\u606f\u7b26\u53f7\u3002", "result": "\u5728AWGN\u548c\u591a\u5f84\u745e\u5229\u8870\u843d\u4fe1\u9053\u4e2d\u8bc4\u4f30\u4e86\u7cfb\u7edf\u6027\u80fd\uff0c\u5206\u6790\u4e86\u63a5\u6536\u5e8f\u5217\u5931\u51c6\u5bf9SER\u7684\u5f71\u54cd\uff0c\u5728\u591a\u5f84\u745e\u5229\u8870\u843d\u4fe1\u9053\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709CSK\u7cfb\u7edf\u3002", "conclusion": "CCS-M-CSK\u7cfb\u7edf\u662f\u5404\u79cd\u5e94\u7528\uff08\u5305\u62ecV2X\uff09\u7684\u6709\u524d\u666f\u5019\u9009\u65b9\u6848\uff0c\u5177\u6709\u65e0\u9700\u6df7\u6c8c\u540c\u6b65\u548c\u4f18\u8d8a\u7684\u6297\u591a\u5f84\u6027\u80fd\u3002"}}
{"id": "2511.02006", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.02006", "abs": "https://arxiv.org/abs/2511.02006", "authors": ["Logan Schexnaydre", "Aman Poovalappil", "Darrell Robinette", "Jeremy Bos"], "title": "A Comparison of Road Grade Preview Signals from Lidar and Maps", "comment": "8 pages, 10 figures, submitted to SAE WCX 2026", "summary": "Road grade can impact the energy efficiency, safety, and comfort associated\nwith automated vehicle control systems. Currently, control systems that attempt\nto compensate for road grade are designed with one of two assumptions. Either\nthe grade is only known once the vehicle is driving over the road segment\nthrough proprioception, or complete knowledge of the oncoming road grade is\nknown from a pre-made map. Both assumptions limit the performance of a control\nsystem, as not having a preview signal prevents proactive grade compensation,\nwhereas relying only on map data potentially subjects the control system to\nmissing or outdated information. These limits can be avoided by measuring the\noncoming grade in real-time using on-board lidar sensors. In this work, we use\npoint returns accumulated during travel to estimate the grade at each waypoint\nalong a path. The estimated grade is defined as the difference in height\nbetween the front and rear wheelbase at a given waypoint. Kalman filtering\ntechniques are used to mitigate the effects of odometry and motion uncertainty\non the grade estimates. This estimator's performance is compared to the\nmeasurements of a map created with a GNSS/INS system via a field experiment.\nWhen compared to the map-based system, the lidar-based estimator produces an\nunbiased error with a standard deviation of 0.6 degrees at an average range of\n52.7 meters. By having similar precision to map-based systems, automotive\nlidar-based grade estimation systems are shown to be a valid approach for\nmeasuring road grade when a map is unavailable or inaccurate. In using lidar as\nan input signal for grade-based control system tasks, autonomous vehicles\nachieve higher redundancy and independence in contrast to existing methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u8f66\u8f7d\u6fc0\u5149\u96f7\u8fbe\u5b9e\u65f6\u4f30\u8ba1\u9053\u8def\u5761\u5ea6\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5361\u5c14\u66fc\u6ee4\u6ce2\u6280\u672f\u5904\u7406\u91cc\u7a0b\u8ba1\u548c\u8fd0\u52a8\u4e0d\u786e\u5b9a\u6027\uff0c\u4e0e\u57fa\u4e8e\u5730\u56fe\u7684\u7cfb\u7edf\u76f8\u6bd4\u5177\u6709\u76f8\u4f3c\u7684\u7cbe\u5ea6\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u63d0\u4f9b\u4e86\u66f4\u9ad8\u7684\u5197\u4f59\u6027\u548c\u72ec\u7acb\u6027\u3002", "motivation": "\u73b0\u6709\u63a7\u5236\u7cfb\u7edf\u8981\u4e48\u53ea\u80fd\u5728\u8f66\u8f86\u884c\u9a76\u65f6\u901a\u8fc7\u672c\u4f53\u611f\u77e5\u83b7\u53d6\u5761\u5ea6\u4fe1\u606f\uff0c\u8981\u4e48\u5b8c\u5168\u4f9d\u8d56\u9884\u8bbe\u5730\u56fe\u6570\u636e\uff0c\u8fd9\u4e24\u79cd\u65b9\u6cd5\u90fd\u6709\u5c40\u9650\u6027\u3002\u524d\u8005\u65e0\u6cd5\u5b9e\u73b0\u4e3b\u52a8\u5761\u5ea6\u8865\u507f\uff0c\u540e\u8005\u53ef\u80fd\u9762\u4e34\u6570\u636e\u7f3a\u5931\u6216\u8fc7\u65f6\u7684\u95ee\u9898\u3002", "method": "\u5229\u7528\u8f66\u8f7d\u6fc0\u5149\u96f7\u8fbe\u5728\u884c\u9a76\u8fc7\u7a0b\u4e2d\u79ef\u7d2f\u7684\u70b9\u4e91\u6570\u636e\uff0c\u901a\u8fc7\u8ba1\u7b97\u7ed9\u5b9a\u8def\u5f84\u70b9\u524d\u540e\u8f6e\u8f74\u4e4b\u95f4\u7684\u9ad8\u5ea6\u5dee\u6765\u4f30\u8ba1\u5761\u5ea6\uff0c\u5e76\u4f7f\u7528\u5361\u5c14\u66fc\u6ee4\u6ce2\u6280\u672f\u6765\u51cf\u8f7b\u91cc\u7a0b\u8ba1\u548c\u8fd0\u52a8\u4e0d\u786e\u5b9a\u6027\u5bf9\u5761\u5ea6\u4f30\u8ba1\u7684\u5f71\u54cd\u3002", "result": "\u4e0e\u57fa\u4e8eGNSS/INS\u7cfb\u7edf\u521b\u5efa\u7684\u5730\u56fe\u6d4b\u91cf\u76f8\u6bd4\uff0c\u6fc0\u5149\u96f7\u8fbe\u4f30\u8ba1\u5668\u4ea7\u751f\u7684\u65e0\u504f\u8bef\u5dee\u6807\u51c6\u5dee\u4e3a0.6\u5ea6\uff0c\u5e73\u5747\u8303\u56f4\u4e3a52.7\u7c73\uff0c\u7cbe\u5ea6\u4e0e\u57fa\u4e8e\u5730\u56fe\u7684\u7cfb\u7edf\u76f8\u5f53\u3002", "conclusion": "\u6fc0\u5149\u96f7\u8fbe\u5761\u5ea6\u4f30\u8ba1\u7cfb\u7edf\u5728\u65e0\u6cd5\u83b7\u53d6\u6216\u5730\u56fe\u4e0d\u51c6\u786e\u65f6\u662f\u4e00\u79cd\u6709\u6548\u7684\u9053\u8def\u5761\u5ea6\u6d4b\u91cf\u65b9\u6cd5\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u63d0\u4f9b\u4e86\u66f4\u9ad8\u7684\u5197\u4f59\u6027\u548c\u72ec\u7acb\u6027\u3002"}}
{"id": "2511.02030", "categories": ["eess.SP", "cs.NI"], "pdf": "https://arxiv.org/pdf/2511.02030", "abs": "https://arxiv.org/abs/2511.02030", "authors": ["Brian Kim", "Justin H. Kong", "Terrence J. Moore", "Fikadu T. Dagefu"], "title": "Deep Reinforcement Learning for Multi-flow Routing in Heterogeneous Wireless Networks", "comment": null, "summary": "Due to the rapid growth of heterogeneous wireless networks (HWNs), where\ndevices with diverse communication technologies coexist, there is increasing\ndemand for efficient and adaptive multi-hop routing with multiple data flows.\nTraditional routing methods, designed for homogeneous environments, fail to\naddress the complexity introduced by links consisting of multiple technologies,\nfrequency-dependent fading, and dynamic topology changes. In this paper, we\npropose a deep reinforcement learning (DRL)-based routing framework using deep\nQ-networks (DQN) to establish routes between multiple source-destination pairs\nin HWNs by enabling each node to jointly select a communication technology, a\nsubband, and a next hop relay that maximizes the rate of the route. Our\napproach incorporates channel and interference-aware neighbor selection\napproaches to improve decision-making beyond conventional distance-based\nheuristics. We further evaluate the robustness and generalizability of the\nproposed method under varying network dynamics, including node mobility,\nchanges in node density, and the number of data flows. Simulation results\ndemonstrate that our DRL-based routing framework significantly enhances\nscalability, adaptability, and end-to-end throughput in complex HWN scenarios.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u8def\u7531\u6846\u67b6\uff0c\u7528\u4e8e\u5f02\u6784\u65e0\u7ebf\u7f51\u7edc\u4e2d\u7684\u591a\u8df3\u8def\u7531\uff0c\u901a\u8fc7\u8054\u5408\u9009\u62e9\u901a\u4fe1\u6280\u672f\u3001\u5b50\u5e26\u548c\u4e2d\u7ee7\u8282\u70b9\u6765\u4f18\u5316\u8def\u7531\u901f\u7387\u3002", "motivation": "\u5f02\u6784\u65e0\u7ebf\u7f51\u7edc\u4e2d\u4f20\u7edf\u8def\u7531\u65b9\u6cd5\u65e0\u6cd5\u5e94\u5bf9\u591a\u6280\u672f\u94fe\u8def\u3001\u9891\u7387\u76f8\u5173\u8870\u843d\u548c\u52a8\u6001\u62d3\u6251\u53d8\u5316\u5e26\u6765\u7684\u590d\u6742\u6027\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u81ea\u9002\u5e94\u7684\u8def\u7531\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u6df1\u5ea6Q\u7f51\u7edc\uff08DQN\uff09\u7684\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u8ba9\u6bcf\u4e2a\u8282\u70b9\u8054\u5408\u9009\u62e9\u901a\u4fe1\u6280\u672f\u3001\u5b50\u5e26\u548c\u4e0b\u4e00\u8df3\u4e2d\u7ee7\uff0c\u7ed3\u5408\u4fe1\u9053\u548c\u5e72\u6270\u611f\u77e5\u7684\u90bb\u5c45\u9009\u62e9\u65b9\u6cd5\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u590d\u6742\u5f02\u6784\u65e0\u7ebf\u7f51\u7edc\u573a\u666f\u4e2d\u663e\u8457\u63d0\u9ad8\u4e86\u53ef\u6269\u5c55\u6027\u3001\u9002\u5e94\u6027\u548c\u7aef\u5230\u7aef\u541e\u5410\u91cf\u3002", "conclusion": "\u57fa\u4e8e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u8def\u7531\u6846\u67b6\u80fd\u591f\u6709\u6548\u89e3\u51b3\u5f02\u6784\u65e0\u7ebf\u7f51\u7edc\u4e2d\u7684\u8def\u7531\u6311\u6218\uff0c\u5728\u5404\u79cd\u7f51\u7edc\u52a8\u6001\u53d8\u5316\u4e0b\u8868\u73b0\u51fa\u826f\u597d\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2511.02074", "categories": ["eess.SP", "cs.ET"], "pdf": "https://arxiv.org/pdf/2511.02074", "abs": "https://arxiv.org/abs/2511.02074", "authors": ["Mart\u00edn Schottlender", "Maximilian Sch\u00e4fer", "Ricardo A. Veiga"], "title": "Neural Network based Distance Estimation for Branched Molecular Communication Systems", "comment": "6 pages, 8 figures, published in International Conference on\n  Nanoscale Computing and Communication (NanoCom 25), 2025, Chengdu, China", "summary": "Molecular Communications (MC) is an emerging research paradigm that utilizes\nmolecules to transmit information, with promising applications in biomedicine\nsuch as targeted drug delivery or tumor detection. It is also envisioned as a\nkey enabler of the Internet of BioNanoThings (IoBNT). In this paper, we propose\nalgorithms based on Recurrent Neural Networks (RNN) for the estimation of\ncommunication channel parameters in MC systems. We focus on a simple branched\ntopology, simulating the molecule movement with a macroscopic MC simulator. The\nDeep Learning architectures proposed for distance estimation demonstrate strong\nperformance within these branched environments, highlighting their potential\nfor future MC applications.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5faa\u73af\u795e\u7ecf\u7f51\u7edc(RNN)\u7684\u7b97\u6cd5\uff0c\u7528\u4e8e\u4f30\u8ba1\u5206\u5b50\u901a\u4fe1\u7cfb\u7edf\u4e2d\u7684\u4fe1\u9053\u53c2\u6570\uff0c\u5728\u5206\u652f\u62d3\u6251\u7ed3\u6784\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u5206\u5b50\u901a\u4fe1\u5728\u751f\u7269\u533b\u5b66\u9886\u57df\u6709\u91cd\u8981\u5e94\u7528\u524d\u666f\uff0c\u5982\u9776\u5411\u836f\u7269\u9012\u9001\u548c\u80bf\u7624\u68c0\u6d4b\uff0c\u4e5f\u662f\u751f\u7269\u7eb3\u7c73\u7269\u8054\u7f51(IoBNT)\u7684\u5173\u952e\u4f7f\u80fd\u6280\u672f\u3002", "method": "\u4f7f\u7528\u5b8f\u89c2\u5206\u5b50\u901a\u4fe1\u6a21\u62df\u5668\u6a21\u62df\u5206\u5b50\u5728\u5206\u652f\u62d3\u6251\u4e2d\u7684\u8fd0\u52a8\uff0c\u63d0\u51fa\u57fa\u4e8e\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u7684\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\u8fdb\u884c\u8ddd\u79bb\u4f30\u8ba1\u3002", "result": "\u5728\u5206\u652f\u73af\u5883\u4e2d\uff0c\u6240\u63d0\u51fa\u7684\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6027\u80fd\u3002", "conclusion": "\u8fd9\u4e9b\u65b9\u6cd5\u5728\u5206\u5b50\u901a\u4fe1\u5e94\u7528\u4e2d\u5177\u6709\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2511.02084", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.02084", "abs": "https://arxiv.org/abs/2511.02084", "authors": ["Pallav Kumar Bera", "Samita Rani Pani"], "title": "Recurrence Plot and Change Quantile-based Deep Supervised and Semi-supervised Protection for Transmission Lines Connected to Photovoltaic Plants", "comment": "29 pages", "summary": "Conventional relays encounter difficulties in protecting transmission lines\n(TLs) connected to converter-based energy sources (CBESs) due to the influence\nof power electronics on fault characteristics. This article proposes a\nsingle-ended intelligent protection method for the TL segment between the grid\nand a Photovoltaic (PV) plant. The approach utilizes a Recurrence Matrix and an\nInceptionTime-based system to identify faults by using the mean change in\nquantiles of 3-phase currents. It determines the fault position and identifies\nthe faulty phase. ReliefF feature selection is applied to extract the optimal\nquantile features. The scheme's performance is assessed under abnormal\nconditions, including faults and capacitor and load-switching events, simulated\nin Power Systems Computer Aided Design / Electromagnetic Transients Program\n(PSCAD/EMTDC) on the Western System Coordinating Council (WSCC) 9-bus system,\nwith various fault and switching parameters. The scheme is also validated on\nthe New England IEEE 39-bus system and in presence of partially rated\nconverters. Additionally, the validation of the proposed strategy takes into\naccount various conditions, including double-circuit line configuration, noise,\nseries compensation, high-impedance faults, current transformer (CT)\nsaturation, evolving and cross-country faults, remote and local faults, as well\nas variations in PV capacity, sampling frequency, and data window size. To\naddress label scarcity and improve generalization, semi-supervised learning\nparadigms including label spreading, label propagation, and self-training are\nintegrated with the InceptionTime framework, enabling near-supervised\nperformance with limited annotated fault data. The results demonstrate that the\napproach is effective in handling different system configurations and\nconditions, ensuring the protection of TLs connected to large PV plants.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9012\u5f52\u77e9\u9635\u548cInceptionTime\u7684\u667a\u80fd\u4fdd\u62a4\u65b9\u6cd5\uff0c\u7528\u4e8e\u4fdd\u62a4\u8fde\u63a5\u5230\u5149\u4f0f\u7535\u7ad9\u7684\u8f93\u7535\u7ebf\u8def\uff0c\u901a\u8fc7\u91cf\u5316\u4e09\u76f8\u7535\u6d41\u53d8\u5316\u6765\u8bc6\u522b\u6545\u969c\u4f4d\u7f6e\u548c\u6545\u969c\u76f8\uff0c\u5e76\u91c7\u7528\u534a\u76d1\u7763\u5b66\u4e60\u89e3\u51b3\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u7ee7\u7535\u5668\u5728\u4fdd\u62a4\u8fde\u63a5\u5230\u57fa\u4e8e\u53d8\u6362\u5668\u7684\u80fd\u6e90\u7cfb\u7edf\u7684\u8f93\u7535\u7ebf\u8def\u65f6\u9047\u5230\u56f0\u96be\uff0c\u56e0\u4e3a\u7535\u529b\u7535\u5b50\u8bbe\u5907\u4f1a\u5f71\u54cd\u6545\u969c\u7279\u6027\u3002", "method": "\u4f7f\u7528\u9012\u5f52\u77e9\u9635\u548c\u57fa\u4e8eInceptionTime\u7684\u7cfb\u7edf\uff0c\u901a\u8fc7\u4e09\u76f8\u7535\u6d41\u5206\u4f4d\u6570\u7684\u5e73\u5747\u53d8\u5316\u6765\u8bc6\u522b\u6545\u969c\uff0c\u5e94\u7528ReliefF\u7279\u5f81\u9009\u62e9\u63d0\u53d6\u6700\u4f18\u5206\u4f4d\u6570\u7279\u5f81\uff0c\u5e76\u96c6\u6210\u534a\u76d1\u7763\u5b66\u4e60\u8303\u5f0f\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u4e0d\u540c\u7cfb\u7edf\u914d\u7f6e\u548c\u6761\u4ef6\u4e0b\u8868\u73b0\u6709\u6548\uff0c\u5305\u62ec\u53cc\u56de\u7ebf\u8def\u914d\u7f6e\u3001\u566a\u58f0\u3001\u4e32\u8054\u8865\u507f\u3001\u9ad8\u963b\u6297\u6545\u969c\u7b49\u591a\u79cd\u590d\u6742\u60c5\u51b5\uff0c\u786e\u4fdd\u4e86\u5927\u578b\u5149\u4f0f\u7535\u7ad9\u8fde\u63a5\u7684\u8f93\u7535\u7ebf\u8def\u4fdd\u62a4\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u667a\u80fd\u4fdd\u62a4\u65b9\u6848\u80fd\u591f\u6709\u6548\u5904\u7406\u4e0d\u540c\u7cfb\u7edf\u914d\u7f6e\u548c\u6761\u4ef6\uff0c\u4e3a\u8fde\u63a5\u5230\u5927\u578b\u5149\u4f0f\u7535\u7ad9\u7684\u8f93\u7535\u7ebf\u8def\u63d0\u4f9b\u53ef\u9760\u4fdd\u62a4\uff0c\u4e14\u901a\u8fc7\u534a\u76d1\u7763\u5b66\u4e60\u5728\u6709\u9650\u6807\u6ce8\u6570\u636e\u4e0b\u5b9e\u73b0\u63a5\u8fd1\u76d1\u7763\u5b66\u4e60\u7684\u6027\u80fd\u3002"}}
{"id": "2511.02105", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.02105", "abs": "https://arxiv.org/abs/2511.02105", "authors": ["Vivien Walter", "Dadi Bi", "Daniel L. Ruiz Blanco", "Yansha Deng"], "title": "CNN-Based Detection of Mixed-Molecule Concentrations in Molecular Communication", "comment": null, "summary": "Molecular communication (MC) is a promising paradigm for applications where\ntraditional electromagnetic communications are impractical. However, decoding\nchemical signals, especially in multi-transmitter systems, remains a key\nchallenge due to interference and complex propagation dynamics. In this paper,\nwe develop a one-dimensional fractal convolutional neural network (fCNN) to\ndetect the concentrations of multiple types of molecules based on the\nabsorbance spectra measured at a receiver. Our model is trained by both\nexperimental and simulated datasets, with the latter enhanced by noise modeling\nto mimic real-world measurements. We demonstrate that a noiseaugmented\nsimulated dataset can effectively be a substitute for experimental data,\nachieving similar decoding accuracy. Our approach successfully detects bit\nsequences in both binary and quadruple concentration shift keying (BCSK and\nQCSK) scenarios, even when transmitters are desynchronized, highlighting the\npotential of machine learning for robust MC signal detection.", "AI": {"tldr": "\u5f00\u53d1\u4e00\u7ef4\u5206\u5f62\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u7528\u4e8e\u5206\u5b50\u901a\u4fe1\u4e2d\u7684\u591a\u5206\u5b50\u6d53\u5ea6\u68c0\u6d4b\uff0c\u901a\u8fc7\u5b9e\u9a8c\u548c\u6a21\u62df\u6570\u636e\u96c6\u8bad\u7ec3\uff0c\u8bc1\u660e\u566a\u58f0\u589e\u5f3a\u7684\u6a21\u62df\u6570\u636e\u53ef\u66ff\u4ee3\u5b9e\u9a8c\u6570\u636e\uff0c\u5728BCSK\u548cQCSK\u573a\u666f\u4e2d\u6210\u529f\u68c0\u6d4b\u6bd4\u7279\u5e8f\u5217\u3002", "motivation": "\u5206\u5b50\u901a\u4fe1\u5728\u4f20\u7edf\u7535\u78c1\u901a\u4fe1\u4e0d\u53ef\u884c\u7684\u5e94\u7528\u4e2d\u5177\u6709\u524d\u666f\uff0c\u4f46\u591a\u53d1\u5c04\u5668\u7cfb\u7edf\u4e2d\u7684\u5316\u5b66\u4fe1\u53f7\u89e3\u7801\u56e0\u5e72\u6270\u548c\u590d\u6742\u4f20\u64ad\u52a8\u6001\u800c\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u4f7f\u7528\u4e00\u7ef4\u5206\u5f62\u5377\u79ef\u795e\u7ecf\u7f51\u7edc(fCNN)\uff0c\u57fa\u4e8e\u63a5\u6536\u5668\u6d4b\u91cf\u7684\u5438\u6536\u5149\u8c31\u68c0\u6d4b\u591a\u79cd\u5206\u5b50\u6d53\u5ea6\uff0c\u901a\u8fc7\u5b9e\u9a8c\u548c\u566a\u58f0\u589e\u5f3a\u7684\u6a21\u62df\u6570\u636e\u96c6\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u566a\u58f0\u589e\u5f3a\u7684\u6a21\u62df\u6570\u636e\u96c6\u53ef\u6709\u6548\u66ff\u4ee3\u5b9e\u9a8c\u6570\u636e\uff0c\u8fbe\u5230\u76f8\u4f3c\u7684\u89e3\u7801\u7cbe\u5ea6\uff0c\u5728\u4e8c\u8fdb\u5236\u548c\u56db\u8fdb\u5236\u6d53\u5ea6\u79fb\u4f4d\u952e\u63a7\u573a\u666f\u4e2d\u6210\u529f\u68c0\u6d4b\u6bd4\u7279\u5e8f\u5217\uff0c\u5373\u4f7f\u5728\u53d1\u5c04\u5668\u4e0d\u540c\u6b65\u7684\u60c5\u51b5\u4e0b\u3002", "conclusion": "\u673a\u5668\u5b66\u4e60\u5728\u5206\u5b50\u901a\u4fe1\u4e2d\u5177\u6709\u5f3a\u5927\u7684\u4fe1\u53f7\u68c0\u6d4b\u6f5c\u529b\uff0c\u7279\u522b\u662f\u5728\u591a\u53d1\u5c04\u5668\u7cfb\u7edf\u4e2d\u5904\u7406\u5e72\u6270\u548c\u590d\u6742\u4f20\u64ad\u52a8\u6001\u65b9\u9762\u3002"}}
{"id": "2511.02260", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.02260", "abs": "https://arxiv.org/abs/2511.02260", "authors": ["Ailton Oliveira", "Amir Khatibi", "Daniel Suzuki", "Ilan Correa", "Jos\u00e9 Rezende", "Aldebaro Klautau"], "title": "DL-Based Beam Management for mmWave Vehicular Networks Exploring Temporal Correlation", "comment": "10 pages, pre-print", "summary": "Millimeter wave communications are essential for modern wireless networks. It\nsupports high data rates but suffers from severe path loss, which requires\nprecise beam alignment to maintain reliable links. This beam management is\nparticularly challenging in highly dynamic scenarios such as\nvehicle-to-infrastructure, and several methods have been presented. In this\nwork, we propose a deep learning-based beam tracking framework that combines a\nposition-aware beam pre-selection strategy with sequential prediction using\nrecurrent neural networks. The proposed architecture can support deep learning\nmodels trained for both classification and regression. In contrast to many\nexisting studies that evaluate beam tracking under predominantly line-of-sight\n(LOS) conditions, our work explicitly includes highly challenging non-LOS\nscenarios - with up to 50% non-LOS incidence in certain datasets - to\nrigorously assess model robustness. Experimental results demonstrate that our\napproach maintains high top-K accuracy, even under adverse conditions, while\nreducing the beam measurement overhead by up to 50%.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u6ce2\u675f\u8ddf\u8e2a\u6846\u67b6\uff0c\u7ed3\u5408\u4f4d\u7f6e\u611f\u77e5\u6ce2\u675f\u9884\u9009\u7b56\u7565\u548c\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u5e8f\u5217\u9884\u6d4b\uff0c\u5728\u5305\u542b50%\u975e\u89c6\u8ddd\u573a\u666f\u7684\u6311\u6218\u6027\u73af\u5883\u4e0b\u4fdd\u6301\u9ad8\u7cbe\u5ea6\uff0c\u540c\u65f6\u51cf\u5c1150%\u7684\u6ce2\u675f\u6d4b\u91cf\u5f00\u9500\u3002", "motivation": "\u6beb\u7c73\u6ce2\u901a\u4fe1\u867d\u7136\u652f\u6301\u9ad8\u6570\u636e\u901f\u7387\uff0c\u4f46\u5b58\u5728\u4e25\u91cd\u7684\u8def\u5f84\u635f\u8017\uff0c\u9700\u8981\u7cbe\u786e\u7684\u6ce2\u675f\u5bf9\u51c6\u3002\u5728\u8f66\u8f86\u5230\u57fa\u7840\u8bbe\u65bd\u7b49\u9ad8\u5ea6\u52a8\u6001\u573a\u666f\u4e2d\uff0c\u6ce2\u675f\u7ba1\u7406\u5c24\u4e3a\u56f0\u96be\u3002", "method": "\u91c7\u7528\u6df1\u5ea6\u5b66\u4e60\u6ce2\u675f\u8ddf\u8e2a\u6846\u67b6\uff0c\u7ed3\u5408\u4f4d\u7f6e\u611f\u77e5\u6ce2\u675f\u9884\u9009\u7b56\u7565\u548c\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u5e8f\u5217\u9884\u6d4b\uff0c\u652f\u6301\u5206\u7c7b\u548c\u56de\u5f52\u4e24\u79cd\u8bad\u7ec3\u6a21\u5f0f\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5373\u4f7f\u5728\u5305\u542b50%\u975e\u89c6\u8ddd\u573a\u666f\u7684\u6076\u52a3\u6761\u4ef6\u4e0b\uff0c\u8be5\u65b9\u6cd5\u4ecd\u80fd\u4fdd\u6301\u9ad8top-K\u51c6\u786e\u7387\uff0c\u540c\u65f6\u5c06\u6ce2\u675f\u6d4b\u91cf\u5f00\u9500\u51cf\u5c1150%\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u6df1\u5ea6\u5b66\u4e60\u6ce2\u675f\u8ddf\u8e2a\u6846\u67b6\u5728\u6311\u6218\u6027\u975e\u89c6\u8ddd\u573a\u666f\u4e0b\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u9c81\u68d2\u6027\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u6ce2\u675f\u7ba1\u7406\u5f00\u9500\u3002"}}
{"id": "2511.02423", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.02423", "abs": "https://arxiv.org/abs/2511.02423", "authors": ["Mingran Sun", "Lu Bai", "Xiang Cheng", "Jianjun Wu"], "title": "LLM4PG: Adapting Large Language Model for Pathloss Map Generation via Synesthesia of Machines", "comment": null, "summary": "In this paper, a novel large language model (LLM)-based pathloss map\ngeneration model, termed LLM4PG, is proposed for sixth-generation (6G)\nAI-native communication systems via Synesthesia of Machines (SoM). To explore\nthe mapping mechanism between sensing images and pathloss maps, a new synthetic\nintelligent multi-modal sensing-communication dataset, SynthSoM-U2G, is\nconstructed, covering multiple scenarios, frequency bands, and flight\naltitudes. By adapting the LLM for cross-modal pathloss map generation for the\nfirst time, LLM4PG establishes an effective cross-domain alignment between the\nmulti-modal sensing-communication and natural language domains. A task-specific\nfine-tuning strategy with a tailored layer selection and activation scheme is\ndesigned to meet the demands of massive-scale, high-quality generation.\nCompared with conventional deep learning artificial intelligence generated\ncontent (AIGC) models, LLM4PG achieves more accurate pathloss map generation\nand stronger generalization across diverse conditions. Results show that LLM4PG\nattains an NMSE of 0.0454, outperforming the conventional AIGC model by over\n2.90 dB, while its cross-condition generalization achieves an NMSE of 0.0492,\nexceeding the baseline by 4.52 dB.", "AI": {"tldr": "\u63d0\u51fa\u4e86LLM4PG\u6a21\u578b\uff0c\u4e00\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8def\u5f84\u635f\u8017\u5730\u56fe\u751f\u6210\u65b9\u6cd5\uff0c\u7528\u4e8e6G AI\u539f\u751f\u901a\u4fe1\u7cfb\u7edf\uff0c\u901a\u8fc7\u673a\u5668\u901a\u611f\u5b9e\u73b0\u591a\u6a21\u6001\u611f\u77e5-\u901a\u4fe1\u6570\u636e\u5230\u8def\u5f84\u635f\u8017\u5730\u56fe\u7684\u8de8\u57df\u6620\u5c04\u3002", "motivation": "\u63a2\u7d22\u611f\u77e5\u56fe\u50cf\u4e0e\u8def\u5f84\u635f\u8017\u5730\u56fe\u4e4b\u95f4\u7684\u6620\u5c04\u673a\u5236\uff0c\u4e3a6G AI\u539f\u751f\u901a\u4fe1\u7cfb\u7edf\u63d0\u4f9b\u5927\u89c4\u6a21\u3001\u9ad8\u8d28\u91cf\u7684\u8def\u5f84\u635f\u8017\u5730\u56fe\u751f\u6210\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u6784\u5efa\u4e86SynthSoM-U2G\u591a\u6a21\u6001\u611f\u77e5-\u901a\u4fe1\u6570\u636e\u96c6\uff0c\u9996\u6b21\u5c06LLM\u9002\u914d\u7528\u4e8e\u8de8\u6a21\u6001\u8def\u5f84\u635f\u8017\u5730\u56fe\u751f\u6210\uff0c\u8bbe\u8ba1\u4e86\u4efb\u52a1\u7279\u5b9a\u7684\u5fae\u8c03\u7b56\u7565\u548c\u5c42\u9009\u62e9\u6fc0\u6d3b\u65b9\u6848\u3002", "result": "LLM4PG\u5b9e\u73b0\u4e86\u66f4\u51c6\u786e\u7684\u8def\u5f84\u635f\u8017\u5730\u56fe\u751f\u6210\u548c\u66f4\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\uff0cNMSE\u8fbe\u52300.0454\uff0c\u6bd4\u4f20\u7edfAIGC\u6a21\u578b\u63d0\u5347\u8d85\u8fc72.90 dB\uff0c\u8de8\u6761\u4ef6\u6cdb\u5316NMSE\u4e3a0.0492\uff0c\u6bd4\u57fa\u7ebf\u63d0\u53474.52 dB\u3002", "conclusion": "LLM4PG\u6a21\u578b\u5728\u8def\u5f84\u635f\u8017\u5730\u56fe\u751f\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u9a8c\u8bc1\u4e86LLM\u5728\u8de8\u6a21\u6001\u901a\u4fe1\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\uff0c\u4e3a6G\u901a\u4fe1\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.02426", "categories": ["eess.SP", "cs.AI", "cs.CV", "cs.IT", "cs.SY", "eess.SY", "math.IT", "68T05 (Learning and adaptive systems)", "I.2.6; I.2.8"], "pdf": "https://arxiv.org/pdf/2511.02426", "abs": "https://arxiv.org/abs/2511.02426", "authors": ["Marios Impraimakis"], "title": "A Kullback-Leibler divergence method for input-system-state identification", "comment": "32 pages, 17 figures, published in Journal of Sound and Vibration", "summary": "The capability of a novel Kullback-Leibler divergence method is examined\nherein within the Kalman filter framework to select the input-parameter-state\nestimation execution with the most plausible results. This identification\nsuffers from the uncertainty related to obtaining different results from\ndifferent initial parameter set guesses, and the examined approach uses the\ninformation gained from the data in going from the prior to the posterior\ndistribution to address the issue. Firstly, the Kalman filter is performed for\na number of different initial parameter sets providing the system\ninput-parameter-state estimation. Secondly, the resulting posterior\ndistributions are compared simultaneously to the initial prior distributions\nusing the Kullback-Leibler divergence. Finally, the identification with the\nleast Kullback-Leibler divergence is selected as the one with the most\nplausible results. Importantly, the method is shown to select the better\nperformed identification in linear, nonlinear, and limited information\napplications, providing a powerful tool for system monitoring.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eKullback-Leibler\u6563\u5ea6\u7684\u65b0\u65b9\u6cd5\uff0c\u5728\u5361\u5c14\u66fc\u6ee4\u6ce2\u6846\u67b6\u4e2d\u9009\u62e9\u6700\u5408\u7406\u7684\u8f93\u5165-\u53c2\u6570-\u72b6\u6001\u4f30\u8ba1\u7ed3\u679c\uff0c\u89e3\u51b3\u4e86\u4e0d\u540c\u521d\u59cb\u53c2\u6570\u96c6\u5bfc\u81f4\u4e0d\u540c\u7ed3\u679c\u7684\u4e0d\u786e\u5b9a\u6027\u95ee\u9898\u3002", "motivation": "\u7cfb\u7edf\u8bc6\u522b\u4e2d\u5b58\u5728\u4e0d\u786e\u5b9a\u6027\uff0c\u4e0d\u540c\u521d\u59cb\u53c2\u6570\u96c6\u731c\u6d4b\u4f1a\u5f97\u5230\u4e0d\u540c\u7ed3\u679c\uff0c\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u9009\u62e9\u6700\u5408\u7406\u7684\u4f30\u8ba1\u7ed3\u679c\u3002", "method": "\u9996\u5148\u5bf9\u591a\u4e2a\u4e0d\u540c\u521d\u59cb\u53c2\u6570\u96c6\u6267\u884c\u5361\u5c14\u66fc\u6ee4\u6ce2\uff0c\u7136\u540e\u4f7f\u7528Kullback-Leibler\u6563\u5ea6\u540c\u65f6\u6bd4\u8f83\u540e\u9a8c\u5206\u5e03\u4e0e\u5148\u9a8c\u5206\u5e03\uff0c\u6700\u540e\u9009\u62e9Kullback-Leibler\u6563\u5ea6\u6700\u5c0f\u7684\u8bc6\u522b\u7ed3\u679c\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u7ebf\u6027\u3001\u975e\u7ebf\u6027\u548c\u4fe1\u606f\u53d7\u9650\u5e94\u7528\u4e2d\u90fd\u80fd\u9009\u62e9\u6027\u80fd\u66f4\u597d\u7684\u8bc6\u522b\u7ed3\u679c\uff0c\u4e3a\u7cfb\u7edf\u76d1\u63a7\u63d0\u4f9b\u4e86\u6709\u529b\u5de5\u5177\u3002", "conclusion": "\u57fa\u4e8eKullback-Leibler\u6563\u5ea6\u7684\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u9009\u62e9\u6700\u5408\u7406\u7684\u7cfb\u7edf\u8bc6\u522b\u7ed3\u679c\uff0c\u89e3\u51b3\u4e86\u521d\u59cb\u53c2\u6570\u4e0d\u786e\u5b9a\u6027\u5e26\u6765\u7684\u95ee\u9898\u3002"}}
{"id": "2511.02444", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.02444", "abs": "https://arxiv.org/abs/2511.02444", "authors": ["Vered Karp", "Aseel Omar", "Alejandro Cohen"], "title": "Adaptive Compressed Integrate-and-Fire Time Encoding Machine", "comment": null, "summary": "Integrate-and-Fire Time Encoding Machine (IF-TEM) is a power-efficient\nasynchronous sampler that converts analog signals into non-uniform time-domain\nsamples. Adaptive IF-TEM (AIF-TEM) improves this machine by adapting its\nprocess to the characteristics of the input signal, thereby reducing the\nsampling rate. Compressed IF-TEM (CIF-TEM) reduces bit usage by performing\nanalog compression before quantization. In this paper, we introduce a combined\nAdaptive Compressed IF-TEM (ACIF-TEM) -- a new sampler that leverages the two\nmachines, AIF-TEM and CIF-TEM, where each reinforces the effectiveness of the\nother. We propose an efficient adaptive clockless time-to-digital converter\n(TDC) architecture for the novel sampler that integrates the compression stage\nwithin the TDC, facilitating the realization of the intended integrated system.\n\\ifconf \\else We analyze the total bit usage, and contrast its performance with\nthat of IF-TEM, AIF-TEM, and CIF-TEM.\\fi Via an evaluation study, we\ndemonstrate that the proposed ACIF-TEM sampler achieves lower Mean Square Error\n(MSE) with fewer bits, offering compression gains of at least 3-bit out of\n9-bits over AIF-TEM and 60\\% compression over IF-TEM, for fixed recovery MSE\nwith real audio signals.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u81ea\u9002\u5e94\u548c\u538b\u7f29\u6280\u672f\u7684ACIF-TEM\u91c7\u6837\u5668\uff0c\u901a\u8fc7\u96c6\u6210AIF-TEM\u548cCIF-TEM\u7684\u4f18\u52bf\uff0c\u5728\u964d\u4f4e\u91c7\u6837\u7387\u548c\u6bd4\u7279\u4f7f\u7528\u7684\u540c\u65f6\u63d0\u9ad8\u4fe1\u53f7\u6062\u590d\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709\u7684IF-TEM\u91c7\u6837\u5668\u867d\u7136\u529f\u7387\u6548\u7387\u9ad8\uff0c\u4f46\u91c7\u6837\u7387\u548c\u6bd4\u7279\u4f7f\u7528\u4ecd\u6709\u4f18\u5316\u7a7a\u95f4\u3002AIF-TEM\u901a\u8fc7\u81ea\u9002\u5e94\u964d\u4f4e\u91c7\u6837\u7387\uff0cCIF-TEM\u901a\u8fc7\u538b\u7f29\u51cf\u5c11\u6bd4\u7279\u4f7f\u7528\uff0c\u4f46\u4e24\u8005\u5355\u72ec\u4f7f\u7528\u6548\u679c\u6709\u9650\u3002", "method": "\u63d0\u51faACIF-TEM\u91c7\u6837\u5668\uff0c\u5c06\u81ea\u9002\u5e94IF-TEM\u548c\u538b\u7f29IF-TEM\u76f8\u7ed3\u5408\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u65e0\u65f6\u949f\u65f6\u95f4\u6570\u5b57\u8f6c\u6362\u5668\u67b6\u6784\uff0c\u5c06\u538b\u7f29\u9636\u6bb5\u96c6\u6210\u5728TDC\u5185\u90e8\u3002", "result": "\u5728\u771f\u5b9e\u97f3\u9891\u4fe1\u53f7\u6d4b\u8bd5\u4e2d\uff0cACIF-TEM\u5728\u56fa\u5b9a\u6062\u590dMSE\u6761\u4ef6\u4e0b\uff0c\u76f8\u6bd4AIF-TEM\u8282\u7701\u81f3\u5c113\u4f4d\uff08\u51719\u4f4d\uff09\uff0c\u76f8\u6bd4IF-TEM\u5b9e\u73b060%\u7684\u538b\u7f29\u7387\uff0c\u4e14MSE\u66f4\u4f4e\u3002", "conclusion": "ACIF-TEM\u901a\u8fc7\u7ed3\u5408\u81ea\u9002\u5e94\u548c\u538b\u7f29\u6280\u672f\uff0c\u5728\u964d\u4f4e\u6bd4\u7279\u4f7f\u7528\u7684\u540c\u65f6\u63d0\u9ad8\u4e86\u4fe1\u53f7\u6062\u590d\u7cbe\u5ea6\uff0c\u4e3a\u9ad8\u6548\u5f02\u6b65\u91c7\u6837\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.02457", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.02457", "abs": "https://arxiv.org/abs/2511.02457", "authors": ["Mohaddese Qaremohammadlou", "Mohammad Bagher Shamsollahi"], "title": "Investigating Brain Connectivity and Information Flow in Mental Workload Using EEG and fNIRS Integration", "comment": null, "summary": "This study investigates brain connectivity and information flow during mental\nworkload (MWL) by integrating electroencephalogram (EEG) and functional\nnear-infrared spectroscopy (fNIRS) signals. Utilizing the N-back task to induce\nvarying levels of MWL in 26 participants, we analyzed both functional and\neffective connectivity across 25 cortical regions derived from combined EEG and\nfNIRS signals. Functional connectivity was assessed using Pearson Correlation\nCoefficient (PCC), Phase Locking Value (PLV), and Magnitude Squared Coherence\n(MSC), while effective connectivity was evaluated using directed Directed\nTransfer Function (dDTF) and generalized Partial Directed Coherence (gPDC). Our\nfindings reveal increased functional connectivity in frontal regions during\nhigher MWL conditions (3-back compared to 0-back). Furthermore, effective\nconnectivity analysis demonstrates a significant directional information flow\nfrom EEG to fNIRS, indicating a dominant influence of neural activity on\nhemodynamic responses. Statistical tests confirm significant differences in\nconnectivity patterns between low and high MWL states. These results underscore\nthe utility of EEG-fNIRS integration for characterizing brain network dynamics\nunder varying cognitive demands and provide insights into neurovascular\ncoupling mechanisms during mental workload.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u6574\u5408EEG\u548cfNIRS\u4fe1\u53f7\uff0c\u5206\u6790\u4e8626\u540d\u53c2\u4e0e\u8005\u5728N-back\u4efb\u52a1\u4e2d\u4e0d\u540c\u8111\u529b\u8d1f\u8377\u6c34\u5e73\u4e0b\u7684\u8111\u8fde\u63a5\u6027\u548c\u4fe1\u606f\u6d41\uff0c\u53d1\u73b0\u9ad8\u8d1f\u8377\u65f6\u989d\u53f6\u529f\u80fd\u8fde\u63a5\u589e\u5f3a\uff0c\u4e14\u5b58\u5728\u4eceEEG\u5230fNIRS\u7684\u5b9a\u5411\u4fe1\u606f\u6d41\u3002", "motivation": "\u7814\u7a76\u8111\u529b\u8d1f\u8377\u4e0b\u7684\u5927\u8111\u7f51\u7edc\u52a8\u6001\u548c\u795e\u7ecf\u8840\u7ba1\u8026\u5408\u673a\u5236\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u795e\u7ecf\u6210\u50cf\u6280\u672f\u66f4\u5168\u9762\u5730\u7406\u89e3\u8ba4\u77e5\u8fc7\u7a0b\u4e2d\u7684\u8111\u529f\u80fd\u8fde\u63a5\u3002", "method": "\u4f7f\u7528N-back\u4efb\u52a1\u8bf1\u53d1\u4e0d\u540c\u8111\u529b\u8d1f\u8377\u6c34\u5e73\uff0c\u7ed3\u5408EEG\u548cfNIRS\u4fe1\u53f7\u5206\u679025\u4e2a\u76ae\u5c42\u533a\u57df\u7684\u529f\u80fd\u8fde\u63a5\uff08PCC\u3001PLV\u3001MSC\uff09\u548c\u6709\u6548\u8fde\u63a5\uff08dDTF\u3001gPDC\uff09\u3002", "result": "\u9ad8\u8111\u529b\u8d1f\u8377\uff083-back\uff09\u65f6\u989d\u53f6\u529f\u80fd\u8fde\u63a5\u589e\u5f3a\uff0c\u6709\u6548\u8fde\u63a5\u5206\u6790\u663e\u793a\u4eceEEG\u5230fNIRS\u7684\u663e\u8457\u5b9a\u5411\u4fe1\u606f\u6d41\uff0c\u7edf\u8ba1\u68c0\u9a8c\u8bc1\u5b9e\u4e0d\u540c\u8d1f\u8377\u72b6\u6001\u4e0b\u7684\u8fde\u63a5\u6a21\u5f0f\u5b58\u5728\u663e\u8457\u5dee\u5f02\u3002", "conclusion": "EEG-fNIRS\u6574\u5408\u6709\u52a9\u4e8e\u8868\u5f81\u4e0d\u540c\u8ba4\u77e5\u9700\u6c42\u4e0b\u7684\u5927\u8111\u7f51\u7edc\u52a8\u6001\uff0c\u5e76\u4e3a\u8111\u529b\u8d1f\u8377\u671f\u95f4\u7684\u795e\u7ecf\u8840\u7ba1\u8026\u5408\u673a\u5236\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\u3002"}}
{"id": "2511.02493", "categories": ["eess.SP", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.02493", "abs": "https://arxiv.org/abs/2511.02493", "authors": ["Ana P\u00e9rez-Neira", "Marc Martinez-Gost", "Miguel \u00c1ngel Lagunas"], "title": "Before AI Takes Over: Rethinking Nonlinear Signal Processing in Communications", "comment": "Submitted to npj Wireless Technology", "summary": "There is an urgent reflection on traditional nonlinear signal processing\nmethods in communications before Artificial Intelligence (AI) dominates the\nfield. It implies a need to reassess or reinterpret established theories and\ntools, highlighting the tension between data-driven and model-based approaches.\nThis paper calls for preserving valuable insights from classical signal\nprocessing while exploring how they can coexist or integrate with emerging AI\nmethods.", "AI": {"tldr": "\u672c\u6587\u547c\u5401\u5728AI\u4e3b\u5bfc\u901a\u4fe1\u9886\u57df\u4e4b\u524d\uff0c\u91cd\u65b0\u8bc4\u4f30\u4f20\u7edf\u975e\u7ebf\u6027\u4fe1\u53f7\u5904\u7406\u65b9\u6cd5\uff0c\u5f3a\u8c03\u6570\u636e\u9a71\u52a8\u4e0e\u6a21\u578b\u9a71\u52a8\u65b9\u6cd5\u4e4b\u95f4\u7684\u5f20\u529b\uff0c\u5e76\u63a2\u7d22\u7ecf\u5178\u4fe1\u53f7\u5904\u7406\u4e0e\u65b0\u5174AI\u65b9\u6cd5\u7684\u5171\u5b58\u4e0e\u6574\u5408\u3002", "motivation": "\u5728\u4eba\u5de5\u667a\u80fd\u5373\u5c06\u4e3b\u5bfc\u901a\u4fe1\u9886\u57df\u4e4b\u9645\uff0c\u9700\u8981\u53cd\u601d\u4f20\u7edf\u975e\u7ebf\u6027\u4fe1\u53f7\u5904\u7406\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u91cd\u65b0\u8bc4\u4f30\u5df2\u5efa\u7acb\u7684\u7406\u8bba\u548c\u5de5\u5177\uff0c\u89e3\u51b3\u6570\u636e\u9a71\u52a8\u4e0e\u6a21\u578b\u9a71\u52a8\u65b9\u6cd5\u4e4b\u95f4\u7684\u51b2\u7a81\u3002", "method": "\u901a\u8fc7\u6279\u5224\u6027\u53cd\u601d\u548c\u7406\u8bba\u5206\u6790\uff0c\u63a2\u8ba8\u4f20\u7edf\u4fe1\u53f7\u5904\u7406\u7406\u8bba\u4e0eAI\u65b9\u6cd5\u7684\u6574\u5408\u53ef\u80fd\u6027\uff0c\u63d0\u51fa\u4e24\u8005\u5171\u5b58\u7684\u6846\u67b6\u3002", "result": "\u8bc6\u522b\u4e86\u4f20\u7edf\u4fe1\u53f7\u5904\u7406\u4e0eAI\u65b9\u6cd5\u4e4b\u95f4\u7684\u5f20\u529b\uff0c\u5f3a\u8c03\u4e86\u4fdd\u7559\u7ecf\u5178\u4fe1\u53f7\u5904\u7406\u5b9d\u8d35\u89c1\u89e3\u7684\u91cd\u8981\u6027\uff0c\u540c\u65f6\u4e3a\u4e24\u8005\u7684\u6574\u5408\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002", "conclusion": "\u5728AI\u65f6\u4ee3\uff0c\u9700\u8981\u5e73\u8861\u4f20\u7edf\u4fe1\u53f7\u5904\u7406\u7406\u8bba\u4e0e\u65b0\u5174AI\u65b9\u6cd5\uff0c\u65e2\u8981\u4fdd\u7559\u7ecf\u5178\u7406\u8bba\u7684\u6d1e\u5bdf\u529b\uff0c\u53c8\u8981\u5145\u5206\u5229\u7528\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u7684\u4f18\u52bf\uff0c\u5b9e\u73b0\u4e24\u8005\u7684\u534f\u540c\u53d1\u5c55\u3002"}}
{"id": "2511.02573", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.02573", "abs": "https://arxiv.org/abs/2511.02573", "authors": ["Anastasios T. Sotiropoulos", "Stavros Tsimpoukis", "Dimitrios Tyrovolas", "Sotiris Ioannidis", "George K. Karagiannidis", "Christos K. Liaskos"], "title": "RIS-Assisted 3D Spherical Splatting for Object Composition Visualization using Detection Transformers", "comment": "Submitted to IEEE ICC 2026", "summary": "The pursuit of immersive and structurally aware multimedia experiences has\nintensified interest in sensing modalities that reconstruct objects beyond the\nlimits of visible light. Conventional optical pipelines degrade under occlusion\nor low illumination, motivating the use of radio-frequency (RF) sensing, whose\nelectromagnetic waves penetrate materials and encode both geometric and\ncompositional information. Yet, uncontrolled multipath propagation restricts\nreconstruction accuracy. Recent advances in Programmable Wireless Environments\n(PWEs) mitigate this limitation by enabling software-defined manipulation of\npropagation through Reconfigurable Intelligent Surfaces (RISs), thereby\nproviding controllable illumination diversity. Building on this capability,\nthis work introduces a PWE-driven RF framework for three-dimensional object\nreconstruction using material-aware spherical primitives. The proposed approach\ncombines RIS-enabled field synthesis with a Detection Transformer (DETR) that\ninfers spatial and material parameters directly from extracted RF features.\nSimulation results confirm the framework's ability to approximate object\ngeometries and classify material composition with an overall accuracy of\n79.35%, marking an initial step toward programmable and physically grounded\nRF-based 3D object composition visualization.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53ef\u7f16\u7a0b\u65e0\u7ebf\u73af\u5883(PWE)\u7684RF\u6846\u67b6\uff0c\u4f7f\u7528\u6750\u6599\u611f\u77e5\u7403\u5f62\u57fa\u5143\u8fdb\u884c\u4e09\u7ef4\u7269\u4f53\u91cd\u5efa\uff0c\u7ed3\u5408RIS\u573a\u5408\u6210\u548cDETR\u68c0\u6d4b\u5668\uff0c\u5b9e\u73b079.35%\u7684\u51e0\u4f55\u548c\u6750\u6599\u5206\u7c7b\u51c6\u786e\u7387\u3002", "motivation": "\u4f20\u7edf\u5149\u5b66\u65b9\u6cd5\u5728\u906e\u6321\u6216\u4f4e\u5149\u7167\u6761\u4ef6\u4e0b\u6027\u80fd\u4e0b\u964d\uff0c\u800c\u5c04\u9891(RF)\u4f20\u611f\u80fd\u7a7f\u900f\u6750\u6599\u5e76\u7f16\u7801\u51e0\u4f55\u548c\u6210\u5206\u4fe1\u606f\uff0c\u4f46\u591a\u5f84\u4f20\u64ad\u9650\u5236\u4e86\u91cd\u5efa\u7cbe\u5ea6\u3002\u53ef\u7f16\u7a0b\u65e0\u7ebf\u73af\u5883(PWEs)\u901a\u8fc7\u8f6f\u4ef6\u5b9a\u4e49\u4f20\u64ad\u63a7\u5236\u63d0\u4f9b\u4e86\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u7ed3\u5408RIS\u4f7f\u80fd\u7684\u573a\u5408\u6210\u4e0e\u68c0\u6d4b\u53d8\u6362\u5668(DETR)\uff0c\u76f4\u63a5\u4ece\u63d0\u53d6\u7684RF\u7279\u5f81\u63a8\u65ad\u7a7a\u95f4\u548c\u6750\u6599\u53c2\u6570\uff0c\u4f7f\u7528\u6750\u6599\u611f\u77e5\u7403\u5f62\u57fa\u5143\u8fdb\u884c\u4e09\u7ef4\u91cd\u5efa\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8bc1\u5b9e\u8be5\u6846\u67b6\u80fd\u591f\u8fd1\u4f3c\u7269\u4f53\u51e0\u4f55\u5f62\u72b6\u5e76\u5206\u7c7b\u6750\u6599\u6210\u5206\uff0c\u603b\u4f53\u51c6\u786e\u7387\u8fbe\u523079.35%\u3002", "conclusion": "\u8fd9\u662f\u5411\u53ef\u7f16\u7a0b\u548c\u7269\u7406\u57fa\u7840\u7684RF\u57fa\u4e09\u7ef4\u7269\u4f53\u6210\u5206\u53ef\u89c6\u5316\u8fc8\u51fa\u7684\u521d\u6b65\u6b65\u9aa4\u3002"}}
{"id": "2511.02672", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.02672", "abs": "https://arxiv.org/abs/2511.02672", "authors": ["Adam Umra", "Aya M. Ahmed", "Aydin Sezgin"], "title": "RL-Aided Cognitive ISAC: Robust Detection and Sensing-Communication Trade-offs", "comment": "29 pages, 14 figures. Invited paper, submitted to the EURASIP Journal\n  on Wireless Communications and Networking (JWCN)", "summary": "This paper proposes a reinforcement learning (RL)-aided cognitive framework\nfor massive MIMO-based integrated sensing and communication (ISAC) systems\nemploying a uniform planar array (UPA). The focus is on enhancing radar sensing\nperformance in environments with unknown and dynamic disturbance\ncharacteristics. A Wald-type detector is employed for robust target detection\nunder non-Gaussian clutter, while a SARSA-based RL algorithm enables adaptive\nestimation of target positions without prior environmental knowledge. Based on\nthe RL-derived sensing information, a joint waveform optimization strategy is\nformulated to balance radar sensing accuracy and downlink communication\nthroughput. The resulting design provides an adaptive trade-off between\ndetection performance and achievable sum rate through an analytically derived\nclosed-form solution. Monte Carlo simulations demonstrate that the proposed\ncognitive ISAC framework achieves significantly improved detection probability\ncompared to orthogonal and non-learning adaptive baselines, while maintaining\ncompetitive communication performance. These results underline the potential of\nRL-assisted sensing for robust and spectrum-efficient ISAC in next-generation\nwireless networks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u8ba4\u77e5\u6846\u67b6\uff0c\u7528\u4e8e\u5927\u89c4\u6a21MIMO\u96c6\u6210\u611f\u77e5\u4e0e\u901a\u4fe1\u7cfb\u7edf\uff0c\u901a\u8fc7Wald\u68c0\u6d4b\u5668\u548cSARSA\u7b97\u6cd5\u5b9e\u73b0\u81ea\u9002\u5e94\u76ee\u6807\u68c0\u6d4b\u4e0e\u5b9a\u4f4d\uff0c\u5e76\u901a\u8fc7\u6ce2\u5f62\u4f18\u5316\u5e73\u8861\u611f\u77e5\u7cbe\u5ea6\u4e0e\u901a\u4fe1\u541e\u5410\u91cf\u3002", "motivation": "\u5728\u672a\u77e5\u548c\u52a8\u6001\u5e72\u6270\u73af\u5883\u4e0b\u63d0\u5347\u96f7\u8fbe\u611f\u77e5\u6027\u80fd\uff0c\u89e3\u51b3\u975e\u9ad8\u65af\u6742\u6ce2\u4e0b\u7684\u76ee\u6807\u68c0\u6d4b\u95ee\u9898\uff0c\u5b9e\u73b0\u611f\u77e5\u4e0e\u901a\u4fe1\u7684\u9891\u8c31\u9ad8\u6548\u96c6\u6210\u3002", "method": "\u91c7\u7528Wald\u578b\u68c0\u6d4b\u5668\u5904\u7406\u975e\u9ad8\u65af\u6742\u6ce2\uff0c\u4f7f\u7528SARSA\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u81ea\u9002\u5e94\u4f30\u8ba1\u76ee\u6807\u4f4d\u7f6e\uff0c\u57fa\u4e8e\u611f\u77e5\u4fe1\u606f\u5236\u5b9a\u8054\u5408\u6ce2\u5f62\u4f18\u5316\u7b56\u7565\u3002", "result": "\u8499\u7279\u5361\u6d1b\u4eff\u771f\u663e\u793a\uff0c\u76f8\u6bd4\u6b63\u4ea4\u548c\u975e\u5b66\u4e60\u81ea\u9002\u5e94\u57fa\u7ebf\u65b9\u6cd5\uff0c\u6240\u63d0\u6846\u67b6\u663e\u8457\u63d0\u9ad8\u4e86\u68c0\u6d4b\u6982\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6709\u7ade\u4e89\u529b\u7684\u901a\u4fe1\u6027\u80fd\u3002", "conclusion": "\u5f3a\u5316\u5b66\u4e60\u8f85\u52a9\u7684\u611f\u77e5\u65b9\u6cd5\u5728\u4e0b\u4e00\u4ee3\u65e0\u7ebf\u7f51\u7edc\u4e2d\u5177\u6709\u5b9e\u73b0\u9c81\u68d2\u4e14\u9891\u8c31\u9ad8\u6548\u96c6\u6210\u611f\u77e5\u4e0e\u901a\u4fe1\u7684\u6f5c\u529b\u3002"}}
{"id": "2511.02673", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2511.02673", "abs": "https://arxiv.org/abs/2511.02673", "authors": ["Adam Umra", "Kevin Weinberger", "Aymen Khaleel", "Aydin Sezgin"], "title": "Short Blocks, Fast Sensing: Finite Blocklength Tradeoffs in RIS-Assisted ISAC", "comment": "6 pages, 4 figures, submitted to IEEE ICC 2026", "summary": "Integrated sensing and communication (ISAC) is a cornerstone for future\nsixth-generation (6G) networks, enabling simultaneous connectivity and\nenvironmental awareness. However, practical realization faces significant\nchallenges, including residual self-interference (SI) in full-duplex systems\nand performance degradation of short-packet transmissions under finite\nblocklength (FBL) constraints. This work studies a reconfigurable intelligent\nsurface (RIS)-assisted full-duplex ISAC system serving multiple downlink users\nwhile tracking a moving target, explicitly accounting for SI and FBL effects in\nboth communication and sensing. We formulate an optimization framework to\nminimize service adaptation gaps while ensuring sensing reliability, solved via\nalternating optimization and successive convex approximation. Numerical results\nshow that short blocklengths enable fast adaptation but raise radar outage from\nfewer pulses and motion sensitivity. Longer blocklengths improve\nsignal-to-interference-plus-noise ratio (SINR) and reduce outages but allow\nmotion to degrade sensing. A \"sweet spot\" arises where blocklength and\nbeamformer allocation optimize throughput and sensing, seen as a local minimum\nin radar SINR variance. RIS-assisted optimization identifies this balance,\nachieving reliable communication and radar sensing jointly.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76RIS\u8f85\u52a9\u7684\u5168\u53cc\u5de5ISAC\u7cfb\u7edf\uff0c\u5728\u8003\u8651\u81ea\u5e72\u6270\u548c\u6709\u9650\u5757\u957f\u5ea6\u7ea6\u675f\u4e0b\uff0c\u901a\u8fc7\u4f18\u5316\u6846\u67b6\u6700\u5c0f\u5316\u670d\u52a1\u9002\u5e94\u5dee\u8ddd\uff0c\u540c\u65f6\u786e\u4fdd\u611f\u77e5\u53ef\u9760\u6027\u3002", "motivation": "\u96c6\u6210\u611f\u77e5\u4e0e\u901a\u4fe1\u662f6G\u7f51\u7edc\u7684\u5173\u952e\u6280\u672f\uff0c\u4f46\u5b9e\u9645\u5b9e\u73b0\u9762\u4e34\u5168\u53cc\u5de5\u7cfb\u7edf\u7684\u81ea\u5e72\u6270\u548c\u6709\u9650\u5757\u957f\u5ea6\u4f20\u8f93\u6027\u80fd\u4e0b\u964d\u7b49\u6311\u6218\u3002", "method": "\u91c7\u7528\u4ea4\u66ff\u4f18\u5316\u548c\u9010\u6b21\u51f8\u903c\u8fd1\u65b9\u6cd5\uff0c\u4f18\u5316RIS\u8f85\u52a9\u7684\u5168\u53cc\u5de5ISAC\u7cfb\u7edf\uff0c\u670d\u52a1\u591a\u4e2a\u4e0b\u884c\u7528\u6237\u540c\u65f6\u8ddf\u8e2a\u79fb\u52a8\u76ee\u6807\u3002", "result": "\u6570\u503c\u7ed3\u679c\u8868\u660e\uff0c\u77ed\u5757\u957f\u5ea6\u53ef\u5b9e\u73b0\u5feb\u901f\u9002\u5e94\u4f46\u4f1a\u589e\u52a0\u96f7\u8fbe\u4e2d\u65ad\u6982\u7387\uff0c\u957f\u5757\u957f\u5ea6\u53ef\u6539\u5584SINR\u4f46\u8fd0\u52a8\u4f1a\u5f71\u54cd\u611f\u77e5\u6027\u80fd\uff0c\u5b58\u5728\u6700\u4f18\u5e73\u8861\u70b9\u3002", "conclusion": "RIS\u8f85\u52a9\u4f18\u5316\u80fd\u591f\u627e\u5230\u901a\u4fe1\u548c\u96f7\u8fbe\u611f\u77e5\u7684\u5e73\u8861\u70b9\uff0c\u5b9e\u73b0\u53ef\u9760\u7684\u8054\u5408\u901a\u4fe1\u548c\u611f\u77e5\u6027\u80fd\u3002"}}
{"id": "2511.02689", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.02689", "abs": "https://arxiv.org/abs/2511.02689", "authors": ["Smilja Stokanovi\u0107", "Jaka Sodnik", "Nadica Miljkovi\u0107"], "title": "Eye Movement Analysis in Simulated Driving Scenarios", "comment": "29 pages, 6 figures, and 6 tables", "summary": "This study investigates eye movement behaviour during three conditions:\nBaseline, Ride (simulated drive under normal visibility), and Fog (simulated\ndrive under reduced visibility). Eye tracking data are analyzed using 31\nparameters, organized into three groups: (1) saccade features, (2) Bivariate\nContour Ellipse Area (BCEA), and (3) blinking features. Specifically, the\nanalysis includes 13 saccade, 13 BCEA, and 5 blinking variables. Across all\nfeature groups, numerous statistically significant differences emerge between\nBaseline and the driving conditions, particularly between Baseline and Ride or\nFog. Between Ride and Fog, saccade features show minimal changes (one out of\n13), whereas BCEA (9 of 13) and blink features (four of 5) exhibit pronounced\ndifferences, highlighting the strong impact of reduced visibility on gaze\nstability and blinking behaviour. In addition to conventional measures such as\nMean Squared Error (MSE) and entropy metrics, a new parameter, Guzik's Index\n(GI), is introduced to quantify fixation asymmetry along the major axis of the\nBCEA. This index utilizes eye tracking data to enhance the understanding of eye\nmovement dynamics during driving conditions. Separately from GI, other\nparameters elicit the largest deviations compared to Ride (e.g., number of\nsaccades: Cliff's $\\delta$ = 0.96, BCEA: Cohen's $\\textit{d}$ = 0.89, and\nstandard deviation of blink duration: Cliff's $\\delta$ = 0.80), underscoring\nthe influence of reduced visibility on visual attention. Overall, these\nfindings demonstrate that combining BCEA with saccade and blink parameters\nprovides a comprehensive understanding of visual attention and gaze stability,\nwhile GI offers additional insights into fixation asymmetry under varying\nvisibility conditions.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u773c\u52a8\u8ffd\u8e2a\u5206\u6790\u6bd4\u8f83\u4e86\u4e09\u79cd\u6761\u4ef6\u4e0b\u7684\u773c\u52a8\u884c\u4e3a\uff1a\u57fa\u7ebf\u3001\u6b63\u5e38\u80fd\u89c1\u5ea6\u9a7e\u9a76\u548c\u4f4e\u80fd\u89c1\u5ea6\u9a7e\u9a76\u3002\u7814\u7a76\u53d1\u73b0\u4f4e\u80fd\u89c1\u5ea6\u663e\u8457\u5f71\u54cd\u6ce8\u89c6\u7a33\u5b9a\u6027\u548c\u7728\u773c\u884c\u4e3a\uff0c\u5e76\u5f15\u5165Guzik\u6307\u6570\u6765\u91cf\u5316\u6ce8\u89c6\u4e0d\u5bf9\u79f0\u6027\u3002", "motivation": "\u7814\u7a76\u4e0d\u540c\u80fd\u89c1\u5ea6\u6761\u4ef6\u4e0b\u9a7e\u9a76\u65f6\u7684\u773c\u52a8\u884c\u4e3a\u53d8\u5316\uff0c\u7279\u522b\u5173\u6ce8\u4f4e\u80fd\u89c1\u5ea6\u5bf9\u89c6\u89c9\u6ce8\u610f\u529b\u548c\u6ce8\u89c6\u7a33\u5b9a\u6027\u7684\u5f71\u54cd\u3002", "method": "\u4f7f\u752831\u4e2a\u773c\u52a8\u53c2\u6570\u5206\u6790\u773c\u52a8\u6570\u636e\uff0c\u5206\u4e3a\u4e09\u7ec4\uff1a\u626b\u89c6\u7279\u5f81\uff0813\u4e2a\uff09\u3001\u53cc\u53d8\u91cf\u8f6e\u5ed3\u692d\u5706\u9762\u79ef\uff08BCEA\uff0c13\u4e2a\uff09\u548c\u7728\u773c\u7279\u5f81\uff085\u4e2a\uff09\u3002\u5f15\u5165\u65b0\u7684\u53c2\u6570Guzik\u6307\u6570\u6765\u91cf\u5316\u6ce8\u89c6\u4e0d\u5bf9\u79f0\u6027\u3002", "result": "\u57fa\u7ebf\u4e0e\u9a7e\u9a76\u6761\u4ef6\u95f4\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u7279\u522b\u662f\u57fa\u7ebf\u4e0e\u6b63\u5e38/\u4f4e\u80fd\u89c1\u5ea6\u9a7e\u9a76\u4e4b\u95f4\u3002\u4f4e\u80fd\u89c1\u5ea6\u5bf9\u626b\u89c6\u7279\u5f81\u5f71\u54cd\u8f83\u5c0f\uff0813\u4e2a\u4e2d1\u4e2a\u663e\u8457\uff09\uff0c\u4f46\u5bf9BCEA\uff0813\u4e2a\u4e2d9\u4e2a\u663e\u8457\uff09\u548c\u7728\u773c\u7279\u5f81\uff085\u4e2a\u4e2d4\u4e2a\u663e\u8457\uff09\u5f71\u54cd\u663e\u8457\u3002Guzik\u6307\u6570\u63d0\u4f9b\u4e86\u6ce8\u89c6\u4e0d\u5bf9\u79f0\u6027\u7684\u989d\u5916\u6d1e\u5bdf\u3002", "conclusion": "\u7ed3\u5408BCEA\u3001\u626b\u89c6\u548c\u7728\u773c\u53c2\u6570\u80fd\u5168\u9762\u7406\u89e3\u89c6\u89c9\u6ce8\u610f\u529b\u548c\u6ce8\u89c6\u7a33\u5b9a\u6027\uff0cGuzik\u6307\u6570\u4e3a\u4e0d\u540c\u80fd\u89c1\u5ea6\u6761\u4ef6\u4e0b\u7684\u6ce8\u89c6\u4e0d\u5bf9\u79f0\u6027\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u8865\u5145\u4fe1\u606f\u3002"}}
{"id": "2511.02728", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.02728", "abs": "https://arxiv.org/abs/2511.02728", "authors": ["Kaluguri Yashaswini", "Anshu Arora", "Satish Mulleti"], "title": "A Non-Uniform Quantization Framework for Time-Encoding Machines", "comment": "5 pages", "summary": "Time encoding machines (TEMs) provide an event-driven alternative to\nclassical uniform sampling, enabling power-efficient representations without a\nglobal clock. While prior work analyzed uniform quantization (UQ) of firing\nintervals, we show that these intervals are inherently non-uniformly\ndistributed, motivating the use of non-uniform quantization (NUQ). We derive\nthe probability distribution of firing intervals for a class of bandlimited\nsignals and design a power-law-based NUQ scheme tailored to this distribution.\nSimulations demonstrate that NUQ significantly outperforms UQ under the same\nbit budget. We also compare TEMs with non-uniform sampling (NUS), where both\namplitudes and timings require quantization, and show that TEM--NUQ achieves\nlower error at half the transmission cost. These results highlight the\nadvantages of distribution-aware quantization and establish TEM--NUQ as an\nefficient alternative to conventional UQ and NUS schemes.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u65f6\u95f4\u7f16\u7801\u673a\u5668(TEMs)\u7684\u975e\u5747\u5300\u91cf\u5316(NUQ)\u65b9\u6848\uff0c\u901a\u8fc7\u5206\u6790\u89e6\u53d1\u95f4\u9694\u7684\u975e\u5747\u5300\u5206\u5e03\u7279\u6027\uff0c\u8bbe\u8ba1\u57fa\u4e8e\u5e42\u5f8b\u7684NUQ\u65b9\u6cd5\uff0c\u5728\u76f8\u540c\u6bd4\u7279\u9884\u7b97\u4e0b\u663e\u8457\u4f18\u4e8e\u5747\u5300\u91cf\u5316(UQ)\uff0c\u4e14\u6bd4\u975e\u5747\u5300\u91c7\u6837(NUS)\u65b9\u6848\u4f20\u8f93\u6210\u672c\u51cf\u534a\u3002", "motivation": "\u4f20\u7edf\u5747\u5300\u91cf\u5316\u5047\u8bbe\u89e6\u53d1\u95f4\u9694\u5747\u5300\u5206\u5e03\uff0c\u4f46\u5b9e\u9645TEMs\u7684\u89e6\u53d1\u95f4\u9694\u5177\u6709\u975e\u5747\u5300\u5206\u5e03\u7279\u6027\uff0c\u8fd9\u4fc3\u4f7f\u7814\u7a76\u66f4\u9002\u5408\u7684\u975e\u5747\u5300\u91cf\u5316\u65b9\u6848\u3002", "method": "\u63a8\u5bfc\u4e86\u5e26\u9650\u4fe1\u53f7\u89e6\u53d1\u95f4\u9694\u7684\u6982\u7387\u5206\u5e03\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u79cd\u57fa\u4e8e\u5e42\u5f8b\u7684\u975e\u5747\u5300\u91cf\u5316\u65b9\u6848\u6765\u5339\u914d\u8be5\u5206\u5e03\u7279\u6027\u3002", "result": "\u4eff\u771f\u8868\u660eNUQ\u5728\u76f8\u540c\u6bd4\u7279\u9884\u7b97\u4e0b\u663e\u8457\u4f18\u4e8eUQ\uff0c\u4e14TEM-NUQ\u65b9\u6848\u76f8\u6bd4NUS\u65b9\u6848\u5728\u4f20\u8f93\u6210\u672c\u51cf\u534a\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u66f4\u4f4e\u7684\u8bef\u5dee\u3002", "conclusion": "\u5206\u5e03\u611f\u77e5\u7684\u91cf\u5316\u65b9\u6cd5\u5177\u6709\u660e\u663e\u4f18\u52bf\uff0cTEM-NUQ\u6210\u4e3a\u4f20\u7edfUQ\u548cNUS\u65b9\u6848\u7684\u9ad8\u6548\u66ff\u4ee3\u65b9\u6848\u3002"}}

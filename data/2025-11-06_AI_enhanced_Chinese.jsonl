{"id": "2511.03084", "categories": ["eess.AS", "cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2511.03084", "abs": "https://arxiv.org/abs/2511.03084", "authors": ["Gowtham Premananth", "Carol Espy-Wilson"], "title": "Quantifying Articulatory Coordination as a Biomarker for Schizophrenia", "comment": "Submitted to ICASSP 2026", "summary": "Advances in artificial intelligence (AI) and deep learning have improved\ndiagnostic capabilities in healthcare, yet limited interpretability continues\nto hinder clinical adoption. Schizophrenia, a complex disorder with diverse\nsymptoms including disorganized speech and social withdrawal, demands tools\nthat capture symptom severity and provide clinically meaningful insights beyond\nbinary diagnosis. Here, we present an interpretable framework that leverages\narticulatory speech features through eigenspectra difference plots and a\nweighted sum with exponential decay (WSED) to quantify vocal tract\ncoordination. Eigenspectra plots effectively distinguished complex from simpler\ncoordination patterns, and WSED scores reliably separated these groups, with\nambiguity confined to a narrow range near zero. Importantly, WSED scores\ncorrelated not only with overall BPRS severity but also with the balance\nbetween positive and negative symptoms, reflecting more complex coordination in\nsubjects with pronounced positive symptoms and the opposite trend for stronger\nnegative symptoms. This approach offers a transparent, severity-sensitive\nbiomarker for schizophrenia, advancing the potential for clinically\ninterpretable speech-based assessment tools.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bed\u97f3\u7279\u5f81\u7684\u53ef\u89e3\u91ca\u6846\u67b6\uff0c\u901a\u8fc7\u7279\u5f81\u8c31\u5dee\u5f02\u56fe\u548c\u52a0\u6743\u6307\u6570\u8870\u51cf\u548c\u6765\u91cf\u5316\u58f0\u5e26\u534f\u8c03\u6027\uff0c\u4e3a\u7cbe\u795e\u5206\u88c2\u75c7\u63d0\u4f9b\u900f\u660e\u3001\u4e25\u91cd\u7a0b\u5ea6\u654f\u611f\u7684\u8bed\u97f3\u751f\u7269\u6807\u5fd7\u7269\u3002", "motivation": "\u5c3d\u7ba1AI\u548c\u6df1\u5ea6\u5b66\u4e60\u5728\u533b\u7597\u8bca\u65ad\u4e2d\u6709\u6240\u8fdb\u5c55\uff0c\u4f46\u6709\u9650\u7684\u53ef\u89e3\u91ca\u6027\u963b\u788d\u4e86\u4e34\u5e8a\u91c7\u7528\u3002\u7cbe\u795e\u5206\u88c2\u75c7\u4f5c\u4e3a\u590d\u6742\u75be\u75c5\uff0c\u9700\u8981\u80fd\u591f\u6355\u6349\u75c7\u72b6\u4e25\u91cd\u7a0b\u5ea6\u5e76\u63d0\u4f9b\u4e34\u5e8a\u610f\u4e49\u6d1e\u5bdf\u7684\u5de5\u5177\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u4e8c\u5143\u8bca\u65ad\u3002", "method": "\u5229\u7528\u53d1\u97f3\u8bed\u97f3\u7279\u5f81\uff0c\u901a\u8fc7\u7279\u5f81\u8c31\u5dee\u5f02\u56fe\u548c\u52a0\u6743\u6307\u6570\u8870\u51cf\u548c\uff08WSED\uff09\u6765\u91cf\u5316\u58f0\u5e26\u534f\u8c03\u6027\u3002\u7279\u5f81\u8c31\u56fe\u6709\u6548\u533a\u5206\u590d\u6742\u4e0e\u7b80\u5355\u534f\u8c03\u6a21\u5f0f\uff0cWSED\u5206\u6570\u53ef\u9760\u5206\u79bb\u8fd9\u4e9b\u7ec4\u522b\u3002", "result": "WSED\u5206\u6570\u4e0d\u4ec5\u4e0e\u603b\u4f53BPRS\u4e25\u91cd\u7a0b\u5ea6\u76f8\u5173\uff0c\u8fd8\u4e0e\u9633\u6027\u548c\u9634\u6027\u75c7\u72b6\u7684\u5e73\u8861\u76f8\u5173\uff0c\u53cd\u6620\u4e86\u9633\u6027\u75c7\u72b6\u660e\u663e\u60a3\u8005\u66f4\u590d\u6742\u7684\u534f\u8c03\u6a21\u5f0f\uff0c\u800c\u9634\u6027\u75c7\u72b6\u660e\u663e\u60a3\u8005\u5219\u5448\u73b0\u76f8\u53cd\u8d8b\u52bf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u7cbe\u795e\u5206\u88c2\u75c7\u63d0\u4f9b\u4e86\u900f\u660e\u3001\u4e25\u91cd\u7a0b\u5ea6\u654f\u611f\u7684\u8bed\u97f3\u751f\u7269\u6807\u5fd7\u7269\uff0c\u63a8\u8fdb\u4e86\u4e34\u5e8a\u53ef\u89e3\u91ca\u7684\u8bed\u97f3\u8bc4\u4f30\u5de5\u5177\u7684\u6f5c\u529b\u3002"}}
{"id": "2511.03086", "categories": ["eess.AS", "eess.SP"], "pdf": "https://arxiv.org/pdf/2511.03086", "abs": "https://arxiv.org/abs/2511.03086", "authors": ["Gowtham Premananth", "Philip Resnik", "Sonia Bansal", "Deanna L. Kelly", "Carol Espy-Wilson"], "title": "Speech-Based Prioritization for Schizophrenia Intervention", "comment": "Submitted for ICASSP 2026", "summary": "Millions of people suffer from mental health conditions, yet many remain\nundiagnosed or receive delayed care due to limited clinical resources and\nlabor-intensive assessment methods. While most machine-assisted approaches\nfocus on diagnostic classification, estimating symptom severity is essential\nfor prioritizing care, particularly in resource-constrained settings.\nSpeech-based AI provides a scalable alternative by enabling automated,\ncontinuous, and remote monitoring, reducing reliance on subjective self-reports\nand time-consuming evaluations. In this paper, we introduce a speech-based\nmodel for pairwise comparison of schizophrenia symptom severity, leveraging\narticulatory and acoustic features. These comparisons are used to generate\nseverity rankings via the Bradley-Terry model. Our approach outperforms\nprevious regression-based models on ranking-based metrics, offering a more\neffective solution for clinical triage and prioritization.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u8bed\u97f3\u7684\u6210\u5bf9\u6bd4\u8f83\u6a21\u578b\uff0c\u7528\u4e8e\u8bc4\u4f30\u7cbe\u795e\u5206\u88c2\u75c7\u75c7\u72b6\u4e25\u91cd\u7a0b\u5ea6\uff0c\u901a\u8fc7Bradley-Terry\u6a21\u578b\u751f\u6210\u4e25\u91cd\u7a0b\u5ea6\u6392\u540d\uff0c\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u5b9e\u73b0\u66f4\u597d\u7684\u4e34\u5e8a\u5206\u8bca\u6548\u679c\u3002", "motivation": "\u89e3\u51b3\u7cbe\u795e\u5065\u5eb7\u8d44\u6e90\u6709\u9650\u5bfc\u81f4\u7684\u8bca\u65ad\u5ef6\u8fdf\u95ee\u9898\uff0c\u4f20\u7edf\u8bc4\u4f30\u65b9\u6cd5\u52b3\u52a8\u5bc6\u96c6\u4e14\u4f9d\u8d56\u4e3b\u89c2\u81ea\u62a5\uff0c\u9700\u8981\u53ef\u6269\u5c55\u7684\u81ea\u52a8\u5316\u75c7\u72b6\u4e25\u91cd\u7a0b\u5ea6\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u5229\u7528\u53d1\u97f3\u548c\u58f0\u5b66\u7279\u5f81\u6784\u5efa\u8bed\u97f3\u6a21\u578b\uff0c\u8fdb\u884c\u75c7\u72b6\u4e25\u91cd\u7a0b\u5ea6\u7684\u6210\u5bf9\u6bd4\u8f83\uff0c\u901a\u8fc7Bradley-Terry\u6a21\u578b\u751f\u6210\u4e25\u91cd\u7a0b\u5ea6\u6392\u540d\u3002", "result": "\u5728\u57fa\u4e8e\u6392\u540d\u7684\u6307\u6807\u4e0a\u4f18\u4e8e\u4e4b\u524d\u7684\u56de\u5f52\u6a21\u578b\uff0c\u4e3a\u4e34\u5e8a\u5206\u8bca\u548c\u4f18\u5148\u7ea7\u6392\u5e8f\u63d0\u4f9b\u66f4\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u8bed\u97f3AI\u65b9\u6cd5\u80fd\u591f\u5b9e\u73b0\u81ea\u52a8\u5316\u3001\u8fde\u7eed\u548c\u8fdc\u7a0b\u76d1\u6d4b\uff0c\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u4e3a\u7cbe\u795e\u5206\u88c2\u75c7\u75c7\u72b6\u4e25\u91cd\u7a0b\u5ea6\u8bc4\u4f30\u63d0\u4f9b\u53ef\u6269\u5c55\u7684\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2511.03310", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2511.03310", "abs": "https://arxiv.org/abs/2511.03310", "authors": ["Jing Peng", "Yi Yang", "Xu Li", "Yu Xi", "Quanwei Tang", "Yangui Fang", "Junjie Li", "Kai Yu"], "title": "TASU: Text-Only Alignment for Speech Understanding", "comment": "This paper is submitted to ICASSP 2026", "summary": "Recent advances in Speech Large Language Models (Speech LLMs) have paved the\nway for unified architectures across diverse speech understanding tasks.\nHowever, prevailing alignment paradigms rely heavily on large-scale audio-text\npaired data and computationally intensive training, yet often exhibit limited\ngeneralization to unseen domains or tasks. To address these limitations, we\npropose TASU (Text-only Alignment for Speech Understanding), a novel alignment\nparadigm that can leverage only unpaired text data to guide cross-modal\nalignment. Experiments show that TASU achieves competitive zero-shot speech\nrecognition. Leveraging this property, it can further function as a\npre-training stage in curriculum learning, enhancing domain generalization in\nspeech recognition. Ultimately, TASU can extend its zero-shot generalization to\na wide range of speech understanding tasks and notably outperforms prominent\nSpeech LLMs including GLM-4-Voice and Step-Audio on the MMSU benchmark,\nestablishing TASU as an efficient and scalable alignment paradigm for Speech\nLLMs.", "AI": {"tldr": "TASU\u662f\u4e00\u79cd\u65b0\u7684\u8bed\u97f3\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u9f50\u8303\u5f0f\uff0c\u4ec5\u4f7f\u7528\u975e\u914d\u5bf9\u7684\u6587\u672c\u6570\u636e\u6765\u5b9e\u73b0\u8de8\u6a21\u6001\u5bf9\u9f50\uff0c\u5728\u96f6\u6837\u672c\u8bed\u97f3\u8bc6\u522b\u548c\u591a\u79cd\u8bed\u97f3\u7406\u89e3\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u5f53\u524d\u8bed\u97f3\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5bf9\u9f50\u65b9\u6cd5\u4f9d\u8d56\u5927\u89c4\u6a21\u97f3\u9891-\u6587\u672c\u914d\u5bf9\u6570\u636e\u548c\u8ba1\u7b97\u5bc6\u96c6\u578b\u8bad\u7ec3\uff0c\u4f46\u5728\u672a\u89c1\u9886\u57df\u6216\u4efb\u52a1\u4e0a\u7684\u6cdb\u5316\u80fd\u529b\u6709\u9650\u3002", "method": "\u63d0\u51faTASU\u8303\u5f0f\uff0c\u4ec5\u5229\u7528\u975e\u914d\u5bf9\u6587\u672c\u6570\u636e\u6307\u5bfc\u8de8\u6a21\u6001\u5bf9\u9f50\uff0c\u53ef\u4f5c\u4e3a\u8bfe\u7a0b\u5b66\u4e60\u7684\u9884\u8bad\u7ec3\u9636\u6bb5\u6765\u589e\u5f3a\u9886\u57df\u6cdb\u5316\u3002", "result": "TASU\u5728\u96f6\u6837\u672c\u8bed\u97f3\u8bc6\u522b\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5728MMSU\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8eGLM-4-Voice\u548cStep-Audio\u7b49\u4e3b\u6d41\u8bed\u97f3\u5927\u8bed\u8a00\u6a21\u578b\u3002", "conclusion": "TASU\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u8bed\u97f3\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u9f50\u8303\u5f0f\uff0c\u80fd\u591f\u6269\u5c55\u5230\u5e7f\u6cdb\u7684\u8bed\u97f3\u7406\u89e3\u4efb\u52a1\u4e2d\u3002"}}
{"id": "2511.03337", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2511.03337", "abs": "https://arxiv.org/abs/2511.03337", "authors": ["Riccardo Tripodi"], "title": "audio2chart: End to End Audio Transcription into playable Guitar Hero charts", "comment": null, "summary": "This work introduces audio2chart, a framework for the automatic generation of\nGuitar Hero style charts directly from raw audio. The task is formalized as a\nsequence prediction problem, where models are trained to generate discrete\nchart tokens aligned with the audio on discrete time steps. An unconditional\nbaseline demonstrates strong predictive performance, while the addition of\naudio conditioning yields consistent improvements across accuracy based\nmetrics. This work demonstrates that incorporating audio conditioning is both\nfeasible and effective for improving note prediction in automatic chart\ngeneration. The complete codebase for training and inference is publicly\navailable on GitHub supporting reproducible research on neural chart\ngeneration. A family of pretrained models is released on Hugging Face.", "AI": {"tldr": "\u97f3\u98912\u56fe\u8868\u6846\u67b6\uff1a\u4ece\u539f\u59cb\u97f3\u9891\u81ea\u52a8\u751f\u6210\u5409\u4ed6\u82f1\u96c4\u98ce\u683c\u56fe\u8868\uff0c\u901a\u8fc7\u5e8f\u5217\u9884\u6d4b\u65b9\u6cd5\u751f\u6210\u4e0e\u97f3\u9891\u5bf9\u9f50\u7684\u79bb\u6563\u56fe\u8868\u6807\u8bb0", "motivation": "\u5f00\u53d1\u80fd\u591f\u76f4\u63a5\u4ece\u97f3\u9891\u81ea\u52a8\u751f\u6210\u5409\u4ed6\u82f1\u96c4\u98ce\u683c\u56fe\u8868\u7684\u7cfb\u7edf\uff0c\u89e3\u51b3\u624b\u52a8\u5236\u4f5c\u56fe\u8868\u8017\u65f6\u8017\u529b\u7684\u95ee\u9898", "method": "\u5c06\u4efb\u52a1\u5f62\u5f0f\u5316\u4e3a\u5e8f\u5217\u9884\u6d4b\u95ee\u9898\uff0c\u8bad\u7ec3\u6a21\u578b\u5728\u79bb\u6563\u65f6\u95f4\u6b65\u4e0a\u751f\u6210\u4e0e\u97f3\u9891\u5bf9\u9f50\u7684\u56fe\u8868\u6807\u8bb0\uff0c\u5305\u62ec\u65e0\u6761\u4ef6\u57fa\u7ebf\u548c\u97f3\u9891\u6761\u4ef6\u5316\u6a21\u578b", "result": "\u65e0\u6761\u4ef6\u57fa\u7ebf\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u9884\u6d4b\u6027\u80fd\uff0c\u97f3\u9891\u6761\u4ef6\u5316\u5728\u6240\u6709\u57fa\u4e8e\u51c6\u786e\u5ea6\u7684\u6307\u6807\u4e0a\u5e26\u6765\u4e00\u81f4\u6539\u8fdb", "conclusion": "\u97f3\u9891\u6761\u4ef6\u5316\u5bf9\u4e8e\u81ea\u52a8\u56fe\u8868\u751f\u6210\u4e2d\u7684\u97f3\u7b26\u9884\u6d4b\u65e2\u53ef\u884c\u53c8\u6709\u6548\uff0c\u4ee3\u7801\u548c\u9884\u8bad\u7ec3\u6a21\u578b\u5df2\u5f00\u6e90\u652f\u6301\u53ef\u91cd\u590d\u7814\u7a76"}}
{"id": "2511.03244", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2511.03244", "abs": "https://arxiv.org/abs/2511.03244", "authors": ["Fei Zhao", "Zhong-Qiu Wang"], "title": "Why Not Put a Microphone Near the Loudspeaker? A New Paradigm for Acoustic Echo Cancellation", "comment": null, "summary": "Acoustic echo cancellation (AEC) remains challenging in real-world\nenvironments due to nonlinear distortions caused by low-cost loudspeakers and\ncomplex room acoustics. To mitigate these issues, we introduce a\ndual-microphone configuration, where an auxiliary reference microphone is\nplaced near the loudspeaker to capture the nonlinearly distorted far-end\nsignal. Although this reference signal is contaminated by near-end speech, we\npropose a preprocessing module based on Wiener filtering to estimate a\ncompressed time-frequency mask to suppress near-end components. This purified\nreference signal enables a more effective linear AEC stage, whose residual\nerror signal is then fed to a deep neural network for joint residual echo and\nnoise suppression. Evaluation results show that our method outperforms baseline\napproaches on matched test sets. To evaluate its robustness under strong\nnonlinearities, we further test it on a mismatched dataset and observe that it\nachieves substantial performance gains. These results demonstrate its\neffectiveness in practical scenarios where the nonlinear distortions are\ntypically unknown.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u9ea6\u514b\u98ce\u914d\u7f6e\u7684\u58f0\u5b66\u56de\u58f0\u6d88\u9664\u65b9\u6cd5\uff0c\u901a\u8fc7\u8f85\u52a9\u53c2\u8003\u9ea6\u514b\u98ce\u6355\u83b7\u975e\u7ebf\u6027\u5931\u771f\u4fe1\u53f7\uff0c\u7ed3\u5408\u7ef4\u7eb3\u6ee4\u6ce2\u9884\u5904\u7406\u548c\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff0c\u6709\u6548\u5904\u7406\u73b0\u5b9e\u73af\u5883\u4e2d\u7684\u975e\u7ebf\u6027\u5931\u771f\u95ee\u9898\u3002", "motivation": "\u73b0\u5b9e\u73af\u5883\u4e2d\u7531\u4e8e\u4f4e\u6210\u672c\u626c\u58f0\u5668\u548c\u590d\u6742\u623f\u95f4\u58f0\u5b66\u5bfc\u81f4\u7684\u975e\u7ebf\u6027\u5931\u771f\u4f7f\u5f97\u58f0\u5b66\u56de\u58f0\u6d88\u9664\u5177\u6709\u6311\u6218\u6027\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u53cc\u9ea6\u514b\u98ce\u914d\u7f6e\uff0c\u8f85\u52a9\u9ea6\u514b\u98ce\u9760\u8fd1\u626c\u58f0\u5668\u6355\u83b7\u5931\u771f\u4fe1\u53f7\uff1b\u901a\u8fc7\u7ef4\u7eb3\u6ee4\u6ce2\u9884\u5904\u7406\u4f30\u8ba1\u538b\u7f29\u65f6\u9891\u63a9\u7801\u6291\u5236\u8fd1\u7aef\u8bed\u97f3\uff1b\u7ebf\u6027AEC\u9636\u6bb5\u540e\uff0c\u7528\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u8054\u5408\u6b8b\u4f59\u56de\u58f0\u548c\u566a\u58f0\u6291\u5236\u3002", "result": "\u5728\u5339\u914d\u6d4b\u8bd5\u96c6\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff1b\u5728\u4e0d\u5339\u914d\u6570\u636e\u96c6\u4e0a\u5bf9\u5f3a\u975e\u7ebf\u6027\u5931\u771f\u8868\u73b0\u51fa\u663e\u8457\u6027\u80fd\u63d0\u5347\uff0c\u8bc1\u660e\u5176\u5728\u672a\u77e5\u975e\u7ebf\u6027\u5931\u771f\u573a\u666f\u4e0b\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u53cc\u9ea6\u514b\u98ce\u914d\u7f6e\u548c\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u5b9e\u73af\u5883\u4e2d\u975e\u7ebf\u6027\u5931\u771f\u5e26\u6765\u7684\u58f0\u5b66\u56de\u58f0\u6d88\u9664\u6311\u6218\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2511.02845", "categories": ["eess.SP", "cs.AI", "physics.ins-det"], "pdf": "https://arxiv.org/pdf/2511.02845", "abs": "https://arxiv.org/abs/2511.02845", "authors": ["Yuxuan Liu", "Chiya Zhang", "Yifeng Yuan", "Chunlong He", "Weizheng Zhang", "Gaojie Chen"], "title": "AI-Enhanced Wi-Fi Sensing Through Single Transceiver Pair", "comment": "12 pages, 11 figures", "summary": "The advancement of next-generation Wi-Fi technology heavily relies on sensing\ncapabilities, which play a pivotal role in enabling sophisticated applications.\nIn response to the growing demand for large-scale deployments, contemporary\nWi-Fi sensing systems strive to achieve high-precision perception while\nmaintaining minimal bandwidth consumption and antenna count requirements.\nRemarkably, various AI-driven perception technologies have demonstrated the\nability to surpass the traditional resolution limitations imposed by radar\ntheory. However, the theoretical underpinnings of this phenomenon have not been\nthoroughly investigated in existing research. In this study, we found that\nunder hardware-constrained conditions, the performance gains brought by AI to\nWi-Fi sensing systems primarily originate from two aspects: prior information\nand temporal correlation. Prior information enables the AI to generate\nplausible details based on vague input, while temporal correlation helps reduce\nthe upper bound of sensing error. We developed an AI-based Wi-Fi sensing system\nusing a single transceiver pair and designed experiments focusing on human pose\nestimation and indoor localization to validate the theoretical claims. The\nresults confirm the performance gains contributed by temporal correlation and\nprior information.", "AI": {"tldr": "AI\u9a71\u52a8\u7684Wi-Fi\u611f\u77e5\u7cfb\u7edf\u5728\u786c\u4ef6\u53d7\u9650\u6761\u4ef6\u4e0b\uff0c\u901a\u8fc7\u5148\u9a8c\u4fe1\u606f\u548c\u65f6\u95f4\u76f8\u5173\u6027\u7a81\u7834\u4f20\u7edf\u96f7\u8fbe\u7406\u8bba\u7684\u5206\u8fa8\u7387\u9650\u5236\uff0c\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1\u548c\u5ba4\u5185\u5b9a\u4f4d\u3002", "motivation": "\u7814\u7a76AI\u9a71\u52a8\u7684Wi-Fi\u611f\u77e5\u7cfb\u7edf\u5982\u4f55\u7a81\u7834\u4f20\u7edf\u96f7\u8fbe\u7406\u8bba\u7684\u5206\u8fa8\u7387\u9650\u5236\uff0c\u7279\u522b\u662f\u5728\u786c\u4ef6\u53d7\u9650\u6761\u4ef6\u4e0b\uff08\u5982\u6700\u5c0f\u5e26\u5bbd\u548c\u5929\u7ebf\u6570\u91cf\uff09\uff0c\u63a2\u7d22\u5176\u6027\u80fd\u63d0\u5347\u7684\u7406\u8bba\u57fa\u7840\u3002", "method": "\u5f00\u53d1\u57fa\u4e8eAI\u7684Wi-Fi\u611f\u77e5\u7cfb\u7edf\uff0c\u4f7f\u7528\u5355\u4e2a\u6536\u53d1\u5668\u5bf9\uff0c\u901a\u8fc7\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1\u548c\u5ba4\u5185\u5b9a\u4f4d\u5b9e\u9a8c\u9a8c\u8bc1\u7406\u8bba\u5047\u8bbe\uff0c\u91cd\u70b9\u5206\u6790\u5148\u9a8c\u4fe1\u606f\u548c\u65f6\u95f4\u76f8\u5173\u6027\u5bf9\u611f\u77e5\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u5b9e\u9a8c\u8bc1\u5b9e\u65f6\u95f4\u76f8\u5173\u6027\u548c\u5148\u9a8c\u4fe1\u606f\u786e\u5b9e\u5e26\u6765\u4e86\u6027\u80fd\u63d0\u5347\uff0c\u9a8c\u8bc1\u4e86AI\u5728Wi-Fi\u611f\u77e5\u7cfb\u7edf\u4e2d\u7a81\u7834\u4f20\u7edf\u5206\u8fa8\u7387\u9650\u5236\u7684\u7406\u8bba\u673a\u5236\u3002", "conclusion": "\u5728\u786c\u4ef6\u53d7\u9650\u7684Wi-Fi\u611f\u77e5\u7cfb\u7edf\u4e2d\uff0cAI\u7684\u6027\u80fd\u63d0\u5347\u4e3b\u8981\u6765\u6e90\u4e8e\u5148\u9a8c\u4fe1\u606f\uff08\u57fa\u4e8e\u6a21\u7cca\u8f93\u5165\u751f\u6210\u5408\u7406\u7ec6\u8282\uff09\u548c\u65f6\u95f4\u76f8\u5173\u6027\uff08\u964d\u4f4e\u611f\u77e5\u8bef\u5dee\u4e0a\u9650\uff09\uff0c\u8fd9\u4e3a\u4e0b\u4e00\u4ee3Wi-Fi\u6280\u672f\u7684\u5927\u89c4\u6a21\u90e8\u7f72\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2511.03361", "categories": ["eess.AS", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.03361", "abs": "https://arxiv.org/abs/2511.03361", "authors": ["Gabriel Pirlogeanu", "Alexandru-Lucian Georgescu", "Horia Cucu"], "title": "Open Source State-Of-the-Art Solution for Romanian Speech Recognition", "comment": "13th Conference on Speech Technology and Human-Computer Dialogue\n  (SpeD 2025), Cluj-Napoca, Romania", "summary": "In this work, we present a new state-of-the-art Romanian Automatic Speech\nRecognition (ASR) system based on NVIDIA's FastConformer architecture--explored\nhere for the first time in the context of Romanian. We train our model on a\nlarge corpus of, mostly, weakly supervised transcriptions, totaling over 2,600\nhours of speech. Leveraging a hybrid decoder with both Connectionist Temporal\nClassification (CTC) and Token-Duration Transducer (TDT) branches, we evaluate\na range of decoding strategies including greedy, ALSD, and CTC beam search with\na 6-gram token-level language model. Our system achieves state-of-the-art\nperformance across all Romanian evaluation benchmarks, including read,\nspontaneous, and domain-specific speech, with up to 27% relative WER reduction\ncompared to previous best-performing systems. In addition to improved\ntranscription accuracy, our approach demonstrates practical decoding\nefficiency, making it suitable for both research and deployment in low-latency\nASR applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u57fa\u4e8eNVIDIA FastConformer\u67b6\u6784\u7684\u7f57\u9a6c\u5c3c\u4e9a\u8bed\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\u7cfb\u7edf\uff0c\u57282600\u5c0f\u65f6\u8bed\u97f3\u6570\u636e\u4e0a\u8bad\u7ec3\uff0c\u4f7f\u7528CTC\u548cTDT\u6df7\u5408\u89e3\u7801\u5668\uff0c\u5728\u591a\u4e2a\u8bc4\u4f30\u57fa\u51c6\u4e0a\u5b9e\u73b0\u4e8627%\u7684\u76f8\u5bf9WER\u964d\u4f4e\u3002", "motivation": "\u4e3a\u7f57\u9a6c\u5c3c\u4e9a\u8bed\u5f00\u53d1\u6700\u5148\u8fdb\u7684\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\u7cfb\u7edf\uff0c\u9996\u6b21\u5728\u8be5\u8bed\u8a00\u4e2d\u63a2\u7d22FastConformer\u67b6\u6784\uff0c\u89e3\u51b3\u73b0\u6709\u7cfb\u7edf\u6027\u80fd\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528FastConformer\u67b6\u6784\uff0c\u57282600\u5c0f\u65f6\u5f31\u76d1\u7763\u8f6c\u5f55\u8bed\u97f3\u6570\u636e\u4e0a\u8bad\u7ec3\uff0c\u91c7\u7528CTC\u548cToken-Duration Transducer\u6df7\u5408\u89e3\u7801\u5668\uff0c\u8bc4\u4f30\u591a\u79cd\u89e3\u7801\u7b56\u7565\u5305\u62ec\u8d2a\u5a6a\u89e3\u7801\u3001ALSD\u548cCTC\u6ce2\u675f\u641c\u7d22\u7ed3\u54086-gram\u8bed\u8a00\u6a21\u578b\u3002", "result": "\u5728\u6240\u6709\u7f57\u9a6c\u5c3c\u4e9a\u8bed\u8bc4\u4f30\u57fa\u51c6\uff08\u5305\u62ec\u6717\u8bfb\u3001\u81ea\u53d1\u548c\u9886\u57df\u7279\u5b9a\u8bed\u97f3\uff09\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u76f8\u6bd4\u4e4b\u524d\u6700\u4f73\u7cfb\u7edf\u5b9e\u73b0\u9ad8\u8fbe27%\u7684\u76f8\u5bf9WER\u964d\u4f4e\uff0c\u540c\u65f6\u5177\u5907\u5b9e\u7528\u7684\u89e3\u7801\u6548\u7387\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u4e0d\u4ec5\u663e\u8457\u63d0\u9ad8\u4e86\u8f6c\u5f55\u51c6\u786e\u6027\uff0c\u8fd8\u5177\u5907\u4f4e\u5ef6\u8fdfASR\u5e94\u7528\u6240\u9700\u7684\u89e3\u7801\u6548\u7387\uff0c\u9002\u5408\u7814\u7a76\u548c\u5b9e\u9645\u90e8\u7f72\u3002"}}
{"id": "2511.03425", "categories": ["cs.SD", "cs.LG", "cs.MM"], "pdf": "https://arxiv.org/pdf/2511.03425", "abs": "https://arxiv.org/abs/2511.03425", "authors": ["Ilya Borovik", "Dmitrii Gavrilev", "Vladimir Viro"], "title": "SyMuPe: Affective and Controllable Symbolic Music Performance", "comment": "ACM Multimedia 2025. Extended version with supplementary material", "summary": "Emotions are fundamental to the creation and perception of music\nperformances. However, achieving human-like expression and emotion through\nmachine learning models for performance rendering remains a challenging task.\nIn this work, we present SyMuPe, a novel framework for developing and training\naffective and controllable symbolic piano performance models. Our flagship\nmodel, PianoFlow, uses conditional flow matching trained to solve diverse\nmulti-mask performance inpainting tasks. By design, it supports both\nunconditional generation and infilling of music performance features. For\ntraining, we use a curated, cleaned dataset of 2,968 hours of aligned musical\nscores and expressive MIDI performances. For text and emotion control, we\nintegrate a piano performance emotion classifier and tune PianoFlow with the\nemotion-weighted Flan-T5 text embeddings provided as conditional inputs.\nObjective and subjective evaluations against transformer-based baselines and\nexisting models show that PianoFlow not only outperforms other approaches, but\nalso achieves performance quality comparable to that of human-recorded and\ntranscribed MIDI samples. For emotion control, we present and analyze samples\ngenerated under different text conditioning scenarios. The developed model can\nbe integrated into interactive applications, contributing to the creation of\nmore accessible and engaging music performance systems.", "AI": {"tldr": "\u63d0\u51faSyMuPe\u6846\u67b6\u548cPianoFlow\u6a21\u578b\uff0c\u7528\u4e8e\u5f00\u53d1\u60c5\u611f\u53ef\u63a7\u7684\u7b26\u53f7\u94a2\u7434\u6f14\u594f\u6a21\u578b\uff0c\u901a\u8fc7\u6761\u4ef6\u6d41\u5339\u914d\u652f\u6301\u65e0\u6761\u4ef6\u751f\u6210\u548c\u97f3\u4e50\u7279\u5f81\u586b\u5145\uff0c\u5728\u60c5\u611f\u63a7\u5236\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u97f3\u4e50\u6f14\u594f\u6e32\u67d3\u4e2d\u96be\u4ee5\u5b9e\u73b0\u7c7b\u4eba\u8868\u8fbe\u548c\u60c5\u611f\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u63a7\u5236\u60c5\u611f\u8868\u8fbe\u7684\u7b26\u53f7\u94a2\u7434\u6f14\u594f\u6a21\u578b\u3002", "method": "\u4f7f\u7528\u6761\u4ef6\u6d41\u5339\u914d\u8bad\u7ec3PianoFlow\u6a21\u578b\uff0c\u89e3\u51b3\u591a\u63a9\u7801\u6f14\u594f\u4fee\u590d\u4efb\u52a1\uff1b\u96c6\u6210\u94a2\u7434\u6f14\u594f\u60c5\u611f\u5206\u7c7b\u5668\uff0c\u4f7f\u7528\u60c5\u611f\u52a0\u6743\u7684Flan-T5\u6587\u672c\u5d4c\u5165\u4f5c\u4e3a\u6761\u4ef6\u8f93\u5165\u3002", "result": "\u5ba2\u89c2\u548c\u4e3b\u89c2\u8bc4\u4f30\u663e\u793aPianoFlow\u4f18\u4e8e\u57fa\u4e8eTransformer\u7684\u57fa\u7ebf\u6a21\u578b\uff0c\u6027\u80fd\u8d28\u91cf\u53ef\u4e0e\u4eba\u7c7b\u5f55\u5236\u548c\u8f6c\u5f55\u7684MIDI\u6837\u672c\u76f8\u5ab2\u7f8e\u3002", "conclusion": "\u8be5\u6a21\u578b\u53ef\u96c6\u6210\u5230\u4ea4\u4e92\u5e94\u7528\u4e2d\uff0c\u6709\u52a9\u4e8e\u521b\u5efa\u66f4\u6613\u8bbf\u95ee\u548c\u5f15\u4eba\u5165\u80dc\u7684\u97f3\u4e50\u6f14\u594f\u7cfb\u7edf\u3002"}}
{"id": "2511.02846", "categories": ["eess.SP", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.02846", "abs": "https://arxiv.org/abs/2511.02846", "authors": ["Zan Li", "Kyongmin Yeo", "Wesley Gifford", "Lara Marcuse", "Madeline Fields", "B\u00fclent Yener"], "title": "Spatio-Temporal Attention Network for Epileptic Seizure Prediction", "comment": null, "summary": "In this study, we present a deep learning framework that learns complex\nspatio-temporal correlation structures of EEG signals through a Spatio-Temporal\nAttention Network (STAN) for accurate predictions of onset of seizures for\nEpilepsy patients. Unlike existing methods, which rely on feature engineering\nand/or assume fixed preictal durations, our approach simultaneously models\nspatio-temporal correlations through STAN and employs an adversarial\ndiscriminator to distinguish preictal from interictal attention patterns,\nenabling patient-specific learning. Evaluation on CHB-MIT and MSSM datasets\ndemonstrates 96.6\\% sensitivity with 0.011/h false detection rate on CHB-MIT,\nand 94.2% sensitivity with 0.063/h FDR on MSSM, significantly outperforming\nstate-of-the-art methods. The framework reliably detects preictal states at\nleast 15 minutes before an onset, with patient-specific windows extending to 45\nminutes, providing sufficient intervention time for clinical applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u65f6\u7a7a\u6ce8\u610f\u529b\u7f51\u7edc(STAN)\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5efa\u6a21EEG\u4fe1\u53f7\u7684\u590d\u6742\u65f6\u7a7a\u76f8\u5173\u6027\u6765\u51c6\u786e\u9884\u6d4b\u766b\u75eb\u53d1\u4f5c\uff0c\u65e0\u9700\u7279\u5f81\u5de5\u7a0b\u6216\u56fa\u5b9a\u9884\u53d1\u4f5c\u6301\u7eed\u65f6\u95f4\u5047\u8bbe\u3002", "motivation": "\u73b0\u6709\u766b\u75eb\u9884\u6d4b\u65b9\u6cd5\u4f9d\u8d56\u7279\u5f81\u5de5\u7a0b\u548c/\u6216\u5047\u8bbe\u56fa\u5b9a\u7684\u9884\u53d1\u4f5c\u6301\u7eed\u65f6\u95f4\uff0c\u65e0\u6cd5\u6709\u6548\u5efa\u6a21EEG\u4fe1\u53f7\u7684\u590d\u6742\u65f6\u7a7a\u76f8\u5173\u6027\u3002", "method": "\u4f7f\u7528\u65f6\u7a7a\u6ce8\u610f\u529b\u7f51\u7edc(STAN)\u540c\u65f6\u5efa\u6a21\u65f6\u7a7a\u76f8\u5173\u6027\uff0c\u5e76\u91c7\u7528\u5bf9\u6297\u5224\u522b\u5668\u533a\u5206\u9884\u53d1\u4f5c\u548c\u53d1\u4f5c\u95f4\u671f\u7684\u6ce8\u610f\u529b\u6a21\u5f0f\uff0c\u5b9e\u73b0\u60a3\u8005\u7279\u5f02\u6027\u5b66\u4e60\u3002", "result": "\u5728CHB-MIT\u6570\u636e\u96c6\u4e0a\u8fbe\u523096.6%\u7684\u654f\u611f\u6027\u548c0.011/h\u7684\u8bef\u68c0\u7387\uff0c\u5728MSSM\u6570\u636e\u96c6\u4e0a\u8fbe\u523094.2%\u7684\u654f\u611f\u6027\u548c0.063/h\u7684\u8bef\u68c0\u7387\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u53ef\u9760\u5730\u81f3\u5c11\u5728\u53d1\u4f5c\u524d15\u5206\u949f\u68c0\u6d4b\u5230\u9884\u53d1\u4f5c\u72b6\u6001\uff0c\u60a3\u8005\u7279\u5f02\u6027\u7a97\u53e3\u53ef\u5ef6\u957f\u81f345\u5206\u949f\uff0c\u4e3a\u4e34\u5e8a\u5e94\u7528\u63d0\u4f9b\u4e86\u8db3\u591f\u7684\u5e72\u9884\u65f6\u95f4\u3002"}}
{"id": "2511.03423", "categories": ["eess.AS", "cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2511.03423", "abs": "https://arxiv.org/abs/2511.03423", "authors": ["Jiyoung Lee", "Song Park", "Sanghyuk Chun", "Soo-Whan Chung"], "title": "Seeing What You Say: Expressive Image Generation from Speech", "comment": "In progress", "summary": "This paper proposes VoxStudio, the first unified and end-to-end\nspeech-to-image model that generates expressive images directly from spoken\ndescriptions by jointly aligning linguistic and paralinguistic information. At\nits core is a speech information bottleneck (SIB) module, which compresses raw\nspeech into compact semantic tokens, preserving prosody and emotional nuance.\nBy operating directly on these tokens, VoxStudio eliminates the need for an\nadditional speech-to-text system, which often ignores the hidden details beyond\ntext, e.g., tone or emotion. We also release VoxEmoset, a large-scale paired\nemotional speech-image dataset built via an advanced TTS engine to affordably\ngenerate richly expressive utterances. Comprehensive experiments on the\nSpokenCOCO, Flickr8kAudio, and VoxEmoset benchmarks demonstrate the feasibility\nof our method and highlight key challenges, including emotional consistency and\nlinguistic ambiguity, paving the way for future research.", "AI": {"tldr": "VoxStudio\u662f\u9996\u4e2a\u7edf\u4e00\u7684\u7aef\u5230\u7aef\u8bed\u97f3\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\uff0c\u901a\u8fc7\u8054\u5408\u5bf9\u9f50\u8bed\u8a00\u548c\u526f\u8bed\u8a00\u4fe1\u606f\uff0c\u76f4\u63a5\u4ece\u8bed\u97f3\u63cf\u8ff0\u751f\u6210\u5bcc\u6709\u8868\u73b0\u529b\u7684\u56fe\u50cf\u3002\u6838\u5fc3\u662f\u8bed\u97f3\u4fe1\u606f\u74f6\u9888\u6a21\u5757\uff0c\u5c06\u539f\u59cb\u8bed\u97f3\u538b\u7f29\u4e3a\u7d27\u51d1\u7684\u8bed\u4e49\u6807\u8bb0\uff0c\u4fdd\u7559\u97f5\u5f8b\u548c\u60c5\u611f\u7ec6\u8282\u3002", "motivation": "\u73b0\u6709\u7684\u8bed\u97f3\u5230\u56fe\u50cf\u7cfb\u7edf\u901a\u5e38\u9700\u8981\u989d\u5916\u7684\u8bed\u97f3\u8f6c\u6587\u672c\u6a21\u5757\uff0c\u8fd9\u4f1a\u5ffd\u7565\u6587\u672c\u4e4b\u5916\u7684\u9690\u85cf\u7ec6\u8282\uff08\u5982\u8bed\u8c03\u6216\u60c5\u611f\uff09\u3002VoxStudio\u65e8\u5728\u76f4\u63a5\u4ece\u8bed\u97f3\u751f\u6210\u56fe\u50cf\uff0c\u4fdd\u7559\u8bed\u97f3\u4e2d\u7684\u60c5\u611f\u548c\u97f5\u5f8b\u4fe1\u606f\u3002", "method": "\u63d0\u51fa\u8bed\u97f3\u4fe1\u606f\u74f6\u9888\u6a21\u5757\u538b\u7f29\u539f\u59cb\u8bed\u97f3\u4e3a\u8bed\u4e49\u6807\u8bb0\uff0c\u76f4\u63a5\u5728\u8fd9\u4e9b\u6807\u8bb0\u4e0a\u64cd\u4f5c\uff0c\u65e0\u9700\u8bed\u97f3\u8f6c\u6587\u672c\u7cfb\u7edf\u3002\u540c\u65f6\u53d1\u5e03\u4e86VoxEmoset\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u5148\u8fdb\u7684TTS\u5f15\u64ce\u751f\u6210\u5bcc\u6709\u8868\u73b0\u529b\u7684\u8bed\u97f3-\u56fe\u50cf\u5bf9\u3002", "result": "\u5728SpokenCOCO\u3001Flickr8kAudio\u548cVoxEmoset\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u7efc\u5408\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u53ef\u884c\u6027\uff0c\u5e76\u7a81\u51fa\u4e86\u60c5\u611f\u4e00\u81f4\u6027\u548c\u8bed\u8a00\u6b67\u4e49\u7b49\u5173\u952e\u6311\u6218\u3002", "conclusion": "VoxStudio\u4e3a\u76f4\u63a5\u4ece\u8bed\u97f3\u751f\u6210\u5bcc\u6709\u8868\u73b0\u529b\u56fe\u50cf\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\uff0c\u5e76\u4e3a\u672a\u6765\u7814\u7a76\u94fa\u5e73\u4e86\u9053\u8def\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u60c5\u611f\u4e00\u81f4\u6027\u548c\u8bed\u8a00\u6b67\u4e49\u65b9\u9762\u3002"}}
{"id": "2511.02848", "categories": ["eess.SP", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.02848", "abs": "https://arxiv.org/abs/2511.02848", "authors": ["Shantanu Sarkar", "Piotr Nabrzyski", "Saurabh Prasad", "Jose Luis Contreras-Vidal"], "title": "EEGReXferNet: A Lightweight Gen-AI Framework for EEG Subspace Reconstruction via Cross-Subject Transfer Learning and Channel-Aware Embedding", "comment": "Accepted for presentation at the NeurIPS 2025 Workshop on Foundation\n  Models for the Brain and Body", "summary": "Electroencephalography (EEG) is a widely used non-invasive technique for\nmonitoring brain activity, but low signal-to-noise ratios (SNR) due to various\nartifacts often compromise its utility. Conventional artifact removal methods\nrequire manual intervention or risk suppressing critical neural features during\nfiltering/reconstruction. Recent advances in generative models, including\nVariational Autoencoders (VAEs) and Generative Adversarial Networks (GANs),\nhave shown promise for EEG reconstruction; however, these approaches often lack\nintegrated temporal-spectral-spatial sensitivity and are computationally\nintensive, limiting their suitability for real-time applications like\nbrain-computer interfaces (BCIs). To overcome these challenges, we introduce\nEEGReXferNet, a lightweight Gen-AI framework for EEG subspace reconstruction\nvia cross-subject transfer learning - developed using Keras TensorFlow\n(v2.15.1). EEGReXferNet employs a modular architecture that leverages volume\nconduction across neighboring channels, band-specific convolution encoding, and\ndynamic latent feature extraction through sliding windows. By integrating\nreference-based scaling, the framework ensures continuity across successive\nwindows and generalizes effectively across subjects. This design improves\nspatial-temporal-spectral resolution (mean PSD correlation >= 0.95; mean\nspectrogram RV-Coefficient >= 0.85), reduces total weights by ~45% to mitigate\noverfitting, and maintains computational efficiency for robust, real-time EEG\npreprocessing in neurophysiological and BCI applications.", "AI": {"tldr": "EEGReXferNet\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u751f\u6210\u5f0fAI\u6846\u67b6\uff0c\u901a\u8fc7\u8de8\u88ab\u8bd5\u8fc1\u79fb\u5b66\u4e60\u8fdb\u884cEEG\u5b50\u7a7a\u95f4\u91cd\u5efa\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfEEG\u53bb\u566a\u65b9\u6cd5\u7684\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u65f6\u7a7a\u9891\u8c31\u5206\u8fa8\u7387\u5e76\u51cf\u5c11\u4e86\u8ba1\u7b97\u8d1f\u62c5\u3002", "motivation": "\u4f20\u7edfEEG\u4fe1\u53f7\u53bb\u566a\u65b9\u6cd5\u9700\u8981\u4eba\u5de5\u5e72\u9884\u6216\u5728\u6ee4\u6ce2/\u91cd\u5efa\u8fc7\u7a0b\u4e2d\u53ef\u80fd\u6291\u5236\u5173\u952e\u795e\u7ecf\u7279\u5f81\uff0c\u800c\u73b0\u6709\u7684\u751f\u6210\u6a21\u578b\u65b9\u6cd5\u7f3a\u4e4f\u96c6\u6210\u7684\u65f6\u7a7a\u9891\u8c31\u654f\u611f\u6027\u4e14\u8ba1\u7b97\u91cf\u5927\uff0c\u4e0d\u9002\u5408\u8111\u673a\u63a5\u53e3\u7b49\u5b9e\u65f6\u5e94\u7528\u3002", "method": "EEGReXferNet\u91c7\u7528\u6a21\u5757\u5316\u67b6\u6784\uff0c\u5229\u7528\u76f8\u90bb\u901a\u9053\u7684\u5bb9\u79ef\u4f20\u5bfc\u3001\u9891\u5e26\u7279\u5b9a\u5377\u79ef\u7f16\u7801\u548c\u6ed1\u52a8\u7a97\u53e3\u7684\u52a8\u6001\u6f5c\u5728\u7279\u5f81\u63d0\u53d6\uff0c\u901a\u8fc7\u57fa\u4e8e\u53c2\u8003\u7684\u7f29\u653e\u786e\u4fdd\u7a97\u53e3\u95f4\u7684\u8fde\u7eed\u6027\uff0c\u5e76\u5b9e\u73b0\u8de8\u88ab\u8bd5\u7684\u6709\u6548\u6cdb\u5316\u3002", "result": "\u8be5\u6846\u67b6\u63d0\u9ad8\u4e86\u65f6\u7a7a\u9891\u8c31\u5206\u8fa8\u7387\uff08\u5e73\u5747PSD\u76f8\u5173\u6027\u22650.95\uff1b\u5e73\u5747\u9891\u8c31\u56feRV\u7cfb\u6570\u22650.85\uff09\uff0c\u603b\u6743\u91cd\u51cf\u5c11\u7ea645%\u4ee5\u51cf\u8f7b\u8fc7\u62df\u5408\uff0c\u5e76\u4fdd\u6301\u4e86\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "EEGReXferNet\u4e3a\u795e\u7ecf\u751f\u7406\u5b66\u548c\u8111\u673a\u63a5\u53e3\u5e94\u7528\u63d0\u4f9b\u4e86\u7a33\u5065\u3001\u5b9e\u65f6\u7684EEG\u9884\u5904\u7406\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u8bbe\u8ba1\u548c\u8de8\u88ab\u8bd5\u8fc1\u79fb\u5b66\u4e60\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2511.02849", "categories": ["eess.SP", "cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2511.02849", "abs": "https://arxiv.org/abs/2511.02849", "authors": ["Beyza Cinar", "Maria Maleshkova"], "title": "Benchmarking ResNet for Short-Term Hypoglycemia Classification with DiaData", "comment": "11 pages, 5 Tables, 4 Figures, BHI 2025 conference (JBHI special\n  issue)", "summary": "Individualized therapy is driven forward by medical data analysis, which\nprovides insight into the patient's context. In particular, for Type 1 Diabetes\n(T1D), which is an autoimmune disease, relationships between demographics,\nsensor data, and context can be analyzed. However, outliers, noisy data, and\nsmall data volumes cannot provide a reliable analysis. Hence, the research\ndomain requires large volumes of high-quality data. Moreover, missing values\ncan lead to information loss. To address this limitation, this study improves\nthe data quality of DiaData, an integration of 15 separate datasets containing\nglucose values from 2510 subjects with T1D. Notably, we make the following\ncontributions: 1) Outliers are identified with the interquartile range (IQR)\napproach and treated by replacing them with missing values. 2) Small gaps\n($\\le$ 25 min) are imputed with linear interpolation and larger gaps ($\\ge$ 30\nand $<$ 120 min) with Stineman interpolation. Based on a visual comparison,\nStineman interpolation provides more realistic glucose estimates than linear\ninterpolation for larger gaps. 3) After data cleaning, the correlation between\nglucose and heart rate is analyzed, yielding a moderate relation between 15 and\n60 minutes before hypoglycemia ($\\le$ 70 mg/dL). 4) Finally, a benchmark for\nhypoglycemia classification is provided with a state-of-the-art ResNet model.\nThe model is trained with the Maindatabase and Subdatabase II of DiaData to\nclassify hypoglycemia onset up to 2 hours in advance. Training with more data\nimproves performance by 7% while using quality-refined data yields a 2-3% gain\ncompared to raw data.", "AI": {"tldr": "\u672c\u7814\u7a76\u6539\u8fdb\u4e86DiaData\u6570\u636e\u96c6\u7684\u8d28\u91cf\uff0c\u901a\u8fc7\u5f02\u5e38\u503c\u5904\u7406\u3001\u7f3a\u5931\u503c\u63d2\u8865\u7b49\u65b9\u6cd5\u63d0\u5347T1D\u6570\u636e\u5206\u6790\u7684\u53ef\u9760\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u8d28\u91cf\u63d0\u5347\u5bf9\u4f4e\u8840\u7cd6\u5206\u7c7b\u6a21\u578b\u6027\u80fd\u7684\u79ef\u6781\u5f71\u54cd\u3002", "motivation": "T1D\u4e2a\u4f53\u5316\u6cbb\u7597\u9700\u8981\u9ad8\u8d28\u91cf\u6570\u636e\u652f\u6301\uff0c\u4f46\u73b0\u6709\u6570\u636e\u5b58\u5728\u5f02\u5e38\u503c\u3001\u566a\u58f0\u548c\u5c0f\u6837\u672c\u95ee\u9898\uff0c\u5f71\u54cd\u5206\u6790\u7684\u53ef\u9760\u6027\u3002", "method": "\u4f7f\u7528IQR\u65b9\u6cd5\u8bc6\u522b\u5f02\u5e38\u503c\u5e76\u7528\u7f3a\u5931\u503c\u66ff\u6362\uff1b\u5c0f\u95f4\u9699\uff08\u226425\u5206\u949f\uff09\u7528\u7ebf\u6027\u63d2\u503c\uff0c\u5927\u95f4\u9699\uff0830-120\u5206\u949f\uff09\u7528Stineman\u63d2\u503c\uff1b\u5206\u6790\u8840\u7cd6\u4e0e\u5fc3\u7387\u76f8\u5173\u6027\uff1b\u4f7f\u7528ResNet\u6a21\u578b\u8fdb\u884c\u4f4e\u8840\u7cd6\u5206\u7c7b\u3002", "result": "Stineman\u63d2\u503c\u6bd4\u7ebf\u6027\u63d2\u503c\u63d0\u4f9b\u66f4\u73b0\u5b9e\u7684\u8840\u7cd6\u4f30\u8ba1\uff1b\u8840\u7cd6\u4e0e\u5fc3\u7387\u5728\u4f4e\u8840\u7cd6\u524d15-60\u5206\u949f\u6709\u4e2d\u7b49\u76f8\u5173\u6027\uff1b\u4f7f\u7528\u66f4\u591a\u6570\u636e\u8bad\u7ec3\u63d0\u5347\u6027\u80fd7%\uff0c\u4f7f\u7528\u8d28\u91cf\u4f18\u5316\u6570\u636e\u6bd4\u539f\u59cb\u6570\u636e\u63d0\u53472-3%\u3002", "conclusion": "\u6570\u636e\u8d28\u91cf\u6539\u8fdb\u663e\u8457\u63d0\u5347\u4e86T1D\u6570\u636e\u5206\u6790\u7684\u53ef\u9760\u6027\uff0c\u4e3a\u4e2a\u4f53\u5316\u6cbb\u7597\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u6570\u636e\u57fa\u7840\u3002"}}
{"id": "2511.02850", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.02850", "abs": "https://arxiv.org/abs/2511.02850", "authors": ["Youssif Abuzied", "Hassan AbdEltawab", "Abdelrhman Gaber", "Tamer ElBatt"], "title": "ECGXtract: Deep Learning-based ECG Feature Extraction for Automated CVD Diagnosis", "comment": null, "summary": "This paper presents ECGXtract, a deep learning-based approach for\ninterpretable ECG feature extraction, addressing the limitations of traditional\nsignal processing and black-box machine learning methods. In particular, we\ndevelop convolutional neural network models capable of extracting both temporal\nand morphological features with strong correlations to a clinically validated\nground truth. Initially, each model is trained to extract a single feature,\nensuring precise and interpretable outputs. A series of experiments is then\ncarried out to evaluate the proposed method across multiple setups, including\nglobal versus lead-specific features, different sampling frequencies, and\ncomparisons with other approaches such as ECGdeli. Our findings show that\nECGXtract achieves robust performance across most features with a mean\ncorrelation score of 0.80 with the ground truth for global features, with lead\nII consistently providing the best results. For lead-specific features,\nECGXtract achieves a mean correlation score of 0.822. Moreover, ECGXtract\nachieves superior results to the state-of-the-art open source ECGdeli as it got\na higher correlation score with the ground truth in 90% of the features.\nFurthermore, we explore the feasibility of extracting multiple features\nsimultaneously utilizing a single model. Semantic grouping is proved to be\neffective for global features, while large-scale grouping and lead-specific\nmulti-output models show notable performance drops. These results highlight the\npotential of structured grouping strategies to balance the computational\nefficiency vs. model accuracy, paving the way for more scalable and clinically\ninterpretable ECG feature extraction systems in limited resource settings.", "AI": {"tldr": "ECGXtract\u662f\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u53ef\u89e3\u91caECG\u7279\u5f81\u63d0\u53d6\u65b9\u6cd5\uff0c\u901a\u8fc7\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u63d0\u53d6\u4e0e\u4e34\u5e8a\u9a8c\u8bc1\u771f\u503c\u5f3a\u76f8\u5173\u7684\u65f6\u57df\u548c\u5f62\u6001\u7279\u5f81\uff0c\u5728\u591a\u4e2a\u5b9e\u9a8c\u8bbe\u7f6e\u4e0b\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u4fe1\u53f7\u5904\u7406\u548c\u9ed1\u76d2\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5728ECG\u7279\u5f81\u63d0\u53d6\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u5f00\u53d1\u53ef\u89e3\u91ca\u4e14\u7cbe\u786e\u7684\u7279\u5f81\u63d0\u53d6\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff0c\u6bcf\u4e2a\u6a21\u578b\u8bad\u7ec3\u63d0\u53d6\u5355\u4e2a\u7279\u5f81\u4ee5\u786e\u4fdd\u7cbe\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u5b9e\u9a8c\u8bc4\u4f30\u5305\u62ec\u5168\u5c40\u4e0e\u5bfc\u8054\u7279\u5b9a\u7279\u5f81\u3001\u4e0d\u540c\u91c7\u6837\u9891\u7387\uff0c\u5e76\u4e0eECGdeli\u7b49\u65b9\u6cd5\u6bd4\u8f83\u3002", "result": "ECGXtract\u5728\u5168\u5c40\u7279\u5f81\u4e0a\u5e73\u5747\u76f8\u5173\u6027\u5f97\u5206\u4e3a0.80\uff0c\u5bfc\u8054II\u8868\u73b0\u6700\u4f73\uff1b\u5bfc\u8054\u7279\u5b9a\u7279\u5f81\u5e73\u5747\u76f8\u5173\u6027\u5f97\u5206\u4e3a0.822\uff1b\u572890%\u7684\u7279\u5f81\u4e0a\u4f18\u4e8eECGdeli\u3002\u8bed\u4e49\u5206\u7ec4\u5bf9\u5168\u5c40\u7279\u5f81\u6709\u6548\uff0c\u4f46\u5927\u89c4\u6a21\u5206\u7ec4\u548c\u5bfc\u8054\u7279\u5b9a\u591a\u8f93\u51fa\u6a21\u578b\u6027\u80fd\u4e0b\u964d\u3002", "conclusion": "\u7ed3\u6784\u5316\u5206\u7ec4\u7b56\u7565\u53ef\u5728\u8ba1\u7b97\u6548\u7387\u548c\u6a21\u578b\u51c6\u786e\u6027\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u5f00\u53d1\u53ef\u6269\u5c55\u4e14\u4e34\u5e8a\u53ef\u89e3\u91ca\u7684ECG\u7279\u5f81\u63d0\u53d6\u7cfb\u7edf\u94fa\u5e73\u9053\u8def\u3002"}}
{"id": "2511.02851", "categories": ["eess.SP", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.02851", "abs": "https://arxiv.org/abs/2511.02851", "authors": ["Rushuang Zhou", "Yuan-Ting Zhang", "M. Jamal Deen", "Yining Dong"], "title": "Approaching Low-Cost Cardiac Intelligence with Semi-Supervised Knowledge Distillation", "comment": null, "summary": "Deploying advanced cardiac artificial intelligence for daily cardiac\nmonitoring is hindered by its reliance on extensive medical data and high\ncomputational resources. Low-cost cardiac intelligence (LCCI) offers a\npromising alternative by using wearable device data, such as 1-lead\nelectrocardiogram (ECG), but it suffers from a significant diagnostic\nperformance gap compared to high-cost cardiac intelligence (HCCI). To bridge\nthis gap, we propose LiteHeart, a semi-supervised knowledge distillation\nframework. LiteHeart introduces a region-aware distillation module to mimic how\ncardiologists focus on diagnostically relevant ECG regions and a cross-layer\nmutual information module to align the decision processes of LCCI and HCCI\nsystems. Using a semi-supervised training strategy, LiteHeart further improves\nmodel robustness under limited supervision. Evaluated on five datasets covering\nover 38 cardiovascular diseases, LiteHeart substantially reduces the\nperformance gap between LCCI and HCCI, outperforming existing methods by 4.27%\nto 7.10% in macro F1 score. These results demonstrate that LiteHeart\nsignificantly enhances the diagnostic capabilities of low-cost cardiac\nintelligence systems, paving the way for scalable, affordable, and accurate\ndaily cardiac healthcare using wearable technologies.", "AI": {"tldr": "LiteHeart\u662f\u4e00\u4e2a\u534a\u76d1\u7763\u77e5\u8bc6\u84b8\u998f\u6846\u67b6\uff0c\u901a\u8fc7\u533a\u57df\u611f\u77e5\u84b8\u998f\u548c\u8de8\u5c42\u4e92\u4fe1\u606f\u6a21\u5757\uff0c\u663e\u8457\u7f29\u5c0f\u4e86\u4f4e\u6210\u672c\u5fc3\u810f\u667a\u80fd\u7cfb\u7edf\u4e0e\u9ad8\u6210\u672c\u7cfb\u7edf\u4e4b\u95f4\u7684\u8bca\u65ad\u6027\u80fd\u5dee\u8ddd\u3002", "motivation": "\u5148\u8fdb\u7684\u5fc3\u810fAI\u5728\u65e5\u5e38\u76d1\u6d4b\u4e2d\u9762\u4e34\u533b\u7597\u6570\u636e\u9700\u6c42\u5927\u3001\u8ba1\u7b97\u8d44\u6e90\u9ad8\u7684\u95ee\u9898\uff0c\u800c\u4f4e\u6210\u672c\u5fc3\u810f\u667a\u80fd\u7cfb\u7edf\u867d\u7136\u4f7f\u7528\u53ef\u7a7f\u6234\u8bbe\u5907\u6570\u636e\uff0c\u4f46\u4e0e\u9ad8\u6210\u672c\u7cfb\u7edf\u5b58\u5728\u663e\u8457\u7684\u8bca\u65ad\u6027\u80fd\u5dee\u8ddd\u3002", "method": "\u63d0\u51faLiteHeart\u6846\u67b6\uff0c\u5305\u542b\u533a\u57df\u611f\u77e5\u84b8\u998f\u6a21\u5757\uff08\u6a21\u62df\u5fc3\u810f\u75c5\u4e13\u5bb6\u5173\u6ce8\u8bca\u65ad\u76f8\u5173ECG\u533a\u57df\uff09\u548c\u8de8\u5c42\u4e92\u4fe1\u606f\u6a21\u5757\uff08\u5bf9\u9f50LCCI\u548cHCCI\u7cfb\u7edf\u7684\u51b3\u7b56\u8fc7\u7a0b\uff09\uff0c\u91c7\u7528\u534a\u76d1\u7763\u8bad\u7ec3\u7b56\u7565\u63d0\u9ad8\u6a21\u578b\u9c81\u68d2\u6027\u3002", "result": "\u5728\u6db5\u76d638\u79cd\u5fc3\u8840\u7ba1\u75be\u75c5\u76845\u4e2a\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0cLiteHeart\u5c06LCCI\u4e0eHCCI\u4e4b\u95f4\u7684\u6027\u80fd\u5dee\u8ddd\u663e\u8457\u7f29\u5c0f\uff0c\u5728\u5b8f\u89c2F1\u5206\u6570\u4e0a\u6bd4\u73b0\u6709\u65b9\u6cd5\u63d0\u53474.27%\u81f37.10%\u3002", "conclusion": "LiteHeart\u663e\u8457\u589e\u5f3a\u4e86\u4f4e\u6210\u672c\u5fc3\u810f\u667a\u80fd\u7cfb\u7edf\u7684\u8bca\u65ad\u80fd\u529b\uff0c\u4e3a\u4f7f\u7528\u53ef\u7a7f\u6234\u6280\u672f\u5b9e\u73b0\u53ef\u6269\u5c55\u3001\u7ecf\u6d4e\u5b9e\u60e0\u4e14\u51c6\u786e\u7684\u65e5\u5e38\u5fc3\u810f\u4fdd\u5065\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2511.02852", "categories": ["eess.SP", "cs.GR", "cs.MM"], "pdf": "https://arxiv.org/pdf/2511.02852", "abs": "https://arxiv.org/abs/2511.02852", "authors": ["Shengze Xue", "Yu Ren", "Jiacheng Hong", "Run Ni", "Shuangjiu Xiao", "Deli Dong"], "title": "Real-Time Interactive Hybrid Ocean: Spectrum-Consistent Wave Particle-FFT Coupling", "comment": null, "summary": "Fast Fourier Transform-based (FFT) spectral oceans are widely adopted for\ntheir efficiency and large-scale realism, but they assume global stationarity\nand spatial homogeneity, making it difficult to represent non-uniform seas and\nnear-field interactions (e.g., ships and floaters). In contrast, wave particles\ncapture local wakes and ripples, yet are costly to maintain at scale and hard\nto match global spectral statistics.We present a real-time interactive hybrid\nocean: a global FFT background coupled with local wave-particle (WP) patch\nregions around interactive objects, jointly driven under a unified set of\nspectral parameters and dispersion. At patch boundaries, particles are injected\naccording to the same directional spectrum as the FFT, aligning the local\nfrequency-direction distribution with the background and matching energy\ndensity, without disturbing the far field.Our approach introduces two main\ninnovations: (1) Hybrid ocean representation. We couple a global FFT background\nwith local WP patches under a unified spectrum, achieving large-scale spectral\nconsistency while supporting localized wakes and ripples.(2) Frequency-bucketed\nimplementation. We design a particle sampling and GPU-parallel synthesis scheme\nbased on frequency buckets, which preserves spectral energy consistency and\nsustains real-time interactive performance.Together, these innovations enable a\nunified framework that delivers both large-scale spectral realism and\nfine-grained interactivity in real time.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5b9e\u65f6\u4ea4\u4e92\u5f0f\u6df7\u5408\u6d77\u6d0b\u6a21\u578b\uff0c\u5c06\u5168\u5c40FFT\u80cc\u666f\u4e0e\u5c40\u90e8\u6ce2\u7c92\u5b50\u533a\u57df\u76f8\u7ed3\u5408\uff0c\u5728\u7edf\u4e00\u9891\u8c31\u53c2\u6570\u4e0b\u5b9e\u73b0\u5927\u89c4\u6a21\u5149\u8c31\u771f\u5b9e\u6027\u548c\u7ec6\u7c92\u5ea6\u4ea4\u4e92\u6027\u3002", "motivation": "\u4f20\u7edfFFT\u6d77\u6d0b\u6a21\u578b\u5047\u8bbe\u5168\u5c40\u5e73\u7a33\u6027\u548c\u7a7a\u95f4\u5747\u5300\u6027\uff0c\u96be\u4ee5\u8868\u793a\u975e\u5747\u5300\u6d77\u6d0b\u548c\u8fd1\u573a\u4ea4\u4e92\uff1b\u800c\u6ce2\u7c92\u5b50\u65b9\u6cd5\u867d\u7136\u80fd\u6355\u6349\u5c40\u90e8\u6ce2\u6d6a\uff0c\u4f46\u5927\u89c4\u6a21\u7ef4\u62a4\u6210\u672c\u9ad8\u4e14\u96be\u4ee5\u5339\u914d\u5168\u5c40\u5149\u8c31\u7edf\u8ba1\u3002", "method": "\u91c7\u7528\u6df7\u5408\u6d77\u6d0b\u8868\u793a\uff1a\u5168\u5c40FFT\u80cc\u666f\u4e0e\u5c40\u90e8\u6ce2\u7c92\u5b50\u533a\u57df\u5728\u7edf\u4e00\u9891\u8c31\u4e0b\u8026\u5408\uff1b\u57fa\u4e8e\u9891\u7387\u5206\u6876\u7684\u7c92\u5b50\u91c7\u6837\u548cGPU\u5e76\u884c\u5408\u6210\u65b9\u6848\uff0c\u4fdd\u6301\u5149\u8c31\u80fd\u91cf\u4e00\u81f4\u6027\u3002", "result": "\u5b9e\u73b0\u4e86\u5927\u89c4\u6a21\u5149\u8c31\u4e00\u81f4\u6027\u7684\u540c\u65f6\u652f\u6301\u5c40\u90e8\u6ce2\u6d6a\u548c\u6d9f\u6f2a\uff0c\u5728\u5b9e\u65f6\u4ea4\u4e92\u6027\u80fd\u4e0b\u63d0\u4f9b\u7edf\u4e00\u6846\u67b6\u3002", "conclusion": "\u8be5\u6df7\u5408\u65b9\u6cd5\u6210\u529f\u7ed3\u5408\u4e86FFT\u6d77\u6d0b\u7684\u5927\u5c3a\u5ea6\u771f\u5b9e\u6027\u548c\u6ce2\u7c92\u5b50\u7684\u5c40\u90e8\u4ea4\u4e92\u80fd\u529b\uff0c\u4e3a\u5b9e\u65f6\u6d77\u6d0b\u6a21\u62df\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.02853", "categories": ["eess.SP", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.02853", "abs": "https://arxiv.org/abs/2511.02853", "authors": ["Young-Seok Kweon", "Gi-Hwan Shin", "Ji-Yong Kim", "Bokyeong Ryu", "Seong-Whan Lee"], "title": "Consciousness-ECG Transformer for Conscious State Estimation System with Real-Time Monitoring", "comment": "30 pages, 8 figures", "summary": "Conscious state estimation is important in various medical settings,\nincluding sleep staging and anesthesia management, to ensure patient safety and\noptimize health outcomes. Traditional methods predominantly utilize\nelectroencephalography (EEG), which faces challenges such as high sensitivity\nto noise and the requirement for controlled environments. In this study, we\npropose the consciousness-ECG transformer that leverages electrocardiography\n(ECG) signals for non-invasive and reliable conscious state estimation. Our\napproach employs a transformer with decoupled query attention to effectively\ncapture heart rate variability features that distinguish between conscious and\nunconscious states. We implemented the conscious state estimation system with\nreal-time monitoring and validated our system on datasets involving sleep\nstaging and anesthesia level monitoring during surgeries. Experimental results\ndemonstrate that our model outperforms baseline models, achieving accuracies of\n0.877 on sleep staging and 0.880 on anesthesia level monitoring. Moreover, our\nmodel achieves the highest area under curve values of 0.786 and 0.895 on sleep\nstaging and anesthesia level monitoring, respectively. The proposed system\noffers a practical and robust alternative to EEG-based methods, particularly\nsuited for dynamic clinical environments. Our results highlight the potential\nof ECG-based consciousness monitoring to enhance patient safety and advance our\nunderstanding of conscious states.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5fc3\u7535\u56fe\u7684\u610f\u8bc6\u72b6\u6001\u4f30\u8ba1\u7cfb\u7edf\uff0c\u4f7f\u7528\u89e3\u8026\u67e5\u8be2\u6ce8\u610f\u529btransformer\u6a21\u578b\uff0c\u5728\u7761\u7720\u5206\u671f\u548c\u9ebb\u9189\u76d1\u6d4b\u4efb\u52a1\u4e2d\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4e3a\u4e34\u5e8a\u73af\u5883\u63d0\u4f9b\u53ef\u9760\u66ff\u4ee3\u65b9\u6848\u3002", "motivation": "\u4f20\u7edf\u610f\u8bc6\u72b6\u6001\u76d1\u6d4b\u4e3b\u8981\u4f9d\u8d56\u8111\u7535\u56fe\uff0c\u4f46\u5b58\u5728\u566a\u58f0\u654f\u611f\u3001\u73af\u5883\u8981\u6c42\u9ad8\u7b49\u95ee\u9898\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u53ef\u9760\u3001\u975e\u4fb5\u5165\u7684\u66ff\u4ee3\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u5fc3\u7535\u56fe\u4fe1\u53f7\uff0c\u6784\u5efaconsciousness-ECG transformer\u6a21\u578b\uff0c\u91c7\u7528\u89e3\u8026\u67e5\u8be2\u6ce8\u610f\u529b\u673a\u5236\u6355\u6349\u5fc3\u7387\u53d8\u5f02\u6027\u7279\u5f81\u6765\u533a\u5206\u610f\u8bc6\u72b6\u6001\u3002", "result": "\u5728\u7761\u7720\u5206\u671f\u4efb\u52a1\u4e2d\u51c6\u786e\u7387\u8fbe0.877\uff0c\u9ebb\u9189\u76d1\u6d4b\u51c6\u786e\u7387\u8fbe0.880\uff0cAUC\u503c\u5206\u522b\u4e3a0.786\u548c0.895\uff0c\u5747\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "\u57fa\u4e8e\u5fc3\u7535\u56fe\u7684\u610f\u8bc6\u76d1\u6d4b\u7cfb\u7edf\u4e3a\u4e34\u5e8a\u73af\u5883\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u9c81\u68d2\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u6709\u671b\u63d0\u5347\u60a3\u8005\u5b89\u5168\u5e76\u4fc3\u8fdb\u5bf9\u610f\u8bc6\u72b6\u6001\u7684\u7406\u89e3\u3002"}}
{"id": "2511.02880", "categories": ["eess.SP", "cs.AI", "cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2511.02880", "abs": "https://arxiv.org/abs/2511.02880", "authors": ["Zehui Zhan", "Yaojun Hu", "Jiajing Zhan", "Wanchen Lian", "Wanqing Wu", "Jintai Chen"], "title": "NEF-NET+: Adapting Electrocardio panorama in the wild", "comment": null, "summary": "Conventional multi-lead electrocardiogram (ECG) systems capture cardiac\nsignals from a fixed set of anatomical viewpoints defined by lead placement.\nHowever, certain cardiac conditions (e.g., Brugada syndrome) require\nadditional, non-standard viewpoints to reveal diagnostically critical patterns\nthat may be absent in standard leads. To systematically overcome this\nlimitation, Nef-Net was recently introduced to reconstruct a continuous\nelectrocardiac field, enabling virtual observation of ECG signals from\narbitrary views (termed Electrocardio Panorama). Despite its promise, Nef-Net\noperates under idealized assumptions and faces in-the-wild challenges, such as\nlong-duration ECG modeling, robustness to device-specific signal artifacts, and\nsuboptimal lead placement calibration. This paper presents NEF-NET+, an\nenhanced framework for realistic panoramic ECG synthesis that supports\narbitrary-length signal synthesis from any desired view, generalizes across ECG\ndevices, and compensates for operator-induced deviations in electrode\nplacement. These capabilities are enabled by a newly designed model\narchitecture that performs direct view transformation, incorporating a workflow\ncomprising offline pretraining, device calibration tuning steps as well as an\non-the-fly calibration step for patient-specific adaptation. To rigorously\nevaluate panoramic ECG synthesis, we construct a new Electrocardio Panorama\nbenchmark, called Panobench, comprising 5367 recordings with 48-view per\nsubject, capturing the full spatial variability of cardiac electrical activity.\nExperimental results show that NEF-NET+ delivers substantial improvements over\nNef-Net, yielding an increase of around 6 dB in PSNR in real-world setting. The\ncode and Panobench will be released in a subsequent publication.", "AI": {"tldr": "NEF-NET+\u662f\u4e00\u4e2a\u589e\u5f3a\u7684\u5fc3\u7535\u56fe\u5168\u666f\u5408\u6210\u6846\u67b6\uff0c\u80fd\u591f\u4ece\u4efb\u610f\u89c6\u89d2\u751f\u6210\u4efb\u610f\u957f\u5ea6\u7684\u5fc3\u7535\u4fe1\u53f7\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u591a\u5bfc\u8054\u5fc3\u7535\u56fe\u7cfb\u7edf\u7684\u56fa\u5b9a\u89c6\u89d2\u9650\u5236\u3002", "motivation": "\u4f20\u7edf\u591a\u5bfc\u8054\u5fc3\u7535\u56fe\u7cfb\u7edf\u53ea\u80fd\u4ece\u56fa\u5b9a\u7684\u89e3\u5256\u89c6\u89d2\u6355\u6349\u5fc3\u810f\u4fe1\u53f7\uff0c\u800c\u67d0\u4e9b\u5fc3\u810f\u75be\u75c5\u9700\u8981\u975e\u6807\u51c6\u89c6\u89d2\u624d\u80fd\u663e\u793a\u8bca\u65ad\u5173\u952e\u6a21\u5f0f\u3002\u73b0\u6709\u7684Nef-Net\u65b9\u6cd5\u5728\u73b0\u5b9e\u5e94\u7528\u4e2d\u9762\u4e34\u957f\u65f6\u7a0b\u5efa\u6a21\u3001\u8bbe\u5907\u7279\u5f02\u6027\u4f2a\u5f71\u548c\u7535\u6781\u653e\u7f6e\u504f\u5dee\u7b49\u6311\u6218\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u65b0\u7684\u6a21\u578b\u67b6\u6784\uff0c\u6267\u884c\u76f4\u63a5\u89c6\u89d2\u53d8\u6362\uff0c\u5305\u542b\u79bb\u7ebf\u9884\u8bad\u7ec3\u3001\u8bbe\u5907\u6821\u51c6\u8c03\u4f18\u6b65\u9aa4\u4ee5\u53ca\u7528\u4e8e\u60a3\u8005\u7279\u5b9a\u9002\u5e94\u7684\u5b9e\u65f6\u6821\u51c6\u6b65\u9aa4\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cNEF-NET+\u76f8\u6bd4Nef-Net\u5728\u771f\u5b9e\u4e16\u754c\u8bbe\u7f6e\u4e2dPSNR\u63d0\u9ad8\u4e86\u7ea66 dB\uff0c\u6784\u5efa\u4e86\u5305\u542b5367\u4e2a\u8bb0\u5f55\u3001\u6bcf\u4e2a\u53d7\u8bd5\u800548\u4e2a\u89c6\u89d2\u7684\u65b0\u57fa\u51c6Panobench\u3002", "conclusion": "NEF-NET+\u663e\u8457\u63d0\u5347\u4e86\u5168\u666f\u5fc3\u7535\u56fe\u5408\u6210\u7684\u6027\u80fd\uff0c\u80fd\u591f\u66f4\u597d\u5730\u9002\u5e94\u73b0\u5b9e\u4e16\u754c\u5e94\u7528\u573a\u666f\uff0c\u4e3a\u5fc3\u810f\u7535\u6d3b\u52a8\u7a7a\u95f4\u53d8\u5f02\u6027\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u529b\u5de5\u5177\u3002"}}
{"id": "2511.02884", "categories": ["eess.SP", "cs.HC"], "pdf": "https://arxiv.org/pdf/2511.02884", "abs": "https://arxiv.org/abs/2511.02884", "authors": ["Dariush Salami", "Nima Bahmani", "H\u00fcseyin Yi\u011fitler", "Stephan Sigg"], "title": "Adaptive Internal Calibration for Temperature-Robust mmWave FMCW Radars", "comment": "Accepted to be published in ACM international joint conference on\n  Pervasive and Ubiquitous Computing (UbiComp)", "summary": "We present a novel internal calibration framework for Millimeter- Wave\n(mmWave) Frequency-Modulated Continuous-Wave (FMCW) radars to ensure robust\nperformance under internal temperature variations, tailored for deployment in\ndense wireless networks. Our approach mitigates the impact of\ntemperature-induced drifts in radar hardware, enhancing reliability. We propose\na temperature compensation model that leverages internal sensor data and signal\nprocessing techniques to maintain measurement accuracy. Experimental results\ndemonstrate improved robustness across a range of internal temperature\nconditions, with minimal computational overhead, ensuring scalability in dense\nnetwork environments. The framework also incorporates ethical design\nprinciples, avoiding reliance on sensitive external data. The proposed scheme\nreduces the Pearson correlation between the amplitude of the Intermediate\nFrequency (IF) signal and internal temperature drift up to 84%, significantly\nmitigating the temperature drift.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u6beb\u7c73\u6ce2FMCW\u96f7\u8fbe\u7684\u5185\u90e8\u6821\u51c6\u6846\u67b6\uff0c\u901a\u8fc7\u6e29\u5ea6\u8865\u507f\u6a21\u578b\u51cf\u5c11\u6e29\u5ea6\u6f02\u79fb\u5bf9\u6d4b\u91cf\u7cbe\u5ea6\u7684\u5f71\u54cd\uff0c\u5728\u5bc6\u96c6\u65e0\u7ebf\u7f51\u7edc\u4e2d\u5b9e\u73b0\u7a33\u5065\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u6beb\u7c73\u6ce2FMCW\u96f7\u8fbe\u5728\u5185\u90e8\u6e29\u5ea6\u53d8\u5316\u4e0b\u6027\u80fd\u4e0d\u7a33\u5b9a\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u5bc6\u96c6\u65e0\u7ebf\u7f51\u7edc\u90e8\u7f72\u4e2d\u786e\u4fdd\u6d4b\u91cf\u53ef\u9760\u6027\u3002", "method": "\u5229\u7528\u5185\u90e8\u4f20\u611f\u5668\u6570\u636e\u548c\u4fe1\u53f7\u5904\u7406\u6280\u672f\u6784\u5efa\u6e29\u5ea6\u8865\u507f\u6a21\u578b\uff0c\u51cf\u5c11\u6e29\u5ea6\u5f15\u8d77\u7684\u786c\u4ef6\u6f02\u79fb\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5c06\u4e2d\u9891\u4fe1\u53f7\u5e45\u5ea6\u4e0e\u5185\u90e8\u6e29\u5ea6\u6f02\u79fb\u7684\u76ae\u5c14\u900a\u76f8\u5173\u6027\u964d\u4f4e\u4e8684%\uff0c\u663e\u8457\u51cf\u8f7b\u4e86\u6e29\u5ea6\u6f02\u79fb\u5f71\u54cd\uff0c\u4e14\u8ba1\u7b97\u5f00\u9500\u5c0f\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u4fdd\u6301\u6d4b\u91cf\u7cbe\u5ea6\u7684\u540c\u65f6\uff0c\u901a\u8fc7\u5185\u90e8\u6821\u51c6\u673a\u5236\u589e\u5f3a\u4e86\u96f7\u8fbe\u5728\u6e29\u5ea6\u53d8\u5316\u73af\u5883\u4e0b\u7684\u9c81\u68d2\u6027\uff0c\u9002\u7528\u4e8e\u5bc6\u96c6\u7f51\u7edc\u90e8\u7f72\u3002"}}
{"id": "2511.02938", "categories": ["eess.SP", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.02938", "abs": "https://arxiv.org/abs/2511.02938", "authors": ["Sepideh KhakzadGharamaleki", "Hassan Rivaz", "Brandon Helfield"], "title": "From Narrow to Wide: Autoencoding Transformers for Ultrasound Bandwidth Recovery", "comment": null, "summary": "Conventional pulse-echo ultrasound suffers when low-cost probes deliver only\nnarrow fractional bandwidths, elongating pulses and erasing high-frequency\ndetail. We address this limitation by learning a data-driven mapping from\nband-limited to broadband spectrogram of radio-frequency (RF) lines. To this\nend, a variation of Tiny Vision Transform (ViT) auto-encoder is trained on\nsimulation data using a curriculum-weighted loss. On heterogeneous speckle-cyst\nphantoms, the network reduces image-domain MSE by 90 percent, boosts PSNR by\n6.7 dB, and raises SSIM to 0.965 compared with the narrow-band input. It also\nsharpens point-target rows in a completely unseen resolution phantom,\ndemonstrating strong out-of-distribution generalisation without sacrificing\nframe rate or phase information. These results indicate that a purely software\nupgrade can endow installed narrow-band probes with broadband-like performance,\npotentially widening access to high-resolution ultrasound in\nresource-constrained settings.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eViT\u81ea\u7f16\u7801\u5668\u7684\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\uff0c\u5c06\u7a84\u5e26\u8d85\u58f0RF\u4fe1\u53f7\u6620\u5c04\u4e3a\u5bbd\u5e26\u9891\u8c31\u56fe\uff0c\u663e\u8457\u63d0\u5347\u56fe\u50cf\u8d28\u91cf\u800c\u4e0d\u727a\u7272\u5e27\u7387\u6216\u76f8\u4f4d\u4fe1\u606f\u3002", "motivation": "\u4f20\u7edf\u8109\u51b2\u56de\u6ce2\u8d85\u58f0\u5728\u4f4e\u6210\u672c\u7a84\u5e26\u63a2\u5934\u4e0b\u6027\u80fd\u53d7\u9650\uff0c\u8109\u51b2\u5ef6\u957f\u5bfc\u81f4\u9ad8\u9891\u7ec6\u8282\u4e22\u5931\uff0c\u9650\u5236\u4e86\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u7684\u9ad8\u5206\u8fa8\u7387\u8d85\u58f0\u5e94\u7528\u3002", "method": "\u4f7f\u7528\u6539\u8fdb\u7684Tiny ViT\u81ea\u7f16\u7801\u5668\u5728\u4eff\u771f\u6570\u636e\u4e0a\u8bad\u7ec3\uff0c\u91c7\u7528\u8bfe\u7a0b\u52a0\u6743\u635f\u5931\u51fd\u6570\uff0c\u5b66\u4e60\u4ece\u7a84\u5e26\u5230\u5bbd\u5e26RF\u9891\u8c31\u56fe\u7684\u6620\u5c04\u3002", "result": "\u5728\u5f02\u8d28\u6591\u5757-\u56ca\u80bf\u4f53\u6a21\u4e0a\uff0c\u56fe\u50cf\u57dfMSE\u964d\u4f4e90%\uff0cPSNR\u63d0\u53476.7 dB\uff0cSSIM\u8fbe\u52300.965\uff1b\u5728\u672a\u89c1\u8fc7\u7684\u5206\u8fa8\u7387\u4f53\u6a21\u4e2d\u4e5f\u80fd\u9510\u5316\u70b9\u76ee\u6807\uff0c\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u5206\u5e03\u5916\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u7eaf\u8f6f\u4ef6\u5347\u7ea7\u53ef\u4f7f\u73b0\u6709\u7a84\u5e26\u63a2\u5934\u83b7\u5f97\u7c7b\u4f3c\u5bbd\u5e26\u7684\u6027\u80fd\uff0c\u6709\u671b\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u63a8\u5e7f\u9ad8\u5206\u8fa8\u7387\u8d85\u58f0\u5e94\u7528\u3002"}}
{"id": "2511.03130", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.03130", "abs": "https://arxiv.org/abs/2511.03130", "authors": ["Ved Prakash Dubey", "Shovan Bhaumik"], "title": "Consensus Tracking of an Underwater Vehicle Using Weighted Harmonic Mean Density", "comment": null, "summary": "This paper addresses an underwater target tracking problem in which a large\nnumber of sonobuoy sensors are deployed on a surveillance region. The region is\ndivided into several sub-regions, where a single tracker, capable of generating\ntrack is installed. Each sonobuoy can measure the direction of arrival of\nacoustic signals (known as bearing angles) and communicate the measurements\nwith the local tracker. Further, each local tracker can communicate with all\nother trackers, where each of them can exchange their estimate and finally a\nconsensus is reached. We propose a weighted harmonic mean density (HMD) based\ntracking to reach a consensus and provide a solution for the fusion of Gaussian\ndensities. In this approach, optimal weights are assigned by minimizing the\nKullback-Leibler divergence measure. Performance of the proposed method is\nmeasured using root mean square error, percentage of track divergence, and\nnormalized estimation error squared. Simulation results demonstrate that the\noptimized HMD-based fusion outperforms existing fusion methods during a\ndistributed tracking.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u52a0\u6743\u8c03\u548c\u5747\u503c\u5bc6\u5ea6\u7684\u6c34\u4e0b\u76ee\u6807\u8ddf\u8e2a\u65b9\u6cd5\uff0c\u901a\u8fc7\u6700\u5c0f\u5316K-L\u6563\u5ea6\u5206\u914d\u6700\u4f18\u6743\u91cd\uff0c\u5728\u5206\u5e03\u5f0f\u8ddf\u8e2a\u4e2d\u4f18\u4e8e\u73b0\u6709\u878d\u5408\u65b9\u6cd5", "motivation": "\u89e3\u51b3\u6c34\u4e0b\u76ee\u6807\u8ddf\u8e2a\u95ee\u9898\uff0c\u5176\u4e2d\u5927\u91cf\u58f0\u7eb3\u6d6e\u6807\u4f20\u611f\u5668\u90e8\u7f72\u5728\u76d1\u89c6\u533a\u57df\uff0c\u9700\u8981\u5b9e\u73b0\u591a\u4e2a\u8ddf\u8e2a\u5668\u4e4b\u95f4\u7684\u5171\u8bc6\u878d\u5408", "method": "\u4f7f\u7528\u52a0\u6743\u8c03\u548c\u5747\u503c\u5bc6\u5ea6\u8fdb\u884c\u8ddf\u8e2a\uff0c\u901a\u8fc7\u6700\u5c0f\u5316K-L\u6563\u5ea6\u6765\u5206\u914d\u6700\u4f18\u6743\u91cd\uff0c\u63d0\u4f9b\u9ad8\u65af\u5bc6\u5ea6\u878d\u5408\u89e3\u51b3\u65b9\u6848", "result": "\u4eff\u771f\u7ed3\u679c\u663e\u793a\uff0c\u4f18\u5316\u7684HMD\u878d\u5408\u65b9\u6cd5\u5728\u5747\u65b9\u6839\u8bef\u5dee\u3001\u8f68\u8ff9\u53d1\u6563\u767e\u5206\u6bd4\u548c\u5f52\u4e00\u5316\u4f30\u8ba1\u8bef\u5dee\u5e73\u65b9\u7b49\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u878d\u5408\u65b9\u6cd5", "conclusion": "\u63d0\u51fa\u7684\u52a0\u6743\u8c03\u548c\u5747\u503c\u5bc6\u5ea6\u878d\u5408\u65b9\u6cd5\u5728\u5206\u5e03\u5f0f\u8ddf\u8e2a\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\uff0c\u80fd\u591f\u6709\u6548\u5b9e\u73b0\u8ddf\u8e2a\u5668\u95f4\u7684\u5171\u8bc6"}}
{"id": "2511.03133", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.03133", "abs": "https://arxiv.org/abs/2511.03133", "authors": ["Ziheng Zhang", "Wen Chen", "Qingqing Wu", "Haoran Qin", "Zhendong Li", "Qiong Wu"], "title": "Analysis and Algorithm for Multi IRS Collaborative Localization via Hybrid Time Angle Estimation", "comment": null, "summary": "This paper proposes a novel multiple intelligent reflecting surfaces (IRSs)\ncollaborative hybrid localization system, which involves deploying multiple\nIRSs near the target area and achieving target localization through joint time\ndelay and angle estimation. Specifically, echo signals from all reflective\nelements are received by each sensor and jointly processed to estimate the time\ndelay and angle parameters. Based on the above model, we derive the Fisher\nInformation Matrix (FIM) for cascaded delay, Angle of Arrival (AOA), and Angle\nof Departure (AOD) estimation in semi passive passive models, along with the\ncorresponding Cramer Rao Bound (CRB). To achieve precise estimation close to\nthe CRB, we design efficient algorithms for angle and location estimation. For\nangle estimation, reflective signals are categorized into three cases based on\ntheir rank, with different signal preprocessing. By constructing an atomic norm\nset and minimizing the atomic norm, the joint angle estimation problem is\ntransformed into a convex optimization problem, and low-complexity estimation\nof multiple AOA and AOD pairs is achieved using the Alternating Direction\nMethod of Multipliers (ADMM). For location estimation, we propose a three-stage\nlocalization algorithm that combines weighted least squares, total least\nsquares, and quadratic correction to handle errors in the coefficient matrix\nand observation vector, thus improving accuracy. Numerical simulations validate\nthe superiority of the proposed system, demonstrating that the system's\ncollaboration, hybrid localization, and distributed deployment provide\nsubstantial benefits, as well as the accuracy of the proposed estimation\nalgorithms, particularly in low signal to noise ratio (SNR) condition.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u667a\u80fd\u53cd\u5c04\u9762\u534f\u4f5c\u7684\u6df7\u5408\u5b9a\u4f4d\u7cfb\u7edf\uff0c\u901a\u8fc7\u8054\u5408\u65f6\u5ef6\u548c\u89d2\u5ea6\u4f30\u8ba1\u5b9e\u73b0\u76ee\u6807\u5b9a\u4f4d\uff0c\u5e76\u8bbe\u8ba1\u4e86\u63a5\u8fd1\u514b\u62c9\u7f8e\u7f57\u754c\u7684\u4f30\u8ba1\u7b97\u6cd5\u3002", "motivation": "\u4f20\u7edf\u5b9a\u4f4d\u7cfb\u7edf\u5728\u4f4e\u4fe1\u566a\u6bd4\u6761\u4ef6\u4e0b\u6027\u80fd\u53d7\u9650\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u7cbe\u786e\u7684\u5b9a\u4f4d\u65b9\u6cd5\u6765\u6ee1\u8db3\u73b0\u4ee3\u901a\u4fe1\u548c\u611f\u77e5\u9700\u6c42\u3002", "method": "\u4f7f\u7528\u591aIRS\u534f\u4f5c\uff0c\u901a\u8fc7\u8054\u5408\u65f6\u5ef6\u548c\u89d2\u5ea6\u4f30\u8ba1\uff0c\u91c7\u7528\u539f\u5b50\u8303\u6570\u6700\u5c0f\u5316\u548cADMM\u7b97\u6cd5\u8fdb\u884c\u89d2\u5ea6\u4f30\u8ba1\uff0c\u4ee5\u53ca\u4e09\u9636\u6bb5\u5b9a\u4f4d\u7b97\u6cd5\u5904\u7406\u8bef\u5dee\u3002", "result": "\u6570\u503c\u4eff\u771f\u9a8c\u8bc1\u4e86\u7cfb\u7edf\u5728\u534f\u4f5c\u3001\u6df7\u5408\u5b9a\u4f4d\u548c\u5206\u5e03\u5f0f\u90e8\u7f72\u65b9\u9762\u7684\u4f18\u52bf\uff0c\u7279\u522b\u662f\u5728\u4f4e\u4fe1\u566a\u6bd4\u6761\u4ef6\u4e0b\u7b97\u6cd5\u7cbe\u5ea6\u663e\u8457\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u591aIRS\u534f\u4f5c\u6df7\u5408\u5b9a\u4f4d\u7cfb\u7edf\u80fd\u591f\u6709\u6548\u63d0\u9ad8\u5b9a\u4f4d\u7cbe\u5ea6\uff0c\u7279\u522b\u662f\u5728\u6076\u52a3\u4fe1\u9053\u6761\u4ef6\u4e0b\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2511.03220", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.03220", "abs": "https://arxiv.org/abs/2511.03220", "authors": ["Tianhao Mao", "Le Liang", "Jie Yang", "Hao Ye", "Shi Jin", "Geoffrey Ye Li"], "title": "Multimodal-Wireless: A Large-Scale Dataset for Sensing and Communication", "comment": null, "summary": "This paper presents Multimodal-Wireless, an open-source multimodal sensing\ndataset designed for wireless communication research. The dataset is generated\nthrough an integrated and customizable data pipeline built upon the CARLA\nsimulator and Sionna framework. It contains approximately 160,000 frames\ncollected across four virtual towns, sixteen communication scenarios, and three\nweather conditions, encompassing multiple sensing modalities--communication\nchannel, light detection and ranging, RGB and depth cameras, inertial\nmeasurement unit, and radar. This paper provides a comprehensive overview of\nthe dataset, outlining its key features, overall framework, and technical\nimplementation details. In addition, it explores potential research\napplications concerning communication and collaborative perception, exemplified\nby beam prediction using a multimodal large language model. The dataset is open\nin https://le-liang.github.io/mmw/.", "AI": {"tldr": "Multimodal-Wireless\u662f\u4e00\u4e2a\u7528\u4e8e\u65e0\u7ebf\u901a\u4fe1\u7814\u7a76\u7684\u5f00\u6e90\u591a\u6a21\u6001\u611f\u77e5\u6570\u636e\u96c6\uff0c\u5305\u542b\u7ea616\u4e07\u5e27\u6570\u636e\uff0c\u6db5\u76d6\u901a\u4fe1\u4fe1\u9053\u3001\u6fc0\u5149\u96f7\u8fbe\u3001RGB\u548c\u6df1\u5ea6\u76f8\u673a\u3001\u60ef\u6027\u6d4b\u91cf\u5355\u5143\u548c\u96f7\u8fbe\u7b49\u591a\u79cd\u4f20\u611f\u6a21\u6001\u3002", "motivation": "\u4e3a\u65e0\u7ebf\u901a\u4fe1\u7814\u7a76\u63d0\u4f9b\u9ad8\u8d28\u91cf\u7684\u591a\u6a21\u6001\u611f\u77e5\u6570\u636e\u96c6\uff0c\u652f\u6301\u901a\u4fe1\u548c\u534f\u540c\u611f\u77e5\u7b49\u7814\u7a76\u65b9\u5411\u3002", "method": "\u57fa\u4e8eCARLA\u6a21\u62df\u5668\u548cSionna\u6846\u67b6\u6784\u5efa\u96c6\u6210\u53ef\u5b9a\u5236\u6570\u636e\u7ba1\u9053\uff0c\u5728\u56db\u4e2a\u865a\u62df\u57ce\u9547\u3001\u5341\u516d\u4e2a\u901a\u4fe1\u573a\u666f\u548c\u4e09\u79cd\u5929\u6c14\u6761\u4ef6\u4e0b\u6536\u96c6\u6570\u636e\u3002", "result": "\u6210\u529f\u521b\u5efa\u5305\u542b\u7ea616\u4e07\u5e27\u591a\u6a21\u6001\u6570\u636e\u7684\u5f00\u653e\u6570\u636e\u96c6\uff0c\u5e76\u5c55\u793a\u4e86\u4f7f\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u6ce2\u675f\u9884\u6d4b\u7684\u5e94\u7528\u793a\u4f8b\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u4e3a\u65e0\u7ebf\u901a\u4fe1\u7814\u7a76\u63d0\u4f9b\u4e86\u5b9d\u8d35\u7684\u8d44\u6e90\uff0c\u652f\u6301\u591a\u6a21\u6001\u611f\u77e5\u548c\u901a\u4fe1\u6280\u672f\u7684\u878d\u5408\u53d1\u5c55\u3002"}}
{"id": "2511.03283", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.03283", "abs": "https://arxiv.org/abs/2511.03283", "authors": ["Zhiyuan Zhai", "Wei Ni", "Xin Wang", "Dusit Niyato", "Ekram Hossain"], "title": "Integrated Sensing and Communication with UAV Swarms via Decentralized Consensus ADMM", "comment": null, "summary": "UAV swarms can form virtual antenna arrays to exploit additional spatial\ndegrees of freedom and enhance integrated sensing and communication (ISAC). The\noptimization of UAV positions is challenging due to the distributed nature of\nswarms and the lack of a global view at individual UAVs.\n  This paper presents a new decentralized optimization framework that allows\nUAVs to decide their locations in parallel and reach consensus on a globally\noptimal swarm geometry for ISAC.\n  Specifically, we derive the achievable uplink rate and Cram\\'er-Rao Bound\n(CRB) as tractable metrics for communication and sensing, respectively.\n  The UAV positions are optimized to balance maximizing the communication rate\nand minimizing the CRB.\n  To solve this non-convex problem with coupled variables, we develop a\ndecentralized consensus alternating direction method of multipliers (ADMM)\nalgorithm, which enables the UAVs to iteratively align their local updates and\nreach consensus.\n  The algorithm decomposes the global objective into local projection updates,\nproxy-assisted consensus coordination, and lightweight dual updates, ensuring\nscalability and consistency throughout the swarm.\n  Simulations demonstrate that the proposed consensus ADMM algorithm converges\nrapidly with strong scalability, and that the UAV swarm significantly\noutperforms fixed-array baselines in both communication and sensing\nperformance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53bb\u4e2d\u5fc3\u5316\u7684\u65e0\u4eba\u673a\u7fa4\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u5171\u8bc6ADMM\u7b97\u6cd5\u8ba9\u65e0\u4eba\u673a\u5e76\u884c\u51b3\u7b56\u4f4d\u7f6e\uff0c\u8fbe\u6210\u5168\u5c40\u6700\u4f18\u7684ISAC\u51e0\u4f55\u6784\u578b\u3002", "motivation": "\u65e0\u4eba\u673a\u7fa4\u5f62\u6210\u865a\u62df\u5929\u7ebf\u9635\u5217\u53ef\u589e\u5f3a\u96c6\u6210\u611f\u77e5\u4e0e\u901a\u4fe1\u6027\u80fd\uff0c\u4f46\u7531\u4e8e\u5206\u5e03\u5f0f\u7279\u6027\u548c\u7f3a\u4e4f\u5168\u5c40\u89c6\u89d2\uff0c\u4f4d\u7f6e\u4f18\u5316\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u4f7f\u7528\u5171\u8bc6\u4ea4\u66ff\u65b9\u5411\u4e58\u5b50\u6cd5(ADMM)\uff0c\u5c06\u5168\u5c40\u76ee\u6807\u5206\u89e3\u4e3a\u5c40\u90e8\u6295\u5f71\u66f4\u65b0\u3001\u4ee3\u7406\u8f85\u52a9\u7684\u5171\u8bc6\u534f\u8c03\u548c\u8f7b\u91cf\u7ea7\u5bf9\u5076\u66f4\u65b0\u3002", "result": "\u7b97\u6cd5\u6536\u655b\u5feb\u901f\u4e14\u6269\u5c55\u6027\u5f3a\uff0c\u65e0\u4eba\u673a\u7fa4\u5728\u901a\u4fe1\u548c\u611f\u77e5\u6027\u80fd\u4e0a\u663e\u8457\u4f18\u4e8e\u56fa\u5b9a\u9635\u5217\u57fa\u7ebf\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u53bb\u4e2d\u5fc3\u5316\u5171\u8bc6ADMM\u6846\u67b6\u80fd\u6709\u6548\u89e3\u51b3\u65e0\u4eba\u673a\u7fa4ISAC\u4f4d\u7f6e\u4f18\u5316\u95ee\u9898\uff0c\u5b9e\u73b0\u5168\u5c40\u6700\u4f18\u6027\u80fd\u3002"}}
{"id": "2511.03284", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.03284", "abs": "https://arxiv.org/abs/2511.03284", "authors": ["Zhiyuan Zhai", "Xiaojun Yuan", "Xin Wang", "Geoffrey Ye Li"], "title": "Decentralized Federated Learning with Distributed Aggregation Weight Optimization", "comment": null, "summary": "Decentralized federated learning (DFL) is an emerging paradigm to enable edge\ndevices collaboratively training a learning model using a device-to-device\n(D2D) communication manner without the coordination of a parameter server (PS).\nAggregation weights, also known as mixing weights, are crucial in DFL process,\nand impact the learning efficiency and accuracy. Conventional design relies on\na so-called central entity to collect all local information and conduct system\noptimization to obtain appropriate weights. In this paper, we develop a\ndistributed aggregation weight optimization algorithm to align with the\ndecentralized nature of DFL. We analyze convergence by quantitatively capturing\nthe impact of the aggregation weights over decentralized communication\nnetworks. Based on the analysis, we then formulate a learning performance\noptimization problem by designing the aggregation weights to minimize the\nderived convergence bound. The optimization problem is further transformed as\nan eigenvalue optimization problem and solved by our proposed subgradient-based\nalgorithm in a distributed fashion. In our algorithm, edge devices only need\nlocal information to obtain the optimal aggregation weights through local (D2D)\ncommunications, just like the learning itself. Therefore, the optimization,\ncommunication, and learning process can be all conducted in a distributed\nfashion, which leads to a genuinely distributed DFL system. Numerical results\ndemonstrate the superiority of the proposed algorithm in practical DFL\ndeployment.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5e03\u5f0f\u805a\u5408\u6743\u91cd\u4f18\u5316\u7b97\u6cd5\uff0c\u7528\u4e8e\u53bb\u4e2d\u5fc3\u5316\u8054\u90a6\u5b66\u4e60\uff0c\u901a\u8fc7\u672c\u5730D2D\u901a\u4fe1\u4f18\u5316\u805a\u5408\u6743\u91cd\u6765\u6700\u5c0f\u5316\u6536\u655b\u8fb9\u754c\u3002", "motivation": "\u4f20\u7edf\u53bb\u4e2d\u5fc3\u5316\u8054\u90a6\u5b66\u4e60\u4f9d\u8d56\u4e2d\u5fc3\u5b9e\u4f53\u6536\u96c6\u4fe1\u606f\u5e76\u4f18\u5316\u6743\u91cd\uff0c\u8fd9\u4e0e\u53bb\u4e2d\u5fc3\u5316\u7279\u6027\u4e0d\u7b26\uff0c\u9700\u8981\u5f00\u53d1\u771f\u6b63\u5206\u5e03\u5f0f\u7684\u6743\u91cd\u4f18\u5316\u65b9\u6cd5\u3002", "method": "\u5c06\u805a\u5408\u6743\u91cd\u4f18\u5316\u95ee\u9898\u8f6c\u5316\u4e3a\u7279\u5f81\u503c\u4f18\u5316\u95ee\u9898\uff0c\u63d0\u51fa\u57fa\u4e8e\u6b21\u68af\u5ea6\u7684\u5206\u5e03\u5f0f\u7b97\u6cd5\uff0c\u4ec5\u9700\u672c\u5730\u4fe1\u606f\u548cD2D\u901a\u4fe1\u5373\u53ef\u83b7\u5f97\u6700\u4f18\u6743\u91cd\u3002", "result": "\u6570\u503c\u7ed3\u679c\u8868\u660e\u8be5\u7b97\u6cd5\u5728\u5b9e\u9645DFL\u90e8\u7f72\u4e2d\u5177\u6709\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "\u5b9e\u73b0\u4e86\u771f\u6b63\u5206\u5e03\u5f0f\u7684DFL\u7cfb\u7edf\uff0c\u4f18\u5316\u3001\u901a\u4fe1\u548c\u5b66\u4e60\u8fc7\u7a0b\u5747\u53ef\u5206\u5e03\u5f0f\u8fdb\u884c\u3002"}}
{"id": "2511.03290", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.03290", "abs": "https://arxiv.org/abs/2511.03290", "authors": ["Jinhao Yi", "Weijun Gao", "Chong Han"], "title": "Diffusion-Driven Terahertz Air-Ground Communications under Dynamic Atmospheric Turbulence", "comment": null, "summary": "The ever-increasing demand for ultra-high data rates in space-air-ground\nintegrated networks (SAGINs) has rendered terahertz THz communications a\npromising technology owing to its exceptionally broad and continuous spectrum\nresources. Nevertheless, in air-ground (AG) scenarios, the high mobility of\naircraft induces intense and rapidly fluctuating turbulence, leading to\nadditional propagation loss that is often overlooked in existing studies. To\nbridge this gap, this paper presents an AI-empowered THz AG communication\nframework that explicitly models turbulence-induced attenuation through fluid\ndynamics and integrates it into an adaptive optimization paradigm for\ncommunication performance enhancement. Specifically, a fluid-dynamics-informed\nattenuation model is established to characterize aircraft-generated turbulence\nand quantify its impact on THz signal propagation. Building upon this model, a\njoint power-attitude optimization problem is formulated to adaptively allocate\ntransmit power and adjust aircraft attitude for maximizing link capacity. The\noptimization problem is efficiently solved using a diffusion-based algorithm\nthat learns the nonlinear relationship between flight configuration and\nturbulence-induced attenuation. Comprehensive numerical evaluations demonstrate\nthat the turbulence-induced attenuation ranges from 18 to 28 dB under attacking\nangles between -10 degree and 10 degree at 0.7 Mach, verifying the pronounced\nimpact of aircraft-induced turbulence on THz propagation. Furthermore, the\nproposed framework attains an average capacity of 11.241 bps/Hz, substantially\noutperforming existing strategies by 22.8% and 66.5%, and approaching\napproximately 98% of the theoretical capacity limit.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cdAI\u8d4b\u80fd\u7684\u592a\u8d6b\u5179\u7a7a-\u5730\u901a\u4fe1\u6846\u67b6\uff0c\u901a\u8fc7\u6d41\u4f53\u52a8\u529b\u5b66\u5efa\u6a21\u98de\u673a\u6e4d\u6d41\u5f15\u8d77\u7684\u8870\u51cf\uff0c\u5e76\u91c7\u7528\u6269\u6563\u7b97\u6cd5\u8054\u5408\u4f18\u5316\u53d1\u5c04\u529f\u7387\u548c\u98de\u673a\u59ff\u6001\uff0c\u663e\u8457\u63d0\u5347\u94fe\u8def\u5bb9\u91cf\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5f80\u5f80\u5ffd\u7565\u98de\u673a\u9ad8\u901f\u79fb\u52a8\u5f15\u8d77\u7684\u5f3a\u70c8\u6e4d\u6d41\u5bf9\u592a\u8d6b\u5179\u901a\u4fe1\u7684\u989d\u5916\u4f20\u64ad\u635f\u8017\uff0c\u9700\u8981\u5efa\u7acb\u7cbe\u786e\u7684\u6e4d\u6d41\u8870\u51cf\u6a21\u578b\u5e76\u4f18\u5316\u901a\u4fe1\u6027\u80fd\u3002", "method": "\u5efa\u7acb\u57fa\u4e8e\u6d41\u4f53\u52a8\u529b\u5b66\u7684\u6e4d\u6d41\u8870\u51cf\u6a21\u578b\uff0c\u6784\u5efa\u8054\u5408\u529f\u7387-\u59ff\u6001\u4f18\u5316\u95ee\u9898\uff0c\u4f7f\u7528\u6269\u6563\u7b97\u6cd5\u5b66\u4e60\u98de\u884c\u914d\u7f6e\u4e0e\u6e4d\u6d41\u8870\u51cf\u7684\u975e\u7ebf\u6027\u5173\u7cfb\u8fdb\u884c\u9ad8\u6548\u6c42\u89e3\u3002", "result": "\u6e4d\u6d41\u5f15\u8d77\u7684\u8870\u51cf\u5728-10\u5ea6\u523010\u5ea6\u653b\u51fb\u89d2\u30010.7\u9a6c\u8d6b\u6761\u4ef6\u4e0b\u8fbe\u523018-28dB\uff1b\u6240\u63d0\u6846\u67b6\u5e73\u5747\u5bb9\u91cf\u8fbe11.241bps/Hz\uff0c\u6bd4\u73b0\u6709\u7b56\u7565\u63d0\u534722.8%\u548c66.5%\uff0c\u63a5\u8fd1\u7406\u8bba\u5bb9\u91cf\u6781\u9650\u768498%\u3002", "conclusion": "\u98de\u673a\u8bf1\u5bfc\u6e4d\u6d41\u5bf9\u592a\u8d6b\u5179\u4f20\u64ad\u6709\u663e\u8457\u5f71\u54cd\uff0c\u6240\u63d0\u51fa\u7684AI\u8d4b\u80fd\u6846\u67b6\u80fd\u6709\u6548\u7f13\u89e3\u6e4d\u6d41\u8870\u51cf\uff0c\u5927\u5e45\u63d0\u5347\u7a7a-\u5730\u592a\u8d6b\u5179\u901a\u4fe1\u6027\u80fd\u3002"}}
{"id": "2511.03291", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.03291", "abs": "https://arxiv.org/abs/2511.03291", "authors": ["Zhiyuan Zhai", "Shuyan Hu", "Wei Ni", "Xiaojun Yuan", "Xin Wang"], "title": "Spectral-Convergent Decentralized Machine Learning: Theory and Application in Space Networks", "comment": null, "summary": "Decentralized machine learning (DML) supports collaborative training in\nlarge-scale networks with no central server. It is sensitive to the quality and\nreliability of inter-device communications that result in time-varying and\nstochastic topologies. This paper studies the impact of unreliable\ncommunication on the convergence of DML and establishes a direct connection\nbetween the spectral properties of the mixing process and the global\nperformance. We provide rigorous convergence guarantees under random topologies\nand derive bounds that characterize the impact of the expected mixing matrix's\nspectral properties on learning. We formulate a spectral optimization problem\nthat minimizes the spectral radius of the expected second-order mixing matrix\nto enhance the convergence rate under probabilistic link failures. To solve\nthis non-smooth spectral problem in a fully decentralized manner, we design an\nefficient subgradient-based algorithm that integrates Chebyshev-accelerated\neigenvector estimation with local update and aggregation weight adjustment,\nwhile ensuring symmetry and stochasticity constraints without central\ncoordination. Experiments on a realistic low Earth orbit (LEO) satellite\nconstellation with time-varying inter-satellite link models and real-world\nremote sensing data demonstrate the feasibility and effectiveness of the\nproposed method. The method significantly improves classification accuracy and\nconvergence efficiency compared to existing baselines, validating its\napplicability in satellite and other decentralized systems.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u53bb\u4e2d\u5fc3\u5316\u673a\u5668\u5b66\u4e60\u5728\u968f\u673a\u62d3\u6251\u7f51\u7edc\u4e2d\u7684\u6536\u655b\u6027\u95ee\u9898\uff0c\u5efa\u7acb\u4e86\u6df7\u5408\u8fc7\u7a0b\u8c31\u7279\u6027\u4e0e\u5168\u5c40\u6027\u80fd\u7684\u76f4\u63a5\u8054\u7cfb\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u8c31\u4f18\u5316\u65b9\u6cd5\u6765\u63d0\u5347\u5728\u6982\u7387\u6027\u94fe\u8def\u6545\u969c\u4e0b\u7684\u6536\u655b\u6548\u7387\u3002", "motivation": "\u53bb\u4e2d\u5fc3\u5316\u673a\u5668\u5b66\u4e60\u5728\u5927\u89c4\u6a21\u7f51\u7edc\u4e2d\u652f\u6301\u65e0\u4e2d\u5fc3\u670d\u52a1\u5668\u7684\u534f\u4f5c\u8bad\u7ec3\uff0c\u4f46\u5bf9\u8bbe\u5907\u95f4\u901a\u4fe1\u8d28\u91cf\u654f\u611f\uff0c\u5bb9\u6613\u51fa\u73b0\u65f6\u53d8\u548c\u968f\u673a\u62d3\u6251\u3002\u9700\u8981\u7814\u7a76\u4e0d\u53ef\u9760\u901a\u4fe1\u5bf9DML\u6536\u655b\u7684\u5f71\u54cd\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u8c31\u4f18\u5316\u95ee\u9898\uff0c\u6700\u5c0f\u5316\u671f\u671b\u4e8c\u9636\u6df7\u5408\u77e9\u9635\u7684\u8c31\u534a\u5f84\u4ee5\u63d0\u5347\u6536\u655b\u901f\u7387\u3002\u8bbe\u8ba1\u4e86\u57fa\u4e8e\u6b21\u68af\u5ea6\u7684\u5206\u5e03\u5f0f\u7b97\u6cd5\uff0c\u7ed3\u5408\u5207\u6bd4\u96ea\u592b\u52a0\u901f\u7279\u5f81\u5411\u91cf\u4f30\u8ba1\u4e0e\u672c\u5730\u66f4\u65b0\u548c\u805a\u5408\u6743\u91cd\u8c03\u6574\uff0c\u786e\u4fdd\u5bf9\u79f0\u6027\u548c\u968f\u673a\u6027\u7ea6\u675f\u3002", "result": "\u5728\u4f4e\u5730\u7403\u8f68\u9053\u536b\u661f\u661f\u5ea7\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u76f8\u6bd4\u73b0\u6709\u57fa\u7ebf\u663e\u8457\u63d0\u9ad8\u4e86\u5206\u7c7b\u51c6\u786e\u6027\u548c\u6536\u655b\u6548\u7387\uff0c\u9a8c\u8bc1\u4e86\u5728\u536b\u661f\u548c\u5176\u4ed6\u53bb\u4e2d\u5fc3\u5316\u7cfb\u7edf\u4e2d\u7684\u9002\u7528\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u8c31\u4f18\u5316\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u5347\u53bb\u4e2d\u5fc3\u5316\u673a\u5668\u5b66\u4e60\u5728\u4e0d\u53ef\u9760\u901a\u4fe1\u73af\u5883\u4e0b\u7684\u6027\u80fd\uff0c\u4e3a\u536b\u661f\u7f51\u7edc\u7b49\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.03292", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.03292", "abs": "https://arxiv.org/abs/2511.03292", "authors": ["Qiuyuan Yang", "Cunhua Pan", "Ruidong Li", "Zhenkun Zhang", "Hong Ren", "Changhong Wang", "Jiangzhou Wang"], "title": "UAV SAR Imaging with 5G NR OFDM Signals in NLOS Environments", "comment": null, "summary": "The integration of sensing and communication (ISAC) has significant potential\nfor future wireless systems, enabling efficient spectrum utilization and novel\napplication scenarios. In this paper, we propose a cooperative ISAC framework\nfor synthetic aperture radar (SAR) imaging by leveraging orthogonal frequency\ndivision multiplexing (OFDM) communication signals. We address the challenge of\nsevere imaging degradation in non-line-of-sight (NLOS) environments under the\nQUAsi Deterministic RadIo channel GenerAtor (QuaDRiGa). To detect weak signals\nand eliminate false points, we develop a two-stage compressed sensing-space\nalternating generalized expectation maximization (CS-SAGE) scheme for\nhigh-precision scatterer localization. In stage I, orthogonal matching pursuit\n(OMP) is employed for coarse estimation to identify the approximate locations\nof dominant scatterers. Then, the SAGE algorithm in stage II performs fine\nestimation to accurately extract scatterer parameters. Simulation results\nvalidate the effectiveness of the proposed cooperative ISAC framework, and\nprovide valuable insights for practical system design.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eOFDM\u901a\u4fe1\u4fe1\u53f7\u7684\u534f\u4f5c\u5f0f\u611f\u77e5\u4e0e\u901a\u4fe1\u4e00\u4f53\u5316SAR\u6210\u50cf\u6846\u67b6\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u538b\u7f29\u611f\u77e5-\u7a7a\u95f4\u4ea4\u66ff\u5e7f\u4e49\u671f\u671b\u6700\u5927\u5316\u65b9\u6848\uff0c\u5728\u975e\u89c6\u8ddd\u73af\u5883\u4e0b\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u6563\u5c04\u4f53\u5b9a\u4f4d\u3002", "motivation": "\u89e3\u51b3\u5728\u975e\u89c6\u8ddd\u73af\u5883\u4e0b\u5408\u6210\u5b54\u5f84\u96f7\u8fbe\u6210\u50cf\u6027\u80fd\u4e25\u91cd\u9000\u5316\u7684\u95ee\u9898\uff0c\u5229\u7528ISAC\u6280\u672f\u5b9e\u73b0\u9ad8\u6548\u7684\u9891\u8c31\u5229\u7528\u548c\u65b0\u578b\u5e94\u7528\u573a\u666f\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5CS-SAGE\u65b9\u6848\uff1a\u7b2c\u4e00\u9636\u6bb5\u4f7f\u7528\u6b63\u4ea4\u5339\u914d\u8ffd\u8e2a\u8fdb\u884c\u7c97\u4f30\u8ba1\uff0c\u8bc6\u522b\u4e3b\u8981\u6563\u5c04\u4f53\u7684\u5927\u81f4\u4f4d\u7f6e\uff1b\u7b2c\u4e8c\u9636\u6bb5\u4f7f\u7528SAGE\u7b97\u6cd5\u8fdb\u884c\u7cbe\u7ec6\u4f30\u8ba1\uff0c\u51c6\u786e\u63d0\u53d6\u6563\u5c04\u4f53\u53c2\u6570\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u9a8c\u8bc1\u4e86\u6240\u63d0\u534f\u4f5cISAC\u6846\u67b6\u7684\u6709\u6548\u6027\uff0c\u4e3a\u5b9e\u9645\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u534f\u4f5cISAC\u6846\u67b6\u5728\u975e\u89c6\u8ddd\u73af\u5883\u4e0b\u80fd\u591f\u6709\u6548\u63d0\u5347SAR\u6210\u50cf\u6027\u80fd\uff0c\u4e3a\u672a\u6765\u65e0\u7ebf\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u6280\u672f\u65b9\u6848\u3002"}}
{"id": "2511.03302", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.03302", "abs": "https://arxiv.org/abs/2511.03302", "authors": ["Xiaoyun Wang", "Yutong Zhang", "Sen Wang", "Sun Qi", "Hanning Wang", "Qixing Wang", "Jing Jin", "Jiwei He", "Nan Li"], "title": "C-RAN Advanced: From a Network Cooperation Perspective", "comment": null, "summary": "Future mobile networks in the sixth generation (6G) are poised for a paradigm\nshift from conventional communication services toward comprehensive information\nservices, driving the evolution of radio access network (RAN) architectures\ntoward enhanced cooperation, intelligence, and service orientation. Building\nupon the concept of centralized, collaborative, cloud, and clean RAN (C-RAN),\nthis article proposes a novel cooperative, intelligent, and service-based RAN\n(CIS-RAN) architecture. Focusing on cooperation, CIS-RAN extends the\ntraditional cooperative communication paradigm by further integrating\ncooperative sensing and cooperative artificial intelligence (AI). To improve\nboth performance and effectiveness across diverse application scenarios,\nCIS-RAN enhances network cooperation throughout the entire process of\nacquisition, transmission, and processing, thereby enabling efficient\ninformation acquisition, diverse cooperative interactions, and intelligent\nfusion decision-making. Key technologies are discussed, with network\ncooperative multiple-input multiple-output (MIMO) examined as a case study,\ndemonstrating superior performance over traditional architectures, as\ndemonstrated by numerical results. Future research directions are outlined,\nemphasizing the continued exploration and advancement of the CIS-RAN\narchitecture, particularly in enhancing network cooperation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684CIS-RAN\u67b6\u6784\uff0c\u5c06\u534f\u4f5c\u901a\u4fe1\u6269\u5c55\u5230\u534f\u4f5c\u611f\u77e5\u548c\u534f\u4f5cAI\uff0c\u901a\u8fc7\u5168\u6d41\u7a0b\u7f51\u7edc\u534f\u4f5c\u63d0\u53476G\u7f51\u7edc\u6027\u80fd\u3002", "motivation": "6G\u7f51\u7edc\u9700\u8981\u4ece\u4f20\u7edf\u901a\u4fe1\u670d\u52a1\u5411\u7efc\u5408\u4fe1\u606f\u670d\u52a1\u8f6c\u53d8\uff0c\u63a8\u52a8RAN\u67b6\u6784\u5411\u589e\u5f3a\u534f\u4f5c\u3001\u667a\u80fd\u5316\u548c\u670d\u52a1\u5bfc\u5411\u6f14\u8fdb\u3002", "method": "\u57fa\u4e8eC-RAN\u6982\u5ff5\uff0c\u63d0\u51faCIS-RAN\u67b6\u6784\uff0c\u6574\u5408\u534f\u4f5c\u901a\u4fe1\u3001\u534f\u4f5c\u611f\u77e5\u548c\u534f\u4f5cAI\uff0c\u5728\u91c7\u96c6\u3001\u4f20\u8f93\u548c\u5904\u7406\u5168\u8fc7\u7a0b\u4e2d\u589e\u5f3a\u7f51\u7edc\u534f\u4f5c\u3002", "result": "\u901a\u8fc7\u7f51\u7edc\u534f\u4f5cMIMO\u6848\u4f8b\u7814\u7a76\uff0c\u6570\u503c\u7ed3\u679c\u663e\u793a\u76f8\u6bd4\u4f20\u7edf\u67b6\u6784\u5177\u6709\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "CIS-RAN\u67b6\u6784\u57286G\u7f51\u7edc\u4e2d\u5177\u6709\u91cd\u8981\u5e94\u7528\u524d\u666f\uff0c\u672a\u6765\u7814\u7a76\u65b9\u5411\u5e94\u7ee7\u7eed\u63a2\u7d22\u548c\u63a8\u8fdb\u7f51\u7edc\u534f\u4f5c\u7684\u589e\u5f3a\u3002"}}
{"id": "2511.03401", "categories": ["eess.SP", "H.1"], "pdf": "https://arxiv.org/pdf/2511.03401", "abs": "https://arxiv.org/abs/2511.03401", "authors": ["Kunrui Cao", "Jingyu Chen", "Panagiotis D. Diamantoulakis", "Lei Zhou", "Xingwang Li", "Yuanwei Liu", "George K. Karagiannidis"], "title": "Performance Analysis of Wireless-Powered Pinching Antenna Systems", "comment": "13 pages, 8 figures", "summary": "Pinching antenna system (PAS) serves as a groundbreaking paradigm that\nenhances wireless communications by flexibly adjusting the position of pinching\nantenna (PA) and establishing a strong line-of-sight (LoS) link, thereby\nreducing the free-space path loss. This paper introduces the concept of\nwireless-powered PAS, and investigates the reliability of wireless-powered PAS\nto explore the advantages of PA in improving the performance of\nwireless-powered communication (WPC) system. In addition, we derive the\nclosed-form expressions of outage probability and ergodic rate for the\npractical lossy waveguide case and ideal lossless waveguide case, respectively,\nand analyze the optimal deployment of waveguides and user to provide valuable\ninsights for guiding their deployments. The results show that an increase in\nthe absorption coefficient and in the dimensions of the user area leads to\nhigher in-waveguide and free-space propagation losses, respectively, which in\nturn increase the outage probability and reduce the ergodic rate of the\nwireless-powered PAS. However, the performance of wireless-powered PAS is\nseverely affected by the absorption coefficient and the waveguide length, e.g.,\nunder conditions of high absorption coefficient and long waveguide, the outage\nprobability of wireless-powered PAS is even worse than that of traditional WPC\nsystem. While the ergodic rate of wireless-powered PAS is better than that of\ntraditional WPC system under conditions of high absorption coefficient and long\nwaveguide. Interestingly, the wireless-powered PAS has the optimal time\nallocation factor and optimal distance between power station (PS) and access\npoint (AP) to minimize the outage probability or maximize the ergodic rate.\nMoreover, the system performance of PS and AP separated at the optimal distance\nbetween PS and AP is superior to that of PS and AP integrated into a hybrid\naccess point.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u65e0\u7ebf\u4f9b\u7535\u7684\u5939\u6301\u5929\u7ebf\u7cfb\u7edf(PAS)\uff0c\u901a\u8fc7\u7075\u6d3b\u8c03\u6574\u5939\u6301\u5929\u7ebf\u4f4d\u7f6e\u5efa\u7acb\u5f3a\u89c6\u8ddd\u94fe\u8def\u6765\u964d\u4f4e\u81ea\u7531\u7a7a\u95f4\u8def\u5f84\u635f\u8017\u3002\u7814\u7a76\u4e86\u65e0\u7ebf\u4f9b\u7535PAS\u7684\u53ef\u9760\u6027\uff0c\u63a8\u5bfc\u4e86\u635f\u8017\u548c\u65e0\u635f\u8017\u6ce2\u5bfc\u60c5\u51b5\u4e0b\u7684\u4e2d\u65ad\u6982\u7387\u548c\u904d\u5386\u901f\u7387\u7684\u95ed\u5f0f\u8868\u8fbe\u5f0f\uff0c\u5206\u6790\u4e86\u6ce2\u5bfc\u548c\u7528\u6237\u7684\u6700\u4f18\u90e8\u7f72\u7b56\u7565\u3002", "motivation": "\u63a2\u7d22\u5939\u6301\u5929\u7ebf\u5728\u6539\u5584\u65e0\u7ebf\u4f9b\u7535\u901a\u4fe1\u7cfb\u7edf\u6027\u80fd\u65b9\u9762\u7684\u4f18\u52bf\uff0c\u901a\u8fc7\u5efa\u7acb\u5f3a\u89c6\u8ddd\u94fe\u8def\u6765\u51cf\u5c11\u81ea\u7531\u7a7a\u95f4\u8def\u5f84\u635f\u8017\uff0c\u63d0\u9ad8\u7cfb\u7edf\u53ef\u9760\u6027\u3002", "method": "\u5f15\u5165\u65e0\u7ebf\u4f9b\u7535PAS\u6982\u5ff5\uff0c\u63a8\u5bfc\u4e86\u635f\u8017\u548c\u65e0\u635f\u8017\u6ce2\u5bfc\u60c5\u51b5\u4e0b\u7684\u4e2d\u65ad\u6982\u7387\u548c\u904d\u5386\u901f\u7387\u7684\u95ed\u5f0f\u8868\u8fbe\u5f0f\uff0c\u5206\u6790\u4e86\u6ce2\u5bfc\u548c\u7528\u6237\u7684\u6700\u4f18\u90e8\u7f72\u7b56\u7565\uff0c\u5305\u62ec\u65f6\u95f4\u5206\u914d\u56e0\u5b50\u548c\u529f\u7387\u7ad9\u4e0e\u63a5\u5165\u70b9\u95f4\u6700\u4f18\u8ddd\u79bb\u7684\u4f18\u5316\u3002", "result": "\u5438\u6536\u7cfb\u6570\u548c\u7528\u6237\u533a\u57df\u5c3a\u5bf8\u7684\u589e\u52a0\u4f1a\u5bfc\u81f4\u66f4\u9ad8\u7684\u6ce2\u5bfc\u5185\u548c\u81ea\u7531\u7a7a\u95f4\u4f20\u64ad\u635f\u8017\uff0c\u4ece\u800c\u63d0\u9ad8\u4e2d\u65ad\u6982\u7387\u5e76\u964d\u4f4e\u904d\u5386\u901f\u7387\u3002\u5728\u9ad8\u5438\u6536\u7cfb\u6570\u548c\u957f\u6ce2\u5bfc\u6761\u4ef6\u4e0b\uff0c\u65e0\u7ebf\u4f9b\u7535PAS\u7684\u4e2d\u65ad\u6982\u7387\u6bd4\u4f20\u7edfWPC\u7cfb\u7edf\u66f4\u5dee\uff0c\u4f46\u904d\u5386\u901f\u7387\u66f4\u597d\u3002\u7cfb\u7edf\u5b58\u5728\u6700\u4f18\u65f6\u95f4\u5206\u914d\u56e0\u5b50\u548cPS-AP\u95f4\u6700\u4f18\u8ddd\u79bb\u6765\u6700\u5c0f\u5316\u4e2d\u65ad\u6982\u7387\u6216\u6700\u5927\u5316\u904d\u5386\u901f\u7387\u3002", "conclusion": "\u65e0\u7ebf\u4f9b\u7535PAS\u5728PS\u548cAP\u5206\u79bb\u90e8\u7f72\u4e8e\u6700\u4f18\u8ddd\u79bb\u65f6\u7684\u7cfb\u7edf\u6027\u80fd\u4f18\u4e8ePS\u548cAP\u96c6\u6210\u5230\u6df7\u5408\u63a5\u5165\u70b9\u7684\u65b9\u6848\u3002\u7cfb\u7edf\u6027\u80fd\u53d7\u5438\u6536\u7cfb\u6570\u548c\u6ce2\u5bfc\u957f\u5ea6\u4e25\u91cd\u5f71\u54cd\uff0c\u9700\u8981\u4f18\u5316\u90e8\u7f72\u7b56\u7565\u6765\u5145\u5206\u53d1\u6325PAS\u7684\u4f18\u52bf\u3002"}}
{"id": "2511.03465", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.03465", "abs": "https://arxiv.org/abs/2511.03465", "authors": ["Javier Gim\u00e9nez", "Jos\u00e9 A. Cort\u00e9s", "Francisco Javier Ca\u00f1ete", "Eduardo Martos-Naya", "Luis D\u00edez"], "title": "A Modified Pulse and Design Framework to Halve the Complexity of OFDM Spectral Shaping Techniques", "comment": "5 pages, 1 figure, journal paper", "summary": "Orthogonal frequency division multiplexing (OFDM) is a widespread modulation\nbut suffers from high out-of-band emissions (OOBE). Spectral shaping strategies\nsuch as precoding, active interference cancellation (AIC) and time-domain\nmethods are effective at reducing the OOBE but entail optimization procedures\nand real-time implementation costs which might be considerable. This letter\nproposes a modification of the conventional OFDM waveform aimed at reducing the\ncost associated to many of the state-of-theart spectral shaping techniques and\nsets a framework for future works that want to benefit from the same reduction.\nThis approach may reduce both the number of coefficients involved in the\noptimization and the number of products of its implementation by up to 50%.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u6539\u8fdb\u7684OFDM\u6ce2\u5f62\uff0c\u53ef\u964d\u4f4e\u9891\u8c31\u6574\u5f62\u6280\u672f\u7684\u4f18\u5316\u548c\u5b9e\u73b0\u6210\u672c\uff0c\u6700\u591a\u51cf\u5c1150%\u7684\u7cfb\u6570\u548c\u4e58\u79ef\u8fd0\u7b97", "motivation": "OFDM\u8c03\u5236\u5b58\u5728\u9ad8\u5e26\u5916\u53d1\u5c04\u95ee\u9898\uff0c\u73b0\u6709\u9891\u8c31\u6574\u5f62\u6280\u672f\u867d\u7136\u6709\u6548\u4f46\u6d89\u53ca\u590d\u6742\u7684\u4f18\u5316\u8fc7\u7a0b\u548c\u5b9e\u65f6\u5b9e\u73b0\u6210\u672c", "method": "\u5bf9\u4f20\u7edfOFDM\u6ce2\u5f62\u8fdb\u884c\u4fee\u6539\uff0c\u4e3a\u9891\u8c31\u6574\u5f62\u6280\u672f\u63d0\u4f9b\u6846\u67b6\u652f\u6301\uff0c\u51cf\u5c11\u4f18\u5316\u7cfb\u6570\u548c\u5b9e\u73b0\u4e58\u79ef\u8fd0\u7b97", "result": "\u8be5\u65b9\u6cd5\u53ef\u5c06\u4f18\u5316\u6d89\u53ca\u7684\u7cfb\u6570\u6570\u91cf\u548c\u5b9e\u73b0\u6240\u9700\u7684\u4e58\u79ef\u8fd0\u7b97\u6570\u91cf\u51cf\u5c11\u9ad8\u8fbe50%", "conclusion": "\u8be5\u6539\u8fdb\u65b9\u6cd5\u663e\u8457\u964d\u4f4e\u4e86\u9891\u8c31\u6574\u5f62\u6280\u672f\u7684\u6210\u672c\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u6210\u672c\u964d\u4f4e\u7684\u6846\u67b6"}}
{"id": "2511.03487", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.03487", "abs": "https://arxiv.org/abs/2511.03487", "authors": ["Yameng Liu", "Jianhua Zhang", "Yuxiang Zhang", "Zhiqiang Yuan", "Chuangxin Jiang", "Junchen Liu", "Wei Hong", "Yingyang Li", "Yan Li", "Guangyi Liu"], "title": "A Novel Multi-Reference-Point Modeling Framework for Monostatic Background Channel: Toward 3GPP ISAC Standardization", "comment": null, "summary": "Integrated Sensing and Communication (ISAC) has been identified as a key 6G\napplication by ITU and 3GPP. A realistic, standard-compatible channel model is\nessential for ISAC system design. To characterize the impact of Sensing Targets\n(STs), 3GPP defines ISAC channel as a combination of target and background\nchannels, comprising multipath components related to STs and those originating\nsolely from the environment, respectively. Although the background channel does\nnot carry direct ST information, its accurate modeling is critical for\nevaluating sensing performance, especially in complex environments. Existing\ncommunication standards characterize propagation between separated transmitter\n(Tx) and receiver (Rx). However, modeling background channels in the ISAC\nmonostatic mode, where the Tx and Rx are co-located, remains a pressing\nchallenge. In this paper, we firstly conduct ISAC monostatic background channel\nmeasurements for an indoor scenario at 28 GHz. Realistic channel parameters are\nextracted, revealing pronounced single-hop propagation and discrete multipath\ndistribution. Inspired by these properties, a novel stochastic model is\nproposed to characterizing the ISAC monostatic background channel as the\nsuperposition of sub-channels between the monostatic Tx&Rx and multiple\ncommunication Rx-like Reference Points (RPs). This model is compatible with\nstandardizations, and a 3GPP-extended implementation framework is introduced.\nFinally, a genetic algorithm-based method is proposed to extract the optimal\nnumber and placement of multi-RPs. The optimization approach and modeling\nframework are validated by comparing measured and simulated channel parameters.\nResults demonstrate that the proposed model effectively captures monostatic\nbackground channel characteristics, addresses a critical gap in ISAC channel\nmodeling, and supports 6G standardization.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e6G\u96c6\u6210\u611f\u77e5\u4e0e\u901a\u4fe1(ISAC)\u5355\u9759\u6001\u80cc\u666f\u4fe1\u9053\u7684\u65b0\u578b\u968f\u673a\u6a21\u578b\uff0c\u901a\u8fc7\u5c06\u4fe1\u9053\u5efa\u6a21\u4e3a\u5355\u9759\u6001\u6536\u53d1\u5668\u4e0e\u591a\u4e2a\u53c2\u8003\u70b9\u4e4b\u95f4\u7684\u5b50\u4fe1\u9053\u53e0\u52a0\uff0c\u89e3\u51b3\u4e863GPP\u6807\u51c6\u517c\u5bb9\u7684ISAC\u4fe1\u9053\u5efa\u6a21\u5173\u952e\u6311\u6218\u3002", "motivation": "\u73b0\u6709\u901a\u4fe1\u6807\u51c6\u4e3b\u8981\u9488\u5bf9\u5206\u79bb\u7684\u6536\u53d1\u5668\u5efa\u6a21\uff0c\u800cISAC\u5355\u9759\u6001\u6a21\u5f0f\uff08\u6536\u53d1\u5668\u5171\u5740\uff09\u7684\u80cc\u666f\u4fe1\u9053\u5efa\u6a21\u4ecd\u662f\u4e00\u4e2a\u7d27\u8feb\u6311\u6218\uff0c\u8fd9\u5bf9\u8bc4\u4f30\u590d\u6742\u73af\u5883\u4e2d\u7684\u611f\u77e5\u6027\u80fd\u81f3\u5173\u91cd\u8981\u3002", "method": "\u9996\u5148\u572828GHz\u9891\u6bb5\u8fdb\u884c\u5ba4\u5185ISAC\u5355\u9759\u6001\u80cc\u666f\u4fe1\u9053\u6d4b\u91cf\uff0c\u63d0\u53d6\u5b9e\u9645\u4fe1\u9053\u53c2\u6570\uff1b\u7136\u540e\u63d0\u51fa\u5c06ISAC\u5355\u9759\u6001\u80cc\u666f\u4fe1\u9053\u5efa\u6a21\u4e3a\u5355\u9759\u6001\u6536\u53d1\u5668\u4e0e\u591a\u4e2a\u901a\u4fe1\u63a5\u6536\u5668\u6837\u53c2\u8003\u70b9\u4e4b\u95f4\u5b50\u4fe1\u9053\u7684\u53e0\u52a0\uff1b\u6700\u540e\u4f7f\u7528\u9057\u4f20\u7b97\u6cd5\u4f18\u5316\u591a\u53c2\u8003\u70b9\u7684\u6570\u91cf\u548c\u4f4d\u7f6e\u3002", "result": "\u6d4b\u91cf\u7ed3\u679c\u663e\u793a\u4fe1\u9053\u5177\u6709\u660e\u663e\u7684\u5355\u8df3\u4f20\u64ad\u548c\u79bb\u6563\u591a\u5f84\u5206\u5e03\u7279\u6027\uff1b\u9a8c\u8bc1\u7ed3\u679c\u8868\u660e\u6240\u63d0\u6a21\u578b\u80fd\u6709\u6548\u6355\u6349\u5355\u9759\u6001\u80cc\u666f\u4fe1\u9053\u7279\u6027\uff0c\u89e3\u51b3\u4e86ISAC\u4fe1\u9053\u5efa\u6a21\u7684\u5173\u952e\u7a7a\u767d\u3002", "conclusion": "\u8be5\u6a21\u578b\u4e0e\u6807\u51c6\u5316\u517c\u5bb9\uff0c\u63d0\u51fa\u76843GPP\u6269\u5c55\u5b9e\u73b0\u6846\u67b6\u652f\u63016G\u6807\u51c6\u5316\uff0c\u4e3aISAC\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u73b0\u5b9e\u4e14\u6807\u51c6\u517c\u5bb9\u7684\u4fe1\u9053\u6a21\u578b\u3002"}}
{"id": "2511.03612", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.03612", "abs": "https://arxiv.org/abs/2511.03612", "authors": ["Yingjie Xu", "Xuesong Cai", "Michiel Sandra", "Sara Willhammar", "Fredrik Tufvesson"], "title": "3D Cooperative User Tracking for Distributed Integrated Sensing and Communication", "comment": null, "summary": "As integrated sensing and communication (ISAC) becomes an integral part of 6G\nnetworks, distributed ISAC (DISAC) is expected to enhance both sensing and\ncommunication performance through its decentralized architecture. This paper\npresents a complete framework to address the challenge of cooperative user\ntracking in DISAC systems. By incorporating a global probability hypothesis\ndensity (PHD) filter and a field-of-view-aware access point (AP) management\nstrategy, the framework enables accurate user tracking using radio signals\nwhile optimizing AP scheduling. In addition, a real-world distributed MIMO\nchannel measurement campaign is performed to evaluate the effectiveness of the\nframework. The results demonstrate that a centimeter-level root mean-square\ntrajectory error can be achieved. Furthermore, the results show that it is not\nnecessary to keep APs active at all times to maintain high tracking accuracy,\nindicating the need for robust and efficient AP management. These findings\nprovide valuable insight into practical deployments and further development of\ncooperative user tracking techniques in DISAC systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5206\u5e03\u5f0fISAC\u7cfb\u7edf\u4e2d\u534f\u540c\u7528\u6237\u8ddf\u8e2a\u7684\u5b8c\u6574\u6846\u67b6\uff0c\u901a\u8fc7\u5168\u5c40PHD\u6ee4\u6ce2\u5668\u548c\u89c6\u573a\u611f\u77e5AP\u7ba1\u7406\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u5398\u7c73\u7ea7\u7cbe\u5ea6\u7684\u7528\u6237\u8f68\u8ff9\u8ddf\u8e2a\uff0c\u5e76\u4f18\u5316\u4e86AP\u8c03\u5ea6\u6548\u7387\u3002", "motivation": "\u968f\u7740ISAC\u6210\u4e3a6G\u7f51\u7edc\u7684\u91cd\u8981\u7ec4\u6210\u90e8\u5206\uff0c\u5206\u5e03\u5f0fISAC\u6709\u671b\u901a\u8fc7\u5176\u53bb\u4e2d\u5fc3\u5316\u67b6\u6784\u63d0\u5347\u611f\u77e5\u548c\u901a\u4fe1\u6027\u80fd\uff0c\u9700\u8981\u89e3\u51b3\u534f\u540c\u7528\u6237\u8ddf\u8e2a\u7684\u6311\u6218\u3002", "method": "\u91c7\u7528\u5168\u5c40\u6982\u7387\u5047\u8bbe\u5bc6\u5ea6(PHD)\u6ee4\u6ce2\u5668\u548c\u89c6\u573a\u611f\u77e5\u7684\u63a5\u5165\u70b9(AP)\u7ba1\u7406\u7b56\u7565\uff0c\u7ed3\u5408\u5206\u5e03\u5f0fMIMO\u4fe1\u9053\u6d4b\u91cf\u6765\u8bc4\u4f30\u6846\u67b6\u6709\u6548\u6027\u3002", "result": "\u5b9e\u73b0\u4e86\u5398\u7c73\u7ea7\u7684\u5747\u65b9\u6839\u8f68\u8ff9\u8bef\u5dee\uff0c\u5e76\u8bc1\u660e\u65e0\u9700\u59cb\u7ec8\u4fdd\u6301\u6240\u6709AP\u6d3b\u8dc3\u5373\u53ef\u7ef4\u6301\u9ad8\u8ddf\u8e2a\u7cbe\u5ea6\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u5206\u5e03\u5f0fISAC\u7cfb\u7edf\u4e2d\u534f\u540c\u7528\u6237\u8ddf\u8e2a\u6280\u672f\u7684\u5b9e\u9645\u90e8\u7f72\u548c\u8fdb\u4e00\u6b65\u53d1\u5c55\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\uff0c\u5f3a\u8c03\u4e86\u7a33\u5065\u9ad8\u6548\u7684AP\u7ba1\u7406\u7684\u5fc5\u8981\u6027\u3002"}}

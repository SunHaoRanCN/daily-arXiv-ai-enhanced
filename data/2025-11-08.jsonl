{"id": "2511.03942", "categories": ["cs.SD", "cs.CL", "cs.MM"], "pdf": "https://arxiv.org/pdf/2511.03942", "abs": "https://arxiv.org/abs/2511.03942", "authors": ["Shih-Lun Wu", "Yoon Kim", "Cheng-Zhi Anna Huang"], "title": "MIDI-LLM: Adapting Large Language Models for Text-to-MIDI Music Generation", "comment": "To appear at NeurIPS 2025 Workshop on AI for Music", "summary": "We present MIDI-LLM, an LLM for generating multitrack MIDI music from\nfree-form text prompts. Our approach expands a text LLM's vocabulary to include\nMIDI tokens, and uses a two-stage training recipe to endow text-to-MIDI\nabilities. By preserving the original LLM's parameter structure, we can\ndirectly leverage the vLLM library for accelerated inference. Experiments show\nthat MIDI-LLM achieves higher quality, better text control, and faster\ninference compared to the recent Text2midi model. Live demo at\nhttps://midi-llm-demo.vercel.app."}
{"id": "2511.04376", "categories": ["cs.SD", "cs.AI", "cs.LG", "cs.MM", "eess.AS"], "pdf": "https://arxiv.org/pdf/2511.04376", "abs": "https://arxiv.org/abs/2511.04376", "authors": ["Ali Boudaghi", "Hadi Zare"], "title": "MusRec: Zero-Shot Text-to-Music Editing via Rectified Flow and Diffusion Transformers", "comment": null, "summary": "Music editing has emerged as an important and practical area of artificial\nintelligence, with applications ranging from video game and film music\nproduction to personalizing existing tracks according to user preferences.\nHowever, existing models face significant limitations, such as being restricted\nto editing synthesized music generated by their own models, requiring highly\nprecise prompts, or necessitating task-specific retraining, thus lacking true\nzero-shot capability. Leveraging recent advances in rectified flow and\ndiffusion transformers, we introduce MusRec, the first zero-shot text-to-music\nediting model capable of performing diverse editing tasks on real-world music\nefficiently and effectively. Experimental results demonstrate that our approach\noutperforms existing methods in preserving musical content, structural\nconsistency, and editing fidelity, establishing a strong foundation for\ncontrollable music editing in real-world scenarios."}
{"id": "2511.04623", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2511.04623", "abs": "https://arxiv.org/abs/2511.04623", "authors": ["Yutong Wen", "Ke Chen", "Prem Seetharaman", "Oriol Nieto", "Jiaqi Su", "Rithesh Kumar", "Minje Kim", "Paris Smaragdis", "Zeyu Jin", "Justin Salamon"], "title": "PromptSep: Generative Audio Separation via Multimodal Prompting", "comment": "Submitted to ICASSP 2026", "summary": "Recent breakthroughs in language-queried audio source separation (LASS) have\nshown that generative models can achieve higher separation audio quality than\ntraditional masking-based approaches. However, two key limitations restrict\ntheir practical use: (1) users often require operations beyond separation, such\nas sound removal; and (2) relying solely on text prompts can be unintuitive for\nspecifying sound sources. In this paper, we propose PromptSep to extend LASS\ninto a broader framework for general-purpose sound separation. PromptSep\nleverages a conditional diffusion model enhanced with elaborated data\nsimulation to enable both audio extraction and sound removal. To move beyond\ntext-only queries, we incorporate vocal imitation as an additional and more\nintuitive conditioning modality for our model, by incorporating Sketch2Sound as\na data augmentation strategy. Both objective and subjective evaluations on\nmultiple benchmarks demonstrate that PromptSep achieves state-of-the-art\nperformance in sound removal and vocal-imitation-guided source separation,\nwhile maintaining competitive results on language-queried source separation."}
{"id": "2511.04533", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2511.04533", "abs": "https://arxiv.org/abs/2511.04533", "authors": ["Vladimir Despotovic", "Peter Pocta", "Andrej Zgank"], "title": "CardioPHON: Quality assessment and self-supervised pretraining for screening of cardiac function based on phonocardiogram recordings", "comment": null, "summary": "Remote monitoring of cardiovascular diseases plays an essential role in early\ndetection of abnormal cardiac function, enabling timely intervention, improved\npreventive care, and personalized patient treatment. Abnormalities in the heart\nsounds can be detected automatically via computer-assisted decision support\nsystems, and used as the first-line screening tool for detection of\ncardiovascular problems, or for monitoring the effects of treatments and\ninterventions. We propose in this paper CardioPHON, an integrated heart sound\nquality assessment and classification tool that can be used for screening of\nabnormal cardiac function from phonocardiogram recordings. The model is\npretrained in a self-supervised fashion on a collection of six small- and\nmid-sized heart sound datasets, enables automatic removal of low quality\nrecordings to ensure that subtle sounds of heart abnormalities are not\nmisdiagnosed, and provides a state-of-the-art performance for the heart sound\nclassification task. The multimodal model that combines audio and\nsocio-demographic features demonstrated superior performance, achieving the\nbest ranking on the official leaderboard of the 2022 George B. Moody PhysioNet\nheart sound challenge, whereas the unimodal model, that is based only on\nphonocardiogram recordings, holds the first position among the unimodal\napproaches (a total rank 4), surpassing the models utilizing multiple\nmodalities. CardioPHON is the first publicly released pretrained model in the\ndomain of heart sound recordings, facilitating the development of\ndata-efficient artificial intelligence models that can generalize to various\ndownstream tasks in cardiovascular diagnostics."}
{"id": "2511.03837", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.03837", "abs": "https://arxiv.org/abs/2511.03837", "authors": ["Saúl Fenollosa", "Narcis Cardona", "Wenfei Yang", "Jian Li"], "title": "Correlation and Temporal Consistency Analysis of Mono-static and Bi-static ISAC Channels", "comment": "6 pages, 7 figures, 2 tables. Accepted for publication at the 2025\n  IEEE Global Communications Conference (GLOBECOM), WS-26: 4th Workshop on\n  Propagation Channel Models and Evaluation Methodologies for 6G", "summary": "Integrated Sensing and Communication (ISAC) is critical for efficient\nspectrum and hardware utilization in future wireless networks like 6G. However,\nexisting channel models lack comprehensive characterization of ISAC-specific\ndynamics, particularly the relationship between mono-static (co-located Tx/Rx)\nand bi-static (separated Tx/Rx) sensing configurations. Empirical measurements\nin dynamic urban microcell (UMi) environments using a 79-GHz FMCW channel\nsounder help bridge this gap. Two key findings are demonstrated: (1)\nmono-static and bi-static channels exhibit consistently low instantaneous\ncorrelation due to divergent propagation geometries; (2) despite low\ninstantaneous correlation, both channels share unified temporal consistency,\nevolving predictably under environmental kinematics. These insights, validated\nacross seven real-world scenarios with moving targets/transceivers, inform\nrobust ISAC system design and future standardization."}
{"id": "2511.04376", "categories": ["cs.SD", "cs.AI", "cs.LG", "cs.MM", "eess.AS"], "pdf": "https://arxiv.org/pdf/2511.04376", "abs": "https://arxiv.org/abs/2511.04376", "authors": ["Ali Boudaghi", "Hadi Zare"], "title": "MusRec: Zero-Shot Text-to-Music Editing via Rectified Flow and Diffusion Transformers", "comment": null, "summary": "Music editing has emerged as an important and practical area of artificial\nintelligence, with applications ranging from video game and film music\nproduction to personalizing existing tracks according to user preferences.\nHowever, existing models face significant limitations, such as being restricted\nto editing synthesized music generated by their own models, requiring highly\nprecise prompts, or necessitating task-specific retraining, thus lacking true\nzero-shot capability. Leveraging recent advances in rectified flow and\ndiffusion transformers, we introduce MusRec, the first zero-shot text-to-music\nediting model capable of performing diverse editing tasks on real-world music\nefficiently and effectively. Experimental results demonstrate that our approach\noutperforms existing methods in preserving musical content, structural\nconsistency, and editing fidelity, establishing a strong foundation for\ncontrollable music editing in real-world scenarios."}
{"id": "2511.03923", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.03923", "abs": "https://arxiv.org/abs/2511.03923", "authors": ["Xianhua Yu", "Dong Li", "Bowen Gu", "Liuqing Yang", "Sumei Sun", "George K. Karagiannidis"], "title": "Adaptive Phase Shift Information Compression for IRS Systems: A Prompt Conditioned Variable Rate Framework", "comment": null, "summary": "Intelligent reflecting surfaces (IRSs) have become a vital technology for\nimproving the spectrum and energy efficiency of forthcoming wireless networks.\nNevertheless, practical implementation is obstructed by the excessive overhead\nassociated with the frequent transmission of phase shift information (PSI) over\nbandwidth-constrained control lines. Current deep learning-based compression\nmethods mitigate this problem but are constrained by elevated decoder\ncomplexity, inadequate flexibility to dynamic channels, and static compression\nratios. This research presents a prompt-conditioned PSI compression system that\nintegrates prompt learning inspired by large models into the PSI compression\nprocess to address these difficulties. A hybrid prompt technique that\nintegrates soft prompt concatenation with feature-wise linear modulation (FiLM)\nfacilitates adaptive encoding across diverse signal-to-noise ratios (SNRs),\nfading kinds, and compression ratios. Furthermore, a variable rate technique\nincorporates the compression ratio into the prompt embeddings through latent\nmasking, enabling a singular model to adeptly balance reconstruction accuracy.\nAdditionally, a lightweight depthwise convolutional gating (DWCG) decoder\nfacilitates precise feature reconstruction with minimal complexity.\nComprehensive simulations indicate that the proposed framework significantly\nreduces NMSE compared to traditional autoencoder baselines, while ensuring\nrobustness across various channel circumstances and accommodating variable\ncompression ratios within a single model. These findings underscore the\nframework's promise as a scalable and efficient solution for real-time IRS\ncontrol in next-generation wireless networks."}
{"id": "2511.04623", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2511.04623", "abs": "https://arxiv.org/abs/2511.04623", "authors": ["Yutong Wen", "Ke Chen", "Prem Seetharaman", "Oriol Nieto", "Jiaqi Su", "Rithesh Kumar", "Minje Kim", "Paris Smaragdis", "Zeyu Jin", "Justin Salamon"], "title": "PromptSep: Generative Audio Separation via Multimodal Prompting", "comment": "Submitted to ICASSP 2026", "summary": "Recent breakthroughs in language-queried audio source separation (LASS) have\nshown that generative models can achieve higher separation audio quality than\ntraditional masking-based approaches. However, two key limitations restrict\ntheir practical use: (1) users often require operations beyond separation, such\nas sound removal; and (2) relying solely on text prompts can be unintuitive for\nspecifying sound sources. In this paper, we propose PromptSep to extend LASS\ninto a broader framework for general-purpose sound separation. PromptSep\nleverages a conditional diffusion model enhanced with elaborated data\nsimulation to enable both audio extraction and sound removal. To move beyond\ntext-only queries, we incorporate vocal imitation as an additional and more\nintuitive conditioning modality for our model, by incorporating Sketch2Sound as\na data augmentation strategy. Both objective and subjective evaluations on\nmultiple benchmarks demonstrate that PromptSep achieves state-of-the-art\nperformance in sound removal and vocal-imitation-guided source separation,\nwhile maintaining competitive results on language-queried source separation."}
{"id": "2511.03967", "categories": ["eess.SP", "math.ST", "stat.ME", "stat.TH"], "pdf": "https://arxiv.org/pdf/2511.03967", "abs": "https://arxiv.org/abs/2511.03967", "authors": ["Wuxia Chen", "Sean Moushegian", "Vahid Tarokh", "Taposh Banerjee"], "title": "Score-Based Quickest Change Detection and Fault Identification for Multi-Stream Signals", "comment": null, "summary": "This paper introduces an approach to multi-stream quickest change detection\nand fault isolation for unnormalized and score-based statistical models.\nTraditional optimal algorithms in the quickest change detection literature\nrequire explicit pre-change and post-change distributions to calculate the\nlikelihood ratio of the observations, which can be computationally expensive\nfor higher-dimensional data and sometimes even infeasible for complex machine\nlearning models. To address these challenges, we propose the min-SCUSUM method,\na Hyvarinen score-based algorithm that computes the difference of score\nfunctions in place of log-likelihood ratios. We provide a delay and false alarm\nanalysis of the proposed algorithm, showing that its asymptotic performance\ndepends on the Fisher divergence between the pre- and post-change\ndistributions. Furthermore, we establish an upper bound on the probability of\nfault misidentification in distinguishing the affected stream from the\nunaffected ones."}
{"id": "2511.03984", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.03984", "abs": "https://arxiv.org/abs/2511.03984", "authors": ["Hanfu Zhang", "Erwu Liu"], "title": "Joint Beamforming and Position Design for Movable Antenna Assisted LEO ISAC Systems", "comment": "13 pages, 8 figures", "summary": "Low earth orbit (LEO) satellite-assisted integrated sensing and\ncommunications (ISAC) systems have been extensively studied to achieve\nubiquitous connectivity. However, the severe signal attenuation and limited\ntransmit power at LEO satellites can degrade ISAC performance. To address this\nissue, this paper investigated movable antenna (MA)-assisted LEO ISAC systems.\nWe derive the communication signal-to-interference-plus-noise ratio (SINR) and\nthe sensing squared position error bound (SPEB) for evaluating the ISAC\nperformance. Then, we jointly optimize the transmit beamforming and the MA\npositions to minimize the SPEB under the SINR constraints, total transmit power\nconstraint, and several inherent physical constraints of the MA array. We first\nsimplify the complex problem using the semidefinite relaxation (SDR). Then, we\npresent a novel alternating optimization (AO)-based algorithm to decouple the\noriginal problem into two subproblems, consequently convexified and solved.\nSimulations demonstrate the convergence and effectiveness of the proposed\nalgorithm. Better trade-off between communication and sensing performance, and\nat least 25% gain in sensing performance are achieved, compared to the\nbenchmarks."}
{"id": "2511.03998", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.03998", "abs": "https://arxiv.org/abs/2511.03998", "authors": ["Abhishek Rajasekaran", "Mehdi Karbalayghareh", "Xiaoyan Ma", "David J. Love", "Christopher G. Brinton"], "title": "Optimal RIS Placement in a Multi-User MISO System with User Randomness", "comment": "6 pages, 3 figures", "summary": "It is well established that the performance of reconfigurable intelligent\nsurface (RIS)-assisted systems critically depends on the optimal placement of\nthe RIS. Previous works consider either simple coverage maximization or\nsimultaneous optimization of the placement of the RIS along with the\nbeamforming and reflection coefficients, most of which assume that the location\nof the RIS, base station (BS), and users are known. However, in practice, only\nthe spatial variation of user density and obstacle configuration are likely to\nbe known prior to deployment of the system. Thus, we formulate a non-convex\nproblem that optimizes the position of the RIS over the expected minimum\nsignal-to-interference-plus-noise ratio (SINR) of the system with user\nrandomness, assuming that the system employs joint beamforming after\ndeployment. To solve this problem, we propose a recursive coarse-to-fine\nmethodology that constructs a set of candidate locations for RIS placement\nbased on the obstacle configuration and evaluates them over multiple\ninstantiations from the user distribution. The search is recursively refined\nwithin the optimal region identified in each stage to determine the final\noptimal region for RIS deployment. Numerical results are presented to\ncorroborate our findings."}
{"id": "2511.04011", "categories": ["eess.SP", "math.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2511.04011", "abs": "https://arxiv.org/abs/2511.04011", "authors": ["Higo T. P. Da Silva", "Hugerles S. Silva", "Felipe A. P. Figueiredo", "Andre A. Dos Anjos", "Rausley A. A. Souza"], "title": "A Survey on Noise-Based Communication", "comment": null, "summary": "The proliferation of sixth-generation (6G) networks and the massive Internet\nof Things (IoT) demand wireless communication technologies that are\nultra-low-power, secure, and covert. Noise-based communication has emerged as a\ntransformative paradigm that meets these demands by encoding information\ndirectly into the statistical properties of noise, rather than using\ntraditional deterministic carriers. This survey provides a comprehensive\nsynthesis of this field, systematically exploring its fundamental principles\nand key methodologies, including thermal noise modulation (TherMod), noise\nmodulation (NoiseMod) and its variants, and the Kirchhoff-law-Johnson-noise\n(KLJN) secure key exchange. We address critical practical challenges such as\nchannel estimation and hardware implementation, and highlight emerging\napplications in simultaneous wireless information and power transfer (SWIPT)\nand non-orthogonal multiple access (NOMA). Our analysis confirms that\nnoise-based systems offer unparalleled advantages in energy efficiency and\ncovertness, and we conclude by outlining future research directions to realize\ntheir potential for enabling the next generation of autonomous and secure\nwireless networks."}
{"id": "2511.04015", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.04015", "abs": "https://arxiv.org/abs/2511.04015", "authors": ["Haotian Zhang", "Shijian Gao", "Xiang Cheng"], "title": "Tiny-WiFo: A Lightweight Wireless Foundation Model for Channel Prediction via Multi-Component Adaptive Knowledge Distillation", "comment": "5 pages, 1 figures, 3 tables", "summary": "The massive scale of Wireless Foundation Models (FMs) hinders their real-time\ndeployment on edge devices. This letter moves beyond standard knowledge\ndistillation by introducing a novel Multi-Component Adaptive Knowledge\nDistillation (MCAKD) framework. Key innovations include a Cross-Attention-Based\nKnowledge Selection (CA-KS) module that selectively identifies critical\nfeatures from the teacher model, and an Autonomous Learning-Passive Learning\n(AL-PL) strategy that balances knowledge transfer with independent learning to\nachieve high training efficiency at a manageable computational cost. When\napplied to the WiFo FM, the distilled Tiny-WiFo model, with only 5.5M\nparameters, achieves a 1.6 ms inference time on edge hardware while retaining\nover 98% of WiFo's performance and its crucial zero-shot generalization\ncapability, making real-time FM deployment viable."}
{"id": "2511.04200", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.04200", "abs": "https://arxiv.org/abs/2511.04200", "authors": ["Yuanhan Ni", "Fan Liu", "Haoran Yin", "Yanqun Tang", "Zulin Wang"], "title": "Ambiguity Function Analysis of AFDM Under Pulse-Shaped Random ISAC Signaling", "comment": null, "summary": "This paper investigates the ambiguity function (AF) of the emerging affine\nfrequency division multiplexing (AFDM) waveform for Integrated Sensing and\nCommunication (ISAC) signaling under a pulse shaping regime. Specifically, we\nfirst derive the closed-form expression of the average squared discrete period\nAF (DPAF) for AFDM waveform without pulse shaping, revealing that the AF\ndepends on the parameter $c_1$ and the kurtosis of random communication data,\nwhile being independent of the parameter $c_2$. As a step further, we conduct a\ncomprehensive analysis on the AFs of various waveforms, including AFDM,\northogonal frequency division multiplexing (OFDM) and orthogonal chirp-division\nmultiplexing (OCDM). Our results indicate that all three waveforms exhibit the\nsame number of regular depressions in the sidelobes of their AFs, which incurs\nperformance loss for detecting and estimating weak targets. However, the AFDM\nwaveform can flexibly control the positions of depressions by adjusting the\nparameter $c_1$, which motivates a novel design approach of the AFDM parameters\nto mitigate the adverse impact of depressions of the strong target on the weak\ntarget. Furthermore, a closed-form expression of the average squared DPAF for\npulse-shaped random AFDM waveform is derived, which demonstrates that the pulse\nshaping filter generates the shaped mainlobe along the delay axis and the rapid\nroll-off sidelobes along the Doppler axis. Numerical results verify the\neffectiveness of our theoretical analysis and proposed design methodology for\nthe AFDM modulation."}
{"id": "2511.04292", "categories": ["eess.SP", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2511.04292", "abs": "https://arxiv.org/abs/2511.04292", "authors": ["Arne Van Den Kerchove", "Hakim Si-Mohammed", "François Cabestaing", "Marc M. Van Hulle"], "title": "BTTDA: Block-Term Tensor Discriminant Analysis for Brain-Computer Interfacing", "comment": "This archive contains 26 pages, 7 figures, 2 tables, 3 appendices and\n  3 ancillary files (erp_results.csv, mi_results.csv, block_theta_results.csv).\n  Source code is available at https://github.com/arnevdk/bttda", "summary": "Brain-computer interfaces (BCIs) allow direct communication between the brain\nand external devices, frequently using electroencephalography (EEG) to record\nneural activity. Dimensionality reduction and structured regularization are\nessential for effectively classifying task-related brain signals, including\nevent-related potentials (ERPs) and motor imagery (MI) rhythms. Current\ntensor-based approaches, such as Tucker and PARAFAC decompositions, often lack\nthe flexibility needed to fully capture the complexity of EEG data. This study\nintroduces Block-Term Tensor Discriminant Analysis (BTTDA): a novel\ntensor-based and supervised feature extraction method designed to enhance\nclassification accuracy by providing flexible multilinear dimensionality\nreduction. Extending Higher Order Discriminant Analysis (HODA), BTTDA uses a\nnovel and interpretable forward model for HODA combined with a deflation scheme\nto iteratively extract discriminant block terms, improving feature\nrepresentation for classification. BTTDA and a sum-of-rank-1-terms variant\nPARAFACDA were evaluated on publicly available ERP (second-order tensors) and\nMI (third-order tensors) EEG datasets from the MOABB benchmarking framework.\nBenchmarking revealed that BTTDA and PARAFACDA significantly outperform the\ntraditional HODA method in ERP decoding, resulting in state-of-the art\nperformance (ROC-AUC = 91.25%). For MI, decoding results of HODA, BTTDA and\nPARAFACDA were subpar, but BTTDA still significantly outperformed HODA (64.52%\n> 61.00%). The block-term structure of BTTDA enables interpretable and more\nefficient dimensionality reduction without compromising discriminative power.\nThis offers a promising and adaptable approach for feature extraction in BCI\nand broader neuroimaging applications."}
{"id": "2511.04351", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.04351", "abs": "https://arxiv.org/abs/2511.04351", "authors": ["Hasan Akgul", "Mari Eplik", "Javier Rojas", "Akira Yamamoto", "Rajesh Kumar", "Maya Singh"], "title": "RCMCL: A Unified Contrastive Learning Framework for Robust Multi-Modal (RGB-D, Skeleton, Point Cloud) Action Understanding", "comment": "11 pages, 6 figures,", "summary": "Human action recognition (HAR) with multi-modal inputs (RGB-D, skeleton,\npoint cloud) can achieve high accuracy but typically relies on large labeled\ndatasets and degrades sharply when sensors fail or are noisy. We present Robust\nCross-Modal Contrastive Learning (RCMCL), a self-supervised framework that\nlearns modality-invariant representations and remains reliable under modality\ndropout and corruption. RCMCL jointly optimizes (i) a cross-modal contrastive\nobjective that aligns heterogeneous streams, (ii) an intra-modal\nself-distillation objective that improves view-invariance and reduces\nredundancy, and (iii) a degradation simulation objective that explicitly trains\nmodels to recover from masked or corrupted inputs. At inference, an Adaptive\nModality Gating (AMG) network assigns data-driven reliability weights to each\nmodality for robust fusion. On NTU RGB+D 120 (CS/CV) and UWA3D-II, RCMCL\nattains state-of-the-art accuracy in standard settings and exhibits markedly\nbetter robustness: under severe dual-modality dropout it shows only an 11.5%\ndegradation, significantly outperforming strong supervised fusion baselines.\nThese results indicate that self-supervised cross-modal alignment, coupled with\nexplicit degradation modeling and adaptive fusion, is key to deployable\nmulti-modal HAR."}
{"id": "2511.04362", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.04362", "abs": "https://arxiv.org/abs/2511.04362", "authors": ["Chiara Telli", "Oleg Antropov", "Anne Lönnqvist", "Marco Lavalle"], "title": "High-Resolution Forest Mapping from L-Band Interferometric SAR Time Series using Deep Learning over Northern Spain", "comment": null, "summary": "In this study, we examine the potential of high-resolution forest mapping\nusing L-band interferometric time series datasets and deep learning modeling.\nOur SAR data are represented by a time series of nine ALOS-2 PALSAR-2 dual-pol\nSAR images acquired at near-zero spatial baseline over a study site in\nAsturias, Northern Spain. Reference data are collected using airborne laser\nscanning. We examine the performance of several candidate deep learning models\nfrom UNet-family with various combinations of input polarimetric and\ninterferometric features. In addition to basic Vanilla UNet, attention\nreinforced UNet model with squeeze-excitation blocks (SeU-Net) and advanced\nUNet model with nested structure and skip pathways are used. Studied features\ninclude dual pol interferometric observables additionally incorporating\nmodel-based derived measures. Results show that adding model-based inverted\nInSAR features or InSAR coherence layers improves retrieval accuracy compared\nto using backscatter intensity only. Use of attention mechanisms and nested\nconnection fusion provides better predictions than using Vanilla UNet or\ntraditional machine learning methods. Forest height retrieval accuracies range\nbetween 3.1-3.8 m (R2 = 0.45--0.55) at 20 m resolution when only intensity data\nare used, and improve to less than 2.8 m when both intensity and\ninterferometric coherence features are included. At 40 m and 60 m resolution,\nretrieval performance further improves, primarily due to higher SNR in both the\nintensity and interferometric layers. When using intensity at 60 m resolution,\nbest achieved RMSE is 2.2 m, while when using all suitable input features the\nachieved error is 1.95 m. We recommend this hybrid approach for L-band SAR\nretrievals also suitable for NISAR and future ROSE-L missions."}
{"id": "2511.04448", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.04448", "abs": "https://arxiv.org/abs/2511.04448", "authors": ["Chu Li", "Kevin Weinberger", "Aydin Sezgin"], "title": "A Lightweight Framework for Integrated Sensing and Communications with RIS", "comment": null, "summary": "Reconfigurable Intelligent Surfaces (RIS) have been recognized as a promising\ntechnology to enhance both communication and sensing performance in integrated\nsensing and communication (ISAC) systems for future 6G networks. However,\nexisting RIS optimization methods for improving ISAC performance are mainly\nbased on semidefinite relaxation (SDR) or iterative algorithms. The former\nsuffers from high computational complexity and limited scalability, especially\nwhen the number of RIS elements becomes large, while the latter yields\nsuboptimal solutions whose performance depends on initialization. In this work,\nwe introduce a lightweight RIS phase design framework that provides a\nclosed-form solution and explicitly accounts for the trade-off between\ncommunication and sensing, as well as proportional beam gain distribution\ntoward multiple sensing targets. The key idea is to partition the RIS\nconfiguration into two parts: the first part is designed to maximize the\ncommunication performance, while the second introduces small perturbations to\ngenerate multiple beams for multi-target sensing. Simulation results validate\nthe effectiveness of the proposed approach and demonstrate that it achieves\nperformance comparable to SDR but with significantly lower computational\ncomplexity."}
{"id": "2511.04635", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.04635", "abs": "https://arxiv.org/abs/2511.04635", "authors": ["Qingbin Li", "Jian Pang"], "title": "An Area-Efficient 20-100-GHz Phase-Invariant Switch-Type Attenuator Achieving 0.1-dB Tuning Step in 65-nm CMOS", "comment": "Accepted paper at IEEE UCMMT 2025. 3 pages, 7 figures. Uploaded as\n  preprint for open access", "summary": "This paper presents a switch-type attenuator working from 20 to 100 GHz. The\nattenuator adopts a capacitive compensation technique to reduce phase error.\nThe small resistors in this work are implemented with metal lines to reduce the\nintrinsic parasitic capacitance, which helps minimize the amplitude and phase\nerrors over a wide frequency range. Moreover, the utilization of metal lines\nalso reduces the chip area. In addition, a continuous tuning attenuation unit\nis employed to improve the overall attenuation accuracy of the attenuator. The\npassive attenuator is designed and fabricated in a standard 65nm CMOS. The\nmeasurement results reveal a relative attenuation range of 7.5 dB with a\ncontinuous tuning step within 20-100 GHz. The insertion loss is 1.6-3.8 dB\nwithin the operation band, while the return losses of all states are better\nthan 11.5 dB. The RMS amplitude and phase errors are below 0.15 dB and\n1.6{\\deg}, respectively."}

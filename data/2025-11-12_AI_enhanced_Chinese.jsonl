{"id": "2511.07610", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.07610", "abs": "https://arxiv.org/abs/2511.07610", "authors": ["Sabarinath Ramachandran", "Venkatesh Khammammetti", "Prasanthi Maddala", "Narayan Mandayam", "Ivan Seskar", "Robert Calderbank"], "title": "Over-the-Air Transmission of Zak-OTFS on mmWave Communications Testbed", "comment": null, "summary": "Millimeter-wave (mmWave) communication offers vast bandwidth for next-generation wireless systems but faces severe path loss, Doppler effects, and hardware impairments. Orthogonal Time Frequency Space (OTFS) modulation has emerged as a robust waveform for high-mobility and doubly dispersive channels, outperforming OFDM under strong Doppler. However, the most studied multicarrier OTFS (MC-OTFS) is not easily predictable because the input-output (I$/$O) relation is not given by (twisted) convolution. Recently, the Zak-transform based OTFS (Zak-OTFS or OTFS 2$.$0) was proposed, which provides a single domain delay Doppler (DD) processing framework with predictable I$/$O behavior. This paper presents one of the first over-the-air (OTA) demonstrations of Zak-OTFS at mmWave frequencies. We design a complete Zak-OTFS based mmWave OTA system featuring root-raised-cosine (RRC) filtering for enhanced DD-domain predictability, higher-order modulations up to 16-QAM, and a low-overhead preamble for synchronization. A comprehensive signal model incorporating carrier frequency offset (CFO) and timing impairments is developed, showing these effects can be jointly captured within the effective DD-domain channel. Experimental validation on the COSMOS testbed confirms the feasibility and robustness of Zak-OTFS under realistic mmWave conditions, highlighting its potential for efficient implementations in beyond-5G and 6G systems.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u5728\u6beb\u7c73\u6ce2\u9891\u6bb5\u5b9e\u73b0\u4e86Zak-OTFS\uff08OTFS 2.0\uff09\u7684\u7a7a\u4e2d\u6f14\u793a\uff0c\u5c55\u793a\u4e86\u5176\u5728\u771f\u5b9e\u6beb\u7c73\u6ce2\u6761\u4ef6\u4e0b\u7684\u53ef\u884c\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u6beb\u7c73\u6ce2\u901a\u4fe1\u9762\u4e34\u4e25\u91cd\u7684\u8def\u5f84\u635f\u8017\u3001\u591a\u666e\u52d2\u6548\u5e94\u548c\u786c\u4ef6\u635f\u4f24\u95ee\u9898\u3002\u867d\u7136OTFS\u8c03\u5236\u5728\u9ad8\u79fb\u52a8\u6027\u548c\u53cc\u8272\u6563\u4fe1\u9053\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u6700\u5e38\u7814\u7a76\u7684\u591a\u8f7d\u6ce2OTFS\uff08MC-OTFS\uff09\u7684\u8f93\u5165\u8f93\u51fa\u5173\u7cfb\u4e0d\u53ef\u9884\u6d4b\u3002Zak-OTFS\u63d0\u4f9b\u4e86\u5177\u6709\u53ef\u9884\u6d4bI/O\u884c\u4e3a\u7684\u5355\u57df\u5ef6\u8fdf\u591a\u666e\u52d2\u5904\u7406\u6846\u67b6\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u5b8c\u6574\u7684\u57fa\u4e8eZak-OTFS\u7684\u6beb\u7c73\u6ce2OTA\u7cfb\u7edf\uff0c\u5305\u62ec\u7528\u4e8e\u589e\u5f3aDD\u57df\u53ef\u9884\u6d4b\u6027\u7684\u6839\u5347\u4f59\u5f26\u6ee4\u6ce2\u3001\u9ad8\u8fbe16-QAM\u7684\u9ad8\u9636\u8c03\u5236\u4ee5\u53ca\u7528\u4e8e\u540c\u6b65\u7684\u4f4e\u5f00\u9500\u524d\u5bfc\u7801\u3002\u5f00\u53d1\u4e86\u5305\u542b\u8f7d\u6ce2\u9891\u7387\u504f\u79fb\u548c\u65f6\u5e8f\u635f\u4f24\u7684\u7efc\u5408\u4fe1\u53f7\u6a21\u578b\u3002", "result": "\u5728COSMOS\u6d4b\u8bd5\u5e73\u53f0\u4e0a\u8fdb\u884c\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u8bc1\u5b9e\u4e86Zak-OTFS\u5728\u771f\u5b9e\u6beb\u7c73\u6ce2\u6761\u4ef6\u4e0b\u7684\u53ef\u884c\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "Zak-OTFS\u5728\u6beb\u7c73\u6ce2\u9891\u6bb5\u5177\u6709\u9ad8\u6548\u5b9e\u73b0\u7684\u6f5c\u529b\uff0c\u9002\u7528\u4e8e5G\u540e\u548c6G\u7cfb\u7edf\u3002"}}
{"id": "2511.07683", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.07683", "abs": "https://arxiv.org/abs/2511.07683", "authors": ["Marko Fidanovski", "Iv\u00e1n Alexander Morales Sandoval", "Kuranage Roche Rayan Ranasinghe", "Giuseppe Thadeu Freitas de Abreu", "Emil Bj\u00f6rnson", "Bruno Clerckx"], "title": "Fractional Programming and Manifold Optimization for Reciprocal BD-RIS Scattering Matrix Design", "comment": null, "summary": "We investigate the problem of maximizing the sum-rate performance of a beyond-diagonal reconfigurable intelligent surface (BD-RIS)-aided multi-user (MU)-multiple-input single-output (MISO) system using fractional programming (FP) techniques. More specifically, we leverage the Lagrangian Dual Transform (LDT) and Quadratic Transform (QT) to derive an equivalent objective function which is then solved iteratively via a manifold optimization framework. It is shown that these techniques reduce the complexity of the optimization problem for the scattering matrix solution, while also providing notable performance gains compared to state-of-the-art (SotA) methods under the same system conditions. Simulation results confirm the effectiveness of the proposed method in improving sum-rate performance.", "AI": {"tldr": "\u4f7f\u7528\u5206\u6570\u89c4\u5212\u6280\u672f\u4f18\u5316BD-RIS\u8f85\u52a9MU-MISO\u7cfb\u7edf\u7684\u548c\u901f\u7387\u6027\u80fd\uff0c\u901a\u8fc7LDT\u548cQT\u65b9\u6cd5\u964d\u4f4e\u4f18\u5316\u590d\u6742\u5ea6\u5e76\u63d0\u5347\u6027\u80fd", "motivation": "\u89e3\u51b3BD-RIS\u8f85\u52a9\u591a\u7528\u6237MISO\u7cfb\u7edf\u7684\u548c\u901f\u7387\u6700\u5927\u5316\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u590d\u6742\u5ea6\u9ad8\u4e14\u6027\u80fd\u6709\u9650", "method": "\u7ed3\u5408\u62c9\u683c\u6717\u65e5\u5bf9\u5076\u53d8\u6362(LDT)\u548c\u4e8c\u6b21\u53d8\u6362(QT)\u63a8\u5bfc\u7b49\u4ef7\u76ee\u6807\u51fd\u6570\uff0c\u901a\u8fc7\u6d41\u5f62\u4f18\u5316\u6846\u67b6\u8fed\u4ee3\u6c42\u89e3", "result": "\u76f8\u6bd4\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\uff0c\u5728\u76f8\u540c\u7cfb\u7edf\u6761\u4ef6\u4e0b\u663e\u8457\u964d\u4f4e\u590d\u6742\u5ea6\u5e76\u63d0\u4f9b\u6027\u80fd\u589e\u76ca", "conclusion": "\u6240\u63d0\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u5347BD-RIS\u8f85\u52a9MU-MISO\u7cfb\u7edf\u7684\u548c\u901f\u7387\u6027\u80fd"}}
{"id": "2511.07777", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.07777", "abs": "https://arxiv.org/abs/2511.07777", "authors": ["Zhenghao Zhou", "Yiyan Li", "Xinjie Yu", "Runlong Liu", "Zelin Guo", "Zheng Yan", "Mo-Yuen Chow", "Yuqi Yang", "Yang Xu"], "title": "A Causal-Guided Multimodal Large Language Model for Generalized Power System Time-Series Data Analytics", "comment": null, "summary": "Power system time series analytics is critical in understanding the system operation conditions and predicting the future trends. Despite the wide adoption of Artificial Intelligence (AI) tools, many AI-based time series analytical models suffer from task-specificity (i.e. one model for one task) and structural rigidity (i.e. the input-output format is fixed), leading to limited model performances and resource wastes. In this paper, we propose a Causal-Guided Multimodal Large Language Model (CM-LLM) that can solve heterogeneous power system time-series analysis tasks. First, we introduce a physics-statistics combined causal discovery mechanism to capture the causal relationship, which is represented by graph, among power system variables. Second, we propose a multimodal data preprocessing framework that can encode and fuse text, graph and time series to enhance the model performance. Last, we formulate a generic \"mask-and-reconstruct\" paradigm and design a dynamic input-output padding mechanism to enable CM-LLM adaptive to heterogeneous time-series analysis tasks with varying sample lengths. Simulation results based on open-source LLM Qwen and real-world dataset demonstrate that, after simple fine-tuning, the proposed CM-LLM can achieve satisfying accuracy and efficiency on three heterogeneous time-series analytics tasks: missing data imputation, forecasting and super resolution.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u56e0\u679c\u5f15\u5bfc\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b(CM-LLM)\uff0c\u7528\u4e8e\u89e3\u51b3\u7535\u529b\u7cfb\u7edf\u5f02\u6784\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u4efb\u52a1\uff0c\u901a\u8fc7\u56e0\u679c\u53d1\u73b0\u3001\u591a\u6a21\u6001\u6570\u636e\u878d\u5408\u548c\u52a8\u6001\u8f93\u5165\u8f93\u51fa\u673a\u5236\uff0c\u5728\u7f3a\u5931\u6570\u636e\u586b\u8865\u3001\u9884\u6d4b\u548c\u8d85\u5206\u8fa8\u7387\u7b49\u4efb\u52a1\u4e0a\u53d6\u5f97\u826f\u597d\u6548\u679c\u3002", "motivation": "\u73b0\u6709AI\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u6a21\u578b\u5b58\u5728\u4efb\u52a1\u7279\u5b9a\u6027\u548c\u7ed3\u6784\u521a\u6027\u95ee\u9898\uff0c\u5bfc\u81f4\u6027\u80fd\u6709\u9650\u548c\u8d44\u6e90\u6d6a\u8d39\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u5904\u7406\u5f02\u6784\u4efb\u52a1\u7684\u901a\u7528\u6a21\u578b\u3002", "method": "1) \u7269\u7406-\u7edf\u8ba1\u7ed3\u5408\u7684\u56e0\u679c\u53d1\u73b0\u673a\u5236\u83b7\u53d6\u53d8\u91cf\u95f4\u56e0\u679c\u5173\u7cfb\u56fe\uff1b2) \u591a\u6a21\u6001\u6570\u636e\u9884\u5904\u7406\u6846\u67b6\u878d\u5408\u6587\u672c\u3001\u56fe\u548c\u65f6\u95f4\u5e8f\u5217\uff1b3) \u901a\u7528\"\u63a9\u7801-\u91cd\u6784\"\u8303\u5f0f\u548c\u52a8\u6001\u8f93\u5165\u8f93\u51fa\u586b\u5145\u673a\u5236\u3002", "result": "\u57fa\u4e8e\u5f00\u6e90LLM Qwen\u548c\u771f\u5b9e\u6570\u636e\u96c6\u7684\u4eff\u771f\u8868\u660e\uff0c\u7ecf\u8fc7\u7b80\u5355\u5fae\u8c03\u540e\uff0cCM-LLM\u5728\u7f3a\u5931\u6570\u636e\u586b\u8865\u3001\u9884\u6d4b\u548c\u8d85\u5206\u8fa8\u7387\u4e09\u4e2a\u5f02\u6784\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u4efb\u52a1\u4e0a\u90fd\u80fd\u8fbe\u5230\u6ee1\u610f\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "conclusion": "CM-LLM\u901a\u8fc7\u56e0\u679c\u5f15\u5bfc\u548c\u591a\u6a21\u6001\u878d\u5408\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u7535\u529b\u7cfb\u7edf\u5f02\u6784\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u4efb\u52a1\uff0c\u4e3a\u901a\u7528\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u6a21\u578b\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.07783", "categories": ["eess.SP", "cs.IT"], "pdf": "https://arxiv.org/pdf/2511.07783", "abs": "https://arxiv.org/abs/2511.07783", "authors": ["Hao Luo", "Saeed R. Khosravirad", "Ahmed Alkhateeb"], "title": "Generative Decoding of Compressed CSI for MIMO Precoding Design", "comment": "6 pages, 4 figures", "summary": "Massive MIMO systems can enhance spectral and energy efficiency, but they require accurate channel state information (CSI), which becomes costly as the number of antennas increases. While machine learning (ML) autoencoders show promise for CSI reconstruction and reducing feedback overhead, they introduce new challenges with standardization, interoperability, and backward compatibility. Also, the significant data collection needed for training makes real-world deployment difficult. To overcome these drawbacks, we propose an ML-based, decoder-only solution for compressed CSI. Our approach uses a standardized encoder for CSI compression on the user side and a site-specific generative decoder at the base station to refine the compressed CSI using environmental knowledge. We introduce two training schemes for the generative decoder: An end-to-end method and a two-stage method, both utilizing a goal-oriented loss function. Furthermore, we reduce the data collection overhead by using a site-specific digital twin to generate synthetic CSI data for training. Our simulations highlight the effectiveness of this solution across various feedback overhead regimes.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u89e3\u7801\u5668\u4e13\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u7528\u4e8e\u538b\u7f29CSI\uff0c\u901a\u8fc7\u6807\u51c6\u5316\u7f16\u7801\u5668\u548c\u7ad9\u70b9\u7279\u5b9a\u751f\u6210\u89e3\u7801\u5668\u6765\u51cf\u5c11\u53cd\u9988\u5f00\u9500\uff0c\u5e76\u5229\u7528\u6570\u5b57\u5b6a\u751f\u751f\u6210\u5408\u6210\u6570\u636e\u964d\u4f4e\u8bad\u7ec3\u6570\u636e\u6536\u96c6\u9700\u6c42\u3002", "motivation": "\u5927\u89c4\u6a21MIMO\u7cfb\u7edf\u9700\u8981\u51c6\u786e\u7684CSI\uff0c\u4f46\u968f\u7740\u5929\u7ebf\u6570\u91cf\u589e\u52a0\uff0c\u83b7\u53d6CSI\u6210\u672c\u9ad8\u6602\u3002\u73b0\u6709\u7684ML\u81ea\u52a8\u7f16\u7801\u5668\u867d\u7136\u80fd\u91cd\u5efaCSI\u5e76\u51cf\u5c11\u53cd\u9988\u5f00\u9500\uff0c\u4f46\u5b58\u5728\u6807\u51c6\u5316\u3001\u4e92\u64cd\u4f5c\u6027\u548c\u5411\u540e\u517c\u5bb9\u6027\u95ee\u9898\uff0c\u4e14\u8bad\u7ec3\u6570\u636e\u6536\u96c6\u56f0\u96be\u3002", "method": "\u4f7f\u7528\u6807\u51c6\u5316\u7f16\u7801\u5668\u5728\u7528\u6237\u7aef\u538b\u7f29CSI\uff0c\u5728\u57fa\u7ad9\u7aef\u4f7f\u7528\u7ad9\u70b9\u7279\u5b9a\u751f\u6210\u89e3\u7801\u5668\uff0c\u5229\u7528\u73af\u5883\u77e5\u8bc6\u7cbe\u70bc\u538b\u7f29\u7684CSI\u3002\u63d0\u51fa\u4e86\u4e24\u79cd\u8bad\u7ec3\u65b9\u6848\uff1a\u7aef\u5230\u7aef\u65b9\u6cd5\u548c\u4e24\u9636\u6bb5\u65b9\u6cd5\uff0c\u5747\u91c7\u7528\u76ee\u6807\u5bfc\u5411\u635f\u5931\u51fd\u6570\uff0c\u5e76\u901a\u8fc7\u7ad9\u70b9\u7279\u5b9a\u6570\u5b57\u5b6a\u751f\u751f\u6210\u5408\u6210CSI\u6570\u636e\u7528\u4e8e\u8bad\u7ec3\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u89e3\u51b3\u65b9\u6848\u5728\u5404\u79cd\u53cd\u9988\u5f00\u9500\u673a\u5236\u4e0b\u90fd\u8868\u73b0\u51fa\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684ML\u89e3\u7801\u5668\u4e13\u7528\u89e3\u51b3\u65b9\u6848\u80fd\u591f\u6709\u6548\u89e3\u51b3CSI\u538b\u7f29\u548c\u53cd\u9988\u5f00\u9500\u95ee\u9898\uff0c\u540c\u65f6\u901a\u8fc7\u6570\u5b57\u5b6a\u751f\u6280\u672f\u964d\u4f4e\u4e86\u6570\u636e\u6536\u96c6\u9700\u6c42\uff0c\u4e3a\u5927\u89c4\u6a21MIMO\u7cfb\u7edf\u7684\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2511.08040", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2511.08040", "abs": "https://arxiv.org/abs/2511.08040", "authors": ["Eloi Moliner", "Marco A. Mart\u00ednez-Ram\u00edrez", "Junghyun Koo", "Wei-Hsiang Liao", "Kin Wai Cheuk", "Joan Serr\u00e0", "Vesa V\u00e4lim\u00e4ki", "Yuki Mitsufuji"], "title": "Automatic Music Mixing using a Generative Model of Effect Embeddings", "comment": "submitted to IEEE ICASSP 2026", "summary": "Music mixing involves combining individual tracks into a cohesive mixture, a task characterized by subjectivity where multiple valid solutions exist for the same input. Existing automatic mixing systems treat this task as a deterministic regression problem, thus ignoring this multiplicity of solutions. Here we introduce MEGAMI (Multitrack Embedding Generative Auto MIxing), a generative framework that models the conditional distribution of professional mixes given unprocessed tracks. MEGAMI uses a track-agnostic effects processor conditioned on per-track generated embeddings, handles arbitrary unlabeled tracks through a permutation-equivariant architecture, and enables training on both dry and wet recordings via domain adaptation. Our objective evaluation using distributional metrics shows consistent improvements over existing methods, while listening tests indicate performances approaching human-level quality across diverse musical genres.", "AI": {"tldr": "MEGAMI\u662f\u4e00\u4e2a\u751f\u6210\u5f0f\u81ea\u52a8\u97f3\u4e50\u6df7\u97f3\u6846\u67b6\uff0c\u901a\u8fc7\u5efa\u6a21\u4e13\u4e1a\u6df7\u97f3\u7684\u6761\u4ef6\u5206\u5e03\u6765\u5904\u7406\u6df7\u97f3\u4efb\u52a1\u7684\u591a\u89e3\u6027\uff0c\u76f8\u6bd4\u73b0\u6709\u786e\u5b9a\u6027\u65b9\u6cd5\u6709\u663e\u8457\u6539\u8fdb\u3002", "motivation": "\u73b0\u6709\u81ea\u52a8\u6df7\u97f3\u7cfb\u7edf\u5c06\u6df7\u97f3\u4efb\u52a1\u89c6\u4e3a\u786e\u5b9a\u6027\u56de\u5f52\u95ee\u9898\uff0c\u5ffd\u7565\u4e86\u6df7\u97f3\u7684\u591a\u89e3\u6027\u7279\u70b9\uff0c\u5373\u540c\u4e00\u8f93\u5165\u53ef\u4ee5\u6709\u591a\u4e2a\u6709\u6548\u6df7\u97f3\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u8f68\u9053\u5d4c\u5165\u7684\u6761\u4ef6\u751f\u6210\u6846\u67b6\uff0c\u91c7\u7528\u8f68\u9053\u65e0\u5173\u7684\u6548\u679c\u5904\u7406\u5668\u548c\u6392\u5217\u7b49\u53d8\u67b6\u6784\u5904\u7406\u4efb\u610f\u672a\u6807\u8bb0\u8f68\u9053\uff0c\u901a\u8fc7\u9886\u57df\u9002\u5e94\u6280\u672f\u540c\u65f6\u652f\u6301\u5e72\u6e7f\u5f55\u97f3\u8bad\u7ec3\u3002", "result": "\u5ba2\u89c2\u8bc4\u4f30\u663e\u793a\u5728\u5206\u5e03\u5ea6\u91cf\u4e0a\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u542c\u97f3\u6d4b\u8bd5\u8868\u660e\u5728\u591a\u79cd\u97f3\u4e50\u6d41\u6d3e\u4e0a\u63a5\u8fd1\u4eba\u7c7b\u6c34\u5e73\u7684\u6df7\u97f3\u8d28\u91cf\u3002", "conclusion": "MEGAMI\u6210\u529f\u5efa\u6a21\u4e86\u6df7\u97f3\u4efb\u52a1\u7684\u591a\u89e3\u6027\uff0c\u4e3a\u81ea\u52a8\u97f3\u4e50\u6df7\u97f3\u63d0\u4f9b\u4e86\u66f4\u63a5\u8fd1\u4eba\u7c7b\u4e13\u4e1a\u6c34\u5e73\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.07493", "categories": ["cs.SD", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.07493", "abs": "https://arxiv.org/abs/2511.07493", "authors": ["Euihyeok Lee", "Seonghyeon Kim", "SangHun Im", "Heung-Seon Oh", "Seungwoo Kang"], "title": "Enabling Automatic Self-Talk Detection via Earables", "comment": null, "summary": "Self-talk-an internal dialogue that can occur silently or be spoken aloud-plays a crucial role in emotional regulation, cognitive processing, and motivation, yet has remained largely invisible and unmeasurable in everyday life. In this paper, we present MutterMeter, a mobile system that automatically detects vocalized self-talk from audio captured by earable microphones in real-world settings. Detecting self-talk is technically challenging due to its diverse acoustic forms, semantic and grammatical incompleteness, and irregular occurrence patterns, which differ fundamentally from assumptions underlying conventional speech understanding models. To address these challenges, MutterMeter employs a hierarchical classification architecture that progressively integrates acoustic, linguistic, and contextual information through a sequential processing pipeline, adaptively balancing accuracy and computational efficiency. We build and evaluate MutterMeter using a first-of-its-kind dataset comprising 31.1 hours of audio collected from 25 participants. Experimental results demonstrate that MutterMeter achieves robust performance with a macro-averaged F1 score of 0.84, outperforming conventional approaches, including LLM-based and speech emotion recognition models.", "AI": {"tldr": "MutterMeter\u662f\u4e00\u4e2a\u79fb\u52a8\u7cfb\u7edf\uff0c\u901a\u8fc7\u8033\u6234\u5f0f\u9ea6\u514b\u98ce\u5728\u73b0\u5b9e\u73af\u5883\u4e2d\u81ea\u52a8\u68c0\u6d4b\u53d1\u58f0\u7684\u81ea\u8a00\u81ea\u8bed\uff0c\u89e3\u51b3\u4e86\u81ea\u8a00\u81ea\u8bed\u68c0\u6d4b\u7684\u6280\u672f\u6311\u6218\uff0c\u5e76\u572831.1\u5c0f\u65f6\u97f3\u9891\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e860.84\u7684\u5b8f\u5e73\u5747F1\u5206\u6570\u3002", "motivation": "\u81ea\u8a00\u81ea\u8bed\u5728\u60c5\u7eea\u8c03\u8282\u3001\u8ba4\u77e5\u5904\u7406\u548c\u52a8\u673a\u4e2d\u8d77\u7740\u5173\u952e\u4f5c\u7528\uff0c\u4f46\u5728\u65e5\u5e38\u751f\u6d3b\u4e2d\u4e00\u76f4\u96be\u4ee5\u6d4b\u91cf\u548c\u89c2\u5bdf\u3002", "method": "\u91c7\u7528\u5206\u5c42\u5206\u7c7b\u67b6\u6784\uff0c\u901a\u8fc7\u987a\u5e8f\u5904\u7406\u7ba1\u9053\u9010\u6b65\u6574\u5408\u58f0\u5b66\u3001\u8bed\u8a00\u548c\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u81ea\u9002\u5e94\u5e73\u8861\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\u3002", "result": "\u572825\u540d\u53c2\u4e0e\u8005\u768431.1\u5c0f\u65f6\u97f3\u9891\u6570\u636e\u96c6\u4e0a\uff0cMutterMeter\u5b9e\u73b0\u4e860.84\u7684\u5b8f\u5e73\u5747F1\u5206\u6570\uff0c\u4f18\u4e8e\u4f20\u7edf\u7684\u57fa\u4e8eLLM\u548c\u8bed\u97f3\u60c5\u611f\u8bc6\u522b\u7684\u65b9\u6cd5\u3002", "conclusion": "MutterMeter\u4e3a\u7814\u7a76\u65e5\u5e38\u751f\u6d3b\u4e2d\u4e0d\u53ef\u89c1\u7684\u81ea\u8a00\u81ea\u8bed\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u81ea\u52a8\u68c0\u6d4b\u5de5\u5177\uff0c\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u9c81\u68d2\u6027\u80fd\u3002"}}
{"id": "2511.07802", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.07802", "abs": "https://arxiv.org/abs/2511.07802", "authors": ["Zhenghao Zhou", "Yiyan Li", "Xinjie Yu", "Jian Ping", "Xiaoyuan Xu", "Zheng Yan", "Mohammad Shahidehpour"], "title": "Deep-Learning-based Frequency-Domain Watermarking for Energy System Time Series Data Asset Protection", "comment": null, "summary": "Data has been regarded as a valuable asset with the fast development of artificial intelligence technologies. In this paper, we introduce deep-learning neural network-based frequency-domain watermarking for protecting energy system time series data assets and secure data authenticity when being shared or traded across communities. First, the concept and desired watermarking characteristics are introduced. Second, a deep-learning neural network-based watermarking model with specially designed loss functions and network structure is proposed to embed watermarks into the original dataset. Third, a frequency-domain data preprocessing method is proposed to eliminate the frequency bias of neural networks when learning time series datasets to enhance the model performances. Last, a comprehensive watermarking performance evaluation framework is designed for measuring its invisibility, restorability, robustness, secrecy, false-positive detection, generalization, and capacity. Case studies based on practical load and photovoltaic time series datasets demonstrate the effectiveness of the proposed method.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u9891\u57df\u6c34\u5370\u65b9\u6cd5\uff0c\u7528\u4e8e\u4fdd\u62a4\u80fd\u6e90\u7cfb\u7edf\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u8d44\u4ea7\uff0c\u786e\u4fdd\u6570\u636e\u5728\u8de8\u793e\u533a\u5171\u4eab\u6216\u4ea4\u6613\u65f6\u7684\u771f\u5b9e\u6027\u3002", "motivation": "\u968f\u7740\u4eba\u5de5\u667a\u80fd\u6280\u672f\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u6570\u636e\u5df2\u6210\u4e3a\u5b9d\u8d35\u8d44\u4ea7\u3002\u9700\u8981\u4fdd\u62a4\u80fd\u6e90\u7cfb\u7edf\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u5728\u5171\u4eab\u6216\u4ea4\u6613\u8fc7\u7a0b\u4e2d\u7684\u771f\u5b9e\u6027\u548c\u5b89\u5168\u6027\u3002", "method": "1) \u63d0\u51fa\u5177\u6709\u7279\u6b8a\u8bbe\u8ba1\u635f\u5931\u51fd\u6570\u548c\u7f51\u7edc\u7ed3\u6784\u7684\u6df1\u5ea6\u5b66\u4e60\u795e\u7ecf\u7f51\u7edc\u6c34\u5370\u6a21\u578b\uff1b2) \u63d0\u51fa\u9891\u57df\u6570\u636e\u9884\u5904\u7406\u65b9\u6cd5\u6d88\u9664\u795e\u7ecf\u7f51\u7edc\u5b66\u4e60\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u65f6\u7684\u9891\u7387\u504f\u5dee\uff1b3) \u8bbe\u8ba1\u7efc\u5408\u6c34\u5370\u6027\u80fd\u8bc4\u4f30\u6846\u67b6\u3002", "result": "\u57fa\u4e8e\u5b9e\u9645\u8d1f\u8377\u548c\u5149\u4f0f\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u96c6\u7684\u6848\u4f8b\u7814\u7a76\u8bc1\u660e\u4e86\u6240\u63d0\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u4fdd\u62a4\u80fd\u6e90\u7cfb\u7edf\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u8d44\u4ea7\uff0c\u786e\u4fdd\u6570\u636e\u5728\u8de8\u793e\u533a\u5171\u4eab\u6216\u4ea4\u6613\u65f6\u7684\u771f\u5b9e\u6027\u548c\u5b89\u5168\u6027\u3002"}}
{"id": "2511.08092", "categories": ["eess.AS", "cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2511.08092", "abs": "https://arxiv.org/abs/2511.08092", "authors": ["Julian Irigoyen", "Arthur S\u00f6hler", "Andreas S\u00f8eborg Kirkedal"], "title": "Pruning as Regularization: Sensitivity-Aware One-Shot Pruning in ASR", "comment": "Submitted to ICASSP 2026", "summary": "We challenge the conventional view of neural network pruning as solely a compression technique, demonstrating that one-shot magnitude pruning serves as a powerful implicit regularizer for ASR. Using Whisper-small, we combine gradient- and Fisher-based sensitivity diagnostics with targeted, component-wise pruning. This reveals architectural asymmetries: decoder FFNs are pruning-fragile, whereas decoder self-attention and the last encoder layers contain redundancy that, when removed, improves generalization. Without fine-tuning, pruning 50% of decoder self-attention reduces WER by 2.38% absolute (20.44% relative) on LibriSpeech test-other; pruning the last four encoder layers at 50% instead yields a 1.72% absolute (14.8% relative) improvement. Gains persisted on Common Voice and TED-LIUM datasets. Beyond regularization benefits, our sensitivity-aware approach enables more aggressive one-shot compression. At 40% sparsity, where established global pruning approaches catastrophically fail, our method preserves near-baseline accuracy. This positions pruning as a first-class architectural design tool: knowing where to prune is as important as how much to prune.", "AI": {"tldr": "\u672c\u6587\u6311\u6218\u4e86\u795e\u7ecf\u7f51\u7edc\u526a\u679d\u4ec5\u4f5c\u4e3a\u538b\u7f29\u6280\u672f\u7684\u4f20\u7edf\u89c2\u70b9\uff0c\u8bc1\u660e\u4e00\u6b21\u6027\u5e45\u5ea6\u526a\u679d\u53ef\u4f5c\u4e3aASR\u7684\u5f3a\u5927\u9690\u5f0f\u6b63\u5219\u5316\u5668\u3002\u901a\u8fc7\u654f\u611f\u6027\u8bca\u65ad\u548c\u7ec4\u4ef6\u7ea7\u526a\u679d\uff0c\u53d1\u73b0\u89e3\u7801\u5668FFN\u5bf9\u526a\u679d\u654f\u611f\uff0c\u800c\u89e3\u7801\u5668\u81ea\u6ce8\u610f\u529b\u548c\u6700\u540e\u7f16\u7801\u5668\u5c42\u5b58\u5728\u5197\u4f59\uff0c\u526a\u9664\u8fd9\u4e9b\u5197\u4f59\u53ef\u63d0\u5347\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u91cd\u65b0\u5ba1\u89c6\u795e\u7ecf\u7f51\u7edc\u526a\u679d\u7684\u4f5c\u7528\uff0c\u63a2\u7d22\u5176\u4f5c\u4e3a\u6b63\u5219\u5316\u5de5\u5177\u800c\u975e\u5355\u7eaf\u538b\u7f29\u6280\u672f\u7684\u6f5c\u529b\uff0c\u7279\u522b\u662f\u5728\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b(ASR)\u4efb\u52a1\u4e2d\u3002", "method": "\u4f7f\u7528Whisper-small\u6a21\u578b\uff0c\u7ed3\u5408\u68af\u5ea6\u548cFisher\u654f\u611f\u6027\u8bca\u65ad\uff0c\u8fdb\u884c\u6709\u9488\u5bf9\u6027\u7684\u7ec4\u4ef6\u7ea7\u526a\u679d\uff0c\u5206\u6790\u4e0d\u540c\u67b6\u6784\u7ec4\u4ef6\u7684\u526a\u679d\u654f\u611f\u6027\u3002", "result": "\u5728\u4e0d\u5fae\u8c03\u7684\u60c5\u51b5\u4e0b\uff0c\u526a\u966450%\u89e3\u7801\u5668\u81ea\u6ce8\u610f\u529b\u4f7fLibriSpeech test-other\u7684WER\u7edd\u5bf9\u964d\u4f4e2.38%(\u76f8\u5bf920.44%)\uff1b\u526a\u9664\u6700\u540e\u56db\u4e2a\u7f16\u7801\u5668\u5c4250%\u53c2\u6570\u4f7fWER\u7edd\u5bf9\u964d\u4f4e1.72%(\u76f8\u5bf914.8%)\u3002\u572840%\u7a00\u758f\u5ea6\u4e0b\uff0c\u4f20\u7edf\u5168\u5c40\u526a\u679d\u65b9\u6cd5\u4f1a\u707e\u96be\u6027\u5931\u8d25\uff0c\u800c\u672c\u6587\u65b9\u6cd5\u80fd\u4fdd\u6301\u63a5\u8fd1\u57fa\u7ebf\u7684\u51c6\u786e\u7387\u3002", "conclusion": "\u526a\u679d\u5e94\u88ab\u89c6\u4e3a\u4e00\u6d41\u7684\u67b6\u6784\u8bbe\u8ba1\u5de5\u5177\uff1a\u77e5\u9053\u5728\u54ea\u91cc\u526a\u679d\u4e0e\u526a\u591a\u5c11\u540c\u7b49\u91cd\u8981\uff0c\u654f\u611f\u6027\u611f\u77e5\u65b9\u6cd5\u65e2\u80fd\u5b9e\u73b0\u6b63\u5219\u5316\u6548\u679c\uff0c\u53c8\u80fd\u5b9e\u73b0\u66f4\u6fc0\u8fdb\u7684\u538b\u7f29\u3002"}}
{"id": "2511.07677", "categories": ["cs.SD", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.07677", "abs": "https://arxiv.org/abs/2511.07677", "authors": ["Feyisayo Olalere", "Kiki van der Heijden", "H. Christiaan Stronks", "Jeroen Briaire", "Johan H. M. Frijns", "Yagmur G\u00fc\u00e7l\u00fct\u00fcrk"], "title": "Speech Separation for Hearing-Impaired Children in the Classroom", "comment": "13 pages", "summary": "Classroom environments are particularly challenging for children with hearing impairments, where background noise, multiple talkers, and reverberation degrade speech perception. These difficulties are greater for children than adults, yet most deep learning speech separation models for assistive devices are developed using adult voices in simplified, low-reverberation conditions. This overlooks both the higher spectral similarity of children's voices, which weakens separation cues, and the acoustic complexity of real classrooms. We address this gap using MIMO-TasNet, a compact, low-latency, multi-channel architecture suited for real-time deployment in bilateral hearing aids or cochlear implants. We simulated naturalistic classroom scenes with moving child-child and child-adult talker pairs under varying noise and distance conditions. Training strategies tested how well the model adapts to children's speech through spatial cues. Models trained on adult speech, classroom data, and finetuned variants were compared to assess data-efficient adaptation. Results show that adult-trained models perform well in clean scenes, but classroom-specific training greatly improves separation quality. Finetuning with only half the classroom data achieved comparable gains, confirming efficient transfer learning. Training with diffuse babble noise further enhanced robustness, and the model preserved spatial awareness while generalizing to unseen distances. These findings demonstrate that spatially aware architectures combined with targeted adaptation can improve speech accessibility for children in noisy classrooms, supporting future on-device assistive technologies.", "AI": {"tldr": "\u8be5\u7814\u7a76\u9488\u5bf9\u542c\u529b\u969c\u788d\u513f\u7ae5\u5728\u5608\u6742\u6559\u5ba4\u73af\u5883\u4e2d\u7684\u8bed\u97f3\u5206\u79bb\u95ee\u9898\uff0c\u5f00\u53d1\u4e86\u57fa\u4e8eMIMO-TasNet\u7684\u4f4e\u5ef6\u8fdf\u591a\u901a\u9053\u6a21\u578b\uff0c\u901a\u8fc7\u6559\u5ba4\u7279\u5b9a\u8bad\u7ec3\u548c\u5fae\u8c03\u7b56\u7565\u663e\u8457\u63d0\u5347\u4e86\u513f\u7ae5\u8bed\u97f3\u5206\u79bb\u8d28\u91cf\u3002", "motivation": "\u542c\u529b\u969c\u788d\u513f\u7ae5\u5728\u6559\u5ba4\u73af\u5883\u4e2d\u9762\u4e34\u4e25\u91cd\u8bed\u97f3\u611f\u77e5\u56f0\u96be\uff0c\u73b0\u6709\u8bed\u97f3\u5206\u79bb\u6a21\u578b\u4e3b\u8981\u57fa\u4e8e\u6210\u4eba\u8bed\u97f3\u5f00\u53d1\uff0c\u5ffd\u7565\u4e86\u513f\u7ae5\u8bed\u97f3\u9891\u8c31\u76f8\u4f3c\u6027\u66f4\u9ad8\u548c\u6559\u5ba4\u58f0\u5b66\u590d\u6742\u6027\u8fd9\u4e24\u4e2a\u5173\u952e\u6311\u6218\u3002", "method": "\u4f7f\u7528MIMO-TasNet\u67b6\u6784\uff0c\u6a21\u62df\u81ea\u7136\u6559\u5ba4\u573a\u666f\u4e2d\u79fb\u52a8\u7684\u513f\u7ae5-\u513f\u7ae5\u548c\u513f\u7ae5-\u6210\u4eba\u8bf4\u8bdd\u8005\u5bf9\uff0c\u6d4b\u8bd5\u4e0d\u540c\u8bad\u7ec3\u7b56\u7565\u5bf9\u513f\u7ae5\u8bed\u97f3\u7a7a\u95f4\u7ebf\u7d22\u7684\u9002\u5e94\u80fd\u529b\uff0c\u6bd4\u8f83\u6210\u4eba\u8bed\u97f3\u8bad\u7ec3\u3001\u6559\u5ba4\u6570\u636e\u8bad\u7ec3\u548c\u5fae\u8c03\u53d8\u4f53\u7684\u6548\u679c\u3002", "result": "\u6210\u4eba\u8bad\u7ec3\u6a21\u578b\u5728\u5e72\u51c0\u573a\u666f\u8868\u73b0\u826f\u597d\uff0c\u4f46\u6559\u5ba4\u7279\u5b9a\u8bad\u7ec3\u663e\u8457\u63d0\u5347\u5206\u79bb\u8d28\u91cf\uff1b\u4ec5\u7528\u4e00\u534a\u6559\u5ba4\u6570\u636e\u5fae\u8c03\u5373\u53ef\u83b7\u5f97\u53ef\u6bd4\u589e\u76ca\uff1b\u6269\u6563\u566a\u58f0\u8bad\u7ec3\u589e\u5f3a\u9c81\u68d2\u6027\uff1b\u6a21\u578b\u4fdd\u6301\u7a7a\u95f4\u611f\u77e5\u5e76\u6cdb\u5316\u5230\u672a\u89c1\u8ddd\u79bb\u3002", "conclusion": "\u7a7a\u95f4\u611f\u77e5\u67b6\u6784\u7ed3\u5408\u9488\u5bf9\u6027\u9002\u5e94\u7b56\u7565\u53ef\u6709\u6548\u6539\u5584\u513f\u7ae5\u5728\u5608\u6742\u6559\u5ba4\u4e2d\u7684\u8bed\u97f3\u53ef\u53ca\u6027\uff0c\u652f\u6301\u672a\u6765\u8bbe\u5907\u7aef\u8f85\u52a9\u6280\u672f\u7684\u53d1\u5c55\u3002"}}
{"id": "2511.07874", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.07874", "abs": "https://arxiv.org/abs/2511.07874", "authors": ["Cixiao Zhang", "Yin Xu", "Xinghao Guo", "Xiaowu Ou", "Dazhi He", "Wenjun Zhang"], "title": "MA-Aided Hierarchical Hybrid Beamforming for Multi-User Wideband Beam Squint Mitigation", "comment": null, "summary": "In wideband near-field arrays, frequency-dependent array responses cause wavefronts at different frequencies to deviate from that at the center frequency, producing beam squint and degrading multi-user performance. True-time-delay (TTD) circuits can realign the frequency dependence but require large delay ranges and intricate calibration, limiting scalability. Another line of work explores one- and two-dimensional array geometries, including linear, circular, and concentric circular, that exhibit distinct broadband behaviors such as different beam-squint sensitivities and focusing characteristics. These observations motivate adapting the array layout to enable wideband-friendly focusing and enhance multi-user performance without TTD networks. We propose a movable antenna (MA) aided architecture based on hierarchical sub-connected hybrid beamforming (HSC-HBF) in which antennas are grouped into tiles and only the tile centers are repositioned, providing slow geometric degrees of freedom that emulate TTD-like broadband focusing while keeping hardware and optimization complexity low. We show that the steering vector is inherently frequency dependent and that reconfiguring tile locations improves broadband focusing. Simulations across wideband near-field scenarios demonstrate robust squint suppression and consistent gains over fixed-layout arrays, achieving up to 5\\% higher sum rate, with the maximum improvement exceeding 140\\%.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53ef\u79fb\u52a8\u5929\u7ebf(MA)\u548c\u5206\u5c42\u5b50\u8fde\u63a5\u6df7\u5408\u6ce2\u675f\u6210\u5f62(HSC-HBF)\u7684\u67b6\u6784\uff0c\u901a\u8fc7\u91cd\u65b0\u914d\u7f6e\u5929\u7ebf\u74e6\u7247\u4f4d\u7f6e\u6765\u6a21\u62dfTTD\u5bbd\u5e26\u805a\u7126\u6548\u679c\uff0c\u6291\u5236\u6ce2\u675f\u659c\u89c6\uff0c\u63d0\u9ad8\u591a\u7528\u6237\u6027\u80fd\u3002", "motivation": "\u5728\u5bbd\u5e26\u8fd1\u573a\u9635\u5217\u4e2d\uff0c\u9891\u7387\u76f8\u5173\u7684\u9635\u5217\u54cd\u5e94\u5bfc\u81f4\u4e0d\u540c\u9891\u7387\u7684\u6ce2\u524d\u504f\u79bb\u4e2d\u5fc3\u9891\u7387\uff0c\u4ea7\u751f\u6ce2\u675f\u659c\u89c6\u5e76\u964d\u4f4e\u591a\u7528\u6237\u6027\u80fd\u3002\u4f20\u7edfTTD\u7535\u8def\u9700\u8981\u5927\u5ef6\u8fdf\u8303\u56f4\u548c\u590d\u6742\u6821\u51c6\uff0c\u9650\u5236\u4e86\u53ef\u6269\u5c55\u6027\u3002", "method": "\u91c7\u7528\u53ef\u79fb\u52a8\u5929\u7ebf\u67b6\u6784\uff0c\u5c06\u5929\u7ebf\u5206\u7ec4\u4e3a\u74e6\u7247\uff0c\u4ec5\u91cd\u65b0\u914d\u7f6e\u74e6\u7247\u4e2d\u5fc3\u4f4d\u7f6e\uff0c\u63d0\u4f9b\u6162\u51e0\u4f55\u81ea\u7531\u5ea6\u6765\u6a21\u62dfTTD\u5bbd\u5e26\u805a\u7126\uff0c\u540c\u65f6\u4fdd\u6301\u786c\u4ef6\u548c\u4f18\u5316\u590d\u6742\u5ea6\u8f83\u4f4e\u3002", "result": "\u4eff\u771f\u663e\u793a\u5728\u5bbd\u5e26\u8fd1\u573a\u573a\u666f\u4e2d\uff0c\u8be5\u65b9\u6cd5\u80fd\u7a33\u5065\u5730\u6291\u5236\u659c\u89c6\uff0c\u76f8\u6bd4\u56fa\u5b9a\u5e03\u5c40\u9635\u5217\u5b9e\u73b0\u4e00\u81f4\u589e\u76ca\uff0c\u8fbe\u5230\u9ad8\u8fbe5%\u7684\u603b\u901f\u7387\u63d0\u5347\uff0c\u6700\u5927\u6539\u8fdb\u8d85\u8fc7140%\u3002", "conclusion": "\u901a\u8fc7\u91cd\u65b0\u914d\u7f6e\u5929\u7ebf\u74e6\u7247\u4f4d\u7f6e\u53ef\u4ee5\u6539\u5584\u5bbd\u5e26\u805a\u7126\u6027\u80fd\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700TTD\u7f51\u7edc\u5373\u53ef\u589e\u5f3a\u5bbd\u5e26\u591a\u7528\u6237\u6027\u80fd\u7684\u6709\u6548\u65b9\u6cd5\u3002"}}
{"id": "2511.08093", "categories": ["eess.AS", "cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2511.08093", "abs": "https://arxiv.org/abs/2511.08093", "authors": ["Arthur S\u00f6hler", "Julian Irigoyen", "Andreas S\u00f8eborg Kirkedal"], "title": "Quantizing Whisper-small: How design choices affect ASR performance", "comment": "Submitted to ICASSP 2026", "summary": "Large speech recognition models like Whisper-small achieve high accuracy but are difficult to deploy on edge devices due to their high computational demand. To this end, we present a unified, cross-library evaluation of post-training quantization (PTQ) on Whisper-small that disentangles the impact of quantization scheme, method, granularity, and bit-width. Our study is based on four libraries: PyTorch, Optimum-Quanto, HQQ, and bitsandbytes. Experiments on LibriSpeech test-clean and test-other show that dynamic int8 quantization with Quanto offers the best trade-off, reducing model size by 57% while improving on the baseline's word error rate. Static quantization performed worse, likely due to Whisper's Transformer architecture, while more aggressive formats (e.g., nf4, int3) achieved up to 71% compression at the cost of accuracy in noisy conditions. Overall, our results demonstrate that carefully chosen PTQ methods can substantially reduce model size and inference cost without retraining, enabling efficient deployment of Whisper-small on constrained hardware.", "AI": {"tldr": "\u5bf9Whisper-small\u6a21\u578b\u8fdb\u884c\u540e\u8bad\u7ec3\u91cf\u5316\u8bc4\u4f30\uff0c\u53d1\u73b0\u52a8\u6001int8\u91cf\u5316\u5728Quanto\u5e93\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u6a21\u578b\u5927\u5c0f\u51cf\u5c1157%\u7684\u540c\u65f6\u5728Word Error Rate\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u3002", "motivation": "\u5927\u578b\u8bed\u97f3\u8bc6\u522b\u6a21\u578b\u5982Whisper-small\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u90e8\u7f72\u56f0\u96be\uff0c\u56e0\u5176\u8ba1\u7b97\u9700\u6c42\u9ad8\u3002\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u540e\u8bad\u7ec3\u91cf\u5316\u51cf\u5c0f\u6a21\u578b\u5927\u5c0f\u548c\u63a8\u7406\u6210\u672c\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u3002", "method": "\u4f7f\u7528\u56db\u4e2a\u5e93\uff08PyTorch\u3001Optimum-Quanto\u3001HQQ\u3001bitsandbytes\uff09\u5bf9Whisper-small\u8fdb\u884c\u7edf\u4e00\u8bc4\u4f30\uff0c\u5206\u6790\u91cf\u5316\u65b9\u6848\u3001\u65b9\u6cd5\u3001\u7c92\u5ea6\u548c\u4f4d\u5bbd\u7684\u5f71\u54cd\u3002\u5728LibriSpeech\u6d4b\u8bd5\u96c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u52a8\u6001int8\u91cf\u5316\u5728Quanto\u5e93\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u6a21\u578b\u5927\u5c0f\u51cf\u5c1157%\u4e14WER\u4f18\u4e8e\u57fa\u7ebf\u3002\u9759\u6001\u91cf\u5316\u8868\u73b0\u8f83\u5dee\uff0c\u66f4\u6fc0\u8fdb\u7684\u683c\u5f0f\uff08\u5982nf4\u3001int3\uff09\u53ef\u5b9e\u73b071%\u538b\u7f29\u4f46\u566a\u58f0\u6761\u4ef6\u4e0b\u51c6\u786e\u7387\u4e0b\u964d\u3002", "conclusion": "\u7cbe\u5fc3\u9009\u62e9\u7684\u540e\u8bad\u7ec3\u91cf\u5316\u65b9\u6cd5\u53ef\u663e\u8457\u51cf\u5c0f\u6a21\u578b\u5927\u5c0f\u548c\u63a8\u7406\u6210\u672c\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\uff0c\u4f7fWhisper-small\u80fd\u591f\u5728\u53d7\u9650\u786c\u4ef6\u4e0a\u9ad8\u6548\u90e8\u7f72\u3002"}}
{"id": "2511.07821", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2511.07821", "abs": "https://arxiv.org/abs/2511.07821", "authors": ["Lu Gan", "Xi Li"], "title": "SynTTS-Commands: A Public Dataset for On-Device KWS via TTS-Synthesized Multilingual Speech", "comment": null, "summary": "The development of high-performance, on-device keyword spotting (KWS) systems for ultra-low-power hardware is critically constrained by the scarcity of specialized, multi-command training datasets. Traditional data collection through human recording is costly, slow, and lacks scalability. This paper introduces SYNTTS-COMMANDS, a novel, multilingual voice command dataset entirely generated using state-of-the-art Text-to-Speech (TTS) synthesis. By leveraging the CosyVoice 2 model and speaker embeddings from public corpora, we created a scalable collection of English and Chinese commands. Extensive benchmarking across a range of efficient acoustic models demonstrates that our synthetic dataset enables exceptional accuracy, achieving up to 99.5\\% on English and 98\\% on Chinese command recognition. These results robustly validate that synthetic speech can effectively replace human-recorded audio for training KWS classifiers. Our work directly addresses the data bottleneck in TinyML, providing a practical, scalable foundation for building private, low-latency, and energy-efficient voice interfaces on resource-constrained edge devices.", "AI": {"tldr": "SYNTTS-COMMANDS\u662f\u4e00\u4e2a\u5b8c\u5168\u4f7f\u7528TTS\u5408\u6210\u751f\u6210\u7684\u591a\u8bed\u8a00\u8bed\u97f3\u547d\u4ee4\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u89e3\u51b3\u8bbe\u5907\u7aef\u5173\u952e\u8bcd\u68c0\u6d4b\u7cfb\u7edf\u7684\u8bad\u7ec3\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u5728\u82f1\u8bed\u548c\u4e2d\u6587\u547d\u4ee4\u8bc6\u522b\u4e0a\u5206\u522b\u8fbe\u523099.5%\u548c98%\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u4f20\u7edf\u7684\u4eba\u5de5\u5f55\u97f3\u6570\u636e\u6536\u96c6\u65b9\u5f0f\u6210\u672c\u9ad8\u3001\u901f\u5ea6\u6162\u4e14\u7f3a\u4e4f\u53ef\u6269\u5c55\u6027\uff0c\u4e25\u91cd\u5236\u7ea6\u4e86\u8d85\u4f4e\u529f\u8017\u786c\u4ef6\u4e0a\u9ad8\u6027\u80fd\u5173\u952e\u8bcd\u68c0\u6d4b\u7cfb\u7edf\u7684\u5f00\u53d1\u3002", "method": "\u5229\u7528CosyVoice 2 TTS\u6a21\u578b\u548c\u516c\u5f00\u8bed\u6599\u5e93\u4e2d\u7684\u8bf4\u8bdd\u4eba\u5d4c\u5165\uff0c\u521b\u5efa\u4e86\u53ef\u6269\u5c55\u7684\u82f1\u8bed\u548c\u4e2d\u6587\u547d\u4ee4\u6570\u636e\u96c6\u3002", "result": "\u5728\u591a\u79cd\u9ad8\u6548\u58f0\u5b66\u6a21\u578b\u4e0a\u7684\u5e7f\u6cdb\u57fa\u51c6\u6d4b\u8bd5\u663e\u793a\uff0c\u5408\u6210\u6570\u636e\u96c6\u5b9e\u73b0\u4e86\u4f18\u5f02\u7684\u51c6\u786e\u7387\uff1a\u82f1\u8bed\u547d\u4ee4\u8bc6\u522b\u8fbe99.5%\uff0c\u4e2d\u6587\u547d\u4ee4\u8bc6\u522b\u8fbe98%\u3002", "conclusion": "\u5408\u6210\u8bed\u97f3\u53ef\u4ee5\u6709\u6548\u66ff\u4ee3\u4eba\u5de5\u5f55\u97f3\u97f3\u9891\u6765\u8bad\u7ec3\u5173\u952e\u8bcd\u68c0\u6d4b\u5206\u7c7b\u5668\uff0c\u4e3a\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u4e0a\u6784\u5efa\u79c1\u6709\u3001\u4f4e\u5ef6\u8fdf\u548c\u8282\u80fd\u7684\u8bed\u97f3\u63a5\u53e3\u63d0\u4f9b\u4e86\u5b9e\u7528\u3001\u53ef\u6269\u5c55\u7684\u57fa\u7840\u3002"}}
{"id": "2511.07881", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.07881", "abs": "https://arxiv.org/abs/2511.07881", "authors": ["Ehsan Alamdari", "Rouhollah Amiri"], "title": "Conical Localization via Modified Polar Representation: A Unified Framework for Robust 3-D Positioning with 1-D Sensor Arrays", "comment": null, "summary": "This paper presents a unified framework for robust three-dimensional (3-D) source localization using a network of sensors equipped with one-dimensional (1-D) linear arrays. While such arrays offer practical advantages in terms of cost and size, existing localization methods suffer from a fundamental limitation: their performance degrades significantly as the source moves into the far-field, a common challenge known as the thresholding effect. To address this issue, we reformulate the localization problem in the modified polar representation (MPR) coordinate system, which parameterizes the source location using its azimuth, elevation, and inverse-range. We have developed a constrained weighted least squares (CWLS) estimator, which is subsequently transformed into a tight semidefinite programming (SDP) problem via semidefinite relaxation, enhanced with additional constraints to improve accuracy. Simulation results demonstrate that the proposed estimator attains the Cramer-Rao lower bound (CRLB) for both angle and inverse-range estimation in near-field scenarios. More importantly, it maintains this optimal performance in the far-field, substantially outperforming state-of-the-art methods, which exhibit significant error at large ranges. The proposed solution thus provides a reliable, unified localization system that is effective irrespective of the source range.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u4e00\u7ef4\u7ebf\u6027\u9635\u5217\u4f20\u611f\u5668\u7f51\u7edc\u8fdb\u884c\u9c81\u68d2\u4e09\u7ef4\u6e90\u5b9a\u4f4d\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u8fdc\u573a\u6027\u80fd\u4e0b\u964d\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u4f7f\u7528\u4e00\u7ef4\u7ebf\u6027\u9635\u5217\u7684\u5b9a\u4f4d\u65b9\u6cd5\u5728\u6e90\u79fb\u52a8\u5230\u8fdc\u573a\u65f6\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u5b58\u5728\u9608\u503c\u6548\u5e94\u95ee\u9898\u3002", "method": "\u5728\u6539\u8fdb\u6781\u5750\u6807\u8868\u793a(MPR)\u5750\u6807\u7cfb\u4e2d\u91cd\u65b0\u8868\u8ff0\u5b9a\u4f4d\u95ee\u9898\uff0c\u5f00\u53d1\u4e86\u7ea6\u675f\u52a0\u6743\u6700\u5c0f\u4e8c\u4e58(CWLS)\u4f30\u8ba1\u5668\uff0c\u901a\u8fc7\u534a\u5b9a\u677e\u5f1b\u8f6c\u5316\u4e3a\u7d27\u534a\u5b9a\u89c4\u5212(SDP)\u95ee\u9898\uff0c\u5e76\u6dfb\u52a0\u989d\u5916\u7ea6\u675f\u63d0\u9ad8\u7cbe\u5ea6\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u4f30\u8ba1\u5668\u5728\u8fd1\u573a\u573a\u666f\u4e0b\u8fbe\u5230\u89d2\u5ea6\u548c\u9006\u8ddd\u79bb\u4f30\u8ba1\u7684Cramer-Rao\u4e0b\u754c(CRLB)\uff0c\u66f4\u91cd\u8981\u7684\u662f\u5728\u8fdc\u573a\u4ecd\u4fdd\u6301\u6700\u4f18\u6027\u80fd\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u89e3\u51b3\u65b9\u6848\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u9760\u3001\u7edf\u4e00\u7684\u5b9a\u4f4d\u7cfb\u7edf\uff0c\u65e0\u8bba\u6e90\u8ddd\u79bb\u5982\u4f55\u90fd\u80fd\u6709\u6548\u5de5\u4f5c\u3002"}}
{"id": "2511.08389", "categories": ["eess.AS", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.08389", "abs": "https://arxiv.org/abs/2511.08389", "authors": ["Yi-Jen Shih", "David Harwath"], "title": "Unifying Model and Layer Fusion for Speech Foundation Models", "comment": "Accepted by IEEE ASRU 2025", "summary": "Speech Foundation Models have gained significant attention recently. Prior works have shown that the fusion of representations from multiple layers of the same model or the fusion of multiple models can improve performance on downstream tasks. We unify these two fusion strategies by proposing an interface module that enables fusion across multiple upstream speech models while integrating information across their layers. We conduct extensive experiments on different self-supervised and supervised models across various speech tasks, including ASR and paralinguistic analysis, and demonstrate that our method outperforms prior fusion approaches. We further analyze its scalability concerning model size and count, highlighting the importance of selecting appropriate upstream models. Our results show that the proposed interface provides an additional performance boost when given a suitable upstream model selection, making it a promising approach for utilizing Speech Foundation Models.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8de8\u591a\u4e2a\u4e0a\u6e38\u8bed\u97f3\u6a21\u578b\u7684\u878d\u5408\u63a5\u53e3\u6a21\u5757\uff0c\u80fd\u591f\u5728\u4e0d\u540c\u6a21\u578b\u95f4\u8fdb\u884c\u5c42\u95f4\u4fe1\u606f\u878d\u5408\uff0c\u5728ASR\u548c\u526f\u8bed\u8a00\u5206\u6790\u7b49\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u878d\u5408\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u8868\u660e\uff0c\u540c\u4e00\u6a21\u578b\u7684\u591a\u5c42\u8868\u5f81\u878d\u5408\u6216\u591a\u4e2a\u6a21\u578b\u7684\u878d\u5408\u80fd\u63d0\u5347\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\uff0c\u4f46\u7f3a\u4e4f\u7edf\u4e00\u7684\u878d\u5408\u7b56\u7565\u6765\u540c\u65f6\u5b9e\u73b0\u8de8\u6a21\u578b\u548c\u8de8\u5c42\u7684\u4fe1\u606f\u6574\u5408\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u63a5\u53e3\u6a21\u5757\uff0c\u652f\u6301\u591a\u4e2a\u4e0a\u6e38\u8bed\u97f3\u6a21\u578b\u4e4b\u95f4\u7684\u878d\u5408\uff0c\u540c\u65f6\u6574\u5408\u8fd9\u4e9b\u6a21\u578b\u5404\u5c42\u7684\u4fe1\u606f\u3002", "result": "\u5728\u591a\u79cd\u81ea\u76d1\u7763\u548c\u76d1\u7763\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728ASR\u548c\u526f\u8bed\u8a00\u5206\u6790\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u878d\u5408\u65b9\u6cd5\uff0c\u4e14\u5728\u9009\u62e9\u5408\u9002\u4e0a\u6e38\u6a21\u578b\u65f6\u80fd\u63d0\u4f9b\u989d\u5916\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u8be5\u63a5\u53e3\u6a21\u5757\u4e3a\u5229\u7528\u8bed\u97f3\u57fa\u7840\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u524d\u666f\u7684\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u9009\u62e9\u5408\u9002\u4e0a\u6e38\u6a21\u578b\u65f6\u80fd\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002"}}
{"id": "2511.07883", "categories": ["cs.SD", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.07883", "abs": "https://arxiv.org/abs/2511.07883", "authors": ["Jiaqi Wang", "Liutao Yu", "Xiongri Shen", "Sihang Guo", "Chenlin Zhou", "Leilei Zhao", "Yi Zhong", "Zhengyu Ma", "Zhiguo Zhang"], "title": "SpikCommander: A High-performance Spiking Transformer with Multi-view Learning for Efficient Speech Command Recognition", "comment": "Accepted by The Fortieth AAAI Conference on Artificial Intelligence (AAAI 2026)", "summary": "Spiking neural networks (SNNs) offer a promising path toward energy-efficient speech command recognition (SCR) by leveraging their event-driven processing paradigm. However, existing SNN-based SCR methods often struggle to capture rich temporal dependencies and contextual information from speech due to limited temporal modeling and binary spike-based representations. To address these challenges, we first introduce the multi-view spiking temporal-aware self-attention (MSTASA) module, which combines effective spiking temporal-aware attention with a multi-view learning framework to model complementary temporal dependencies in speech commands. Building on MSTASA, we further propose SpikCommander, a fully spike-driven transformer architecture that integrates MSTASA with a spiking contextual refinement channel MLP (SCR-MLP) to jointly enhance temporal context modeling and channel-wise feature integration. We evaluate our method on three benchmark datasets: the Spiking Heidelberg Dataset (SHD), the Spiking Speech Commands (SSC), and the Google Speech Commands V2 (GSC). Extensive experiments demonstrate that SpikCommander consistently outperforms state-of-the-art (SOTA) SNN approaches with fewer parameters under comparable time steps, highlighting its effectiveness and efficiency for robust speech command recognition.", "AI": {"tldr": "\u63d0\u51fa\u4e86SpikCommander\uff0c\u4e00\u79cd\u5b8c\u5168\u8109\u51b2\u9a71\u52a8\u7684\u53d8\u538b\u5668\u67b6\u6784\uff0c\u901a\u8fc7\u591a\u89c6\u56fe\u8109\u51b2\u65f6\u95f4\u611f\u77e5\u81ea\u6ce8\u610f\u529b\u6a21\u5757\u548c\u8109\u51b2\u4e0a\u4e0b\u6587\u7ec6\u5316\u901a\u9053MLP\uff0c\u5728\u66f4\u5c11\u53c2\u6570\u548c\u53ef\u6bd4\u65f6\u95f4\u6b65\u957f\u4e0b\u4f18\u4e8e\u73b0\u6709SNN\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eSNN\u7684\u8bed\u97f3\u547d\u4ee4\u8bc6\u522b\u65b9\u6cd5\u7531\u4e8e\u6709\u9650\u7684\u65f6\u95f4\u5efa\u6a21\u548c\u4e8c\u8fdb\u5236\u8109\u51b2\u8868\u793a\uff0c\u96be\u4ee5\u6355\u6349\u8bed\u97f3\u4e2d\u7684\u4e30\u5bcc\u65f6\u95f4\u4f9d\u8d56\u6027\u548c\u4e0a\u4e0b\u6587\u4fe1\u606f\u3002", "method": "\u5f15\u5165\u591a\u89c6\u56fe\u8109\u51b2\u65f6\u95f4\u611f\u77e5\u81ea\u6ce8\u610f\u529b\u6a21\u5757\uff0c\u7ed3\u5408\u8109\u51b2\u65f6\u95f4\u611f\u77e5\u6ce8\u610f\u529b\u548c\u591a\u89c6\u56fe\u5b66\u4e60\u6846\u67b6\uff1b\u6784\u5efaSpikCommander\u67b6\u6784\uff0c\u96c6\u6210MSTASA\u4e0e\u8109\u51b2\u4e0a\u4e0b\u6587\u7ec6\u5316\u901a\u9053MLP\u3002", "result": "\u5728SHD\u3001SSC\u548cGSC\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cSpikCommander\u5728\u53ef\u6bd4\u65f6\u95f4\u6b65\u957f\u4e0b\u4ee5\u66f4\u5c11\u53c2\u6570\u6301\u7eed\u4f18\u4e8e\u6700\u5148\u8fdb\u7684SNN\u65b9\u6cd5\u3002", "conclusion": "SpikCommander\u5c55\u793a\u4e86\u5728\u9c81\u68d2\u8bed\u97f3\u547d\u4ee4\u8bc6\u522b\u65b9\u9762\u7684\u6709\u6548\u6027\u548c\u6548\u7387\uff0c\u4e3a\u80fd\u91cf\u9ad8\u6548\u7684SNN\u8bed\u97f3\u5904\u7406\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.07891", "categories": ["eess.SP", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.07891", "abs": "https://arxiv.org/abs/2511.07891", "authors": ["Yeon-Woo Choi", "Hye-Bin Shin", "Dan Li"], "title": "Toward Adaptive BCIs: Enhancing Decoding Stability via User State-Aware EEG Filtering", "comment": "4 pages, 3 figures, conference", "summary": "Brain-computer interfaces (BCIs) often suffer from limited robustness and poor long-term adaptability. Model performance rapidly degrades when user attention fluctuates, brain states shift over time, or irregular artifacts appear during interaction. To mitigate these issues, we introduce a user state-aware electroencephalogram (EEG) filtering framework that refines neural representations before decoding user intentions. The proposed method continuously estimates the user's cognitive state (e.g., focus or distraction) from EEG features and filters unreliable segments by applying adaptive weighting based on the estimated attention level. This filtering stage suppresses noisy or out-of-focus epochs, thereby reducing distributional drift and improving the consistency of subsequent decoding. Experiments on multiple EEG datasets that emulate real BCI scenarios demonstrate that the proposed state-aware filtering enhances classification accuracy and stability across different user states and sessions compared with conventional preprocessing pipelines. These findings highlight that leveraging brain-derived state information--even without additional user labels--can substantially improve the reliability of practical EEG-based BCIs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u6237\u72b6\u6001\u611f\u77e5\u7684EEG\u8fc7\u6ee4\u6846\u67b6\uff0c\u901a\u8fc7\u4f30\u8ba1\u7528\u6237\u8ba4\u77e5\u72b6\u6001\u5e76\u57fa\u4e8e\u6ce8\u610f\u529b\u6c34\u5e73\u8fdb\u884c\u81ea\u9002\u5e94\u52a0\u6743\uff0c\u8fc7\u6ee4\u4e0d\u53ef\u9760\u7684EEG\u7247\u6bb5\uff0c\u4ece\u800c\u63d0\u9ad8\u8111\u673a\u63a5\u53e3\u7684\u9c81\u68d2\u6027\u548c\u957f\u671f\u9002\u5e94\u6027\u3002", "motivation": "\u8111\u673a\u63a5\u53e3\u5728\u7528\u6237\u6ce8\u610f\u529b\u6ce2\u52a8\u3001\u8111\u72b6\u6001\u968f\u65f6\u95f4\u53d8\u5316\u6216\u51fa\u73b0\u4e0d\u89c4\u5219\u4f2a\u5f71\u65f6\u6027\u80fd\u4f1a\u8fc5\u901f\u4e0b\u964d\uff0c\u9700\u8981\u63d0\u9ad8\u5176\u9c81\u68d2\u6027\u548c\u957f\u671f\u9002\u5e94\u6027\u3002", "method": "\u5f00\u53d1\u4e86\u7528\u6237\u72b6\u6001\u611f\u77e5\u7684EEG\u8fc7\u6ee4\u6846\u67b6\uff0c\u4eceEEG\u7279\u5f81\u4e2d\u8fde\u7eed\u4f30\u8ba1\u7528\u6237\u7684\u8ba4\u77e5\u72b6\u6001\uff08\u5982\u4e13\u6ce8\u6216\u5206\u5fc3\uff09\uff0c\u5e76\u57fa\u4e8e\u4f30\u8ba1\u7684\u6ce8\u610f\u529b\u6c34\u5e73\u5e94\u7528\u81ea\u9002\u5e94\u52a0\u6743\u6765\u8fc7\u6ee4\u4e0d\u53ef\u9760\u7684\u7247\u6bb5\u3002", "result": "\u5728\u591a\u4e2a\u6a21\u62df\u771f\u5b9eBCI\u573a\u666f\u7684EEG\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u4f20\u7edf\u7684\u9884\u5904\u7406\u6d41\u7a0b\u76f8\u6bd4\uff0c\u6240\u63d0\u51fa\u7684\u72b6\u6001\u611f\u77e5\u8fc7\u6ee4\u65b9\u6cd5\u5728\u4e0d\u540c\u7528\u6237\u72b6\u6001\u548c\u4f1a\u8bdd\u4e2d\u63d0\u9ad8\u4e86\u5206\u7c7b\u51c6\u786e\u6027\u548c\u7a33\u5b9a\u6027\u3002", "conclusion": "\u5229\u7528\u8111\u6e90\u6027\u72b6\u6001\u4fe1\u606f\uff08\u5373\u4f7f\u6ca1\u6709\u989d\u5916\u7684\u7528\u6237\u6807\u7b7e\uff09\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u57fa\u4e8eEEG\u7684\u5b9e\u7528\u8111\u673a\u63a5\u53e3\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2511.08252", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2511.08252", "abs": "https://arxiv.org/abs/2511.08252", "authors": ["Yi Yang", "Haowen Li", "Tianxiang Li", "Boyu Cao", "Xiaohan Zhang", "Liqun Chen", "Qi Liu"], "title": "Melodia: Training-Free Music Editing Guided by Attention Probing in Diffusion Models", "comment": "AAAI 2026", "summary": "Text-to-music generation technology is progressing rapidly, creating new opportunities for musical composition and editing. However, existing music editing methods often fail to preserve the source music's temporal structure, including melody and rhythm, when altering particular attributes like instrument, genre, and mood. To address this challenge, this paper conducts an in-depth probing analysis on attention maps within AudioLDM 2, a diffusion-based model commonly used as the backbone for existing music editing methods. We reveal a key finding: cross-attention maps encompass details regarding distinct musical characteristics, and interventions on these maps frequently result in ineffective modifications. In contrast, self-attention maps are essential for preserving the temporal structure of the source music during its conversion into the target music. Building upon this understanding, we present Melodia, a training-free technique that selectively manipulates self-attention maps in particular layers during the denoising process and leverages an attention repository to store source music information, achieving accurate modification of musical characteristics while preserving the original structure without requiring textual descriptions of the source music. Additionally, we propose two novel metrics to better evaluate music editing methods. Both objective and subjective experiments demonstrate that our approach achieves superior results in terms of textual adherence and structural integrity across various datasets. This research enhances comprehension of internal mechanisms within music generation models and provides improved control for music creation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Melodia\uff0c\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u97f3\u4e50\u7f16\u8f91\u65b9\u6cd5\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u64cd\u4f5c\u81ea\u6ce8\u610f\u529b\u6620\u5c04\u6765\u4fee\u6539\u97f3\u4e50\u7279\u5f81\uff0c\u540c\u65f6\u4fdd\u6301\u6e90\u97f3\u4e50\u7684\u65cb\u5f8b\u548c\u8282\u594f\u7ed3\u6784\u3002", "motivation": "\u73b0\u6709\u97f3\u4e50\u7f16\u8f91\u65b9\u6cd5\u5728\u4fee\u6539\u4e50\u5668\u3001\u6d41\u6d3e\u548c\u60c5\u7eea\u7b49\u5c5e\u6027\u65f6\uff0c\u5f80\u5f80\u65e0\u6cd5\u4fdd\u6301\u6e90\u97f3\u4e50\u7684\u65f6\u95f4\u7ed3\u6784\uff08\u5305\u62ec\u65cb\u5f8b\u548c\u8282\u594f\uff09\u3002", "method": "\u5bf9AudioLDM 2\u4e2d\u7684\u6ce8\u610f\u529b\u6620\u5c04\u8fdb\u884c\u6df1\u5165\u5206\u6790\uff0c\u53d1\u73b0\u81ea\u6ce8\u610f\u529b\u6620\u5c04\u5bf9\u4fdd\u6301\u65f6\u95f4\u7ed3\u6784\u81f3\u5173\u91cd\u8981\u3002\u63d0\u51faMelodia\u65b9\u6cd5\uff0c\u5728\u53bb\u566a\u8fc7\u7a0b\u4e2d\u9009\u62e9\u6027\u64cd\u4f5c\u7279\u5b9a\u5c42\u7684\u81ea\u6ce8\u610f\u529b\u6620\u5c04\uff0c\u5e76\u5229\u7528\u6ce8\u610f\u529b\u5b58\u50a8\u5e93\u5b58\u50a8\u6e90\u97f3\u4e50\u4fe1\u606f\u3002", "result": "\u4e3b\u5ba2\u89c2\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5404\u79cd\u6570\u636e\u96c6\u4e0a\u5728\u6587\u672c\u9075\u5faa\u5ea6\u548c\u7ed3\u6784\u5b8c\u6574\u6027\u65b9\u9762\u90fd\u53d6\u5f97\u4e86\u4f18\u8d8a\u7684\u7ed3\u679c\u3002", "conclusion": "\u8fd9\u9879\u7814\u7a76\u589e\u5f3a\u4e86\u5bf9\u97f3\u4e50\u751f\u6210\u6a21\u578b\u5185\u90e8\u673a\u5236\u7684\u7406\u89e3\uff0c\u5e76\u4e3a\u97f3\u4e50\u521b\u4f5c\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u63a7\u5236\u80fd\u529b\u3002"}}
{"id": "2511.07931", "categories": ["cs.SD", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.07931", "abs": "https://arxiv.org/abs/2511.07931", "authors": ["Xueyao Zhang", "Chaoren Wang", "Huan Liao", "Ziniu Li", "Yuancheng Wang", "Li Wang", "Dongya Jia", "Yuanzhe Chen", "Xiulin Li", "Zhuo Chen", "Zhizheng Wu"], "title": "SpeechJudge: Towards Human-Level Judgment for Speech Naturalness", "comment": "Project Page: https://speechjudge.github.io/", "summary": "Aligning large generative models with human feedback is a critical challenge. In speech synthesis, this is particularly pronounced due to the lack of a large-scale human preference dataset, which hinders the development of models that truly align with human perception. To address this, we introduce SpeechJudge, a comprehensive suite comprising a dataset, a benchmark, and a reward model centered on naturalness--one of the most fundamental subjective metrics for speech synthesis. First, we present SpeechJudge-Data, a large-scale human feedback corpus of 99K speech pairs. The dataset is constructed using a diverse set of advanced zero-shot text-to-speech (TTS) models across diverse speech styles and multiple languages, with human annotations for both intelligibility and naturalness preference. From this, we establish SpeechJudge-Eval, a challenging benchmark for speech naturalness judgment. Our evaluation reveals that existing metrics and AudioLLMs struggle with this task; the leading model, Gemini-2.5-Flash, achieves less than 70% agreement with human judgment, highlighting a significant gap for improvement. To bridge this gap, we develop SpeechJudge-GRM, a generative reward model (GRM) based on Qwen2.5-Omni-7B. It is trained on SpeechJudge-Data via a two-stage post-training process: Supervised Fine-Tuning (SFT) with Chain-of-Thought rationales followed by Reinforcement Learning (RL) with GRPO on challenging cases. On the SpeechJudge-Eval benchmark, the proposed SpeechJudge-GRM demonstrates superior performance, achieving 77.2% accuracy (and 79.4% after inference-time scaling @10) compared to a classic Bradley-Terry reward model (72.7%). Furthermore, SpeechJudge-GRM can be also employed as a reward function during the post-training of speech generation models to facilitate their alignment with human preferences.", "AI": {"tldr": "\u63d0\u51fa\u4e86SpeechJudge\u5957\u4ef6\uff0c\u5305\u62ec\u5927\u89c4\u6a21\u4eba\u7c7b\u504f\u597d\u6570\u636e\u96c6\u3001\u8bc4\u4f30\u57fa\u51c6\u548c\u751f\u6210\u5f0f\u5956\u52b1\u6a21\u578b\uff0c\u7528\u4e8e\u89e3\u51b3\u8bed\u97f3\u5408\u6210\u4e2d\u7f3a\u4e4f\u4eba\u7c7b\u53cd\u9988\u6570\u636e\u7684\u95ee\u9898\uff0c\u63d0\u5347\u8bed\u97f3\u81ea\u7136\u5ea6\u8bc4\u4f30\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u8bed\u97f3\u5408\u6210\u9886\u57df\u7f3a\u4e4f\u5927\u89c4\u6a21\u4eba\u7c7b\u504f\u597d\u6570\u636e\u96c6\uff0c\u963b\u788d\u4e86\u5f00\u53d1\u771f\u6b63\u7b26\u5408\u4eba\u7c7b\u611f\u77e5\u7684\u6a21\u578b\u3002\u81ea\u7136\u5ea6\u4f5c\u4e3a\u8bed\u97f3\u5408\u6210\u6700\u91cd\u8981\u7684\u4e3b\u89c2\u6307\u6807\u4e4b\u4e00\uff0c\u9700\u8981\u66f4\u597d\u7684\u5bf9\u9f50\u65b9\u6cd5\u3002", "method": "1) \u6784\u5efaSpeechJudge-Data\u6570\u636e\u96c6\uff0899K\u8bed\u97f3\u5bf9\uff0c\u591a\u8bed\u8a00\u591a\u98ce\u683c\uff09\uff1b2) \u5efa\u7acbSpeechJudge-Eval\u8bc4\u4f30\u57fa\u51c6\uff1b3) \u5f00\u53d1SpeechJudge-GRM\u751f\u6210\u5f0f\u5956\u52b1\u6a21\u578b\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\uff1aSFT\u5e26\u601d\u7ef4\u94fe\u63a8\u7406 + RL\u5f3a\u5316\u5b66\u4e60\u5904\u7406\u56f0\u96be\u6848\u4f8b\u3002", "result": "SpeechJudge-GRM\u5728\u8bc4\u4f30\u57fa\u51c6\u4e0a\u8fbe\u523077.2%\u51c6\u786e\u7387\uff08\u63a8\u7406\u65f6\u7f29\u653e\u540e79.4%\uff09\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edfBradley-Terry\u5956\u52b1\u6a21\u578b\uff0872.7%\uff09\u548c\u73b0\u6709\u6700\u4f73\u6a21\u578bGemini-2.5-Flash\uff08<70%\uff09\u3002", "conclusion": "SpeechJudge\u5957\u4ef6\u6709\u6548\u89e3\u51b3\u4e86\u8bed\u97f3\u5408\u6210\u4e2d\u4eba\u7c7b\u504f\u597d\u5bf9\u9f50\u7684\u6311\u6218\uff0c\u63d0\u51fa\u7684\u751f\u6210\u5f0f\u5956\u52b1\u6a21\u578b\u5728\u81ea\u7136\u5ea6\u8bc4\u4f30\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u53ef\u4f5c\u4e3a\u5956\u52b1\u51fd\u6570\u7528\u4e8e\u8bed\u97f3\u751f\u6210\u6a21\u578b\u7684\u540e\u8bad\u7ec3\u5bf9\u9f50\u3002"}}
{"id": "2511.08107", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.08107", "abs": "https://arxiv.org/abs/2511.08107", "authors": ["Chao Zhou", "Changsheng You", "Cong Zhou", "Hai Lin", "Yi Gong"], "title": "MA-enhanced Mixed Near-field and Far-field Covert Communications", "comment": "13 pages, 10 figures, submitted to IEEE for possible publication", "summary": "In this paper, we propose to employ a modular-based movable extremely large-scale array (XL-array) at Alice for enhancing covert communication performance. Compared with existing work that mostly considered either far-field or near-field covert communications, we consider in this paper a more general and practical mixed-field scenario, where multiple Bobs are located in either the near-field or far-field of Alice, in the presence of multiple near-field Willies. Specifically, we first consider a two-Bob-one-Willie system and show that conventional fixed-position XL-arrays suffer degraded sum-rate performance due to the energy-spread effect in mixed-field systems, which, however, can be greatly improved by subarray movement. On the other hand, for transmission covertness, it is revealed that sufficient angle difference between far-field Bob and Willie as well as adequate range difference between near-field Bob and Willie are necessary for ensuring covertness in fixed-position XL-array systems, while this requirement can be relaxed in movable XL-array systems thanks to flexible channel correlation control between Bobs and Willie. Next, for general system setups, we formulate an optimization problem to maximize the achievable sum-rate under covertness constraint. To solve this non-convex optimization problem, we first decompose it into two subproblems, corresponding to an inner problem for beamforming optimization given positions of subarrays and an outer problem for subarray movement optimization. Although these two subproblems are still non-convex, we obtain their high-quality solutions by using the successive convex approximation technique and devising a customized differential evolution algorithm, respectively. Last, numerical results demonstrate the effectiveness of proposed movable XL-array in balancing sum-rate and covert communication requirements.", "AI": {"tldr": "\u63d0\u51fa\u4f7f\u7528\u6a21\u5757\u5316\u53ef\u79fb\u52a8\u8d85\u5927\u89c4\u6a21\u9635\u5217\u6765\u589e\u5f3a\u9690\u853d\u901a\u4fe1\u6027\u80fd\uff0c\u89e3\u51b3\u4e86\u6df7\u5408\u573a\u573a\u666f\u4e0b\u7684\u80fd\u91cf\u6269\u6563\u95ee\u9898\uff0c\u901a\u8fc7\u5b50\u9635\u5217\u79fb\u52a8\u548c\u6ce2\u675f\u6210\u5f62\u4f18\u5316\u6765\u5e73\u8861\u901f\u7387\u548c\u9690\u853d\u6027\u8981\u6c42\u3002", "motivation": "\u73b0\u6709\u9690\u853d\u901a\u4fe1\u7814\u7a76\u4e3b\u8981\u8003\u8651\u8fdc\u573a\u6216\u8fd1\u573a\u573a\u666f\uff0c\u800c\u5b9e\u9645\u7cfb\u7edf\u5f80\u5f80\u662f\u6df7\u5408\u573a\u73af\u5883\uff0c\u591a\u4e2a\u7528\u6237\u53ef\u80fd\u4f4d\u4e8e\u4e0d\u540c\u573a\u57df\u3002\u56fa\u5b9a\u4f4d\u7f6e\u8d85\u5927\u89c4\u6a21\u9635\u5217\u5728\u6df7\u5408\u573a\u4e2d\u4f1a\u56e0\u80fd\u91cf\u6269\u6563\u6548\u5e94\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\uff0c\u9700\u8981\u66f4\u7075\u6d3b\u7684\u9635\u5217\u914d\u7f6e\u65b9\u6848\u3002", "method": "\u91c7\u7528\u6a21\u5757\u5316\u53ef\u79fb\u52a8\u8d85\u5927\u89c4\u6a21\u9635\u5217\uff0c\u5c06\u4f18\u5316\u95ee\u9898\u5206\u89e3\u4e3a\u6ce2\u675f\u6210\u5f62\u548c\u5b50\u9635\u5217\u79fb\u52a8\u4e24\u4e2a\u5b50\u95ee\u9898\uff0c\u5206\u522b\u4f7f\u7528\u9010\u6b21\u51f8\u8fd1\u4f3c\u6280\u672f\u548c\u5b9a\u5236\u5dee\u5206\u8fdb\u5316\u7b97\u6cd5\u6c42\u89e3\u3002", "result": "\u6570\u503c\u7ed3\u679c\u8868\u660e\uff0c\u53ef\u79fb\u52a8\u8d85\u5927\u89c4\u6a21\u9635\u5217\u80fd\u6709\u6548\u5e73\u8861\u53ef\u8fbe\u901f\u7387\u548c\u9690\u853d\u901a\u4fe1\u8981\u6c42\uff0c\u76f8\u6bd4\u56fa\u5b9a\u4f4d\u7f6e\u9635\u5217\u5728\u6df7\u5408\u573a\u573a\u666f\u4e0b\u6027\u80fd\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u53ef\u79fb\u52a8\u8d85\u5927\u89c4\u6a21\u9635\u5217\u901a\u8fc7\u7075\u6d3b\u63a7\u5236\u4fe1\u9053\u76f8\u5173\u6027\uff0c\u80fd\u591f\u514b\u670d\u6df7\u5408\u573a\u4e2d\u7684\u80fd\u91cf\u6269\u6563\u95ee\u9898\uff0c\u4e3a\u9690\u853d\u901a\u4fe1\u63d0\u4f9b\u66f4\u4f18\u7684\u6027\u80fd\u4fdd\u969c\u3002"}}
{"id": "2511.08496", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2511.08496", "abs": "https://arxiv.org/abs/2511.08496", "authors": ["Bingsong Bai", "Yizhong Geng", "Fengping Wang", "Cong Wang", "Puyuan Guo", "Yingming Gao", "Ya Li"], "title": "HQ-SVC: Towards High-Quality Zero-Shot Singing Voice Conversion in Low-Resource Scenarios", "comment": "Accepted by AAAI 2026 main technical track", "summary": "Zero-shot singing voice conversion (SVC) transforms a source singer's timbre to an unseen target speaker's voice while preserving melodic content without fine-tuning. Existing methods model speaker timbre and vocal content separately, losing essential acoustic information that degrades output quality while requiring significant computational resources. To overcome these limitations, we propose HQ-SVC, an efficient framework for high-quality zero-shot SVC. HQ-SVC first extracts jointly content and speaker features using a decoupled codec. It then enhances fidelity through pitch and volume modeling, preserving critical acoustic information typically lost in separate modeling approaches, and progressively refines outputs via differentiable signal processing and diffusion techniques. Evaluations confirm HQ-SVC significantly outperforms state-of-the-art zero-shot SVC methods in conversion quality and efficiency. Beyond voice conversion, HQ-SVC achieves superior voice naturalness compared to specialized audio super-resolution methods while natively supporting voice super-resolution tasks.", "AI": {"tldr": "HQ-SVC\u662f\u4e00\u4e2a\u9ad8\u6548\u7684\u9ad8\u8d28\u91cf\u96f6\u6837\u672c\u6b4c\u58f0\u8f6c\u6362\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u8026\u7f16\u89e3\u7801\u5668\u8054\u5408\u63d0\u53d6\u5185\u5bb9\u548c\u8bf4\u8bdd\u4eba\u7279\u5f81\uff0c\u7ed3\u5408\u97f3\u9ad8\u548c\u97f3\u91cf\u5efa\u6a21\u4ee5\u53ca\u53ef\u5fae\u4fe1\u53f7\u5904\u7406\u548c\u6269\u6563\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u8f6c\u6362\u8d28\u91cf\u548c\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5206\u522b\u5efa\u6a21\u8bf4\u8bdd\u4eba\u97f3\u8272\u548c\u58f0\u4e50\u5185\u5bb9\uff0c\u4e22\u5931\u4e86\u91cd\u8981\u7684\u58f0\u5b66\u4fe1\u606f\uff0c\u5bfc\u81f4\u8f93\u51fa\u8d28\u91cf\u4e0b\u964d\u4e14\u9700\u8981\u5927\u91cf\u8ba1\u7b97\u8d44\u6e90\u3002", "method": "\u4f7f\u7528\u89e3\u8026\u7f16\u89e3\u7801\u5668\u8054\u5408\u63d0\u53d6\u5185\u5bb9\u548c\u8bf4\u8bdd\u4eba\u7279\u5f81\uff0c\u901a\u8fc7\u97f3\u9ad8\u548c\u97f3\u91cf\u5efa\u6a21\u589e\u5f3a\u4fdd\u771f\u5ea6\uff0c\u91c7\u7528\u53ef\u5fae\u4fe1\u53f7\u5904\u7406\u548c\u6269\u6563\u6280\u672f\u9010\u6b65\u4f18\u5316\u8f93\u51fa\u3002", "result": "HQ-SVC\u5728\u8f6c\u6362\u8d28\u91cf\u548c\u6548\u7387\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u96f6\u6837\u672c\u6b4c\u58f0\u8f6c\u6362\u65b9\u6cd5\uff0c\u5728\u8bed\u97f3\u81ea\u7136\u5ea6\u65b9\u9762\u751a\u81f3\u4f18\u4e8e\u4e13\u95e8\u7684\u97f3\u9891\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\u3002", "conclusion": "HQ-SVC\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9ad8\u8d28\u91cf\u3001\u9ad8\u6548\u7684\u96f6\u6837\u672c\u6b4c\u58f0\u8f6c\u6362\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u4fdd\u7559\u5173\u952e\u58f0\u5b66\u4fe1\u606f\uff0c\u540c\u65f6\u539f\u751f\u652f\u6301\u8bed\u97f3\u8d85\u5206\u8fa8\u7387\u4efb\u52a1\u3002"}}
{"id": "2511.07955", "categories": ["cs.SD", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.07955", "abs": "https://arxiv.org/abs/2511.07955", "authors": ["Ziqian Zhang", "Min Huang", "Zhongzhe Xiao"], "title": "Speech Emotion Recognition with Phonation Excitation Information and Articulatory Kinematics", "comment": null, "summary": "Speech emotion recognition (SER) has advanced significantly for the sake of deep-learning methods, while textual information further enhances its performance. However, few studies have focused on the physiological information during speech production, which also encompasses speaker traits, including emotional states. To bridge this gap, we conducted a series of experiments to investigate the potential of the phonation excitation information and articulatory kinematics for SER. Due to the scarcity of training data for this purpose, we introduce a portrayed emotional dataset, STEM-E2VA, which includes audio and physiological data such as electroglottography (EGG) and electromagnetic articulography (EMA). EGG and EMA provide information of phonation excitation and articulatory kinematics, respectively. Additionally, we performed emotion recognition using estimated physiological data derived through inversion methods from speech, instead of collected EGG and EMA, to explore the feasibility of applying such physiological information in real-world SER. Experimental results confirm the effectiveness of incorporating physiological information about speech production for SER and demonstrate its potential for practical use in real-world scenarios.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5728\u8bed\u97f3\u60c5\u611f\u8bc6\u522b\u4e2d\u5f15\u5165\u751f\u7406\u4fe1\u606f\u7684\u6709\u6548\u6027\uff0c\u7279\u522b\u662f\u53d1\u58f0\u6fc0\u52b1\u4fe1\u606f\u548c\u53d1\u97f3\u8fd0\u52a8\u5b66\u4fe1\u606f\uff0c\u5e76\u901a\u8fc7STEM-E2VA\u6570\u636e\u96c6\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002", "motivation": "\u5f53\u524d\u8bed\u97f3\u60c5\u611f\u8bc6\u522b\u4e3b\u8981\u4f9d\u8d56\u6df1\u5ea6\u5b66\u4e60\u548c\u6587\u672c\u4fe1\u606f\uff0c\u4f46\u5f88\u5c11\u5173\u6ce8\u8bed\u97f3\u4ea7\u751f\u8fc7\u7a0b\u4e2d\u7684\u751f\u7406\u4fe1\u606f\uff0c\u8fd9\u4e9b\u4fe1\u606f\u540c\u6837\u5305\u542b\u8bf4\u8bdd\u8005\u7684\u60c5\u611f\u72b6\u6001\u7279\u5f81\u3002", "method": "\u5f15\u5165STEM-E2VA\u6570\u636e\u96c6\uff0c\u5305\u542b\u97f3\u9891\u548c\u751f\u7406\u6570\u636e\uff08EGG\u548cEMA\uff09\uff1b\u901a\u8fc7\u5012\u63a8\u65b9\u6cd5\u4ece\u8bed\u97f3\u4e2d\u4f30\u8ba1\u751f\u7406\u6570\u636e\uff0c\u8fdb\u884c\u60c5\u611f\u8bc6\u522b\u5b9e\u9a8c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u7ed3\u5408\u8bed\u97f3\u4ea7\u751f\u7684\u751f\u7406\u4fe1\u606f\u80fd\u6709\u6548\u63d0\u5347\u8bed\u97f3\u60c5\u611f\u8bc6\u522b\u6027\u80fd\uff0c\u5e76\u8bc1\u660e\u4e86\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u5e94\u7528\u7684\u53ef\u884c\u6027\u3002", "conclusion": "\u751f\u7406\u4fe1\u606f\u5728\u8bed\u97f3\u60c5\u611f\u8bc6\u522b\u4e2d\u5177\u6709\u91cd\u8981\u4ef7\u503c\uff0c\u80fd\u591f\u589e\u5f3a\u8bc6\u522b\u6027\u80fd\u5e76\u5177\u6709\u5b9e\u9645\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2511.08112", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.08112", "abs": "https://arxiv.org/abs/2511.08112", "authors": ["Tian Qiu", "Ruidong Li", "Cunhua Pan", "Taihaon Zhang", "Dongnan Xia", "Changhong Wang", "Hong Ren"], "title": "Mutual Coupling Aware Channel Estimation for RIS-Aided Multi-User mmWave Systems", "comment": null, "summary": "This paper proposes a three-stage uplink channel estimation protocol for reconfigurable intelligent surface (RIS)-aided multi-user (MU) millimeter-wave (mmWave) multiple-input single-output (MISO) systems, where both the base station (BS) and the RIS are equipped with uniform planar arrays (UPAs). The proposed approach explicitly accounts for the mutual coupling (MC) effect, modeled via scattering parameter multiport network theory. In Stage~I, a dimension-reduced subspace-based method is proposed to estimate the common angle of arrival (AoA) at the BS using the received signals across all users. In Stage~II, MC-aware cascaded channel estimation is performed for a typical user. The equivalent measurement vectors for each cascaded path are extracted and the reference column is reconstructed using a compressed sensing (CS)-based approach. By leveraging the structure of the cascaded channel, the reference column is rearranged to estimate the AoA at the RIS, thereby reducing the computational complexity associated with estimating other columns. Additionally, the common angle of departure (AoD) at the RIS is also obtained in this stage, which significantly reduces the pilot overhead for estimating the cascaded channels of other users in Stage~III. The RIS phase shift training matrix is designed to optimize performance in the presence of MC and outperforms random phase scheme. Simulation results validate that the proposed method yields better performance than the MC-unaware and existing approaches in terms of estimation accuracy and pilot efficiency.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u4e09\u9636\u6bb5\u4e0a\u884c\u94fe\u8def\u4fe1\u9053\u4f30\u8ba1\u534f\u8bae\uff0c\u7528\u4e8eRIS\u8f85\u52a9\u7684\u591a\u7528\u6237\u6beb\u7c73\u6ce2MISO\u7cfb\u7edf\uff0c\u8003\u8651\u4e86\u4e92\u8026\u6548\u5e94\uff0c\u901a\u8fc7\u538b\u7f29\u611f\u77e5\u65b9\u6cd5\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u5e76\u63d0\u9ad8\u4f30\u8ba1\u7cbe\u5ea6\u3002", "motivation": "\u5728RIS\u8f85\u52a9\u7684\u6beb\u7c73\u6ce2\u7cfb\u7edf\u4e2d\uff0c\u4f20\u7edf\u4fe1\u9053\u4f30\u8ba1\u65b9\u6cd5\u5ffd\u7565\u4e86\u4e92\u8026\u6548\u5e94\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002\u9700\u8981\u5f00\u53d1\u80fd\u591f\u5904\u7406\u4e92\u8026\u5e76\u964d\u4f4e\u5bfc\u9891\u5f00\u9500\u7684\u9ad8\u6548\u4f30\u8ba1\u65b9\u6cd5\u3002", "method": "\u4e09\u9636\u6bb5\u534f\u8bae\uff1a\u7b2c\u4e00\u9636\u6bb5\u4f7f\u7528\u964d\u7ef4\u5b50\u7a7a\u95f4\u65b9\u6cd5\u4f30\u8ba1BS\u7684\u516c\u5171\u5230\u8fbe\u89d2\uff1b\u7b2c\u4e8c\u9636\u6bb5\u8fdb\u884cMC\u611f\u77e5\u7ea7\u8054\u4fe1\u9053\u4f30\u8ba1\uff0c\u4f7f\u7528CS\u65b9\u6cd5\u91cd\u6784\u53c2\u8003\u5217\uff1b\u7b2c\u4e09\u9636\u6bb5\u5229\u7528\u516c\u5171\u79bb\u5f00\u89d2\u4f30\u8ba1\u5176\u4ed6\u7528\u6237\u7684\u7ea7\u8054\u4fe1\u9053\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6cd5\u5728\u4f30\u8ba1\u7cbe\u5ea6\u548c\u5bfc\u9891\u6548\u7387\u65b9\u9762\u4f18\u4e8e\u4e0d\u8003\u8651\u4e92\u8026\u7684\u73b0\u6709\u65b9\u6cd5\uff0cRIS\u76f8\u4f4d\u8bad\u7ec3\u77e9\u9635\u8bbe\u8ba1\u5728\u5b58\u5728\u4e92\u8026\u65f6\u6027\u80fd\u4f18\u4e8e\u968f\u673a\u76f8\u4f4d\u65b9\u6848\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86RIS\u8f85\u52a9\u6beb\u7c73\u6ce2\u7cfb\u7edf\u4e2d\u7684\u4e92\u8026\u95ee\u9898\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6\u548c\u5bfc\u9891\u5f00\u9500\uff0c\u63d0\u9ad8\u4e86\u4fe1\u9053\u4f30\u8ba1\u6027\u80fd\u3002"}}
{"id": "2511.08012", "categories": ["cs.SD", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.08012", "abs": "https://arxiv.org/abs/2511.08012", "authors": ["Haowen Li", "Zhengding Luo", "Dongyuan Shi", "Boxiang Wang", "Junwei Ji", "Ziyi Yang", "Woon-Seng Gan"], "title": "DOA Estimation with Lightweight Network on LLM-Aided Simulated Acoustic Scenes", "comment": null, "summary": "Direction-of-Arrival (DOA) estimation is critical in spatial audio and acoustic signal processing, with wide-ranging applications in real-world. Most existing DOA models are trained on synthetic data by convolving clean speech with room impulse responses (RIRs), which limits their generalizability due to constrained acoustic diversity. In this paper, we revisit DOA estimation using a recently introduced dataset constructed with the assistance of large language models (LLMs), which provides more realistic and diverse spatial audio scenes. We benchmark several representative neural-based DOA methods on this dataset and propose LightDOA, a lightweight DOA estimation model based on depthwise separable convolutions, specifically designed for mutil-channel input in varying environments. Experimental results show that LightDOA achieves satisfactory accuracy and robustness across various acoustic scenes while maintaining low computational complexity. This study not only highlights the potential of spatial audio synthesized with the assistance of LLMs in advancing robust and efficient DOA estimation research, but also highlights LightDOA as efficient solution for resource-constrained applications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86LightDOA\uff0c\u4e00\u4e2a\u57fa\u4e8e\u6df1\u5ea6\u53ef\u5206\u79bb\u5377\u79ef\u7684\u8f7b\u91cf\u7ea7DOA\u4f30\u8ba1\u6a21\u578b\uff0c\u5728LLM\u8f85\u52a9\u6784\u5efa\u7684\u591a\u6837\u5316\u7a7a\u95f4\u97f3\u9891\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u517c\u987e\u7cbe\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u73b0\u6709DOA\u6a21\u578b\u5927\u591a\u57fa\u4e8e\u5408\u6210\u6570\u636e\u8bad\u7ec3\uff0c\u58f0\u5b66\u591a\u6837\u6027\u6709\u9650\uff0c\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u3002\u672c\u6587\u65e8\u5728\u5229\u7528LLM\u8f85\u52a9\u6784\u5efa\u7684\u66f4\u771f\u5b9e\u591a\u6837\u7684\u7a7a\u95f4\u97f3\u9891\u6570\u636e\u96c6\uff0c\u5f00\u53d1\u66f4\u9c81\u68d2\u9ad8\u6548\u7684DOA\u4f30\u8ba1\u65b9\u6cd5\u3002", "method": "\u63d0\u51faLightDOA\u6a21\u578b\uff0c\u57fa\u4e8e\u6df1\u5ea6\u53ef\u5206\u79bb\u5377\u79ef\u4e13\u95e8\u8bbe\u8ba1\u7528\u4e8e\u591a\u901a\u9053\u8f93\u5165\uff0c\u9002\u5e94\u4e0d\u540c\u73af\u5883\u6761\u4ef6\uff0c\u4fdd\u6301\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660eLightDOA\u5728\u5404\u79cd\u58f0\u5b66\u573a\u666f\u4e0b\u90fd\u80fd\u8fbe\u5230\u6ee1\u610f\u7684\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u7ef4\u6301\u8f83\u4f4e\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "conclusion": "LLM\u8f85\u52a9\u5408\u6210\u7684\u7a7a\u95f4\u97f3\u9891\u6570\u636e\u5728\u63a8\u8fdb\u9c81\u68d2\u9ad8\u6548DOA\u4f30\u8ba1\u7814\u7a76\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0cLightDOA\u662f\u8d44\u6e90\u53d7\u9650\u5e94\u7528\u7684\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.08125", "categories": ["eess.SP", "cs.IT"], "pdf": "https://arxiv.org/pdf/2511.08125", "abs": "https://arxiv.org/abs/2511.08125", "authors": ["Askin Altinoklu", "Leila Musavian"], "title": "DMA-aided MU-MISO Systems for Power Splitting SWIPT via Lorentzian-Constrained Holography", "comment": "Submitted to IEEE ICC 2026", "summary": "This paper presents an optimal power splitting and beamforming design for co-located simultaneous wireless information and power transfer (SWIPT) users in Dynamic Metasurface Antenna (DMA)-aided multiuser multiple-input single-output (MISO) systems. The objective is to minimize transmit power while meeting users signal-to-interference-plus-noise ratio (SINR) and energy harvesting (EH) requirements. The problem is solved via an alternating optimization framework based on semidefinite programming (SDP), where metasurface tunability follows Lorentzian-constrained holography (LCH). In contrast to traditional beamforming architectures, DMA-assisted architectures reduce the need for RF chains and phase shifters but require optimization under the Lorentzian constraint limiting the amplitude and phase optimizations. Hence, the proposed method integrates several LCH schemes, including the recently proposed adaptive-radius LCH (ARLCH), and evaluates nonlinear EH models and circuit noise effects. Simulation results show that the proposed design significantly reduces transmit power compared with baseline methods, highlighting the efficiency of ARLCH and optimal power splitting in DMA-assisted SWIPT systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8eDMA\u8f85\u52a9\u591a\u7528\u6237MISO\u7cfb\u7edf\u4e2dSWIPT\u7528\u6237\u7684\u6700\u4f18\u529f\u7387\u5206\u914d\u548c\u6ce2\u675f\u6210\u5f62\u8bbe\u8ba1\uff0c\u901a\u8fc7\u4ea4\u66ff\u4f18\u5316\u6846\u67b6\u6700\u5c0f\u5316\u53d1\u5c04\u529f\u7387\uff0c\u540c\u65f6\u6ee1\u8db3\u7528\u6237\u7684SINR\u548c\u80fd\u91cf\u6536\u96c6\u8981\u6c42\u3002", "motivation": "\u4f20\u7edf\u6ce2\u675f\u6210\u5f62\u67b6\u6784\u9700\u8981\u5927\u91cfRF\u94fe\u548c\u79fb\u76f8\u5668\uff0c\u800cDMA\u8f85\u52a9\u67b6\u6784\u53ef\u4ee5\u964d\u4f4e\u8fd9\u4e9b\u9700\u6c42\uff0c\u4f46\u5728\u6d1b\u4f26\u5179\u7ea6\u675f\u4e0b\u9700\u8981\u4f18\u5316\uff0c\u8be5\u7ea6\u675f\u9650\u5236\u4e86\u5e45\u5ea6\u548c\u76f8\u4f4d\u4f18\u5316\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u534a\u5b9a\u89c4\u5212\u7684\u4ea4\u66ff\u4f18\u5316\u6846\u67b6\uff0c\u96c6\u6210\u591a\u79cdLCH\u65b9\u6848\uff08\u5305\u62ec\u81ea\u9002\u5e94\u534a\u5f84LCH\uff09\uff0c\u5e76\u8bc4\u4f30\u975e\u7ebf\u6027EH\u6a21\u578b\u548c\u7535\u8def\u566a\u58f0\u6548\u5e94\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u8bbe\u8ba1\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u663e\u8457\u964d\u4f4e\u4e86\u53d1\u5c04\u529f\u7387\uff0c\u7a81\u663e\u4e86ARLCH\u548c\u6700\u4f18\u529f\u7387\u5206\u914d\u5728DMA\u8f85\u52a9SWIPT\u7cfb\u7edf\u4e2d\u7684\u6548\u7387\u3002", "conclusion": "DMA\u8f85\u52a9\u67b6\u6784\u7ed3\u5408\u81ea\u9002\u5e94\u534a\u5f84LCH\u65b9\u6848\u548c\u6700\u4f18\u529f\u7387\u5206\u914d\u7b56\u7565\uff0c\u80fd\u591f\u6709\u6548\u964d\u4f4e\u7cfb\u7edf\u53d1\u5c04\u529f\u7387\uff0c\u63d0\u5347SWIPT\u7cfb\u7edf\u7684\u6027\u80fd\u3002"}}
{"id": "2511.08237", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.08237", "abs": "https://arxiv.org/abs/2511.08237", "authors": ["Humera Hameed", "Waqas Aman", "Muhammad Mahboob Ur Rahman", "Ali Arshad Nasir"], "title": "Effective Capacity Analysis of Joint Near and Far-Field Communication in 6G URLLC Networks", "comment": "8 pages, 5 figures, under review with a conference", "summary": "The emergence of 6G networks enables simultaneous near-field and far-field communications through extremely large antenna arrays and high carrier frequencies. While these regimes enhance spatial multiplexing and link capacity, their coexistence poses new challenges in ensuring quality-of-service (QoS) guarantees for delay-sensitive applications. This paper presents an effective capacity (EC) analysis framework that jointly models near- and far-field communication regimes under distance estimation uncertainty. The user location is modeled as a random variable spanning both propagation regions, and tractable closed-form expression for the EC is derived to quantify delay performance. Numerical results illustrate the impact of estimation variance, QoS exponent, far-field boundary and near- field boundary (Fraunhofer distance) on EC performance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u6709\u6548\u5bb9\u91cf\u5206\u6790\u6846\u67b6\uff0c\u8054\u5408\u5efa\u6a21\u8fd1\u573a\u548c\u8fdc\u573a\u901a\u4fe1\u673a\u5236\uff0c\u8003\u8651\u8ddd\u79bb\u4f30\u8ba1\u4e0d\u786e\u5b9a\u6027\uff0c\u63a8\u5bfc\u51fa\u6709\u6548\u5bb9\u91cf\u7684\u95ed\u5f0f\u8868\u8fbe\u5f0f\u6765\u91cf\u5316\u5ef6\u8fdf\u6027\u80fd\u3002", "motivation": "6G\u7f51\u7edc\u4e2d\u6781\u5927\u5929\u7ebf\u9635\u5217\u548c\u9ad8\u8f7d\u6ce2\u9891\u7387\u4f7f\u5f97\u8fd1\u573a\u548c\u8fdc\u573a\u901a\u4fe1\u5171\u5b58\uff0c\u8fd9\u5bf9\u786e\u4fdd\u5ef6\u8fdf\u654f\u611f\u5e94\u7528\u7684QoS\u4fdd\u8bc1\u63d0\u51fa\u4e86\u65b0\u6311\u6218\u3002", "method": "\u5c06\u7528\u6237\u4f4d\u7f6e\u5efa\u6a21\u4e3a\u8de8\u8d8a\u4e24\u4e2a\u4f20\u64ad\u533a\u57df\u7684\u968f\u673a\u53d8\u91cf\uff0c\u5728\u8ddd\u79bb\u4f30\u8ba1\u4e0d\u786e\u5b9a\u6027\u7684\u60c5\u51b5\u4e0b\u63a8\u5bfc\u51fa\u6709\u6548\u5bb9\u91cf\u7684\u95ed\u5f0f\u8868\u8fbe\u5f0f\u3002", "result": "\u6570\u503c\u7ed3\u679c\u663e\u793a\u4e86\u4f30\u8ba1\u65b9\u5dee\u3001QoS\u6307\u6570\u3001\u8fdc\u573a\u8fb9\u754c\u548c\u8fd1\u573a\u8fb9\u754c\u5bf9\u6709\u6548\u5bb9\u91cf\u6027\u80fd\u7684\u5f71\u54cd\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u591f\u91cf\u5316\u8fd1\u573a\u548c\u8fdc\u573a\u901a\u4fe1\u5171\u5b58\u573a\u666f\u4e0b\u7684\u5ef6\u8fdf\u6027\u80fd\uff0c\u4e3a6G\u7f51\u7edcQoS\u4fdd\u8bc1\u63d0\u4f9b\u5206\u6790\u5de5\u5177\u3002"}}
{"id": "2511.08261", "categories": ["cs.SD", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.08261", "abs": "https://arxiv.org/abs/2511.08261", "authors": ["Raphael Schwinger", "Ben McEwen", "Vincent S. Kather", "Ren\u00e9 Heinrich", "Lukas Rauch", "Sven Tomforde"], "title": "Uncertainty Calibration of Multi-Label Bird Sound Classifiers", "comment": "Under review at ICAART 2026", "summary": "Passive acoustic monitoring enables large-scale biodiversity assessment, but reliable classification of bioacoustic sounds requires not only high accuracy but also well-calibrated uncertainty estimates to ground decision-making. In bioacoustics, calibration is challenged by overlapping vocalisations, long-tailed species distributions, and distribution shifts between training and deployment data. The calibration of multi-label deep learning classifiers within the domain of bioacoustics has not yet been assessed. We systematically benchmark the calibration of four state-of-the-art multi-label bird sound classifiers on the BirdSet benchmark, evaluating both global, per-dataset and per-class calibration using threshold-free calibration metrics (ECE, MCS) alongside discrimination metrics (cmAP). Model calibration varies significantly across datasets and classes. While Perch v2 and ConvNeXt$_{BS}$ show better global calibration, results vary between datasets. Both models indicate consistent underconfidence, while AudioProtoPNet and BirdMAE are mostly overconfident. Surprisingly, calibration seems to be better for less frequent classes. Using simple post hoc calibration methods we demonstrate a straightforward way to improve calibration. A small labelled calibration set is sufficient to significantly improve calibration with Platt scaling, while global calibration parameters suffer from dataset variability. Our findings highlight the importance of evaluating and improving uncertainty calibration in bioacoustic classifiers.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u4e86\u56db\u79cd\u591a\u6807\u7b7e\u9e1f\u7c7b\u58f0\u97f3\u5206\u7c7b\u5668\u7684\u6821\u51c6\u6027\u80fd\uff0c\u53d1\u73b0\u6a21\u578b\u6821\u51c6\u5728\u4e0d\u540c\u6570\u636e\u96c6\u548c\u7c7b\u522b\u95f4\u5dee\u5f02\u663e\u8457\uff0c\u5e76\u8bc1\u660e\u7b80\u5355\u7684\u540e\u5904\u7406\u6821\u51c6\u65b9\u6cd5\u80fd\u663e\u8457\u6539\u5584\u6821\u51c6\u6548\u679c\u3002", "motivation": "\u88ab\u52a8\u58f0\u5b66\u76d1\u6d4b\u9700\u8981\u4e0d\u4ec5\u9ad8\u7cbe\u5ea6\u8fd8\u8981\u826f\u597d\u6821\u51c6\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\uff0c\u4f46\u751f\u7269\u58f0\u5b66\u4e2d\u7684\u6821\u51c6\u9762\u4e34\u91cd\u53e0\u53d1\u58f0\u3001\u957f\u5c3e\u7269\u79cd\u5206\u5e03\u548c\u8bad\u7ec3\u90e8\u7f72\u6570\u636e\u5206\u5e03\u504f\u79fb\u7b49\u6311\u6218\u3002", "method": "\u5728BirdSet\u57fa\u51c6\u4e0a\u7cfb\u7edf\u8bc4\u4f30\u56db\u79cd\u6700\u5148\u8fdb\u7684\u591a\u6807\u7b7e\u9e1f\u7c7b\u58f0\u97f3\u5206\u7c7b\u5668\u7684\u6821\u51c6\u6027\u80fd\uff0c\u4f7f\u7528\u65e0\u9608\u503c\u6821\u51c6\u6307\u6807(ECE, MCS)\u548c\u5224\u522b\u6307\u6807(cmAP)\uff0c\u5e76\u5e94\u7528\u7b80\u5355\u7684\u540e\u5904\u7406\u6821\u51c6\u65b9\u6cd5\u3002", "result": "\u6a21\u578b\u6821\u51c6\u5728\u4e0d\u540c\u6570\u636e\u96c6\u548c\u7c7b\u522b\u95f4\u5dee\u5f02\u663e\u8457\uff0cPerch v2\u548cConvNeXtBS\u663e\u793a\u66f4\u597d\u7684\u5168\u5c40\u6821\u51c6\u4f46\u5b58\u5728\u4e00\u81f4\u7684\u4e0d\u81ea\u4fe1\uff0cAudioProtoPNet\u548cBirdMAE\u5927\u591a\u8fc7\u5ea6\u81ea\u4fe1\u3002\u4ee4\u4eba\u60ca\u8bb6\u7684\u662f\uff0c\u8f83\u5c11\u51fa\u73b0\u7684\u7c7b\u522b\u6821\u51c6\u66f4\u597d\u3002", "conclusion": "\u8bc4\u4f30\u548c\u6539\u8fdb\u751f\u7269\u58f0\u5b66\u5206\u7c7b\u5668\u7684\u4e0d\u786e\u5b9a\u6027\u6821\u51c6\u81f3\u5173\u91cd\u8981\uff0c\u4f7f\u7528\u5c0f\u578b\u6807\u8bb0\u6821\u51c6\u96c6\u901a\u8fc7Platt\u7f29\u653e\u53ef\u663e\u8457\u6539\u5584\u6821\u51c6\u6548\u679c\u3002"}}
{"id": "2511.08273", "categories": ["eess.SP", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.08273", "abs": "https://arxiv.org/abs/2511.08273", "authors": ["Minh Xuan Bui", "Nguyen Thien Dat", "Van Hong Lam", "Tran Le Anh Quan", "Pham Hung Anh", "Mai Dong Xuan", "Ke Wang"], "title": "Wide Tuning Range and Low Noise Voltage Control Oscillators for 5G Technology", "comment": null, "summary": "This paper presents the analytical design of a new wide tuning range and low-noise millimeter-wave voltage control oscillators (VCO) for 5G technology. The small signal model analysis and phase noise of the VCOs will be presented to evaluate the start-up oscillation condition, oscillation frequency, and phase noise affecting factors. Theoretical analysis and simulation results show the outperformance of the proposed cascode cross-couple LC VCO topology compared to the conventional cross-coupled LC VCO in terms of frequency tuning range, VCO gain and phase noise level.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e5G\u6280\u672f\u7684\u5bbd\u8c03\u8c10\u8303\u56f4\u3001\u4f4e\u566a\u58f0\u6beb\u7c73\u6ce2\u538b\u63a7\u632f\u8361\u5668(VCO)\u7684\u8bbe\u8ba1\u65b9\u6848\uff0c\u901a\u8fc7\u5c0f\u4fe1\u53f7\u6a21\u578b\u5206\u6790\u548c\u76f8\u4f4d\u566a\u58f0\u8bc4\u4f30\uff0c\u5c55\u793a\u4e86\u7ea7\u8054\u4ea4\u53c9\u8026\u5408LC VCO\u62d3\u6251\u5728\u9891\u7387\u8c03\u8c10\u8303\u56f4\u3001VCO\u589e\u76ca\u548c\u76f8\u4f4d\u566a\u58f0\u65b9\u9762\u7684\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u4e3a5G\u6280\u672f\u5f00\u53d1\u5177\u6709\u5bbd\u8c03\u8c10\u8303\u56f4\u548c\u4f4e\u566a\u58f0\u7279\u6027\u7684\u6beb\u7c73\u6ce2\u538b\u63a7\u632f\u8361\u5668\uff0c\u4ee5\u6ee1\u8db35G\u901a\u4fe1\u7cfb\u7edf\u5bf9\u9ad8\u6027\u80fd\u9891\u7387\u6e90\u7684\u9700\u6c42\u3002", "method": "\u91c7\u7528\u7ea7\u8054\u4ea4\u53c9\u8026\u5408LC VCO\u62d3\u6251\u7ed3\u6784\uff0c\u8fdb\u884c\u5c0f\u4fe1\u53f7\u6a21\u578b\u5206\u6790\u548c\u76f8\u4f4d\u566a\u58f0\u5206\u6790\uff0c\u8bc4\u4f30\u8d77\u632f\u6761\u4ef6\u3001\u632f\u8361\u9891\u7387\u548c\u76f8\u4f4d\u566a\u58f0\u5f71\u54cd\u56e0\u7d20\uff0c\u5e76\u4e0e\u4f20\u7edf\u4ea4\u53c9\u8026\u5408LC VCO\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u7406\u8bba\u5206\u6790\u548c\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u7ea7\u8054\u4ea4\u53c9\u8026\u5408LC VCO\u62d3\u6251\u5728\u9891\u7387\u8c03\u8c10\u8303\u56f4\u3001VCO\u589e\u76ca\u548c\u76f8\u4f4d\u566a\u58f0\u6c34\u5e73\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u4ea4\u53c9\u8026\u5408LC VCO\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u7684\u7ea7\u8054\u4ea4\u53c9\u8026\u5408LC VCO\u62d3\u6251\u4e3a5G\u6beb\u7c73\u6ce2\u5e94\u7528\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6027\u80fd\u7684\u538b\u63a7\u632f\u8361\u5668\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u5173\u952e\u6027\u80fd\u6307\u6807\u4e0a\u4f18\u4e8e\u4f20\u7edf\u7ed3\u6784\u3002"}}
{"id": "2511.08383", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.08383", "abs": "https://arxiv.org/abs/2511.08383", "authors": ["Dhrumil Bhatt"], "title": "Dynamic Hybrid Resource Utilisation and MCS-based Intelligent Layering", "comment": null, "summary": "The coexistence of heterogeneous service classes in 5G Enhanced Mobile Broadband (eMBB), Ultra-Reliable Low Latency Communication (URLLC), and Massive Machine-Type Communication (mMTC) poses major challenges for meeting diverse Quality-of-Service (QoS) requirements under limited spectrum and power resources. Existing radio access network (RAN) slicing schemes typically optimise isolated layers or objectives, lacking physical-layer realism, slot-level adaptability, and interpretable per-slice performance metrics. This paper presents a joint optimisation framework that integrates Dynamic Hybrid Resource Utilisation with MCS-Based Intelligent Layering, formulated as a mixed-integer linear program (MILP) that jointly allocates bandwidth, power, and modulation and coding scheme (MCS) indices per slice. The model incorporates finite blocklength effects, channel misreporting, and correlated fading to ensure realistic operation. Two modes are implemented: a Baseline Mode that ensures resource-efficient QoS feasibility, and an Ideal-Chaser Mode that minimises deviation from ideal per-slice rates. Simulation results show that the proposed approach achieves energy efficiencies above $10^7$~kb/J in Baseline Mode and sub-millisecond latency with near-ideal throughput in Ideal-Chaser Mode, outperforming recent optimisation and learning-based methods in delay, fairness, and reliability. The framework provides a unified, interpretable, and computationally tractable solution for dynamic cross-layer resource management in 5G and beyond networks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8054\u5408\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u6df7\u5408\u8d44\u6e90\u5229\u7528\u548c\u57fa\u4e8eMCS\u7684\u667a\u80fd\u5206\u5c42\uff0c\u89e3\u51b35G\u5f02\u6784\u670d\u52a1\u7c7b\u522b\u7684\u8d44\u6e90\u5206\u914d\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u8de8\u5c42\u8d44\u6e90\u7ba1\u7406\u3002", "motivation": "5G\u7f51\u7edc\u4e2deMBB\u3001URLLC\u548cmMTC\u7b49\u5f02\u6784\u670d\u52a1\u7c7b\u522b\u5171\u5b58\uff0c\u5728\u6709\u9650\u7684\u9891\u8c31\u548c\u529f\u7387\u8d44\u6e90\u4e0b\u6ee1\u8db3\u591a\u6837\u5316QoS\u9700\u6c42\u9762\u4e34\u91cd\u5927\u6311\u6218\u3002\u73b0\u6709RAN\u5207\u7247\u65b9\u6848\u901a\u5e38\u4f18\u5316\u5b64\u7acb\u5c42\u6216\u76ee\u6807\uff0c\u7f3a\u4e4f\u7269\u7406\u5c42\u771f\u5b9e\u6027\u3001\u65f6\u9699\u7ea7\u9002\u5e94\u6027\u548c\u53ef\u89e3\u91ca\u7684\u5207\u7247\u6027\u80fd\u6307\u6807\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u6df7\u5408\u6574\u6570\u7ebf\u6027\u89c4\u5212\uff08MILP\uff09\u6846\u67b6\uff0c\u8054\u5408\u5206\u914d\u6bcf\u4e2a\u5207\u7247\u7684\u5e26\u5bbd\u3001\u529f\u7387\u548c\u8c03\u5236\u7f16\u7801\u65b9\u6848\uff08MCS\uff09\u7d22\u5f15\u3002\u8be5\u6a21\u578b\u5305\u542b\u6709\u9650\u5757\u957f\u6548\u5e94\u3001\u4fe1\u9053\u8bef\u62a5\u548c\u76f8\u5173\u8870\u843d\uff0c\u786e\u4fdd\u5b9e\u9645\u8fd0\u884c\u3002\u5b9e\u73b0\u4e86\u4e24\u79cd\u6a21\u5f0f\uff1a\u786e\u4fdd\u8d44\u6e90\u9ad8\u6548QoS\u53ef\u884c\u6027\u7684\u57fa\u7ebf\u6a21\u5f0f\uff0c\u4ee5\u53ca\u6700\u5c0f\u5316\u4e0e\u7406\u60f3\u5207\u7247\u901f\u7387\u504f\u5dee\u7684\u7406\u60f3\u8ffd\u8e2a\u6a21\u5f0f\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u663e\u793a\uff0c\u57fa\u7ebf\u6a21\u5f0f\u5b9e\u73b0\u4e86\u8d85\u8fc710^7 kb/J\u7684\u80fd\u6548\uff0c\u7406\u60f3\u8ffd\u8e2a\u6a21\u5f0f\u5b9e\u73b0\u4e86\u4e9a\u6beb\u79d2\u7ea7\u5ef6\u8fdf\u548c\u63a5\u8fd1\u7406\u60f3\u7684\u541e\u5410\u91cf\uff0c\u5728\u5ef6\u8fdf\u3001\u516c\u5e73\u6027\u548c\u53ef\u9760\u6027\u65b9\u9762\u4f18\u4e8e\u6700\u8fd1\u7684\u4f18\u5316\u548c\u57fa\u4e8e\u5b66\u4e60\u7684\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a5G\u53ca\u672a\u6765\u7f51\u7edc\u4e2d\u7684\u52a8\u6001\u8de8\u5c42\u8d44\u6e90\u7ba1\u7406\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u3001\u53ef\u89e3\u91ca\u4e14\u8ba1\u7b97\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.08416", "categories": ["eess.SP", "cs.IT", "cs.LG", "cs.MM"], "pdf": "https://arxiv.org/pdf/2511.08416", "abs": "https://arxiv.org/abs/2511.08416", "authors": ["Hai-Long Qin", "Jincheng Dai", "Guo Lu", "Shuo Shao", "Sixian Wang", "Tongda Xu", "Wenjun Zhang", "Ping Zhang", "Khaled B. Letaief"], "title": "Generative AI Meets 6G and Beyond: Diffusion Models for Semantic Communications", "comment": "Under review, GitHub repository: https://github.com/qin-jingyun/Awesome-DiffComm", "summary": "Semantic communications mark a paradigm shift from bit-accurate transmission toward meaning-centric communication, essential as wireless systems approach theoretical capacity limits. The emergence of generative AI has catalyzed generative semantic communications, where receivers reconstruct content from minimal semantic cues by leveraging learned priors. Among generative approaches, diffusion models stand out for their superior generation quality, stable training dynamics, and rigorous theoretical foundations. However, the field currently lacks systematic guidance connecting diffusion techniques to communication system design, forcing researchers to navigate disparate literatures. This article provides the first comprehensive tutorial on diffusion models for generative semantic communications. We present score-based diffusion foundations and systematically review three technical pillars: conditional diffusion for controllable generation, efficient diffusion for accelerated inference, and generalized diffusion for cross-domain adaptation. In addition, we introduce an inverse problem perspective that reformulates semantic decoding as posterior inference, bridging semantic communications with computational imaging. Through analysis of human-centric, machine-centric, and agent-centric scenarios, we illustrate how diffusion models enable extreme compression while maintaining semantic fidelity and robustness. By bridging generative AI innovations with communication system design, this article aims to establish diffusion models as foundational components of next-generation wireless networks and beyond.", "AI": {"tldr": "\u672c\u6587\u63d0\u4f9b\u4e86\u5173\u4e8e\u6269\u6563\u6a21\u578b\u5728\u751f\u6210\u5f0f\u8bed\u4e49\u901a\u4fe1\u4e2d\u7684\u9996\u4e2a\u5168\u9762\u6559\u7a0b\uff0c\u7cfb\u7edf\u4ecb\u7ecd\u4e86\u6761\u4ef6\u6269\u6563\u3001\u9ad8\u6548\u6269\u6563\u548c\u5e7f\u4e49\u6269\u6563\u4e09\u5927\u6280\u672f\u652f\u67f1\uff0c\u5e76\u5c06\u8bed\u4e49\u89e3\u7801\u91cd\u65b0\u8868\u8ff0\u4e3a\u540e\u9a8c\u63a8\u7406\u95ee\u9898\u3002", "motivation": "\u968f\u7740\u65e0\u7ebf\u7cfb\u7edf\u63a5\u8fd1\u7406\u8bba\u5bb9\u91cf\u6781\u9650\uff0c\u8bed\u4e49\u901a\u4fe1\u4ece\u6bd4\u7279\u7cbe\u786e\u4f20\u8f93\u8f6c\u5411\u4ee5\u610f\u4e49\u4e3a\u4e2d\u5fc3\u7684\u901a\u4fe1\u3002\u751f\u6210\u5f0fAI\u7684\u51fa\u73b0\u50ac\u751f\u4e86\u751f\u6210\u5f0f\u8bed\u4e49\u901a\u4fe1\uff0c\u4f46\u8be5\u9886\u57df\u7f3a\u4e4f\u5c06\u6269\u6563\u6280\u672f\u4e0e\u901a\u4fe1\u7cfb\u7edf\u8bbe\u8ba1\u8fde\u63a5\u7684\u7cfb\u7edf\u6027\u6307\u5bfc\u3002", "method": "\u63d0\u51fa\u4e86\u57fa\u4e8e\u5206\u6570\u7684\u6269\u6563\u6a21\u578b\u57fa\u7840\uff0c\u7cfb\u7edf\u56de\u987e\u4e86\u4e09\u5927\u6280\u672f\u652f\u67f1\uff1a\u6761\u4ef6\u6269\u6563\u7528\u4e8e\u53ef\u63a7\u751f\u6210\u3001\u9ad8\u6548\u6269\u6563\u7528\u4e8e\u52a0\u901f\u63a8\u7406\u3001\u5e7f\u4e49\u6269\u6563\u7528\u4e8e\u8de8\u57df\u9002\u5e94\u3002\u5f15\u5165\u4e86\u9006\u95ee\u9898\u89c6\u89d2\uff0c\u5c06\u8bed\u4e49\u89e3\u7801\u91cd\u65b0\u8868\u8ff0\u4e3a\u540e\u9a8c\u63a8\u7406\u3002", "result": "\u901a\u8fc7\u5206\u6790\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u3001\u4ee5\u673a\u5668\u4e3a\u4e2d\u5fc3\u548c\u4ee5\u667a\u80fd\u4f53\u4e3a\u4e2d\u5fc3\u7684\u573a\u666f\uff0c\u5c55\u793a\u4e86\u6269\u6563\u6a21\u578b\u5982\u4f55\u5728\u4fdd\u6301\u8bed\u4e49\u4fdd\u771f\u5ea6\u548c\u9c81\u68d2\u6027\u7684\u540c\u65f6\u5b9e\u73b0\u6781\u7aef\u538b\u7f29\u3002", "conclusion": "\u901a\u8fc7\u5c06\u751f\u6210\u5f0fAI\u521b\u65b0\u4e0e\u901a\u4fe1\u7cfb\u7edf\u8bbe\u8ba1\u76f8\u7ed3\u5408\uff0c\u672c\u6587\u65e8\u5728\u5c06\u6269\u6563\u6a21\u578b\u786e\u7acb\u4e3a\u4e0b\u4e00\u4ee3\u65e0\u7ebf\u7f51\u7edc\u53ca\u66f4\u5e7f\u6cdb\u9886\u57df\u7684\u57fa\u7840\u7ec4\u4ef6\u3002"}}
{"id": "2511.08474", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.08474", "abs": "https://arxiv.org/abs/2511.08474", "authors": ["Jialiang Zhu", "Hamza Haif", "Abdelali Arous", "Huseyin Arslan", "Arman Farhang"], "title": "Waveform-domain NOMA: An Enabler for ISAC in Uplink Transmission", "comment": null, "summary": "According to the recent 3GPP decisions on 6G air interface, orthogonal frequency-division multiplexing (OFDM)-based waveforms are the primary candidates for future integrated sensing and communication (ISAC) systems. In this paper, we consider a monostatic sensing scenario in which OFDM is used for the downlink and its reflected echo signal is used for sensing. OFDM and discrete Fourier transform-spread OFDM (DFT-s-OFDM) are the options for uplink transmission. When OFDM is used in the uplink, the power difference between this signal and the echo signal leads to a power-domain non-orthogonal multiple access (PD-NOMA) scenario. In contrast, adopting DFT-s-OFDM as uplink signal enables a waveform-domain NOMA(WD-NOMA). Affine frequency-division multiplexing (AFDM) and orthogonal time frequency space (OTFS) have been proven to be DFT-s-OFDM based waveforms. This work focuses on such a WD-NOMA system, where AFDM or OTFS is used as uplink waveform and OFDM is employed for downlink transmission and sensing. We show that the OFDM signal exhibits additive white Gaussian noise (AWGN)-like behavior in the affine domain, allowing it to be modeled as white noise in uplink symbol detection. To enable accurate data detection performance, an AFDM frame design and a noise power estimation (NPE) method are developed. Furthermore, a two-dimensional orthogonal matching pursuit (2D-OMP) algorithm is applied for sensing by iteratively identifying delay-Doppler components of each target. Simulation results demonstrate that the WD-NOMA ISAC system, employing either AFDM or OTFS, outperforms the PD-NOMA ISAC system that uses only the OFDM waveform in terms of bit error rate (BER) performance. Furthermore, the proposed NPE method yields additional improvements in BER.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u57286G\u96c6\u6210\u611f\u77e5\u4e0e\u901a\u4fe1\u7cfb\u7edf\u4e2d\uff0c\u4f7f\u7528AFDM\u6216OTFS\u4f5c\u4e3a\u4e0a\u884c\u6ce2\u5f62\u3001OFDM\u4f5c\u4e3a\u4e0b\u884c\u548c\u611f\u77e5\u6ce2\u5f62\u7684\u6ce2\u5f62\u57df\u975e\u6b63\u4ea4\u591a\u5740\u7cfb\u7edf\uff0c\u76f8\u6bd4\u529f\u7387\u57dfNOMA\u7cfb\u7edf\u5728\u8bef\u7801\u7387\u6027\u80fd\u4e0a\u6709\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u6839\u636e3GPP\u5bf96G\u7a7a\u53e3\u7684\u51b3\u7b56\uff0cOFDM\u6ce2\u5f62\u662f\u672a\u6765ISAC\u7cfb\u7edf\u7684\u4e3b\u8981\u5019\u9009\u65b9\u6848\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u4e0a\u884cOFDM\u4e0e\u4e0b\u884c\u611f\u77e5\u56de\u6ce2\u4fe1\u53f7\u4e4b\u95f4\u7684\u529f\u7387\u5dee\u5f02\u95ee\u9898\uff0c\u63a2\u7d22\u66f4\u4f18\u7684\u6ce2\u5f62\u7ec4\u5408\u65b9\u6848\u3002", "method": "\u91c7\u7528AFDM\u6216OTFS\u4f5c\u4e3a\u4e0a\u884c\u6ce2\u5f62\uff0cOFDM\u7528\u4e8e\u4e0b\u884c\u4f20\u8f93\u548c\u611f\u77e5\u3002\u5f00\u53d1\u4e86AFDM\u5e27\u8bbe\u8ba1\u548c\u566a\u58f0\u529f\u7387\u4f30\u8ba1\u65b9\u6cd5\uff0c\u5e76\u5e94\u7528\u4e8c\u7ef4\u6b63\u4ea4\u5339\u914d\u8ffd\u8e2a\u7b97\u6cd5\u8fdb\u884c\u611f\u77e5\uff0c\u8fed\u4ee3\u8bc6\u522b\u76ee\u6807\u7684\u65f6\u5ef6-\u591a\u666e\u52d2\u5206\u91cf\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u4f7f\u7528AFDM\u6216OTFS\u7684WD-NOMA ISAC\u7cfb\u7edf\u5728\u8bef\u7801\u7387\u6027\u80fd\u4e0a\u4f18\u4e8e\u4ec5\u4f7f\u7528OFDM\u6ce2\u5f62\u7684PD-NOMA\u7cfb\u7edf\uff0c\u4e14\u63d0\u51fa\u7684NPE\u65b9\u6cd5\u8fdb\u4e00\u6b65\u6539\u5584\u4e86BER\u6027\u80fd\u3002", "conclusion": "\u6ce2\u5f62\u57dfNOMA\u7cfb\u7edf\u5728\u96c6\u6210\u611f\u77e5\u4e0e\u901a\u4fe1\u5e94\u7528\u4e2d\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0cAFDM\u548cOTFS\u4f5c\u4e3a\u4e0a\u884c\u6ce2\u5f62\u80fd\u591f\u6709\u6548\u63d0\u5347\u7cfb\u7edf\u6027\u80fd\uff0c\u4e3a6G ISAC\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.08504", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.08504", "abs": "https://arxiv.org/abs/2511.08504", "authors": ["Kailong Wang", "Athina Petropulu"], "title": "Low Overhead Channel Estimation in MIMO OTFS Wireless Communication Systems", "comment": null, "summary": "Orthogonal Time Frequency Space (OTFS) modulation has recently garnered attention due to its robustness in high-mobility wireless communication environments. In OTFS, the data symbols are mapped to the Doppler-Delay (DD) domain. In this paper, we address bandwidth-efficient estimation of channel state information (CSI) for MIMO OTFS systems. Existing channel estimation techniques either require non-overlapped DD-domain pilots and associated guard regions across multiple antennas, sacrificing significant communication rate as the number of transmit antennas increases, or sophisticated algorithms to handle overlapped pilots, escalating the cost and complexity of receivers. We introduce a novel pilot-aided channel estimation method that enjoys low overhead while achieving high performance. Our approach embeds pilots within each OTFS burst in the Time-Frequency (TF) domain. We propose a novel use of TF and DD guard bins, aiming to preserve waveform orthogonality on the pilot bins and DD data integrity, respectively. The receiver first obtains low-complexity coarse estimates of the channel parameters. Leveraging the orthogonality, a virtual array (VA) is constructed. This enables the formulation of a sparse signal recovery (SSR) problem, in which the coarse estimates are used to build a low-dimensional dictionary matrix. The SSR solution yields high-resolution estimates of channel parameters. Simulation results show that the proposed approach achieves good performance with only a small number of pilots and guard bins. Furthermore, the required overhead is independent of the number of transmit antennas, ensuring good scalability of the proposed method for large MIMO arrays. The proposed approach considers practical rectangular transmit pulse-shaping and receiver matched filtering, and also accounts for fractional Doppler effects.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7528\u4e8eMIMO OTFS\u7cfb\u7edf\u7684\u4f4e\u5f00\u9500\u4fe1\u9053\u4f30\u8ba1\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u65f6\u9891\u57df\u5d4c\u5165\u5bfc\u9891\u5e76\u6784\u5efa\u865a\u62df\u9635\u5217\uff0c\u4f7f\u7528\u7a00\u758f\u4fe1\u53f7\u6062\u590d\u6280\u672f\u83b7\u5f97\u9ad8\u5206\u8fa8\u7387\u4fe1\u9053\u53c2\u6570\u4f30\u8ba1\u3002", "motivation": "\u73b0\u6709OTFS\u4fe1\u9053\u4f30\u8ba1\u6280\u672f\u8981\u4e48\u9700\u8981\u975e\u91cd\u53e0\u7684DD\u57df\u5bfc\u9891\u548c\u9632\u62a4\u533a\u57df\uff0c\u727a\u7272\u901a\u4fe1\u901f\u7387\uff1b\u8981\u4e48\u9700\u8981\u590d\u6742\u7b97\u6cd5\u5904\u7406\u91cd\u53e0\u5bfc\u9891\uff0c\u589e\u52a0\u63a5\u6536\u673a\u6210\u672c\u548c\u590d\u6742\u5ea6\u3002", "method": "\u5728\u65f6\u9891\u57df\u5d4c\u5165\u5bfc\u9891\uff0c\u5229\u7528TF\u548cDD\u9632\u62a4\u5355\u5143\u4fdd\u6301\u6ce2\u5f62\u6b63\u4ea4\u6027\u548c\u6570\u636e\u5b8c\u6574\u6027\u3002\u63a5\u6536\u673a\u9996\u5148\u83b7\u5f97\u7c97\u7565\u4fe1\u9053\u4f30\u8ba1\uff0c\u6784\u5efa\u865a\u62df\u9635\u5217\uff0c\u7136\u540e\u901a\u8fc7\u7a00\u758f\u4fe1\u53f7\u6062\u590d\u95ee\u9898\u83b7\u5f97\u9ad8\u5206\u8fa8\u7387\u4fe1\u9053\u53c2\u6570\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u4ec5\u9700\u5c11\u91cf\u5bfc\u9891\u548c\u9632\u62a4\u5355\u5143\u5373\u53ef\u83b7\u5f97\u826f\u597d\u6027\u80fd\uff0c\u4e14\u6240\u9700\u5f00\u9500\u4e0e\u53d1\u5c04\u5929\u7ebf\u6570\u91cf\u65e0\u5173\uff0c\u5177\u6709\u826f\u597d\u7684\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u8003\u8651\u4e86\u5b9e\u9645\u77e9\u5f62\u53d1\u5c04\u8109\u51b2\u6210\u5f62\u548c\u63a5\u6536\u673a\u5339\u914d\u6ee4\u6ce2\uff0c\u5e76\u80fd\u5904\u7406\u5206\u6570\u591a\u666e\u52d2\u6548\u5e94\uff0c\u4e3a\u5927\u89c4\u6a21MIMO OTFS\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u4fe1\u9053\u4f30\u8ba1\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.08534", "categories": ["eess.SP", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.08534", "abs": "https://arxiv.org/abs/2511.08534", "authors": ["Panagiotis Gavriilidis", "Kyriakos Stylianopoulos", "George C. Alexandropoulos"], "title": "MIMO Communications with 1-bit RIS: Asymptotic Analysis and Over-the-Air Channel Diagonalization", "comment": "8 pages, 6 figures, IEEE Conference", "summary": "This paper presents an asymptotic analysis of Multiple-Input Multiple-Output (MIMO) systems assisted by a 1-bit Reconfigurable Intelligent Surface (RIS) under Ricean fading conditions. Using random matrix theory, we show that, in the asymptotic regime, the dominant singular values and vectors of the transmitter-RIS and RIS-receiver channels converge to their deterministic Line-of-Sight (LoS) components, almost irrespective of the Ricean factors. This enables RIS phase configuration using only LoS information through a closed-form Sign Alignment (SA) rule that maximizes the channel gain. Furthermore, when the RIS is asymptotically larger than the transceiver arrays, proper RIS configuration can render the end-to-end MIMO channel in the capacity formula asymptotically diagonal, thereby eliminating inter-stream interference and enabling Over-The-Air (OTA) spatial multiplexing without channel knowledge at the transmitter. Building on this result, a waterfilling-inspired SA algorithm that allocates RIS elements to spatial streams, based on the asymptotic singular values and statistical channel parameters, is proposed. Simulation results validate the theoretical analyses, demonstrating that the proposed schemes achieve performance comparable to conventional Riemannian manifold optimization, but with orders of magnitude lower runtime.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5206\u6790\u4e861\u4f4d\u53ef\u91cd\u6784\u667a\u80fd\u8868\u9762\u8f85\u52a9\u7684MIMO\u7cfb\u7edf\u5728Ricean\u8870\u843d\u4e0b\u7684\u6e10\u8fd1\u6027\u80fd\uff0c\u63d0\u51fa\u57fa\u4e8e\u89c6\u8ddd\u5206\u91cf\u7684\u7b26\u53f7\u5bf9\u9f50\u76f8\u4f4d\u914d\u7f6e\u89c4\u5219\uff0c\u53ef\u5728\u65e0\u53d1\u5c04\u673a\u4fe1\u9053\u4fe1\u606f\u4e0b\u5b9e\u73b0\u7a7a\u4e2d\u7a7a\u95f4\u590d\u7528\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "motivation": "\u7814\u7a761\u4f4dRIS\u8f85\u52a9MIMO\u7cfb\u7edf\u5728Ricean\u8870\u843d\u4e0b\u7684\u6e10\u8fd1\u6027\u80fd\uff0c\u63a2\u7d22\u5982\u4f55\u5229\u7528\u89c6\u8ddd\u5206\u91cf\u7b80\u5316RIS\u76f8\u4f4d\u914d\u7f6e\uff0c\u5b9e\u73b0\u9ad8\u6548\u7684\u7a7a\u95f4\u590d\u7528\u800c\u65e0\u9700\u53d1\u5c04\u673a\u4fe1\u9053\u4fe1\u606f\u3002", "method": "\u4f7f\u7528\u968f\u673a\u77e9\u9635\u7406\u8bba\u8fdb\u884c\u6e10\u8fd1\u5206\u6790\uff0c\u63d0\u51fa\u57fa\u4e8e\u89c6\u8ddd\u5206\u91cf\u7684\u7b26\u53f7\u5bf9\u9f50\u76f8\u4f4d\u914d\u7f6e\u89c4\u5219\uff0c\u4ee5\u53ca\u53d7\u6ce8\u6c34\u7b97\u6cd5\u542f\u53d1\u7684RIS\u5143\u7d20\u5206\u914d\u7b97\u6cd5\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660e\uff0c\u5728\u6e10\u8fd1\u6761\u4ef6\u4e0b\uff0c\u4fe1\u9053\u4e3b\u5bfc\u5947\u5f02\u503c\u548c\u5411\u91cf\u6536\u655b\u4e8e\u89c6\u8ddd\u5206\u91cf\uff0c\u63d0\u51fa\u7684\u65b9\u6848\u6027\u80fd\u4e0e\u4f20\u7edf\u9ece\u66fc\u6d41\u5f62\u4f18\u5316\u76f8\u5f53\uff0c\u4f46\u8fd0\u884c\u65f6\u95f4\u663e\u8457\u964d\u4f4e\u3002", "conclusion": "1\u4f4dRIS\u8f85\u52a9MIMO\u7cfb\u7edf\u53ef\u901a\u8fc7\u89c6\u8ddd\u4fe1\u606f\u5b9e\u73b0\u9ad8\u6548\u76f8\u4f4d\u914d\u7f6e\uff0c\u5728\u65e0\u53d1\u5c04\u673a\u4fe1\u9053\u4fe1\u606f\u4e0b\u5b9e\u73b0\u7a7a\u95f4\u590d\u7528\uff0c\u5927\u5e45\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u3002"}}
{"id": "2511.08553", "categories": ["eess.SP", "cs.IT"], "pdf": "https://arxiv.org/pdf/2511.08553", "abs": "https://arxiv.org/abs/2511.08553", "authors": ["Mohamed Siala", "Abdullah Al-Nafisah", "Tareq Al-Naffouri"], "title": "Nyquist Signaling Modulation (NSM): An FTN-Inspired Paradigm Shift in Modulation Design for 6G and Beyond", "comment": "363 pages, 12 algorithms, 81 figures, 80 tables. Comprehensive theoretical report introducing the Nyquist Signaling Modulation (NSM) and analog LDGM paradigms for unified physical-layer design. Keywords: NSM; FTN; MS-PRS; multidimensional NSM; LDGM; joint source-channel-modulation; 6G", "summary": "Nyquist Signaling Modulations (NSMs) are a new signaling paradigm inspired by faster-than-Nyquist principles but based on a distinct approach that enables controlled inter-symbol interference through carefully designed finite-impulse-response filters. NSMs can operate in any number of dimensions, including mixed-dimensional configurations, offering wide flexibility in filter design, optional energy balancing, and preservation of the 2-ASK minimum squared Euclidean distance (MSED). Both real and rational tapped filters are investigated, and closed-form expressions are derived for the optimal real-tap filters in the one-dimensional case (MS-PRS), providing analytical insight and strong agreement with simulated bit-error behavior across wide SNR ranges. The paradigm is conceptually expanded through an analog Low-Density Generator Matrix (LDGM) formulation, which broadens the NSM family and unifies modulation and coding within a single, structurally coherent framework. When combined with LDPC coding, it enables efficient and naturally synergistic interaction between the analog modulation and the digital LDPC code. Alternatively, when analog LDGM is employed for both source coding and modulation, a simple and harmonious joint source-channel-modulation structure emerges. In both configurations, the constituent blocks exhibit dual graph-based architectures suited to message passing, achieving high flexibility and complexity-efficient operation. Collectively, these results establish promising physical-layer directions for future 6G communication systems.", "AI": {"tldr": "Nyquist Signaling Modulations (NSMs) \u662f\u4e00\u79cd\u57fa\u4e8e\u6709\u9650\u8109\u51b2\u54cd\u5e94\u6ee4\u6ce2\u5668\u8bbe\u8ba1\u7684\u65b0\u578b\u4fe1\u53f7\u8303\u5f0f\uff0c\u901a\u8fc7\u53ef\u63a7\u7684\u7b26\u53f7\u95f4\u5e72\u6270\u5b9e\u73b0\u7075\u6d3b\u8c03\u5236\uff0c\u652f\u6301\u4efb\u610f\u7ef4\u5ea6\u914d\u7f6e\uff0c\u4fdd\u6301\u6700\u5c0f\u6b27\u6c0f\u8ddd\u79bb\uff0c\u5e76\u4e0eLDPC\u7f16\u7801\u6216LDGM\u6e90\u7f16\u7801\u81ea\u7136\u534f\u540c\u3002", "motivation": "\u53d7\u8d85\u5948\u594e\u65af\u7279\u539f\u7406\u542f\u53d1\uff0c\u5f00\u53d1\u4e00\u79cd\u65b0\u7684\u4fe1\u53f7\u8c03\u5236\u8303\u5f0f\uff0c\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u6ee4\u6ce2\u5668\u5b9e\u73b0\u53ef\u63a7\u7b26\u53f7\u95f4\u5e72\u6270\uff0c\u4e3a\u672a\u67656G\u901a\u4fe1\u7cfb\u7edf\u63d0\u4f9b\u7075\u6d3b\u9ad8\u6548\u7684\u7269\u7406\u5c42\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u6709\u9650\u8109\u51b2\u54cd\u5e94\u6ee4\u6ce2\u5668\u8bbe\u8ba1NSM\uff0c\u7814\u7a76\u5b9e\u6570\u548c\u6709\u7406\u6570\u62bd\u5934\u6ee4\u6ce2\u5668\uff0c\u63a8\u5bfc\u4e00\u7ef4\u60c5\u51b5\u4e0b\u6700\u4f18\u5b9e\u62bd\u5934\u6ee4\u6ce2\u5668\u7684\u95ed\u5f0f\u89e3\uff0c\u5e76\u901a\u8fc7\u6a21\u62dfLDGM\u6269\u5c55\u6982\u5ff5\uff0c\u5c06\u8c03\u5236\u548c\u7f16\u7801\u7edf\u4e00\u5728\u57fa\u4e8e\u6d88\u606f\u4f20\u9012\u7684\u56fe\u7ed3\u6784\u6846\u67b6\u4e2d\u3002", "result": "NSM\u5728\u5bbdSNR\u8303\u56f4\u5185\u4e0e\u6a21\u62df\u6bd4\u7279\u8bef\u7801\u884c\u4e3a\u9ad8\u5ea6\u4e00\u81f4\uff0c\u4e0eLDPC\u7f16\u7801\u7ed3\u5408\u5b9e\u73b0\u9ad8\u6548\u534f\u540c\uff0c\u6216\u4e0eLDGM\u6e90\u7f16\u7801\u7ed3\u5408\u5f62\u6210\u7b80\u5355\u548c\u8c10\u7684\u8054\u5408\u6e90-\u4fe1\u9053-\u8c03\u5236\u7ed3\u6784\uff0c\u4e24\u79cd\u914d\u7f6e\u5747\u652f\u6301\u590d\u6742\u5ea6\u9ad8\u6548\u7684\u64cd\u4f5c\u3002", "conclusion": "NSM\u53ca\u5176\u4e0eLDPC/LDGM\u7684\u534f\u540c\u4e3a\u672a\u67656G\u901a\u4fe1\u7cfb\u7edf\u5efa\u7acb\u4e86\u6709\u524d\u666f\u7684\u7269\u7406\u5c42\u65b9\u5411\uff0c\u63d0\u4f9b\u4e86\u9ad8\u7075\u6d3b\u6027\u548c\u590d\u6742\u5ea6\u6548\u7387\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}

{"id": "2511.10897", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.10897", "abs": "https://arxiv.org/abs/2511.10897", "authors": ["Xianxin Song", "Xianghao Yu", "Jie Xu", "Derrick Wing Kwan Ng"], "title": "Detection in Bistatic ISAC with Deterministic Sensing and Gaussian Information Signals", "comment": "6 pages, 5 figures", "summary": "Integrated sensing and communications (ISAC) is a disruptive technology enabling future sixth-generation (6G) networks. This paper investigates target detection in a bistatic ISAC system, in which the base station (BS) transmits superimposed ISAC signals comprising both Gaussian information-bearing and deterministic sensing components to simultaneously provide communication and sensing functionalities. First, we develop a Neyman-Pearson (NP)-based detector that effectively utilizes both the deterministic sensing and random communication signals. Closed-form analysis reveals that both signal components contribute to improving the overall detection performance. Subsequently, we optimize the BS transmit beamforming to maximize the detection probability, subject to a minimum signal-to-interference-plus-noise ratio (SINR) constraint for the communication user (CU) and a total transmit power budget at the BS. The resulting non-convex beamforming optimization problem is addressed via semi-definite relaxation (SDR) and successive convex approximation (SCA) techniques. Simulation results demonstrate the superiority of the proposed NP-based detector, which leverages both types of signals, over benchmark schemes that treat information signals as interference. They also reveal that a higher communication-rate threshold directs more transmit power to Gaussian information-bearing signals, thereby diminishing deterministic-signal power and weakening detection performance.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u53cc\u57fa\u5730ISAC\u7cfb\u7edf\u4e2d\u7684\u76ee\u6807\u68c0\u6d4b\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eNeyman-Pearson\u7684\u68c0\u6d4b\u5668\uff0c\u80fd\u591f\u6709\u6548\u5229\u7528\u786e\u5b9a\u6027\u611f\u77e5\u4fe1\u53f7\u548c\u968f\u673a\u901a\u4fe1\u4fe1\u53f7\uff0c\u5e76\u901a\u8fc7\u4f18\u5316\u6ce2\u675f\u6210\u5f62\u6765\u6700\u5927\u5316\u68c0\u6d4b\u6982\u7387\u3002", "motivation": "\u96c6\u6210\u611f\u77e5\u4e0e\u901a\u4fe1(ISAC)\u662f6G\u7f51\u7edc\u7684\u5173\u952e\u6280\u672f\uff0c\u9700\u8981\u540c\u65f6\u5b9e\u73b0\u901a\u4fe1\u548c\u611f\u77e5\u529f\u80fd\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5c06\u4fe1\u606f\u4fe1\u53f7\u89c6\u4e3a\u5e72\u6270\uff0c\u672a\u80fd\u5145\u5206\u5229\u7528\u6240\u6709\u4fe1\u53f7\u5206\u91cf\u6765\u63d0\u5347\u68c0\u6d4b\u6027\u80fd\u3002", "method": "\u5f00\u53d1\u4e86\u57fa\u4e8eNeyman-Pearson\u7684\u68c0\u6d4b\u5668\uff0c\u5229\u7528\u786e\u5b9a\u6027\u611f\u77e5\u4fe1\u53f7\u548c\u968f\u673a\u901a\u4fe1\u4fe1\u53f7\uff1b\u91c7\u7528\u534a\u5b9a\u677e\u5f1b\u548c\u9010\u6b21\u51f8\u8fd1\u4f3c\u6280\u672f\u4f18\u5316\u57fa\u7ad9\u53d1\u5c04\u6ce2\u675f\u6210\u5f62\uff0c\u5728\u6ee1\u8db3\u7528\u6237SINR\u7ea6\u675f\u548c\u603b\u53d1\u5c04\u529f\u7387\u9650\u5236\u4e0b\u6700\u5927\u5316\u68c0\u6d4b\u6982\u7387\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684NP\u68c0\u6d4b\u5668\u4f18\u4e8e\u5c06\u4fe1\u606f\u4fe1\u53f7\u89c6\u4e3a\u5e72\u6270\u7684\u57fa\u51c6\u65b9\u6848\uff1b\u66f4\u9ad8\u7684\u901a\u4fe1\u901f\u7387\u9608\u503c\u4f1a\u5bfc\u81f4\u66f4\u591a\u529f\u7387\u5206\u914d\u7ed9\u9ad8\u65af\u4fe1\u606f\u627f\u8f7d\u4fe1\u53f7\uff0c\u4ece\u800c\u964d\u4f4e\u786e\u5b9a\u6027\u4fe1\u53f7\u529f\u7387\u5e76\u524a\u5f31\u68c0\u6d4b\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684NP\u68c0\u6d4b\u5668\u80fd\u591f\u6709\u6548\u5229\u7528ISAC\u7cfb\u7edf\u4e2d\u7684\u6240\u6709\u4fe1\u53f7\u5206\u91cf\uff0c\u5728\u53cc\u57fa\u5730ISAC\u7cfb\u7edf\u4e2d\u5b9e\u73b0\u4e86\u4f18\u8d8a\u7684\u76ee\u6807\u68c0\u6d4b\u6027\u80fd\uff0c\u63ed\u793a\u4e86\u901a\u4fe1\u548c\u611f\u77e5\u9700\u6c42\u4e4b\u95f4\u7684\u6743\u8861\u5173\u7cfb\u3002"}}
{"id": "2511.10990", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.10990", "abs": "https://arxiv.org/abs/2511.10990", "authors": ["Qingqing Wu", "Qiaoyan Peng", "Ziheng Zhang", "Xiaodan Shao", "Yang Liu", "Yifan Jiang", "Yapeng Zhao", "Yanze Zhu", "Yilong Chen", "Zixiang Ren", "Jie Xu", "Wen Chen", "Rui Zhang"], "title": "Intelligent Reflecting Surfaces for Integrated Sensing and Communications: A Survey", "comment": null, "summary": "The rapid development of sixth-generation (6G) wireless networks requires seamless integration of communication and sensing to support ubiquitous intelligence and real-time, high-reliability applications. Integrated sensing and communication (ISAC) has emerged as a key solution for achieving this convergence, offering joint utilization of spectral, hardware, and computing resources. However, realizing high-performance ISAC remains challenging due to environmental line-of-sight (LoS) blockage, limited spatial resolution, and the inherent coverage asymmetry and resource coupling between sensing and communication. Intelligent reflecting surfaces (IRSs), featuring low-cost, energy-efficient, and programmable electromagnetic reconfiguration, provide a promising solution to overcome these limitations. This article presents a comprehensive overview of IRS-aided wireless sensing and ISAC technologies, including IRS architectures, target detection and estimation techniques, beamforming designs, and performance metrics. It further explores IRS-enabled new opportunities for more efficient performance balancing, coexistence, and networking in ISAC systems, focuses on current design bottlenecks, and outlines future research directions. This article aims to offer a unified design framework that guides the development of practical and scalable IRS-aided ISAC systems for the next-generation wireless network.", "AI": {"tldr": "IRS\u8f85\u52a9\u7684\u65e0\u7ebf\u611f\u77e5\u4e0eISAC\u6280\u672f\u7efc\u8ff0\uff0c\u5305\u62ecIRS\u67b6\u6784\u3001\u76ee\u6807\u68c0\u6d4b\u4e0e\u4f30\u8ba1\u6280\u672f\u3001\u6ce2\u675f\u6210\u5f62\u8bbe\u8ba1\u548c\u6027\u80fd\u6307\u6807\uff0c\u63a2\u7d22IRS\u5728ISAC\u7cfb\u7edf\u4e2d\u7684\u65b0\u673a\u9047\uff0c\u5e76\u6307\u51fa\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "6G\u65e0\u7ebf\u7f51\u7edc\u9700\u8981\u901a\u4fe1\u4e0e\u611f\u77e5\u7684\u65e0\u7f1d\u96c6\u6210\uff0c\u4ee5\u652f\u6301\u6cdb\u5728\u667a\u80fd\u548c\u5b9e\u65f6\u9ad8\u53ef\u9760\u6027\u5e94\u7528\u3002ISAC\u662f\u5b9e\u73b0\u8fd9\u79cd\u878d\u5408\u7684\u5173\u952e\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u9762\u4e34\u73af\u5883\u906e\u6321\u3001\u7a7a\u95f4\u5206\u8fa8\u7387\u6709\u9650\u4ee5\u53ca\u611f\u77e5\u4e0e\u901a\u4fe1\u4e4b\u95f4\u7684\u8986\u76d6\u4e0d\u5bf9\u79f0\u548c\u8d44\u6e90\u8026\u5408\u7b49\u6311\u6218\u3002", "method": "\u5229\u7528\u667a\u80fd\u53cd\u5c04\u9762(IRS)\u7684\u4f4e\u6210\u672c\u3001\u9ad8\u80fd\u6548\u548c\u53ef\u7f16\u7a0b\u7535\u78c1\u91cd\u6784\u7279\u6027\uff0c\u63d0\u4f9bIRS\u8f85\u52a9\u7684\u65e0\u7ebf\u611f\u77e5\u548cISAC\u6280\u672f\u6846\u67b6\uff0c\u5305\u62ecIRS\u67b6\u6784\u3001\u76ee\u6807\u68c0\u6d4b\u4e0e\u4f30\u8ba1\u3001\u6ce2\u675f\u6210\u5f62\u8bbe\u8ba1\u7b49\u3002", "result": "IRS\u4e3aISAC\u7cfb\u7edf\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u6027\u80fd\u5e73\u8861\u3001\u5171\u5b58\u548c\u7ec4\u7f51\u7684\u65b0\u673a\u9047\uff0c\u80fd\u591f\u514b\u670d\u4f20\u7edfISAC\u7cfb\u7edf\u7684\u5c40\u9650\u6027\u3002", "conclusion": "\u672c\u6587\u65e8\u5728\u63d0\u4f9b\u4e00\u4e2a\u7edf\u4e00\u7684\u8bbe\u8ba1\u6846\u67b6\uff0c\u6307\u5bfc\u4e0b\u4e00\u4ee3\u65e0\u7ebf\u7f51\u7edc\u4e2d\u5b9e\u7528\u4e14\u53ef\u6269\u5c55\u7684IRS\u8f85\u52a9ISAC\u7cfb\u7edf\u7684\u5f00\u53d1\u3002"}}
{"id": "2511.11193", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.11193", "abs": "https://arxiv.org/abs/2511.11193", "authors": ["Yan Zhang", "Indrakshi Dey", "Nicola Marchetti"], "title": "Blockage-aware Hierarchical Codebook Design for RIS-Assisted Movable Antenna Systems", "comment": "17 pages", "summary": "In this paper, we propose a novel blockage-aware hierarchical beamforming framework for movable antenna (MA) systems operating at millimeter-wave (mm-Wave) frequencies. While existing works on MA systems have demonstrated performance gains over conventional systems, they often neglect the design of specialized codebooks to leverage MA's unique capabilities and address the challenges of increased energy consumption and latency inherent to MA systems. To address these aspects, we first integrate blockage detection into the codebook design process based on the Gerchberg-Saxton (GS) algorithm, significantly reducing inefficiencies due to beam evaluations done in blocked directions. Then, we use a two-stage approach to reduce the complexity of the joint beamforming and Reconfigurable Intelligent Surfaces (RIS) optimization problem. The simulations demonstrate that the proposed adaptive codebook successfully improves the Energy Efficiency (EE) and reduces the beam training overhead, substantially boosting the practical deployment potential of RIS-assisted MA systems in future wireless networks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u6beb\u7c73\u6ce2\u53ef\u79fb\u52a8\u5929\u7ebf\u7cfb\u7edf\u7684\u963b\u585e\u611f\u77e5\u5206\u5c42\u6ce2\u675f\u8d4b\u5f62\u6846\u67b6\uff0c\u901a\u8fc7\u96c6\u6210\u963b\u585e\u68c0\u6d4b\u548c\u4e24\u9636\u6bb5\u4f18\u5316\u65b9\u6cd5\uff0c\u63d0\u9ad8\u80fd\u91cf\u6548\u7387\u5e76\u51cf\u5c11\u6ce2\u675f\u8bad\u7ec3\u5f00\u9500\u3002", "motivation": "\u73b0\u6709\u53ef\u79fb\u52a8\u5929\u7ebf\u7cfb\u7edf\u7814\u7a76\u5f80\u5f80\u5ffd\u89c6\u4e86\u4e13\u7528\u7801\u672c\u8bbe\u8ba1\uff0c\u672a\u80fd\u5145\u5206\u5229\u7528MA\u7684\u72ec\u7279\u80fd\u529b\uff0c\u540c\u65f6\u9762\u4e34\u80fd\u8017\u548c\u5ef6\u8fdf\u589e\u52a0\u7684\u6311\u6218\u3002", "method": "\u9996\u5148\u57fa\u4e8eGerchberg-Saxton\u7b97\u6cd5\u5c06\u963b\u585e\u68c0\u6d4b\u96c6\u6210\u5230\u7801\u672c\u8bbe\u8ba1\u4e2d\uff0c\u7136\u540e\u91c7\u7528\u4e24\u9636\u6bb5\u65b9\u6cd5\u964d\u4f4e\u8054\u5408\u6ce2\u675f\u8d4b\u5f62\u548c\u53ef\u91cd\u6784\u667a\u80fd\u8868\u9762\u4f18\u5316\u7684\u590d\u6742\u5ea6\u3002", "result": "\u4eff\u771f\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u81ea\u9002\u5e94\u7801\u672c\u6210\u529f\u63d0\u9ad8\u4e86\u80fd\u91cf\u6548\u7387\uff0c\u51cf\u5c11\u4e86\u6ce2\u675f\u8bad\u7ec3\u5f00\u9500\u3002", "conclusion": "\u8be5\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86RIS\u8f85\u52a9MA\u7cfb\u7edf\u5728\u672a\u6765\u65e0\u7ebf\u7f51\u7edc\u4e2d\u7684\u5b9e\u9645\u90e8\u7f72\u6f5c\u529b\u3002"}}
{"id": "2511.11205", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.11205", "abs": "https://arxiv.org/abs/2511.11205", "authors": ["Rick Luiken", "Lorenzo Pes", "Manil Dev Gomony", "Sander Stuijk"], "title": "LOKI: a 0.266 pJ/SOP Digital SNN Accelerator with Multi-Cycle Clock-Gated SRAM in 22nm", "comment": null, "summary": "Bio-inspired sensors like Dynamic Vision Sensors (DVS) and silicon cochleas are often combined with Spiking Neural Networks (SNNs), enabling efficient, event-driven processing similar to biological sensory systems. To realize the low-power constraints of the edge, the SNN should run on a hardware architecture that can exploit the sparse nature of the spikes. In this paper, we introduce LOKI, a digital architecture for Fully-Connected (FC) SNNs. By using Multi-Cycle Clock-Gated (MCCG) SRAMs, LOKI can operate at 0.59 V, while running at a clock frequency of 667 MHz. At full throughput, LOKI only consumes 0.266 pJ/SOP. We evaluate LOKI on both the Neuromorphic MNIST (N-MNIST) and the Keyword Spotting k(KWS) tasks, achieving 98.0 % accuracy at 119.8 nJ/inference and 93.0 % accuracy at 546.5 nJ/inference respectively.", "AI": {"tldr": "LOKI\u662f\u4e00\u79cd\u7528\u4e8e\u5168\u8fde\u63a5\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u7684\u6570\u5b57\u67b6\u6784\uff0c\u91c7\u7528\u591a\u5468\u671f\u65f6\u949f\u95e8\u63a7SRAM\uff0c\u57280.59V\u7535\u538b\u4e0b\u8fd0\u884c\uff0c\u65f6\u949f\u9891\u7387667MHz\uff0c\u80fd\u8017\u4ec50.266 pJ/SOP\uff0c\u5728N-MNIST\u548cKWS\u4efb\u52a1\u4e0a\u5206\u522b\u8fbe\u523098.0%\u548c93.0%\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u751f\u7269\u542f\u53d1\u4f20\u611f\u5668\u4e0e\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u7ed3\u5408\u9700\u8981\u80fd\u591f\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u6ee1\u8db3\u4f4e\u529f\u8017\u7ea6\u675f\u7684\u786c\u4ef6\u67b6\u6784\uff0c\u4ee5\u5229\u7528\u8109\u51b2\u7684\u7a00\u758f\u7279\u6027\u3002", "method": "\u63d0\u51faLOKI\u6570\u5b57\u67b6\u6784\uff0c\u4f7f\u7528\u591a\u5468\u671f\u65f6\u949f\u95e8\u63a7SRAM\u6280\u672f\uff0c\u5b9e\u73b0\u57280.59V\u4f4e\u7535\u538b\u4e0b\u7684\u9ad8\u6548\u8fd0\u884c\u3002", "result": "\u5728N-MNIST\u4efb\u52a1\u4e0a\u8fbe\u523098.0%\u51c6\u786e\u7387\uff0c\u80fd\u8017119.8 nJ/\u63a8\u7406\uff1b\u5728KWS\u4efb\u52a1\u4e0a\u8fbe\u523093.0%\u51c6\u786e\u7387\uff0c\u80fd\u8017546.5 nJ/\u63a8\u7406\u3002", "conclusion": "LOKI\u67b6\u6784\u8bc1\u660e\u4e86\u5728\u4f4e\u7535\u538b\u4e0b\u5b9e\u73b0\u9ad8\u6548\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u5904\u7406\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u8fb9\u7f18\u8ba1\u7b97\u63d0\u4f9b\u4e86\u4f4e\u529f\u8017\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.10790", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2511.10790", "abs": "https://arxiv.org/abs/2511.10790", "authors": ["Girish", "Mohd Mujtaba Akhtar", "Farhan Sheth", "Muskaan Singh"], "title": "Towards Attribution of Generators and Emotional Manipulation in Cross-Lingual Synthetic Speech using Geometric Learning", "comment": "Accepted to IJCNLP-AACL 2025", "summary": "In this work, we address the problem of finegrained traceback of emotional and manipulation characteristics from synthetically manipulated speech. We hypothesize that combining semantic-prosodic cues captured by Speech Foundation Models (SFMs) with fine-grained spectral dynamics from auditory representations can enable more precise tracing of both emotion and manipulation source. To validate this hypothesis, we introduce MiCuNet, a novel multitask framework for fine-grained tracing of emotional and manipulation attributes in synthetically generated speech. Our approach integrates SFM embeddings with spectrogram-based auditory features through a mixed-curvature projection mechanism that spans Hyperbolic, Euclidean, and Spherical spaces guided by a learnable temporal gating mechanism. Our proposed method adopts a multitask learning setup to simultaneously predict original emotions, manipulated emotions, and manipulation sources on the EmoFake dataset (EFD) across both English and Chinese subsets. MiCuNet yields consistent improvements, consistently surpassing conventional fusion strategies. To the best of our knowledge, this work presents the first study to explore a curvature-adaptive framework specifically tailored for multitask tracking in synthetic speech.", "AI": {"tldr": "\u63d0\u51faMiCuNet\u591a\u4efb\u52a1\u6846\u67b6\uff0c\u7ed3\u5408\u8bed\u97f3\u57fa\u7840\u6a21\u578b\u548c\u542c\u89c9\u7279\u5f81\uff0c\u901a\u8fc7\u6df7\u5408\u66f2\u7387\u6295\u5f71\u673a\u5236\u5728\u53cc\u66f2\u3001\u6b27\u51e0\u91cc\u5f97\u548c\u7403\u9762\u7a7a\u95f4\u4e2d\u7cbe\u7ec6\u8ffd\u8e2a\u5408\u6210\u8bed\u97f3\u4e2d\u7684\u60c5\u611f\u548c\u64cd\u7eb5\u5c5e\u6027", "motivation": "\u89e3\u51b3\u4ece\u5408\u6210\u64cd\u7eb5\u8bed\u97f3\u4e2d\u7cbe\u7ec6\u8ffd\u8e2a\u60c5\u611f\u548c\u64cd\u7eb5\u7279\u5f81\u7684\u95ee\u9898\uff0c\u5047\u8bbe\u7ed3\u5408\u8bed\u97f3\u57fa\u7840\u6a21\u578b\u7684\u8bed\u4e49-\u97f5\u5f8b\u7ebf\u7d22\u4e0e\u542c\u89c9\u7279\u5f81\u7684\u7cbe\u7ec6\u9891\u8c31\u52a8\u6001\u80fd\u66f4\u7cbe\u786e\u5730\u8ffd\u8e2a\u60c5\u611f\u548c\u64cd\u7eb5\u6e90", "method": "\u5f15\u5165MiCuNet\u591a\u4efb\u52a1\u6846\u67b6\uff0c\u901a\u8fc7\u6df7\u5408\u66f2\u7387\u6295\u5f71\u673a\u5236\u6574\u5408SFM\u5d4c\u5165\u548c\u57fa\u4e8e\u9891\u8c31\u56fe\u7684\u542c\u89c9\u7279\u5f81\uff0c\u91c7\u7528\u53ef\u5b66\u4e60\u65f6\u5e8f\u95e8\u63a7\u673a\u5236\uff0c\u5728\u591a\u4efb\u52a1\u5b66\u4e60\u8bbe\u7f6e\u4e0b\u540c\u65f6\u9884\u6d4b\u539f\u59cb\u60c5\u611f\u3001\u64cd\u7eb5\u60c5\u611f\u548c\u64cd\u7eb5\u6e90", "result": "\u5728EmoFake\u6570\u636e\u96c6\uff08\u82f1\u8bed\u548c\u4e2d\u6587\u5b50\u96c6\uff09\u4e0a\u53d6\u5f97\u4e00\u81f4\u6539\u8fdb\uff0c\u6301\u7eed\u8d85\u8d8a\u4f20\u7edf\u878d\u5408\u7b56\u7565", "conclusion": "\u8fd9\u662f\u9996\u4e2a\u4e13\u95e8\u4e3a\u5408\u6210\u8bed\u97f3\u4e2d\u591a\u4efb\u52a1\u8ffd\u8e2a\u91cf\u8eab\u5b9a\u5236\u7684\u66f2\u7387\u81ea\u9002\u5e94\u6846\u67b6\u7814\u7a76"}}
{"id": "2511.10692", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2511.10692", "abs": "https://arxiv.org/abs/2511.10692", "authors": ["Hongyi Li", "Chengxuan Zhou", "Chu Wang", "Sicheng Liang", "Yanting Chen", "Qinlin Xie", "Jiawei Ye", "Jie Wu"], "title": "StyleBreak: Revealing Alignment Vulnerabilities in Large Audio-Language Models via Style-Aware Audio Jailbreak", "comment": "Accepted by AAAI 2026", "summary": "Large Audio-language Models (LAMs) have recently enabled powerful speech-based interactions by coupling audio encoders with Large Language Models (LLMs). However, the security of LAMs under adversarial attacks remains underexplored, especially through audio jailbreaks that craft malicious audio prompts to bypass alignment. Existing efforts primarily rely on converting text-based attacks into speech or applying shallow signal-level perturbations, overlooking the impact of human speech's expressive variations on LAM alignment robustness. To address this gap, we propose StyleBreak, a novel style-aware audio jailbreak framework that systematically investigates how diverse human speech attributes affect LAM alignment robustness. Specifically, StyleBreak employs a two-stage style-aware transformation pipeline that perturbs both textual content and audio to control linguistic, paralinguistic, and extralinguistic attributes. Furthermore, we develop a query-adaptive policy network that automatically searches for adversarial styles to enhance the efficiency of LAM jailbreak exploration. Extensive evaluations demonstrate that LAMs exhibit critical vulnerabilities when exposed to diverse human speech attributes. Moreover, StyleBreak achieves substantial improvements in attack effectiveness and efficiency across multiple attack paradigms, highlighting the urgent need for more robust alignment in LAMs.", "AI": {"tldr": "StyleBreak\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u98ce\u683c\u611f\u77e5\u97f3\u9891\u8d8a\u72f1\u6846\u67b6\uff0c\u901a\u8fc7\u7cfb\u7edf\u7814\u7a76\u4eba\u7c7b\u8bed\u97f3\u5c5e\u6027\u5bf9\u5927\u578b\u97f3\u9891\u8bed\u8a00\u6a21\u578b\u5bf9\u9f50\u9c81\u68d2\u6027\u7684\u5f71\u54cd\uff0c\u63ed\u793a\u4e86LAMs\u5728\u9762\u5bf9\u591a\u6837\u5316\u8bed\u97f3\u98ce\u683c\u65f6\u7684\u5173\u952e\u6f0f\u6d1e\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u4f9d\u8d56\u5c06\u57fa\u4e8e\u6587\u672c\u7684\u653b\u51fb\u8f6c\u6362\u4e3a\u8bed\u97f3\u6216\u5e94\u7528\u6d45\u5c42\u4fe1\u53f7\u7ea7\u6270\u52a8\uff0c\u5ffd\u89c6\u4e86\u4eba\u7c7b\u8bed\u97f3\u8868\u8fbe\u53d8\u5316\u5bf9LAM\u5bf9\u9f50\u9c81\u68d2\u6027\u7684\u5f71\u54cd\uff0c\u9700\u8981\u586b\u8865\u8fd9\u4e00\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u98ce\u683c\u611f\u77e5\u8f6c\u6362\u6d41\u7a0b\uff0c\u6270\u52a8\u6587\u672c\u5185\u5bb9\u548c\u97f3\u9891\u4ee5\u63a7\u5236\u8bed\u8a00\u3001\u526f\u8bed\u8a00\u548c\u8d85\u8bed\u8a00\u5c5e\u6027\uff0c\u5e76\u5f00\u53d1\u67e5\u8be2\u81ea\u9002\u5e94\u7b56\u7565\u7f51\u7edc\u81ea\u52a8\u641c\u7d22\u5bf9\u6297\u6027\u98ce\u683c\u3002", "result": "\u5e7f\u6cdb\u8bc4\u4f30\u663e\u793aLAMs\u5728\u9762\u5bf9\u591a\u6837\u5316\u4eba\u7c7b\u8bed\u97f3\u5c5e\u6027\u65f6\u5b58\u5728\u5173\u952e\u6f0f\u6d1e\uff0cStyleBreak\u5728\u591a\u79cd\u653b\u51fb\u8303\u5f0f\u4e2d\u663e\u8457\u63d0\u9ad8\u4e86\u653b\u51fb\u6548\u679c\u548c\u6548\u7387\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86LAMs\u9700\u8981\u66f4\u5f3a\u5927\u7684\u5bf9\u9f50\u673a\u5236\u6765\u5e94\u5bf9\u8bed\u97f3\u98ce\u683c\u53d8\u5316\u5e26\u6765\u7684\u5b89\u5168\u5a01\u80c1\u3002"}}
{"id": "2511.11224", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.11224", "abs": "https://arxiv.org/abs/2511.11224", "authors": ["Sukhsagar", "Nagendra Kumar", "Ambuj Kumar Mishra", "Vimal Bhatia", "Ondrej Krejcar"], "title": "3D-HQAM Constellation Design and Performance Evaluation under AWGN", "comment": "5 pages, 22 figures, 2 tables, 1 algorithm", "summary": "This paper proposes a simple and effective method for constructing higher-order three-dimensional (3D) signal constellations, aiming to enhance the reliability of digital communication systems. The approach systematically extends the conventional two-dimensional hexagonal quadrature amplitude modulation (2D-HQAM) constellation into a 3D-HQAM signal space, forming structured lattice configurations. To address the increased decision complexity resulting from a larger number of constellation points, a dimension reduction (DR) technique is introduced, allowing the derivation of closed-form symbol error probability (SEP) expressions under additive white Gaussian noise (AWGN) conditions. Theoretical SEPs closely match simulation results, validating the accuracy of the proposed method. The minimum Euclidean distance (MED) of the 3D constellations shows a minimum increase of 12.14% over 2D constellation for 8-HQAM, reaching up to 160.81% for 1024-HQAM constellations. This significant improvement in MED leads to enhanced error performance. Therefore, the proposed 3D constellations are promising candidates for high-quality and reliable next-generation digital communication systems.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u6784\u5efa\u9ad8\u96363D\u4fe1\u53f7\u661f\u5ea7\u7684\u65b9\u6cd5\uff0c\u5c062D\u516d\u8fb9\u5f62QAM\u6269\u5c55\u52303D\u7a7a\u95f4\uff0c\u901a\u8fc7\u964d\u7ef4\u6280\u672f\u964d\u4f4e\u51b3\u7b56\u590d\u6742\u5ea6\uff0c\u663e\u8457\u63d0\u5347\u6700\u5c0f\u6b27\u51e0\u91cc\u5f97\u8ddd\u79bb\u548c\u8bef\u7801\u6027\u80fd\u3002", "motivation": "\u63d0\u9ad8\u6570\u5b57\u901a\u4fe1\u7cfb\u7edf\u7684\u53ef\u9760\u6027\uff0c\u901a\u8fc7\u6784\u5efa3D\u4fe1\u53f7\u661f\u5ea7\u6765\u589e\u5f3a\u4f20\u7edf2D\u661f\u5ea7\u7684\u6027\u80fd\u3002", "method": "\u5c062D\u516d\u8fb9\u5f62QAM\u7cfb\u7edf\u6027\u5730\u6269\u5c55\u52303D-HQAM\u4fe1\u53f7\u7a7a\u95f4\uff0c\u5f62\u6210\u7ed3\u6784\u5316\u6676\u683c\u914d\u7f6e\uff0c\u5e76\u5f15\u5165\u964d\u7ef4\u6280\u672f\u6765\u964d\u4f4e\u51b3\u7b56\u590d\u6742\u5ea6\u3002", "result": "\u7406\u8bbaSEP\u4e0e\u4eff\u771f\u7ed3\u679c\u9ad8\u5ea6\u4e00\u81f4\uff0c3D\u661f\u5ea7\u7684\u6700\u5c0f\u6b27\u51e0\u91cc\u5f97\u8ddd\u79bb\u76f8\u6bd42D\u661f\u5ea7\u63d0\u534712.14%\u5230160.81%\uff0c\u663e\u8457\u6539\u5584\u4e86\u8bef\u7801\u6027\u80fd\u3002", "conclusion": "\u6240\u63d0\u51fa\u76843D\u661f\u5ea7\u662f\u9ad8\u8d28\u91cf\u3001\u53ef\u9760\u7684\u65b0\u4e00\u4ee3\u6570\u5b57\u901a\u4fe1\u7cfb\u7edf\u7684\u6709\u524d\u666f\u5019\u9009\u65b9\u6848\u3002"}}
{"id": "2511.10793", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2511.10793", "abs": "https://arxiv.org/abs/2511.10793", "authors": ["Farhan Sheth", "Girish", "Mohd Mujtaba Akhtar", "Muskaan Singh"], "title": "Curved Worlds, Clear Boundaries: Generalizing Speech Deepfake Detection using Hyperbolic and Spherical Geometry Spaces", "comment": "Accepted to IJCNLP-AACL 2025", "summary": "In this work, we address the challenge of generalizable audio deepfake detection (ADD) across diverse speech synthesis paradigms-including conventional text-to-speech (TTS) systems and modern diffusion or flow-matching (FM) based generators. Prior work has mostly targeted individual synthesis families and often fails to generalize across paradigms due to overfitting to generation-specific artifacts. We hypothesize that synthetic speech, irrespective of its generative origin, leaves behind shared structural distortions in the embedding space that can be aligned through geometry-aware modeling. To this end, we propose RHYME, a unified detection framework that fuses utterance-level embeddings from diverse pretrained speech encoders using non-Euclidean projections. RHYME maps representations into hyperbolic and spherical manifolds-where hyperbolic geometry excels at modeling hierarchical generator families, and spherical projections capture angular, energy-invariant cues such as periodic vocoder artifacts. The fused representation is obtained via Riemannian barycentric averaging, enabling synthesis-invariant alignment. RHYME outperforms individual PTMs and homogeneous fusion baselines, achieving top performance and setting new state-of-the-art in cross-paradigm ADD.", "AI": {"tldr": "RHYME\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u97f3\u9891\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u975e\u6b27\u51e0\u91cc\u5f97\u6295\u5f71\u878d\u5408\u591a\u79cd\u9884\u8bad\u7ec3\u8bed\u97f3\u7f16\u7801\u5668\u7684\u5d4c\u5165\uff0c\u5728\u53cc\u66f2\u548c\u7403\u9762\u6d41\u5f62\u4e2d\u5efa\u6a21\u5408\u6210\u8bed\u97f3\u7684\u7ed3\u6784\u5931\u771f\uff0c\u5b9e\u73b0\u8de8\u5408\u6210\u8303\u5f0f\u7684\u6cdb\u5316\u68c0\u6d4b\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u97f3\u9891\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u65b9\u6cd5\u5728\u8de8\u4e0d\u540c\u8bed\u97f3\u5408\u6210\u8303\u5f0f\uff08\u5982\u4f20\u7edfTTS\u548c\u73b0\u4ee3\u6269\u6563/\u6d41\u5339\u914d\u751f\u6210\u5668\uff09\u65f6\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u907f\u514d\u5bf9\u7279\u5b9a\u751f\u6210\u4f2a\u5f71\u7684\u8fc7\u62df\u5408\u3002", "method": "\u63d0\u51faRHYME\u6846\u67b6\uff1a\u878d\u5408\u591a\u79cd\u9884\u8bad\u7ec3\u8bed\u97f3\u7f16\u7801\u5668\u7684\u8bed\u53e5\u7ea7\u5d4c\u5165\uff0c\u901a\u8fc7\u53cc\u66f2\u51e0\u4f55\u5efa\u6a21\u5c42\u6b21\u5316\u751f\u6210\u5668\u5bb6\u65cf\uff0c\u7403\u9762\u6295\u5f71\u6355\u6349\u89d2\u5ea6\u548c\u80fd\u91cf\u4e0d\u53d8\u7ebf\u7d22\uff08\u5982\u5468\u671f\u6027\u58f0\u7801\u5668\u4f2a\u5f71\uff09\uff0c\u4f7f\u7528\u9ece\u66fc\u91cd\u5fc3\u5e73\u5747\u83b7\u5f97\u878d\u5408\u8868\u793a\u3002", "result": "RHYME\u8d85\u8d8a\u4e86\u5355\u4e2a\u9884\u8bad\u7ec3\u6a21\u578b\u548c\u540c\u8d28\u878d\u5408\u57fa\u7ebf\uff0c\u5728\u8de8\u8303\u5f0f\u97f3\u9891\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u4e2d\u8fbe\u5230\u6700\u4f73\u6027\u80fd\uff0c\u521b\u9020\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "\u5408\u6210\u8bed\u97f3\u5728\u5d4c\u5165\u7a7a\u95f4\u4e2d\u7559\u4e0b\u5171\u4eab\u7684\u7ed3\u6784\u5931\u771f\uff0c\u901a\u8fc7\u51e0\u4f55\u611f\u77e5\u5efa\u6a21\u53ef\u4ee5\u5bf9\u9f50\u8fd9\u4e9b\u5931\u771f\uff0c\u5b9e\u73b0\u8de8\u5408\u6210\u8303\u5f0f\u7684\u7edf\u4e00\u68c0\u6d4b\u3002"}}
{"id": "2511.10697", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2511.10697", "abs": "https://arxiv.org/abs/2511.10697", "authors": ["De Hu", "Junsheng Hu", "Cuicui Jiang"], "title": "Graph Neural Field with Spatial-Correlation Augmentation for HRTF Personalization", "comment": null, "summary": "To achieve immersive spatial audio rendering on VR/AR devices, high-quality Head-Related Transfer Functions (HRTFs) are essential. In general, HRTFs are subject-dependent and position-dependent, and their measurement is time-consuming and tedious. To address this challenge, we propose the Graph Neural Field with Spatial-Correlation Augmentation (GraphNF-SCA) for HRTF personalization, which can be used to generate individual HRTFs for unseen subjects. The GraphNF-SCA consists of three key components: an HRTF personalization (HRTF-P) module, an HRTF upsampling (HRTF-U) module, and a fine-tuning stage. In the HRTF-P module, we predict HRTFs of the target subject via the Graph Neural Network (GNN) with an encoder-decoder architecture, where the encoder extracts universal features and the decoder incorporates the target-relevant features and produces individualized HRTFs. The HRTF-U module employs another GNN to model spatial correlations across HRTFs. This module is fine-tuned using the output of the HRTF-P module, thereby enhancing the spatial consistency of the predicted HRTFs. Unlike existing methods that estimate individual HRTFs position-by-position without spatial correlation modeling, the GraphNF-SCA effectively leverages inherent spatial correlations across HRTFs to enhance the performance of HRTF personalization. Experimental results demonstrate that the GraphNF-SCA achieves state-of-the-art results.", "AI": {"tldr": "\u63d0\u51faGraphNF-SCA\u65b9\u6cd5\u7528\u4e8e\u4e2a\u6027\u5316HRTF\u751f\u6210\uff0c\u901a\u8fc7\u56fe\u795e\u7ecf\u7f51\u7edc\u5efa\u6a21\u7a7a\u95f4\u76f8\u5173\u6027\uff0c\u5728VR/AR\u8bbe\u5907\u4e0a\u5b9e\u73b0\u6c89\u6d78\u5f0f\u7a7a\u95f4\u97f3\u9891\u6e32\u67d3\u3002", "motivation": "HRTF\u6d4b\u91cf\u8017\u65f6\u4e14\u4f9d\u8d56\u4e2a\u4f53\u5dee\u5f02\uff0c\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u7a7a\u95f4\u76f8\u5173\u6027\u5efa\u6a21\uff0c\u9700\u8981\u9ad8\u6548\u751f\u6210\u4e2a\u6027\u5316HRTF\u7684\u65b9\u6cd5\u3002", "method": "\u5305\u542bHRTF\u4e2a\u6027\u5316\u6a21\u5757\u3001HRTF\u4e0a\u91c7\u6837\u6a21\u5757\u548c\u5fae\u8c03\u9636\u6bb5\uff0c\u4f7f\u7528\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784\u7684\u56fe\u795e\u7ecf\u7f51\u7edc\u63d0\u53d6\u901a\u7528\u7279\u5f81\u5e76\u751f\u6210\u4e2a\u6027\u5316HRTF\uff0c\u901a\u8fc7\u7a7a\u95f4\u76f8\u5173\u6027\u589e\u5f3a\u63d0\u5347\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660eGraphNF-SCA\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u6c34\u5e73\u3002", "conclusion": "GraphNF-SCA\u901a\u8fc7\u6709\u6548\u5229\u7528HRTF\u95f4\u7684\u7a7a\u95f4\u76f8\u5173\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86HRTF\u4e2a\u6027\u5316\u6027\u80fd\uff0c\u4e3aVR/AR\u8bbe\u5907\u63d0\u4f9b\u9ad8\u8d28\u91cf\u7684\u6c89\u6d78\u5f0f\u7a7a\u95f4\u97f3\u9891\u6e32\u67d3\u3002"}}
{"id": "2511.11251", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.11251", "abs": "https://arxiv.org/abs/2511.11251", "authors": ["Tianzheng Miao", "Thomas Feys", "Gilles Callebaut", "Jarne Van Mulders", "Md Arifur Rahman", "Fran\u00e7ois Rottenberg"], "title": "Testbed Evaluation of AI-based Precoding in Distributed MIMO Systems", "comment": "6 pages, conference", "summary": "Distributed MIMO (D-MIMO) has emerged as a key architecture for future sixth-generation (6G) networks, enabling cooperative transmission across spatially distributed access points (APs). However, most existing studies rely on idealized channel models and lack hardware validation, leaving a gap between algorithmic design and practical deployment. Meanwhile, recent advances in artificial intelligence (AI)-driven precoding have shown strong potential for learning nonlinear channel-to-precoder mappings, but their real-world deployment remains limited due to challenges in data collection and model generalization. This work presents a framework for implementing and validating an AI-based precoder on a D-MIMO testbed with hardware reciprocity calibration. A pre-trained graph neural network (GNN)-based model is fine-tuned using real-world channel state information (CSI) collected from the Techtile platform and evaluated under both interpolation and extrapolation scenarios before end-to-end validation. Experimental results demonstrate a 15.7% performance gain over the pre-trained model in the multi-user case after fine-tuning, while in the single-user scenario the model achieves near-maximum ratio transmission (MRT) performance with less than 0.7 bits/channel use degradation out of a total throughput of 5.19 bits/channel use on unseen positions. Further analysis confirms the data efficiency of real-world measurements, showing consistent gains with increasing training samples, and end-to-end validation verifies coherent power focusing comparable to MRT.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5728\u5206\u5e03\u5f0fMIMO\u6d4b\u8bd5\u5e73\u53f0\u4e0a\u5b9e\u73b0\u548c\u9a8c\u8bc1\u57fa\u4e8eAI\u7684\u9884\u7f16\u7801\u5668\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u786c\u4ef6\u4e92\u6613\u6027\u6821\u51c6\u548c\u771f\u5b9e\u4e16\u754cCSI\u6570\u636e\u5fae\u8c03GNN\u6a21\u578b\uff0c\u5728\u5355\u7528\u6237\u548c\u591a\u7528\u6237\u573a\u666f\u4e0b\u90fd\u53d6\u5f97\u4e86\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u73b0\u6709D-MIMO\u7814\u7a76\u5927\u591a\u4f9d\u8d56\u7406\u60f3\u5316\u4fe1\u9053\u6a21\u578b\u4e14\u7f3a\u4e4f\u786c\u4ef6\u9a8c\u8bc1\uff0cAI\u9a71\u52a8\u7684\u9884\u7f16\u7801\u65b9\u6cd5\u5728\u73b0\u5b9e\u90e8\u7f72\u4e2d\u9762\u4e34\u6570\u636e\u6536\u96c6\u548c\u6a21\u578b\u6cdb\u5316\u6311\u6218\uff0c\u9700\u8981\u5728\u771f\u5b9e\u786c\u4ef6\u5e73\u53f0\u4e0a\u8fdb\u884c\u9a8c\u8bc1\u3002", "method": "\u5728\u5177\u6709\u786c\u4ef6\u4e92\u6613\u6027\u6821\u51c6\u7684D-MIMO\u6d4b\u8bd5\u5e73\u53f0\u4e0a\uff0c\u4f7f\u7528\u4eceTechtile\u5e73\u53f0\u6536\u96c6\u7684\u771f\u5b9e\u4e16\u754cCSI\u6570\u636e\u5bf9\u9884\u8bad\u7ec3\u7684GNN\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\uff0c\u5e76\u5728\u63d2\u503c\u548c\u5916\u63a8\u573a\u666f\u4e0b\u8bc4\u4f30\uff0c\u6700\u540e\u8fdb\u884c\u7aef\u5230\u7aef\u9a8c\u8bc1\u3002", "result": "\u591a\u7528\u6237\u573a\u666f\u4e0b\u5fae\u8c03\u540e\u6027\u80fd\u6bd4\u9884\u8bad\u7ec3\u6a21\u578b\u63d0\u534715.7%\uff1b\u5355\u7528\u6237\u573a\u666f\u4e0b\u6a21\u578b\u5728\u672a\u89c1\u4f4d\u7f6e\u4e0a\u5b9e\u73b0\u63a5\u8fd1MRT\u7684\u6027\u80fd\uff0c\u541e\u5410\u91cf\u4ec5\u4e0b\u964d\u4e0d\u52300.7\u6bd4\u7279/\u4fe1\u9053\u4f7f\u7528\uff08\u603b\u541e\u5410\u91cf5.19\u6bd4\u7279/\u4fe1\u9053\u4f7f\u7528\uff09\uff1b\u7aef\u5230\u7aef\u9a8c\u8bc1\u786e\u8ba4\u4e86\u4e0eMRT\u76f8\u5f53\u7684\u76f8\u5e72\u529f\u7387\u805a\u7126\u80fd\u529b\u3002", "conclusion": "\u8be5\u6846\u67b6\u6210\u529f\u9a8c\u8bc1\u4e86AI\u9884\u7f16\u7801\u5668\u5728\u771f\u5b9eD-MIMO\u7cfb\u7edf\u4e2d\u7684\u6709\u6548\u6027\uff0c\u8bc1\u660e\u4e86\u771f\u5b9e\u4e16\u754c\u6d4b\u91cf\u7684\u6570\u636e\u6548\u7387\uff0c\u4e3a6G\u7f51\u7edc\u4e2dAI\u9a71\u52a8\u9884\u7f16\u7801\u7684\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u3002"}}
{"id": "2511.10913", "categories": ["cs.SD", "cs.AI", "cs.CR", "cs.MM", "eess.AS"], "pdf": "https://arxiv.org/pdf/2511.10913", "abs": "https://arxiv.org/abs/2511.10913", "authors": ["Guangke Chen", "Yuhui Wang", "Shouling Ji", "Xiapu Luo", "Ting Wang"], "title": "Synthetic Voices, Real Threats: Evaluating Large Text-to-Speech Models in Generating Harmful Audio", "comment": null, "summary": "Modern text-to-speech (TTS) systems, particularly those built on Large Audio-Language Models (LALMs), generate high-fidelity speech that faithfully reproduces input text and mimics specified speaker identities. While prior misuse studies have focused on speaker impersonation, this work explores a distinct content-centric threat: exploiting TTS systems to produce speech containing harmful content. Realizing such threats poses two core challenges: (1) LALM safety alignment frequently rejects harmful prompts, yet existing jailbreak attacks are ill-suited for TTS because these systems are designed to faithfully vocalize any input text, and (2) real-world deployment pipelines often employ input/output filters that block harmful text and audio.\n  We present HARMGEN, a suite of five attacks organized into two families that address these challenges. The first family employs semantic obfuscation techniques (Concat, Shuffle) that conceal harmful content within text. The second leverages audio-modality exploits (Read, Spell, Phoneme) that inject harmful content through auxiliary audio channels while maintaining benign textual prompts. Through evaluation across five commercial LALMs-based TTS systems and three datasets spanning two languages, we demonstrate that our attacks substantially reduce refusal rates and increase the toxicity of generated speech.\n  We further assess both reactive countermeasures deployed by audio-streaming platforms and proactive defenses implemented by TTS providers. Our analysis reveals critical vulnerabilities: deepfake detectors underperform on high-fidelity audio; reactive moderation can be circumvented by adversarial perturbations; while proactive moderation detects 57-93% of attacks. Our work highlights a previously underexplored content-centric misuse vector for TTS and underscore the need for robust cross-modal safeguards throughout training and deployment.", "AI": {"tldr": "HARMGEN\u662f\u4e00\u5957\u9488\u5bf9\u5927\u578b\u97f3\u9891\u8bed\u8a00\u6a21\u578b\u6587\u672c\u8f6c\u8bed\u97f3\u7cfb\u7edf\u7684\u653b\u51fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bed\u4e49\u6df7\u6dc6\u548c\u97f3\u9891\u6a21\u6001\u5229\u7528\u6280\u672f\u7ed5\u8fc7\u5b89\u5168\u9632\u62a4\uff0c\u751f\u6210\u6709\u5bb3\u8bed\u97f3\u5185\u5bb9\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u8bf4\u8bdd\u4eba\u6a21\u4eff\u5a01\u80c1\uff0c\u800c\u672c\u6587\u63a2\u7d22\u5185\u5bb9\u4e2d\u5fc3\u5a01\u80c1\uff1a\u5229\u7528TTS\u7cfb\u7edf\u751f\u6210\u5305\u542b\u6709\u5bb3\u5185\u5bb9\u7684\u8bed\u97f3\uff0c\u73b0\u6709\u5b89\u5168\u9632\u62a4\u5bf9\u6b64\u7c7b\u653b\u51fb\u5b58\u5728\u6f0f\u6d1e\u3002", "method": "\u5f00\u53d1\u4e86\u4e94\u7c7b\u653b\u51fb\u65b9\u6cd5\uff0c\u5206\u4e3a\u4e24\u4e2a\u5bb6\u65cf\uff1a\u8bed\u4e49\u6df7\u6dc6\u6280\u672f\uff08\u62fc\u63a5\u3001\u91cd\u6392\uff09\u5728\u6587\u672c\u4e2d\u9690\u85cf\u6709\u5bb3\u5185\u5bb9\uff1b\u97f3\u9891\u6a21\u6001\u5229\u7528\u6280\u672f\uff08\u6717\u8bfb\u3001\u62fc\u5199\u3001\u97f3\u7d20\uff09\u901a\u8fc7\u8f85\u52a9\u97f3\u9891\u901a\u9053\u6ce8\u5165\u6709\u5bb3\u5185\u5bb9\u3002", "result": "\u5728\u4e94\u4e2a\u5546\u4e1aLALM TTS\u7cfb\u7edf\u548c\u4e09\u4e2a\u53cc\u8bed\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u653b\u51fb\u663e\u8457\u964d\u4f4e\u4e86\u62d2\u7edd\u7387\u5e76\u589e\u52a0\u4e86\u751f\u6210\u8bed\u97f3\u7684\u6bd2\u6027\u3002", "conclusion": "\u63ed\u793a\u4e86TTS\u7cfb\u7edf\u5728\u5185\u5bb9\u4e2d\u5fc3\u6ee5\u7528\u65b9\u9762\u7684\u5173\u952e\u6f0f\u6d1e\uff0c\u5f3a\u8c03\u9700\u8981\u5728\u8bad\u7ec3\u548c\u90e8\u7f72\u8fc7\u7a0b\u4e2d\u5efa\u7acb\u5f3a\u5927\u7684\u8de8\u6a21\u6001\u5b89\u5168\u9632\u62a4\u673a\u5236\u3002"}}
{"id": "2511.11335", "categories": ["eess.SP", "cs.IT"], "pdf": "https://arxiv.org/pdf/2511.11335", "abs": "https://arxiv.org/abs/2511.11335", "authors": ["Yark\u0131n Gevez", "Aymen Khaleel", "Ertugrul Basar"], "title": "A Novel Partitioning Scheme for RIS Identification and Beamforming", "comment": "Accepted for publication in IEEE Wireless Communications Letters", "summary": "This letter introduces a novel partitioning scheme for reconfigurable intelligent surfaces (RISs) that simultaneously consider RIS identification and beamforming. The proposed scheme dynamicly and efficiently allocates RIS elements between identification and beamforming users, considering the different performance metrics associated with each of them. By employing a dynamic partitioning algorithm that efficiently manage the RIS resources (elements), the scheme significantly enhances the signal-to-noise ratio (SNR) while maintaining reliable identification performance. Finally, theoretical analysis and computer simulations are provided to demonstrate the validity of the proposed scheme.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u53ef\u91cd\u6784\u667a\u80fd\u8868\u9762\u5206\u533a\u65b9\u6848\uff0c\u540c\u65f6\u8003\u8651RIS\u8bc6\u522b\u548c\u6ce2\u675f\u6210\u5f62\u529f\u80fd\uff0c\u901a\u8fc7\u52a8\u6001\u5206\u914dRIS\u5355\u5143\u8d44\u6e90\u6765\u4f18\u5316\u7cfb\u7edf\u6027\u80fd", "motivation": "\u4f20\u7edfRIS\u7cfb\u7edf\u5728\u8bc6\u522b\u548c\u6ce2\u675f\u6210\u5f62\u529f\u80fd\u4e4b\u95f4\u5b58\u5728\u8d44\u6e90\u5206\u914d\u51b2\u7a81\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u540c\u65f6\u517c\u987e\u4e24\u79cd\u529f\u80fd\u6027\u80fd\u7684\u52a8\u6001\u5206\u533a\u65b9\u6848", "method": "\u91c7\u7528\u52a8\u6001\u5206\u533a\u7b97\u6cd5\uff0c\u6839\u636e\u8bc6\u522b\u7528\u6237\u548c\u6ce2\u675f\u6210\u5f62\u7528\u6237\u7684\u4e0d\u540c\u6027\u80fd\u6307\u6807\uff0c\u9ad8\u6548\u5206\u914dRIS\u5355\u5143\u8d44\u6e90", "result": "\u8be5\u65b9\u6848\u663e\u8457\u63d0\u5347\u4e86\u4fe1\u566a\u6bd4\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u53ef\u9760\u7684\u8bc6\u522b\u6027\u80fd\uff0c\u7406\u8bba\u5206\u6790\u548c\u4eff\u771f\u9a8c\u8bc1\u4e86\u65b9\u6848\u7684\u6709\u6548\u6027", "conclusion": "\u6240\u63d0\u51fa\u7684\u52a8\u6001\u5206\u533a\u65b9\u6848\u80fd\u591f\u6709\u6548\u5e73\u8861RIS\u8bc6\u522b\u548c\u6ce2\u675f\u6210\u5f62\u529f\u80fd\uff0c\u5b9e\u73b0\u7cfb\u7edf\u6027\u80fd\u7684\u6574\u4f53\u4f18\u5316"}}
{"id": "2511.10935", "categories": ["cs.SD", "cs.LG", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2511.10935", "abs": "https://arxiv.org/abs/2511.10935", "authors": ["Yifan Zhuang", "Calvin Huang", "Zepeng Yu", "Yongjie Zou", "Jiawei Ju"], "title": "CAT-Net: A Cross-Attention Tone Network for Cross-Subject EEG-EMG Fusion Tone Decoding", "comment": "This is the extended version with technical appendices. The version of record appears in AAAI-26. Please cite the AAAI version", "summary": "Brain-computer interface (BCI) speech decoding has emerged as a promising tool for assisting individuals with speech impairments. In this context, the integration of electroencephalography (EEG) and electromyography (EMG) signals offers strong potential for enhancing decoding performance. Mandarin tone classification presents particular challenges, as tonal variations convey distinct meanings even when phonemes remain identical. In this study, we propose a novel cross-subject multimodal BCI decoding framework that fuses EEG and EMG signals to classify four Mandarin tones under both audible and silent speech conditions. Inspired by the cooperative mechanisms of neural and muscular systems in speech production, our neural decoding architecture combines spatial-temporal feature extraction branches with a cross-attention fusion mechanism, enabling informative interaction between modalities. We further incorporate domain-adversarial training to improve cross-subject generalization. We collected 4,800 EEG trials and 4,800 EMG trials from 10 participants using only twenty EEG and five EMG channels, demonstrating the feasibility of minimal-channel decoding. Despite employing lightweight modules, our model outperforms state-of-the-art baselines across all conditions, achieving average classification accuracies of 87.83% for audible speech and 88.08% for silent speech. In cross-subject evaluations, it still maintains strong performance with accuracies of 83.27% and 85.10% for audible and silent speech, respectively. We further conduct ablation studies to validate the effectiveness of each component. Our findings suggest that tone-level decoding with minimal EEG-EMG channels is feasible and potentially generalizable across subjects, contributing to the development of practical BCI applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u8de8\u88ab\u8bd5\u591a\u6a21\u6001\u8111\u673a\u63a5\u53e3\u89e3\u7801\u6846\u67b6\uff0c\u878d\u5408EEG\u548cEMG\u4fe1\u53f7\u6765\u5206\u7c7b\u666e\u901a\u8bdd\u56db\u58f0\uff0c\u5728\u53ef\u542c\u548c\u65e0\u58f0\u8bed\u97f3\u6761\u4ef6\u4e0b\u5747\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u666e\u901a\u8bdd\u58f0\u8c03\u5206\u7c7b\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u4e3a\u58f0\u8c03\u53d8\u5316\u5373\u4f7f\u97f3\u7d20\u76f8\u540c\u4e5f\u4f1a\u4f20\u8fbe\u4e0d\u540c\u542b\u4e49\u3002EEG\u548cEMG\u4fe1\u53f7\u7684\u6574\u5408\u6709\u671b\u63d0\u9ad8\u89e3\u7801\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u8f85\u52a9\u8a00\u8bed\u969c\u788d\u4eba\u7fa4\u65b9\u9762\u3002", "method": "\u91c7\u7528\u8de8\u6ce8\u610f\u529b\u878d\u5408\u673a\u5236\u7684\u795e\u7ecf\u89e3\u7801\u67b6\u6784\uff0c\u7ed3\u5408\u65f6\u7a7a\u7279\u5f81\u63d0\u53d6\u5206\u652f\uff0c\u5e76\u878d\u5165\u9886\u57df\u5bf9\u6297\u8bad\u7ec3\u4ee5\u6539\u5584\u8de8\u88ab\u8bd5\u6cdb\u5316\u80fd\u529b\u3002\u4ec5\u4f7f\u752820\u4e2aEEG\u901a\u9053\u548c5\u4e2aEMG\u901a\u9053\u8fdb\u884c\u6700\u5c0f\u901a\u9053\u89e3\u7801\u3002", "result": "\u5728\u6240\u6709\u6761\u4ef6\u4e0b\u5747\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u53ef\u542c\u8bed\u97f3\u5e73\u5747\u5206\u7c7b\u51c6\u786e\u7387\u4e3a87.83%\uff0c\u65e0\u58f0\u8bed\u97f3\u4e3a88.08%\u3002\u8de8\u88ab\u8bd5\u8bc4\u4f30\u4e2d\u4ecd\u4fdd\u6301\u5f3a\u52b2\u6027\u80fd\uff0c\u53ef\u542c\u8bed\u97f3\u51c6\u786e\u738783.27%\uff0c\u65e0\u58f0\u8bed\u97f385.10%\u3002", "conclusion": "\u4f7f\u7528\u6700\u5c0fEEG-EMG\u901a\u9053\u8fdb\u884c\u58f0\u8c03\u7ea7\u89e3\u7801\u662f\u53ef\u884c\u7684\uff0c\u4e14\u53ef\u80fd\u8de8\u88ab\u8bd5\u6cdb\u5316\uff0c\u6709\u52a9\u4e8e\u5f00\u53d1\u5b9e\u7528\u7684BCI\u5e94\u7528\u3002"}}
{"id": "2511.11386", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.11386", "abs": "https://arxiv.org/abs/2511.11386", "authors": ["Junzhe Song", "Ruisi He", "Mi Yang", "Zhengyu Zhang", "Shuaiqi Gao", "Bo Ai"], "title": "A Geometry Map-Based Propagation Model for Urban Channels", "comment": null, "summary": "With the rapid deployment of 5G and future 6G networks, accurate modeling of urban radio propagation has become critical for system design and network planning. However, conventional statistical or empirical models fail to fully capture the influence of detailed geometric features in dense urban environments. In this paper, we propose a geometry map-based propagation model that directly extracts key parameters from a 3D geometry map and incorporates the Uniform Theory of Diffraction (UTD) to recursively compute multiple diffraction fields, thereby enabling accurate prediction of large-scale path loss and time-varying Doppler characteristics in urban channels. A significant buildings identification algorithm is developed to efficiently detect buildings that significantly affect signal propagation. The proposed model is validated using urban measurement data, showing excellent agreement with path loss in both LOS and NLOS conditions. In particular, for NLOS scenarios with complex diffraction mechanisms, it outperforms the 3GPP and simplified models, reducing the RMSE by 7.1 dB and 3.18 dB, respectively. Doppler analysis further demonstrates its accuracy in capturing time-varying propagation characteristics, confirming the scalability and generalization capability of the model in urban environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u51e0\u4f55\u5730\u56fe\u7684\u4f20\u64ad\u6a21\u578b\uff0c\u901a\u8fc7\u4ece3D\u51e0\u4f55\u5730\u56fe\u63d0\u53d6\u5173\u952e\u53c2\u6570\u5e76\u7ed3\u5408\u5747\u5300\u884d\u5c04\u7406\u8bba\uff0c\u80fd\u591f\u51c6\u786e\u9884\u6d4b\u57ce\u5e02\u73af\u5883\u4e2d\u7684\u8def\u5f84\u635f\u8017\u548c\u591a\u666e\u52d2\u7279\u6027\u3002", "motivation": "\u968f\u77405G/6G\u7f51\u7edc\u90e8\u7f72\uff0c\u4f20\u7edf\u7edf\u8ba1\u6a21\u578b\u65e0\u6cd5\u5145\u5206\u6355\u6349\u5bc6\u96c6\u57ce\u5e02\u73af\u5883\u4e2d\u51e0\u4f55\u7279\u5f81\u5bf9\u65e0\u7ebf\u7535\u4f20\u64ad\u7684\u5f71\u54cd\uff0c\u9700\u8981\u66f4\u7cbe\u786e\u7684\u5efa\u6a21\u65b9\u6cd5\u3002", "method": "\u5f00\u53d1\u4e86\u57fa\u4e8e3D\u51e0\u4f55\u5730\u56fe\u7684\u4f20\u64ad\u6a21\u578b\uff0c\u91c7\u7528\u663e\u8457\u5efa\u7b51\u7269\u8bc6\u522b\u7b97\u6cd5\u68c0\u6d4b\u5f71\u54cd\u4f20\u64ad\u7684\u5173\u952e\u5efa\u7b51\u7269\uff0c\u5e76\u5229\u7528\u5747\u5300\u884d\u5c04\u7406\u8bba\u9012\u5f52\u8ba1\u7b97\u591a\u6b21\u884d\u5c04\u573a\u3002", "result": "\u6a21\u578b\u9a8c\u8bc1\u663e\u793a\u5728LOS\u548cNLOS\u6761\u4ef6\u4e0b\u4e0e\u5b9e\u6d4b\u6570\u636e\u9ad8\u5ea6\u4e00\u81f4\uff0c\u5728\u590d\u6742NLOS\u573a\u666f\u4e2d\u6bd43GPP\u548c\u7b80\u5316\u6a21\u578b\u7684RMSE\u5206\u522b\u964d\u4f4e\u4e867.1 dB\u548c3.18 dB\uff0c\u591a\u666e\u52d2\u5206\u6790\u4e5f\u8bc1\u660e\u4e86\u5176\u51c6\u786e\u6027\u3002", "conclusion": "\u8be5\u6a21\u578b\u5728\u57ce\u5e02\u73af\u5883\u4e2d\u5177\u6709\u826f\u597d\u7684\u53ef\u6269\u5c55\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u80fd\u591f\u51c6\u786e\u9884\u6d4b\u5927\u89c4\u6a21\u8def\u5f84\u635f\u8017\u548c\u65f6\u53d8\u591a\u666e\u52d2\u7279\u6027\u3002"}}
{"id": "2511.11000", "categories": ["cs.SD", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11000", "abs": "https://arxiv.org/abs/2511.11000", "authors": ["HongYu Liu", "Junxin Li", "Changxi Guo", "Hao Chen", "Yaqian Huang", "Yifu Guo", "Huan Yang", "Lihua Cai"], "title": "DialogGraph-LLM: Graph-Informed LLMs for End-to-End Audio Dialogue Intent Recognition", "comment": "8 pages, 2 figures; Series: Frontiers in Artificial Intelligence and Applications, Volume 413: ECAI 2025", "summary": "Recognizing speaker intent in long audio dialogues among speakers has a wide range of applications, but is a non-trivial AI task due to complex inter-dependencies in speaker utterances and scarce annotated data. To address these challenges, an end-to-end framework, namely DialogGraph-LLM, is proposed in the current work. DialogGraph-LLM combines a novel Multi-Relational Dialogue Attention Network (MR-DAN) architecture with multimodal foundation models (e.g., Qwen2.5-Omni-7B) for direct acoustic-to-intent inference. An adaptive semi-supervised learning strategy is designed using LLM with a confidence-aware pseudo-label generation mechanism based on dual-threshold filtering using both global and class confidences, and an entropy-based sample selection process that prioritizes high-information unlabeled instances. Extensive evaluations on the proprietary MarketCalls corpus and the publicly available MIntRec 2.0 benchmark demonstrate DialogGraph-LLM's superiority over strong audio and text-driven baselines. The framework demonstrates strong performance and efficiency in intent recognition in real world scenario audio dialogues, proving its practical value for audio-rich domains with limited supervision. Our code is available at https://github.com/david188888/DialogGraph-LLM.", "AI": {"tldr": "\u63d0\u51fa\u4e86DialogGraph-LLM\u6846\u67b6\uff0c\u7ed3\u5408\u591a\u5173\u7cfb\u5bf9\u8bdd\u6ce8\u610f\u529b\u7f51\u7edc\u548c\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\uff0c\u7528\u4e8e\u957f\u97f3\u9891\u5bf9\u8bdd\u4e2d\u7684\u8bf4\u8bdd\u4eba\u610f\u56fe\u8bc6\u522b\uff0c\u901a\u8fc7\u534a\u76d1\u7763\u5b66\u4e60\u7b56\u7565\u5728\u6709\u9650\u6807\u6ce8\u6570\u636e\u4e0b\u5b9e\u73b0\u9ad8\u6548\u610f\u56fe\u8bc6\u522b\u3002", "motivation": "\u89e3\u51b3\u957f\u97f3\u9891\u5bf9\u8bdd\u4e2d\u8bf4\u8bdd\u4eba\u610f\u56fe\u8bc6\u522b\u7684\u6311\u6218\uff0c\u5305\u62ec\u590d\u6742\u7684\u8bf4\u8bdd\u4eba\u8bdd\u8bed\u76f8\u4e92\u4f9d\u8d56\u6027\u548c\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u591a\u5173\u7cfb\u5bf9\u8bdd\u6ce8\u610f\u529b\u7f51\u7edc(MR-DAN)\u67b6\u6784\u7ed3\u5408\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\uff0c\u91c7\u7528\u57fa\u4e8e\u53cc\u9608\u503c\u8fc7\u6ee4\u548c\u71b5\u7684\u6837\u672c\u9009\u62e9\u7684\u534a\u76d1\u7763\u5b66\u4e60\u7b56\u7565\u3002", "result": "\u5728MarketCalls\u8bed\u6599\u5e93\u548cMIntRec 2.0\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u5f3a\u5927\u7684\u97f3\u9891\u548c\u6587\u672c\u9a71\u52a8\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728\u771f\u5b9e\u573a\u666f\u97f3\u9891\u5bf9\u8bdd\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6027\u80fd\u548c\u6548\u7387\u3002", "conclusion": "DialogGraph-LLM\u6846\u67b6\u5728\u97f3\u9891\u4e30\u5bcc\u4e14\u76d1\u7763\u6709\u9650\u7684\u9886\u57df\u4e2d\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u8bf4\u8bdd\u4eba\u610f\u56fe\u8bc6\u522b\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2511.11392", "categories": ["eess.SP", "cs.RO"], "pdf": "https://arxiv.org/pdf/2511.11392", "abs": "https://arxiv.org/abs/2511.11392", "authors": ["Owen A. Maute", "Blake A. Roberts", "Berker Pek\u00f6z"], "title": "RadAround: A Field-Expedient Direction Finder for Contested IoT Sensing & EM Situational Awareness", "comment": "6 pages. Cite as O. Maute, B. A. Roberts, and B. Pek\u00f6z, \"RadAround: A field-expedient direction finder for contested IoT sensing & EM situational awareness,\" in Proc. 2025 IEEE Military Commun. Conf. (MILCOM), Los Angeles, USA, Oct. 2025, pp. 1-6", "summary": "This paper presents RadAround, a passive 2-D direction-finding system designed for adversarial IoT sensing in contested environments. Using mechanically steered narrow-beam antennas and field-deployable SCADA software, it generates high-resolution electromagnetic (EM) heatmaps using low-cost COTS or 3D-printed components. The microcontroller-deployable SCADA coordinates antenna positioning and SDR sampling in real time for resilient, on-site operation. Its modular design enables rapid adaptation for applications such as EMC testing in disaster-response deployments, battlefield spectrum monitoring, electronic intrusion detection, and tactical EM situational awareness (EMSA). Experiments show RadAround detecting computing machinery through walls, assessing utilization, and pinpointing EM interference (EMI) leakage sources from Faraday enclosures.", "AI": {"tldr": "RadAround\u662f\u4e00\u4e2a\u7528\u4e8e\u5bf9\u6297\u6027\u7269\u8054\u7f51\u4f20\u611f\u7684\u88ab\u52a82D\u6d4b\u5411\u7cfb\u7edf\uff0c\u4f7f\u7528\u673a\u68b0\u8f6c\u5411\u7a84\u6ce2\u675f\u5929\u7ebf\u548c\u73b0\u573a\u53ef\u90e8\u7f72\u7684SCADA\u8f6f\u4ef6\uff0c\u901a\u8fc7\u4f4e\u6210\u672c\u5546\u7528\u62163D\u6253\u5370\u7ec4\u4ef6\u751f\u6210\u9ad8\u5206\u8fa8\u7387\u7535\u78c1\u70ed\u56fe\u3002", "motivation": "\u5728\u7ade\u4e89\u73af\u5883\u4e2d\u8fdb\u884c\u5bf9\u6297\u6027\u7269\u8054\u7f51\u4f20\u611f\uff0c\u9700\u8981\u80fd\u591f\u5728\u73b0\u573a\u64cd\u4f5c\u3001\u5177\u6709\u5f39\u6027\u7684\u7535\u78c1\u76d1\u6d4b\u7cfb\u7edf\uff0c\u7528\u4e8e\u707e\u96be\u54cd\u5e94\u90e8\u7f72\u3001\u6218\u573a\u9891\u8c31\u76d1\u6d4b\u3001\u7535\u5b50\u5165\u4fb5\u68c0\u6d4b\u548c\u6218\u672f\u7535\u78c1\u6001\u52bf\u611f\u77e5\u7b49\u5e94\u7528\u3002", "method": "\u4f7f\u7528\u673a\u68b0\u8f6c\u5411\u7a84\u6ce2\u675f\u5929\u7ebf\u548c\u5fae\u63a7\u5236\u5668\u53ef\u90e8\u7f72\u7684SCADA\u8f6f\u4ef6\uff0c\u5b9e\u65f6\u534f\u8c03\u5929\u7ebf\u5b9a\u4f4d\u548c\u8f6f\u4ef6\u5b9a\u4e49\u65e0\u7ebf\u7535\u91c7\u6837\uff0c\u91c7\u7528\u6a21\u5757\u5316\u8bbe\u8ba1\u4fbf\u4e8e\u5feb\u901f\u9002\u5e94\u4e0d\u540c\u5e94\u7528\u573a\u666f\u3002", "result": "\u5b9e\u9a8c\u663e\u793aRadAround\u80fd\u591f\u7a7f\u900f\u5899\u58c1\u68c0\u6d4b\u8ba1\u7b97\u8bbe\u5907\u3001\u8bc4\u4f30\u8bbe\u5907\u5229\u7528\u7387\uff0c\u5e76\u7cbe\u786e\u5b9a\u4f4d\u6cd5\u62c9\u7b2c\u5916\u58f3\u7684\u7535\u78c1\u5e72\u6270\u6cc4\u6f0f\u6e90\u3002", "conclusion": "RadAround\u63d0\u4f9b\u4e86\u4e00\u4e2a\u4f4e\u6210\u672c\u3001\u9ad8\u5206\u8fa8\u7387\u7684\u88ab\u52a8\u7535\u78c1\u6d4b\u5411\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u5404\u79cd\u5bf9\u6297\u6027\u73af\u5883\u4e2d\u7684\u7535\u78c1\u76d1\u6d4b\u548c\u4f20\u611f\u5e94\u7528\u3002"}}
{"id": "2511.11006", "categories": ["cs.SD", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11006", "abs": "https://arxiv.org/abs/2511.11006", "authors": ["HongYu Liu", "Ruijie Wan", "Yueju Han", "Junxin Li", "Liuxing Lu", "Chao He", "Lihua Cai"], "title": "MSMT-FN: Multi-segment Multi-task Fusion Network for Marketing Audio Classification", "comment": "Accepted at The 21st International Conference on Advanced Data Mining and Applications (ADMA 2025). In book: Advanced Data Mining and Applications (pp.306-320)", "summary": "Audio classification plays an essential role in sentiment analysis and emotion recognition, especially for analyzing customer attitudes in marketing phone calls. Efficiently categorizing customer purchasing propensity from large volumes of audio data remains challenging. In this work, we propose a novel Multi-Segment Multi-Task Fusion Network (MSMT-FN) that is uniquely designed for addressing this business demand. Evaluations conducted on our proprietary MarketCalls dataset, as well as established benchmarks (CMU-MOSI, CMU-MOSEI, and MELD), show MSMT-FN consistently outperforms or matches state-of-the-art methods. Additionally, our newly curated MarketCalls dataset will be available upon request, and the code base is made accessible at GitHub Repository MSMT-FN, to facilitate further research and advancements in audio classification domain.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u591a\u6bb5\u591a\u4efb\u52a1\u878d\u5408\u7f51\u7edc(MSMT-FN)\uff0c\u7528\u4e8e\u4ece\u97f3\u9891\u6570\u636e\u4e2d\u9ad8\u6548\u5206\u7c7b\u5ba2\u6237\u8d2d\u4e70\u503e\u5411\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u6216\u5339\u914d\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\u3002", "motivation": "\u97f3\u9891\u5206\u7c7b\u5728\u60c5\u611f\u5206\u6790\u548c\u60c5\u7eea\u8bc6\u522b\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u7279\u522b\u662f\u5728\u5206\u6790\u8425\u9500\u7535\u8bdd\u4e2d\u7684\u5ba2\u6237\u6001\u5ea6\u65f6\uff0c\u4ece\u5927\u91cf\u97f3\u9891\u6570\u636e\u4e2d\u9ad8\u6548\u5206\u7c7b\u5ba2\u6237\u8d2d\u4e70\u503e\u5411\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u591a\u6bb5\u591a\u4efb\u52a1\u878d\u5408\u7f51\u7edc(MSMT-FN)\uff0c\u4e13\u95e8\u9488\u5bf9\u8fd9\u4e00\u4e1a\u52a1\u9700\u6c42\u8bbe\u8ba1\uff0c\u5e76\u5728\u81ea\u6709\u7684MarketCalls\u6570\u636e\u96c6\u4ee5\u53caCMU-MOSI\u3001CMU-MOSEI\u548cMELD\u7b49\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "MSMT-FN\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u6301\u7eed\u4f18\u4e8e\u6216\u5339\u914d\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\uff0c\u65b0\u6784\u5efa\u7684MarketCalls\u6570\u636e\u96c6\u548c\u4ee3\u7801\u5e93\u5c06\u516c\u5f00\u4ee5\u4fc3\u8fdb\u97f3\u9891\u5206\u7c7b\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u7814\u7a76\u3002", "conclusion": "MSMT-FN\u7f51\u7edc\u6709\u6548\u89e3\u51b3\u4e86\u4ece\u97f3\u9891\u6570\u636e\u5206\u7c7b\u5ba2\u6237\u8d2d\u4e70\u503e\u5411\u7684\u4e1a\u52a1\u9700\u6c42\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u97f3\u9891\u5206\u7c7b\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u548c\u6570\u636e\u96c6\u8d44\u6e90\u3002"}}
{"id": "2511.11414", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.11414", "abs": "https://arxiv.org/abs/2511.11414", "authors": ["Giulia Mariani", "Chiara Lambranzi", "Nicholas Cartocci", "Giacinto Barresi", "Christian Di Natali", "Elena De Momi", "Jesus Ortiz"], "title": "Physiological Measures of the Mental Workload in Users of a Lower Limb Exosuit: A Comparison of Subjective and Objective Metrics", "comment": "to be published in 2025 IEEE SMC proceedings", "summary": "Lower-limb exosuits are particularly relevant for individuals with some degree of mobility impairment, such as post-stroke patients or older adults with reduced movement capabilities. This study aims to investigate the mental workload (MWL) assessment of XoSoft, a lower-limb soft exoskeleton, using and comparing subjective and objective physiological metrics. The NASA-TLX questionnaire, the average percentage change in pupil size (APCPS), and the Baevsky stress index (SI) are compared. The experiments were conducted on 18 healthy subjects while walking and involved mathematical tasks to create a double-task condition. The results show a complex interaction between task difficulty, exoskeleton activation, and pupillary dynamics, suggesting that the subject might reach a saturated condition under a high mental load. Besides, the data indicate that pupil diameter may be an objective mental workload indicator that correlates with subjective NASA-TLX questionnaires. The discordant indications from the stress index suggest how different metrics of the ocular and cardiac levels respond differently to various stimuli and dynamics. Research has also revealed ocular asymmetry, with the right eye more sensitive to cognitive load.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86XoSoft\u4e0b\u80a2\u8f6f\u5916\u9aa8\u9abc\u7684\u5fc3\u7406\u8d1f\u8377\uff0c\u6bd4\u8f83\u4e86\u4e3b\u89c2\u95ee\u5377\u548c\u5ba2\u89c2\u751f\u7406\u6307\u6807\uff0c\u53d1\u73b0\u5728\u9ad8\u8ba4\u77e5\u8d1f\u8377\u4e0b\u53d7\u8bd5\u8005\u53ef\u80fd\u8fbe\u5230\u9971\u548c\u72b6\u6001\uff0c\u77b3\u5b54\u76f4\u5f84\u53ef\u4f5c\u4e3a\u5ba2\u89c2\u5fc3\u7406\u8d1f\u8377\u6307\u6807\u3002", "motivation": "\u8bc4\u4f30\u4e0b\u80a2\u5916\u9aa8\u9abc\u5bf9\u7528\u6237\u5fc3\u7406\u8d1f\u8377\u7684\u5f71\u54cd\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u884c\u52a8\u80fd\u529b\u53d7\u635f\u7684\u4eba\u7fa4\u5982\u4e2d\u98ce\u60a3\u8005\u6216\u8001\u5e74\u4eba\uff0c\u9700\u8981\u53ef\u9760\u7684\u5fc3\u7406\u8d1f\u8377\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528NASA-TLX\u95ee\u5377\u3001\u77b3\u5b54\u5927\u5c0f\u5e73\u5747\u53d8\u5316\u767e\u5206\u6bd4\u548cBaevsky\u538b\u529b\u6307\u6570\uff0c\u5bf918\u540d\u5065\u5eb7\u53d7\u8bd5\u8005\u5728\u884c\u8d70\u548c\u6570\u5b66\u4efb\u52a1\u53cc\u91cd\u4efb\u52a1\u6761\u4ef6\u4e0b\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u4efb\u52a1\u96be\u5ea6\u3001\u5916\u9aa8\u9abc\u6fc0\u6d3b\u548c\u77b3\u5b54\u52a8\u6001\u4e4b\u95f4\u5b58\u5728\u590d\u6742\u4ea4\u4e92\u4f5c\u7528\uff1b\u77b3\u5b54\u76f4\u5f84\u4e0eNASA-TLX\u95ee\u5377\u76f8\u5173\uff0c\u53ef\u4f5c\u4e3a\u5ba2\u89c2\u5fc3\u7406\u8d1f\u8377\u6307\u6807\uff1b\u53f3\u773c\u5bf9\u8ba4\u77e5\u8d1f\u8377\u66f4\u654f\u611f\u3002", "conclusion": "\u4e0d\u540c\u751f\u7406\u6307\u6807\u5bf9\u523a\u6fc0\u54cd\u5e94\u4e0d\u540c\uff0c\u77b3\u5b54\u76f4\u5f84\u662f\u6709\u6548\u7684\u5fc3\u7406\u8d1f\u8377\u5ba2\u89c2\u6307\u6807\uff0c\u4f46\u9700\u8981\u7efc\u5408\u8003\u8651\u591a\u79cd\u6307\u6807\u6765\u5168\u9762\u8bc4\u4f30\u5916\u9aa8\u9abc\u4f7f\u7528\u65f6\u7684\u5fc3\u7406\u8d1f\u8377\u3002"}}
{"id": "2511.11039", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2511.11039", "abs": "https://arxiv.org/abs/2511.11039", "authors": ["Hualei Wang", "Yiming Li", "Shuo Ma", "Hong Liu", "Xiangdong Wang"], "title": "TimeAudio: Bridging Temporal Gaps in Large Audio-Language Models", "comment": "Accepted by The Fortieth AAAI Conference on Artificial Intelligence (AAAI 2026)", "summary": "Recent Large Audio-Language Models (LALMs) exhibit impressive capabilities in understanding audio content for conversational QA tasks. However, these models struggle to accurately understand timestamps for temporal localization (e.g., Temporal Audio Grounding) and are restricted to short audio perception, leading to constrained capabilities on fine-grained tasks. We identify three key aspects that limit their temporal localization and long audio understanding: (i) timestamp representation, (ii) architecture, and (iii) data. To address this, we introduce TimeAudio, a novel method that empowers LALMs to connect their understanding of audio content with precise temporal perception. Specifically, we incorporate unique temporal markers to improve time-sensitive reasoning and apply an absolute time-aware encoding that explicitly grounds the acoustic features with absolute time information. Moreover, to achieve end-to-end long audio understanding, we introduce a segment-level token merging module to substantially reduce audio token redundancy and enhance the efficiency of information extraction. Due to the lack of suitable datasets and evaluation metrics, we consolidate existing audio datasets into a new dataset focused on temporal tasks and establish a series of metrics to evaluate the fine-grained performance. Evaluations show strong performance across a variety of fine-grained tasks, such as dense captioning, temporal grounding, and timeline speech summarization, demonstrating TimeAudio's robust temporal localization and reasoning capabilities.", "AI": {"tldr": "TimeAudio\u662f\u4e00\u79cd\u589e\u5f3a\u5927\u578b\u97f3\u9891\u8bed\u8a00\u6a21\u578b\u65f6\u95f4\u5b9a\u4f4d\u548c\u957f\u97f3\u9891\u7406\u89e3\u80fd\u529b\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u65f6\u95f4\u6807\u8bb0\u5668\u3001\u7edd\u5bf9\u65f6\u95f4\u611f\u77e5\u7f16\u7801\u548c\u6bb5\u7ea7token\u5408\u5e76\u6a21\u5757\u89e3\u51b3\u73b0\u6709\u6a21\u578b\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u5927\u578b\u97f3\u9891\u8bed\u8a00\u6a21\u578b\u5728\u65f6\u95f4\u5b9a\u4f4d\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u4e14\u53d7\u9650\u4e8e\u77ed\u97f3\u9891\u611f\u77e5\uff0c\u65e0\u6cd5\u5904\u7406\u7ec6\u7c92\u5ea6\u4efb\u52a1\u3002\u4e3b\u8981\u95ee\u9898\u5305\u62ec\u65f6\u95f4\u6233\u8868\u793a\u3001\u67b6\u6784\u548c\u6570\u636e\u4e09\u4e2a\u65b9\u9762\u7684\u9650\u5236\u3002", "method": "\u5f15\u5165\u72ec\u7279\u7684\u65f6\u95f4\u6807\u8bb0\u5668\u6539\u8fdb\u65f6\u95f4\u654f\u611f\u63a8\u7406\uff0c\u5e94\u7528\u7edd\u5bf9\u65f6\u95f4\u611f\u77e5\u7f16\u7801\u5c06\u58f0\u5b66\u7279\u5f81\u4e0e\u7edd\u5bf9\u65f6\u95f4\u4fe1\u606f\u663e\u5f0f\u5173\u8054\uff0c\u4f7f\u7528\u6bb5\u7ea7token\u5408\u5e76\u6a21\u5757\u51cf\u5c11\u97f3\u9891token\u5197\u4f59\u5e76\u63d0\u9ad8\u4fe1\u606f\u63d0\u53d6\u6548\u7387\u3002", "result": "\u5728\u5bc6\u96c6\u63cf\u8ff0\u3001\u65f6\u95f4\u5b9a\u4f4d\u548c\u65f6\u95f4\u7ebf\u8bed\u97f3\u6458\u8981\u7b49\u591a\u79cd\u7ec6\u7c92\u5ea6\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u5f3a\u5927\u6027\u80fd\uff0c\u5c55\u793a\u4e86\u7a33\u5065\u7684\u65f6\u95f4\u5b9a\u4f4d\u548c\u63a8\u7406\u80fd\u529b\u3002", "conclusion": "TimeAudio\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u5927\u578b\u97f3\u9891\u8bed\u8a00\u6a21\u578b\u5728\u65f6\u95f4\u5b9a\u4f4d\u548c\u957f\u97f3\u9891\u7406\u89e3\u65b9\u9762\u7684\u5173\u952e\u9650\u5236\uff0c\u4e3a\u7ec6\u7c92\u5ea6\u97f3\u9891\u7406\u89e3\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u529b\u5de5\u5177\u3002"}}
{"id": "2511.11449", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.11449", "abs": "https://arxiv.org/abs/2511.11449", "authors": ["Chibuzor Henry Amadi", "Akaniyene Benard Obot", "Kufre Monday Udofia", "Olaoluwa Ayodeji Adegboye"], "title": "Analytical Modelling of the Impact of Foliage Cover on the Propagation Loss in a Smart Farming Wireless IOT Application", "comment": null, "summary": "This study presents an analytical model of foliage cover impact on propagation loss in smart farming IoT applications.", "AI": {"tldr": "\u5206\u6790\u53f6\u51a0\u8986\u76d6\u5bf9\u667a\u80fd\u519c\u4e1a\u7269\u8054\u7f51\u4f20\u64ad\u635f\u8017\u5f71\u54cd\u7684\u6a21\u578b", "motivation": "\u667a\u80fd\u519c\u4e1a\u7269\u8054\u7f51\u5e94\u7528\u4e2d\uff0c\u690d\u88ab\u8986\u76d6\u5bf9\u65e0\u7ebf\u4fe1\u53f7\u4f20\u64ad\u7684\u5f71\u54cd\u9700\u8981\u91cf\u5316\u5206\u6790", "method": "\u5f00\u53d1\u5206\u6790\u6a21\u578b\u6765\u7814\u7a76\u53f6\u51a0\u8986\u76d6\u5bf9\u4f20\u64ad\u635f\u8017\u7684\u5f71\u54cd", "result": "\u63d0\u51fa\u4e86\u80fd\u591f\u8bc4\u4f30\u53f6\u51a0\u8986\u76d6\u5bf9\u4f20\u64ad\u635f\u8017\u5f71\u54cd\u7684\u5206\u6790\u6a21\u578b", "conclusion": "\u8be5\u6a21\u578b\u6709\u52a9\u4e8e\u4f18\u5316\u667a\u80fd\u519c\u4e1a\u7269\u8054\u7f51\u7cfb\u7edf\u7684\u65e0\u7ebf\u901a\u4fe1\u6027\u80fd"}}
{"id": "2511.11104", "categories": ["cs.SD", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.11104", "abs": "https://arxiv.org/abs/2511.11104", "authors": ["Crystal Min Hui Poon", "Pai Chet Ng", "Xiaoxiao Miao", "Immanuel Jun Kai Loh", "Bowen Zhang", "Haoyu Song", "Ian Mcloughlin"], "title": "CLARITY: Contextual Linguistic Adaptation and Accent Retrieval for Dual-Bias Mitigation in Text-to-Speech Generation", "comment": "Submitted to ICASSP 2026", "summary": "Instruction-guided text-to-speech (TTS) research has reached a maturity level where excellent speech generation quality is possible on demand, yet two coupled biases persist: accent bias, where models default to dominant phonetic patterns, and linguistic bias, where dialect-specific lexical and cultural cues are ignored. These biases are interdependent, as authentic accent generation requires both accent fidelity and localized text. We present Contextual Linguistic Adaptation and Retrieval for Inclusive TTS sYnthesis (CLARITY), a backbone-agnostic framework that addresses these biases through dual-signal optimization: (i) contextual linguistic adaptation that localizes input text to the target dialect, and (ii) retrieval-augmented accent prompting (RAAP) that supplies accent-consistent speech prompts. Across twelve English accents, CLARITY improves accent accuracy and fairness while maintaining strong perceptual quality.", "AI": {"tldr": "CLARITY\u662f\u4e00\u4e2a\u89e3\u51b3TTS\u7cfb\u7edf\u4e2d\u53e3\u97f3\u504f\u89c1\u548c\u8bed\u8a00\u504f\u89c1\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u4e0a\u4e0b\u6587\u8bed\u8a00\u9002\u5e94\u548c\u68c0\u7d22\u589e\u5f3a\u7684\u53e3\u97f3\u63d0\u793a\u6765\u4f18\u5316\u53cc\u4fe1\u53f7\u3002", "motivation": "\u5f53\u524d\u6307\u4ee4\u5f15\u5bfc\u7684TTS\u7cfb\u7edf\u5b58\u5728\u4e24\u4e2a\u76f8\u4e92\u5173\u8054\u7684\u504f\u89c1\uff1a\u53e3\u97f3\u504f\u89c1\uff08\u6a21\u578b\u9ed8\u8ba4\u4f7f\u7528\u4e3b\u5bfc\u8bed\u97f3\u6a21\u5f0f\uff09\u548c\u8bed\u8a00\u504f\u89c1\uff08\u5ffd\u7565\u65b9\u8a00\u7279\u5b9a\u7684\u8bcd\u6c47\u548c\u6587\u5316\u7ebf\u7d22\uff09\u3002", "method": "CLARITY\u91c7\u7528\u53cc\u4fe1\u53f7\u4f18\u5316\uff1a\u4e0a\u4e0b\u6587\u8bed\u8a00\u9002\u5e94\u5c06\u8f93\u5165\u6587\u672c\u672c\u5730\u5316\u4e3a\u76ee\u6807\u65b9\u8a00\uff1b\u68c0\u7d22\u589e\u5f3a\u7684\u53e3\u97f3\u63d0\u793a\u63d0\u4f9b\u53e3\u97f3\u4e00\u81f4\u7684\u8bed\u97f3\u63d0\u793a\u3002", "result": "\u572812\u79cd\u82f1\u8bed\u53e3\u97f3\u4e0a\uff0cCLARITY\u63d0\u9ad8\u4e86\u53e3\u97f3\u51c6\u786e\u6027\u548c\u516c\u5e73\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5f3a\u5927\u7684\u611f\u77e5\u8d28\u91cf\u3002", "conclusion": "CLARITY\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86TTS\u7cfb\u7edf\u4e2d\u7684\u53e3\u97f3\u548c\u8bed\u8a00\u504f\u89c1\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u5305\u5bb9\u7684\u8bed\u97f3\u5408\u6210\u3002"}}
{"id": "2511.11451", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.11451", "abs": "https://arxiv.org/abs/2511.11451", "authors": ["Ya Liu", "Junbin Liu", "Wing-Kin Ma", "Aritra Konar"], "title": "A Scalable and Exact Relaxation for Densest $k$-Subgraph via Error Bounds", "comment": null, "summary": "Given an undirected graph and a size parameter $k$, the Densest $k$-Subgraph (D$k$S) problem extracts the subgraph on $k$ vertices with the largest number of induced edges. While D$k$S is NP--hard and difficult to approximate, penalty-based continuous relaxations of the problem have recently enjoyed practical success for real-world instances of D$k$S. In this work, we propose a scalable and exact continuous penalization approach for D$k$S using the error bound principle, which enables the design of suitable penalty functions. Notably, we develop new theoretical guarantees ensuring that both the global and local optima of the penalized problem match those of the original problem. The proposed penalized reformulation enables the use of first-order continuous optimization methods. In particular, we develop a non-convex proximal gradient algorithm, where the non-convex proximal operator can be computed in closed form, resulting in low per-iteration complexity. We also provide convergence analysis of the algorithm. Experiments on large-scale instances of the D$k$S problem and one of its variants, the Densest ($k_1, k_2$) Bipartite Subgraph (D$k_1k_2$BS) problem, demonstrate that our method achieves a favorable balance between computation cost and solution quality.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bef\u5dee\u754c\u539f\u7406\u7684\u53ef\u6269\u5c55\u7cbe\u786e\u8fde\u7eed\u60e9\u7f5a\u65b9\u6cd5\u6765\u89e3\u51b3Densest k-Subgraph\u95ee\u9898\uff0c\u5f00\u53d1\u4e86\u975e\u51f8\u8fd1\u7aef\u68af\u5ea6\u7b97\u6cd5\uff0c\u5728\u8ba1\u7b97\u6210\u672c\u548c\u6c42\u89e3\u8d28\u91cf\u4e4b\u95f4\u53d6\u5f97\u4e86\u826f\u597d\u5e73\u8861\u3002", "motivation": "DkS\u95ee\u9898\u662fNP\u96be\u4e14\u96be\u4ee5\u8fd1\u4f3c\u7684\u95ee\u9898\uff0c\u73b0\u6709\u7684\u57fa\u4e8e\u60e9\u7f5a\u7684\u8fde\u7eed\u677e\u5f1b\u65b9\u6cd5\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u53d6\u5f97\u4e86\u6210\u529f\uff0c\u4f46\u9700\u8981\u5f00\u53d1\u66f4\u53ef\u6269\u5c55\u548c\u7cbe\u786e\u7684\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u8bef\u5dee\u754c\u539f\u7406\u8bbe\u8ba1\u5408\u9002\u7684\u60e9\u7f5a\u51fd\u6570\uff0c\u63d0\u51fa\u4e86\u975e\u51f8\u8fd1\u7aef\u68af\u5ea6\u7b97\u6cd5\uff0c\u5176\u4e2d\u975e\u51f8\u8fd1\u7aef\u7b97\u5b50\u53ef\u4ee5\u95ed\u5f0f\u6c42\u89e3\uff0c\u5177\u6709\u8f83\u4f4e\u7684\u6bcf\u6b21\u8fed\u4ee3\u590d\u6742\u5ea6\u3002", "result": "\u5728DkS\u95ee\u9898\u53ca\u5176\u53d8\u4f53Dk1k2BS\u95ee\u9898\u7684\u5927\u89c4\u6a21\u5b9e\u4f8b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u8ba1\u7b97\u6210\u672c\u548c\u6c42\u89e3\u8d28\u91cf\u4e4b\u95f4\u8fbe\u5230\u4e86\u6709\u5229\u7684\u5e73\u8861\u3002", "conclusion": "\u63d0\u51fa\u7684\u60e9\u7f5a\u91cd\u6784\u65b9\u6cd5\u4f7f\u5f97\u80fd\u591f\u4f7f\u7528\u4e00\u9636\u8fde\u7eed\u4f18\u5316\u65b9\u6cd5\uff0c\u4e3aDkS\u95ee\u9898\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u7cbe\u786e\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.11465", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.11465", "abs": "https://arxiv.org/abs/2511.11465", "authors": ["Deqiao Gan", "Xiaoxia Xu", "Xiaohu Ge", "Yue Liu", "Yuanwei Liu"], "title": "Enabling Wireless Power Transfer (WPT) in Pinching Antenna Systems (PASS)", "comment": null, "summary": "A novel pinching antenna system (PASS) enabled wireless power transfer (WPT) framework is proposed, where energy harvesting receivers (EHRs) and information decoding receivers (IDRs) coexist. By activating pinching antennas (PAs) near both receivers and flexibly adjusting PAs' power radiation ratios, both energy harvesting efficiency and communication quality can be enhanced. A bi-level optimization problem is formulated to overcome the strong coupling between optimization variables. The upper level jointly optimizes transmit beamforming, PA positions, and feasible interval of power radiation ratios for power conversion efficiency (PCE) maximization under rate requirements, while the lower level refines power radiation ratio for the sum rate maximization. Efficient solutions are developed for both two-user and multi-user scenarios. 1) For the two-user case, where an EHR and an IDR coexist, the alternating optimization (AO)-based and weighted minimum mean square error (WMMSE)-based algorithms are developed to achieve the stationary solutions of transmit beamforming, PA positions, and power radiation ratios. 2) For the multi-user case, a quadratic transform-Lagrangian dual transform (QT-LDT) algorithm is proposed to iteratively update PCE and sum rate by optimizing PA positions and power radiation ratios individually. Closed-form solutions are derived for both maximization problems. Numerical simulation results demonstrate that the proposed PASS-WPT framework significantly outperforms conventional MIMO and the baseline PASS with fixed power radiation, which demonstrates that: i) Compared to the conventional MIMO and baseline PASS, the proposed PASS-WPT framework achieves 81.45% and 43.19% improvements in PCE of EHRs, and ii) also increases the sum rate by 77.81% and 31.91% for IDRs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u5939\u6301\u5929\u7ebf\u7cfb\u7edf\u65e0\u7ebf\u529f\u7387\u4f20\u8f93\u6846\u67b6\uff0c\u901a\u8fc7\u6fc0\u6d3b\u63a5\u6536\u5668\u9644\u8fd1\u7684\u5939\u6301\u5929\u7ebf\u5e76\u7075\u6d3b\u8c03\u6574\u529f\u7387\u8f90\u5c04\u6bd4\uff0c\u540c\u65f6\u63d0\u5347\u80fd\u91cf\u6536\u96c6\u6548\u7387\u548c\u901a\u4fe1\u8d28\u91cf\u3002", "motivation": "\u89e3\u51b3\u80fd\u91cf\u6536\u96c6\u63a5\u6536\u5668\u548c\u4fe1\u606f\u89e3\u7801\u63a5\u6536\u5668\u5171\u5b58\u573a\u666f\u4e2d\uff0c\u80fd\u91cf\u6536\u96c6\u6548\u7387\u548c\u901a\u4fe1\u8d28\u91cf\u4e4b\u95f4\u7684\u8026\u5408\u4f18\u5316\u95ee\u9898\u3002", "method": "\u91c7\u7528\u53cc\u5c42\u4f18\u5316\u65b9\u6cd5\uff1a\u4e0a\u5c42\u8054\u5408\u4f18\u5316\u53d1\u5c04\u6ce2\u675f\u6210\u5f62\u3001PA\u4f4d\u7f6e\u548c\u529f\u7387\u8f90\u5c04\u6bd4\u53ef\u884c\u533a\u95f4\u4ee5\u6700\u5927\u5316\u529f\u7387\u8f6c\u6362\u6548\u7387\uff1b\u4e0b\u5c42\u4f18\u5316\u529f\u7387\u8f90\u5c04\u6bd4\u4ee5\u6700\u5927\u5316\u603b\u901f\u7387\u3002\u9488\u5bf9\u53cc\u7528\u6237\u548c\u591a\u7528\u6237\u573a\u666f\u5206\u522b\u5f00\u53d1\u4e86AO-WMMSE\u7b97\u6cd5\u548cQT-LDT\u7b97\u6cd5\u3002", "result": "\u4e0e\u4f20\u7edf\u7684MIMO\u548c\u56fa\u5b9a\u529f\u7387\u8f90\u5c04\u6bd4\u7684\u57fa\u7ebfPASS\u76f8\u6bd4\uff0c\u6240\u63d0\u6846\u67b6\u5728EHRs\u7684\u529f\u7387\u8f6c\u6362\u6548\u7387\u4e0a\u5206\u522b\u63d0\u5347\u4e8681.45%\u548c43.19%\uff0c\u5728IDRs\u7684\u603b\u901f\u7387\u4e0a\u5206\u522b\u63d0\u5347\u4e8677.81%\u548c31.91%\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684PASS-WPT\u6846\u67b6\u901a\u8fc7\u7075\u6d3b\u8c03\u6574\u5939\u6301\u5929\u7ebf\u7684\u529f\u7387\u8f90\u5c04\u6bd4\uff0c\u80fd\u591f\u6709\u6548\u63d0\u5347\u65e0\u7ebf\u529f\u7387\u4f20\u8f93\u7cfb\u7edf\u7684\u80fd\u91cf\u6536\u96c6\u6548\u7387\u548c\u901a\u4fe1\u6027\u80fd\u3002"}}
{"id": "2511.11503", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.11503", "abs": "https://arxiv.org/abs/2511.11503", "authors": ["Junlong Chen", "Ziwei Huang", "Xuesong Cai", "Xiang Cheng", "Liuqing Yang"], "title": "SynthSoM-Twin: A Multi-Modal Sensing-Communication Digital-Twin Dataset for Sim2Real Transfer via Synesthesia of Machines", "comment": null, "summary": "This paper constructs a novel multi-modal sensing-communication digital-twin dataset, named SynthSoM-Twin, which is spatio-temporally consistent with the real world, for Sim2Real transfer via Synesthesia of Machines (SoM). To construct the SynthSoM-Twin dataset, we propose a new framework that can extend the quantity and missing modality of existing real-world multi-modal sensing-communication dataset. Specifically, we exploit multi-modal sensing-assisted object detection and tracking algorithms to ensure spatio-temporal consistency of static objects and dynamic objects across real world and simulation environments. The constructed scenario is imported into three high-fidelity simulators, i.e., AirSim, WaveFarer, and Sionna RT. The SynthSoM-Twin dataset contains spatio-temporally consistent data with the real world, including 66,868 snapshots of synthetic RGB images, depth maps, light detection and ranging (LiDAR) point clouds, millimeter wave (mmWave) radar point clouds, and large-scale and small-scale channel fading data. To validate the utility of SynthSoM-Twin dataset, we conduct Sim2Real transfer investigation by implementing two cross-modal downstream tasks via cross-modal generative models (CMGMs), i.e., cross-modal channel generation model and multi-modal sensing-assisted beam generation model. Based on the downstream tasks, we explore the threshold of real-world data injection that can achieve a decent trade-off between real-world data usage and models' practical performance. Experimental results show that the model training on the SynthSoM-Twin dataset achieves a decent practical performance, and the injection of real-world data further facilitates Sim2Real transferability. Based on the SynthSoM-Twin dataset, injecting less than 15% of real-world data can achieve similar and even better performance compared to that trained with all the real-world data only.", "AI": {"tldr": "\u672c\u6587\u6784\u5efa\u4e86\u4e00\u4e2a\u65b0\u9896\u7684\u591a\u6a21\u6001\u611f\u77e5-\u901a\u4fe1\u6570\u5b57\u5b6a\u751f\u6570\u636e\u96c6SynthSoM-Twin\uff0c\u8be5\u6570\u636e\u96c6\u5728\u65f6\u7a7a\u4e0a\u4e0e\u771f\u5b9e\u4e16\u754c\u4e00\u81f4\uff0c\u901a\u8fc7\u673a\u5668\u8054\u89c9\u5b9e\u73b0Sim2Real\u8fc1\u79fb\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u771f\u5b9e\u4e16\u754c\u591a\u6a21\u6001\u611f\u77e5-\u901a\u4fe1\u6570\u636e\u96c6\u6570\u91cf\u6709\u9650\u548c\u6a21\u6001\u7f3a\u5931\u7684\u95ee\u9898\uff0c\u6784\u5efa\u65f6\u7a7a\u4e00\u81f4\u7684\u6570\u5b57\u5b6a\u751f\u6570\u636e\u96c6\u4ee5\u4fc3\u8fdbSim2Real\u8fc1\u79fb\u3002", "method": "\u5229\u7528\u591a\u6a21\u6001\u611f\u77e5\u8f85\u52a9\u7684\u76ee\u6807\u68c0\u6d4b\u548c\u8ddf\u8e2a\u7b97\u6cd5\u786e\u4fdd\u9759\u6001\u548c\u52a8\u6001\u5bf9\u8c61\u5728\u771f\u5b9e\u4e16\u754c\u548c\u4eff\u771f\u73af\u5883\u4e2d\u7684\u65f6\u7a7a\u4e00\u81f4\u6027\uff0c\u5c06\u6784\u5efa\u7684\u573a\u666f\u5bfc\u5165\u4e09\u4e2a\u9ad8\u4fdd\u771f\u4eff\u771f\u5668\uff08AirSim\u3001WaveFarer\u3001Sionna RT\uff09\u3002", "result": "\u6784\u5efa\u4e86\u5305\u542b66,868\u4e2a\u5408\u6210RGB\u56fe\u50cf\u3001\u6df1\u5ea6\u56fe\u3001LiDAR\u70b9\u4e91\u3001\u6beb\u7c73\u6ce2\u96f7\u8fbe\u70b9\u4e91\u4ee5\u53ca\u5927\u89c4\u6a21\u548c\u5c0f\u89c4\u6a21\u4fe1\u9053\u8870\u843d\u6570\u636e\u7684SynthSoM-Twin\u6570\u636e\u96c6\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u5728SynthSoM-Twin\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u6a21\u578b\u53ef\u5b9e\u73b0\u826f\u597d\u7684\u5b9e\u9645\u6027\u80fd\uff0c\u6ce8\u5165\u5c11\u4e8e15%\u7684\u771f\u5b9e\u4e16\u754c\u6570\u636e\u5373\u53ef\u8fbe\u5230\u4e0e\u4ec5\u4f7f\u7528\u6240\u6709\u771f\u5b9e\u4e16\u754c\u6570\u636e\u76f8\u5f53\u751a\u81f3\u66f4\u597d\u7684\u6027\u80fd\u3002", "conclusion": "SynthSoM-Twin\u6570\u636e\u96c6\u6709\u6548\u4fc3\u8fdb\u4e86Sim2Real\u8fc1\u79fb\uff0c\u901a\u8fc7\u5c11\u91cf\u771f\u5b9e\u4e16\u754c\u6570\u636e\u6ce8\u5165\u5373\u53ef\u5b9e\u73b0\u6027\u80fd\u4e0e\u5168\u771f\u5b9e\u6570\u636e\u8bad\u7ec3\u76f8\u5f53\u7684\u7ed3\u679c\uff0c\u4e3a\u591a\u6a21\u6001\u611f\u77e5-\u901a\u4fe1\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u8d44\u6e90\u3002"}}

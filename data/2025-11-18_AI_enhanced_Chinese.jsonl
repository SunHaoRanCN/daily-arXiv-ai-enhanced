{"id": "2511.11615", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2511.11615", "abs": "https://arxiv.org/abs/2511.11615", "authors": ["Wendy Lomas", "Andrew Gascoyne", "Colin Dubreuil", "Stefano Vaglio", "Liam Naughton"], "title": "Lightweight Hopfield Neural Networks for Bioacoustic Detection and Call Monitoring of Captive Primates", "comment": "16 pages, 3 figures, Proceedings of the Future Technologies Conference (FTC) 2025, Volume 1", "summary": "Passive acoustic monitoring is a sustainable method of monitoring wildlife and environments that leads to the generation of large datasets and, currently, a processing backlog. Academic research into automating this process is focused on the application of resource intensive convolutional neural networks which require large pre-labelled datasets for training and lack flexibility in application. We present a viable alternative relevant in both wild and captive settings; a transparent, lightweight and fast-to-train associative memory AI model with Hopfield neural network (HNN) architecture. Adapted from a model developed to detect bat echolocation calls, this model monitors captive endangered black-and-white ruffed lemur Varecia variegata vocalisations. Lemur social calls of interest when monitoring welfare are stored in the HNN in order to detect other call instances across the larger acoustic dataset. We make significant model improvements by storing an additional signal caused by movement and achieve an overall accuracy of 0.94. The model can perform $340$ classifications per second, processing over 5.5 hours of audio data per minute, on a standard laptop running other applications. It has broad applicability and trains in milliseconds. Our lightweight solution reduces data-to-insight turnaround times and can accelerate decision making in both captive and wild settings.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684Hopfield\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff0c\u7528\u4e8e\u81ea\u52a8\u76d1\u6d4b\u6fd2\u5371\u9ed1\u767d\u9886\u72d0\u7334\u7684\u53eb\u58f0\uff0c\u66ff\u4ee3\u8d44\u6e90\u5bc6\u96c6\u7684\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\u3002", "motivation": "\u88ab\u52a8\u58f0\u5b66\u76d1\u6d4b\u4ea7\u751f\u5927\u91cf\u6570\u636e\u96c6\u4f46\u5904\u7406\u79ef\u538b\uff0c\u73b0\u6709\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\u8d44\u6e90\u5bc6\u96c6\u3001\u9700\u8981\u5927\u91cf\u9884\u6807\u8bb0\u6570\u636e\u4e14\u7f3a\u4e4f\u7075\u6d3b\u6027\u3002", "method": "\u4f7f\u7528Hopfield\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u7684\u5173\u8054\u8bb0\u5fc6AI\u6a21\u578b\uff0c\u5b58\u50a8\u611f\u5174\u8da3\u7684\u72d0\u7334\u793e\u4ea4\u53eb\u58f0\u548c\u8fd0\u52a8\u4fe1\u53f7\uff0c\u5728\u5927\u89c4\u6a21\u58f0\u5b66\u6570\u636e\u96c6\u4e2d\u68c0\u6d4b\u5176\u4ed6\u53eb\u58f0\u5b9e\u4f8b\u3002", "result": "\u6a21\u578b\u6574\u4f53\u51c6\u786e\u7387\u8fbe\u52300.94\uff0c\u6bcf\u79d2\u53ef\u8fdb\u884c340\u6b21\u5206\u7c7b\uff0c\u6bcf\u5206\u949f\u5904\u7406\u8d85\u8fc75.5\u5c0f\u65f6\u97f3\u9891\u6570\u636e\uff0c\u5728\u6807\u51c6\u7b14\u8bb0\u672c\u7535\u8111\u4e0a\u8bad\u7ec3\u4ec5\u9700\u6beb\u79d2\u7ea7\u65f6\u95f4\u3002", "conclusion": "\u8fd9\u79cd\u8f7b\u91cf\u7ea7\u89e3\u51b3\u65b9\u6848\u663e\u8457\u51cf\u5c11\u4e86\u6570\u636e\u5230\u6d1e\u5bdf\u7684\u5468\u8f6c\u65f6\u95f4\uff0c\u80fd\u591f\u52a0\u901f\u5708\u517b\u548c\u91ce\u5916\u73af\u5883\u4e2d\u7684\u51b3\u7b56\u5236\u5b9a\uff0c\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027\u3002"}}
{"id": "2511.11825", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2511.11825", "abs": "https://arxiv.org/abs/2511.11825", "authors": ["Behnaz Bahmei", "Siamak Arzanpour", "Elina Birmingham"], "title": "Real-Time Speech Enhancement via a Hybrid ViT: A Dual-Input Acoustic-Image Feature Fusion", "comment": null, "summary": "Speech quality and intelligibility are significantly degraded in noisy environments. This paper presents a novel transformer-based learning framework to address the single-channel noise suppression problem for real-time applications. Although existing deep learning networks have shown remarkable improvements in handling stationary noise, their performance often diminishes in real-world environments characterized by non-stationary noise (e.g., dog barking, baby crying). The proposed dual-input acoustic-image feature fusion using a hybrid ViT framework effectively models both temporal and spectral dependencies in noisy signals. Designed for real-world audio environments, the proposed framework is computationally lightweight and suitable for implementation on embedded devices. To evaluate its effectiveness, four standard and commonly used quality measurements, namely PESQ, STOI, Seg SNR, and LLR, are utilized. Experimental results obtained using the Librispeech dataset as the clean speech source and the UrbanSound8K and Google Audioset datasets as the noise sources, demonstrate that the proposed method significantly improves noise reduction, speech intelligibility, and perceptual quality compared to the noisy input signal, achieving performance close to the clean reference.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eTransformer\u7684\u8f7b\u91cf\u7ea7\u5b9e\u65f6\u566a\u58f0\u6291\u5236\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u8f93\u5165\u58f0\u5b66\u56fe\u50cf\u7279\u5f81\u878d\u5408\u6709\u6548\u5904\u7406\u975e\u5e73\u7a33\u566a\u58f0\uff0c\u5728\u5d4c\u5165\u5f0f\u8bbe\u5907\u4e0a\u5b9e\u73b0\u63a5\u8fd1\u5e72\u51c0\u53c2\u8003\u7684\u8bed\u97f3\u8d28\u91cf\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728\u5e73\u7a33\u566a\u58f0\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u9762\u5bf9\u975e\u5e73\u7a33\u566a\u58f0\uff08\u5982\u72d7\u53eb\u3001\u5a74\u513f\u54ed\u58f0\uff09\u65f6\u6027\u80fd\u4e0b\u964d\uff0c\u9700\u8981\u5f00\u53d1\u9002\u7528\u4e8e\u771f\u5b9e\u73af\u5883\u7684\u5b9e\u65f6\u566a\u58f0\u6291\u5236\u65b9\u6848\u3002", "method": "\u91c7\u7528\u6df7\u5408ViT\u6846\u67b6\u7684\u53cc\u8f93\u5165\u58f0\u5b66\u56fe\u50cf\u7279\u5f81\u878d\u5408\u65b9\u6cd5\uff0c\u6709\u6548\u5efa\u6a21\u566a\u58f0\u4fe1\u53f7\u4e2d\u7684\u65f6\u95f4\u548c\u9891\u8c31\u4f9d\u8d56\u5173\u7cfb\uff0c\u8bbe\u8ba1\u8f7b\u91cf\u7ea7\u67b6\u6784\u9002\u5408\u5d4c\u5165\u5f0f\u8bbe\u5907\u90e8\u7f72\u3002", "result": "\u5728Librispeech\u3001UrbanSound8K\u548cGoogle Audioset\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728PESQ\u3001STOI\u3001Seg SNR\u548cLLR\u56db\u4e2a\u6807\u51c6\u6307\u6807\u4e0a\u663e\u8457\u63d0\u5347\u566a\u58f0\u6291\u5236\u3001\u8bed\u97f3\u6e05\u6670\u5ea6\u548c\u611f\u77e5\u8d28\u91cf\uff0c\u6027\u80fd\u63a5\u8fd1\u5e72\u51c0\u53c2\u8003\u4fe1\u53f7\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684Transformer\u6846\u67b6\u5728\u771f\u5b9e\u566a\u58f0\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u5d4c\u5165\u5f0f\u8bbe\u5907\u4e0a\u7684\u5b9e\u65f6\u8bed\u97f3\u589e\u5f3a\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.12074", "categories": ["cs.SD", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12074", "abs": "https://arxiv.org/abs/2511.12074", "authors": ["Xinyue Yu", "Youqing Fang", "Pingyu Wu", "Guoyang Ye", "Wenbo Zhou", "Weiming Zhang", "Song Xiao"], "title": "MF-Speech: Achieving Fine-Grained and Compositional Control in Speech Generation via Factor Disentanglement", "comment": null, "summary": "Generating expressive and controllable human speech is one of the core goals of generative artificial intelligence, but its progress has long been constrained by two fundamental challenges: the deep entanglement of speech factors and the coarse granularity of existing control mechanisms. To overcome these challenges, we have proposed a novel framework called MF-Speech, which consists of two core components: MF-SpeechEncoder and MF-SpeechGenerator. MF-SpeechEncoder acts as a factor purifier, adopting a multi-objective optimization strategy to decompose the original speech signal into highly pure and independent representations of content, timbre, and emotion. Subsequently, MF-SpeechGenerator functions as a conductor, achieving precise, composable and fine-grained control over these factors through dynamic fusion and Hierarchical Style Adaptive Normalization (HSAN). Experiments demonstrate that in the highly challenging multi-factor compositional speech generation task, MF-Speech significantly outperforms current state-of-the-art methods, achieving a lower word error rate (WER=4.67%), superior style control (SECS=0.5685, Corr=0.68), and the highest subjective evaluation scores(nMOS=3.96, sMOS_emotion=3.86, sMOS_style=3.78). Furthermore, the learned discrete factors exhibit strong transferability, demonstrating their significant potential as a general-purpose speech representation.", "AI": {"tldr": "MF-Speech\u662f\u4e00\u4e2a\u89e3\u51b3\u8bed\u97f3\u751f\u6210\u4e2d\u56e0\u7d20\u7ea0\u7f20\u548c\u7c97\u7c92\u5ea6\u63a7\u5236\u95ee\u9898\u7684\u6846\u67b6\uff0c\u5305\u542b\u56e0\u7d20\u7eaf\u5316\u7f16\u7801\u5668\u548c\u751f\u6210\u5668\uff0c\u5728\u591a\u56e0\u7d20\u7ec4\u5408\u8bed\u97f3\u751f\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u89e3\u51b3\u8bed\u97f3\u751f\u6210\u4e2d\u4e24\u4e2a\u57fa\u672c\u6311\u6218\uff1a\u8bed\u97f3\u56e0\u7d20\u7684\u6df1\u5ea6\u7ea0\u7f20\u548c\u73b0\u6709\u63a7\u5236\u673a\u5236\u7684\u7c97\u7c92\u5ea6\u95ee\u9898\uff0c\u5b9e\u73b0\u66f4\u7cbe\u786e\u3001\u53ef\u7ec4\u5408\u7684\u8bed\u97f3\u63a7\u5236\u3002", "method": "\u4f7f\u7528MF-SpeechEncoder\u8fdb\u884c\u56e0\u7d20\u7eaf\u5316\uff0c\u901a\u8fc7\u591a\u76ee\u6807\u4f18\u5316\u5c06\u8bed\u97f3\u5206\u89e3\u4e3a\u5185\u5bb9\u3001\u97f3\u8272\u548c\u60c5\u611f\u7684\u72ec\u7acb\u8868\u793a\uff1b\u4f7f\u7528MF-SpeechGenerator\u901a\u8fc7\u52a8\u6001\u878d\u5408\u548c\u5c42\u6b21\u98ce\u683c\u81ea\u9002\u5e94\u5f52\u4e00\u5316\u5b9e\u73b0\u7cbe\u7ec6\u63a7\u5236\u3002", "result": "\u5728\u591a\u56e0\u7d20\u7ec4\u5408\u8bed\u97f3\u751f\u6210\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff1a\u8bcd\u9519\u8bef\u73874.67%\uff0c\u98ce\u683c\u63a7\u5236\u6307\u6807SECS=0.5685\u3001Corr=0.68\uff0c\u4e3b\u89c2\u8bc4\u5206nMOS=3.96\u3001sMOS_emotion=3.86\u3001sMOS_style=3.78\u3002", "conclusion": "MF-Speech\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u8bed\u97f3\u56e0\u7d20\u7ea0\u7f20\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u7cbe\u7ec6\u7c92\u5ea6\u7684\u8bed\u97f3\u63a7\u5236\uff0c\u5b66\u4e60\u5230\u7684\u79bb\u6563\u56e0\u7d20\u5177\u6709\u5f3a\u8fc1\u79fb\u6027\uff0c\u6709\u671b\u6210\u4e3a\u901a\u7528\u8bed\u97f3\u8868\u793a\u3002"}}
{"id": "2511.13146", "categories": ["cs.SD", "cs.MM"], "pdf": "https://arxiv.org/pdf/2511.13146", "abs": "https://arxiv.org/abs/2511.13146", "authors": ["Junyu Wu", "Jie Liu", "Tianrui Pan", "Jie Tang", "Gangshan Wu"], "title": "Towards Practical Real-Time Low-Latency Music Source Separation", "comment": null, "summary": "In recent years, significant progress has been made in the field of deep learning for music demixing. However, there has been limited attention on real-time, low-latency music demixing, which holds potential for various applications, such as hearing aids, audio stream remixing, and live performances. Additionally, a notable tendency has emerged towards the development of larger models, limiting their applicability in certain scenarios. In this paper, we introduce a lightweight real-time low-latency model called Real-Time Single-Path TFC-TDF UNET (RT-STT), which is based on the Dual-Path TFC-TDF UNET (DTTNet). In RT-STT, we propose a feature fusion technique based on channel expansion. We also demonstrate the superiority of single-path modeling over dual-path modeling in real-time models. Moreover, we investigate the method of quantization to further reduce inference time. RT-STT exhibits superior performance with significantly fewer parameters and shorter inference times compared to state-of-the-art models.", "AI": {"tldr": "\u63d0\u51faRT-STT\u8f7b\u91cf\u7ea7\u5b9e\u65f6\u4f4e\u5ef6\u8fdf\u97f3\u4e50\u5206\u79bb\u6a21\u578b\uff0c\u57fa\u4e8e\u5355\u8def\u5f84\u67b6\u6784\u548c\u901a\u9053\u6269\u5c55\u7279\u5f81\u878d\u5408\uff0c\u53c2\u6570\u66f4\u5c11\u3001\u63a8\u7406\u65f6\u95f4\u66f4\u77ed", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u97f3\u4e50\u5206\u79bb\u65b9\u6cd5\u7f3a\u4e4f\u5bf9\u5b9e\u65f6\u4f4e\u5ef6\u8fdf\u5e94\u7528\u7684\u5173\u6ce8\uff0c\u4e14\u6a21\u578b\u8d8b\u5411\u5927\u578b\u5316\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u573a\u666f", "method": "\u57fa\u4e8eDTTNet\u7684\u5355\u8def\u5f84TFC-TDF UNET\u67b6\u6784\uff0c\u91c7\u7528\u901a\u9053\u6269\u5c55\u7279\u5f81\u878d\u5408\u6280\u672f\uff0c\u5e76\u7814\u7a76\u91cf\u5316\u65b9\u6cd5\u51cf\u5c11\u63a8\u7406\u65f6\u95f4", "result": "RT-STT\u5728\u53c2\u6570\u6570\u91cf\u548c\u63a8\u7406\u65f6\u95f4\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u6a21\u578b\uff0c\u540c\u65f6\u4fdd\u6301\u4f18\u8d8a\u6027\u80fd", "conclusion": "\u5355\u8def\u5f84\u5efa\u6a21\u5728\u5b9e\u65f6\u6a21\u578b\u4e2d\u4f18\u4e8e\u53cc\u8def\u5f84\u5efa\u6a21\uff0cRT-STT\u4e3a\u5b9e\u65f6\u97f3\u4e50\u5206\u79bb\u5e94\u7528\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2511.12285", "categories": ["eess.AS", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.12285", "abs": "https://arxiv.org/abs/2511.12285", "authors": ["Minu Kim", "Ji Sub Um", "Hoirin Kim"], "title": "How Far Do SSL Speech Models Listen for Tone? Temporal Focus of Tone Representation under Low-resource Transfer", "comment": "5 pages, 7 figures, submitted to ICASSP 2026", "summary": "Lexical tone is central to many languages but remains underexplored in self-supervised learning (SSL) speech models, especially beyond Mandarin. We study four languages with complex and diverse tone systems: Burmese, Thai, Lao, and Vietnamese, to examine how far such models listen for tone and how transfer operates in low-resource conditions. As a baseline reference, we estimate the temporal span of tone cues to be about 100 ms in Burmese and Thai, and about 180 ms in Lao and Vietnamese. Probes and gradient analyses on fine-tuned SSL models reveal that tone transfer varies by downstream task: automatic speech recognition fine-tuning aligns spans with language-specific tone cues, while prosody- and voice-related tasks bias the model toward overly long spans. These findings indicate that tone transfer is shaped by downstream task, highlighting task effects on temporal focus in tone modeling.", "AI": {"tldr": "\u7814\u7a76\u81ea\u76d1\u7763\u5b66\u4e60\u8bed\u97f3\u6a21\u578b\u5728\u56db\u79cd\u590d\u6742\u58f0\u8c03\u8bed\u8a00\uff08\u7f05\u7538\u8bed\u3001\u6cf0\u8bed\u3001\u8001\u631d\u8bed\u3001\u8d8a\u5357\u8bed\uff09\u4e2d\u7684\u58f0\u8c03\u611f\u77e5\u80fd\u529b\uff0c\u53d1\u73b0\u4e0b\u6e38\u4efb\u52a1\u5bf9\u58f0\u8c03\u8f6c\u79fb\u7684\u65f6\u57df\u5173\u6ce8\u8303\u56f4\u6709\u663e\u8457\u5f71\u54cd\u3002", "motivation": "\u58f0\u8c03\u5728\u8bb8\u591a\u8bed\u8a00\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5728\u81ea\u76d1\u7763\u5b66\u4e60\u8bed\u97f3\u6a21\u578b\u4e2d\u7814\u7a76\u4e0d\u8db3\uff0c\u7279\u522b\u662f\u5728\u666e\u901a\u8bdd\u4ee5\u5916\u7684\u8bed\u8a00\u4e2d\u3002\u9700\u8981\u63a2\u7d22\u8fd9\u7c7b\u6a21\u578b\u5982\u4f55\u611f\u77e5\u58f0\u8c03\u4ee5\u53ca\u5728\u4f4e\u8d44\u6e90\u6761\u4ef6\u4e0b\u7684\u8f6c\u79fb\u673a\u5236\u3002", "method": "\u9996\u5148\u4f30\u8ba1\u4e86\u56db\u79cd\u8bed\u8a00\u4e2d\u58f0\u8c03\u7ebf\u7d22\u7684\u65f6\u95f4\u8de8\u5ea6\u57fa\u7ebf\uff08\u7f05\u7538\u8bed\u548c\u6cf0\u8bed\u7ea6100\u6beb\u79d2\uff0c\u8001\u631d\u8bed\u548c\u8d8a\u5357\u8bed\u7ea6180\u6beb\u79d2\uff09\uff0c\u7136\u540e\u901a\u8fc7\u63a2\u9488\u548c\u68af\u5ea6\u5206\u6790\u7814\u7a76\u4e86\u5fae\u8c03\u540e\u7684SSL\u6a21\u578b\u3002", "result": "\u58f0\u8c03\u8f6c\u79fb\u56e0\u4e0b\u6e38\u4efb\u52a1\u800c\u5f02\uff1a\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\u5fae\u8c03\u4f7f\u6a21\u578b\u5173\u6ce8\u8303\u56f4\u4e0e\u8bed\u8a00\u7279\u5b9a\u7684\u58f0\u8c03\u7ebf\u7d22\u5bf9\u9f50\uff0c\u800c\u97f5\u5f8b\u548c\u8bed\u97f3\u76f8\u5173\u4efb\u52a1\u5219\u4f7f\u6a21\u578b\u504f\u5411\u8fc7\u957f\u7684\u5173\u6ce8\u8de8\u5ea6\u3002", "conclusion": "\u4e0b\u6e38\u4efb\u52a1\u5851\u9020\u4e86\u58f0\u8c03\u8f6c\u79fb\u8fc7\u7a0b\uff0c\u5f3a\u8c03\u4e86\u4efb\u52a1\u6548\u5e94\u5bf9\u58f0\u8c03\u5efa\u6a21\u4e2d\u65f6\u57df\u5173\u6ce8\u7684\u5f71\u54cd\u3002"}}
{"id": "2511.11844", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.11844", "abs": "https://arxiv.org/abs/2511.11844", "authors": ["Olaoluwa A. Adegboye", "Kufre M. Udofia", "Akaninyene Obot"], "title": "Inverted C-Shaped Slots Loaded Exponential Tapered Triple Band Notched Ultra Wideband (UWB) Antenna", "comment": null, "summary": "This research presents a simple strategy for designing an exponentially tapered, triple-notched ultrawideband antenna. The antenna's microstrip line feed and radiating patch are matched using an exponential tapered transformer. This method inserts antenna notch elements, by cutting two inverted C-shaped slots in the radiating patch; frequency rejection can be achieved for WI-MAX and wireless LAN. The X-band is rejected by etching a U-shaped slot in the feedline. When embedding the notch elements, cross-coupling was minimized. The desired antenna was designed, simulated, and measured. The measured results and graphs show that our proposed design is reliable. This band notched antenna rejects 3.5 GHz (Wi-MAX band, 3.3 to 3.7 GHz), 5.5 GHz (WLAN 2 band, 5.15 to 5.825 GHz), and 7.5 GHz (for satellite downlink X - band-7.25 GHz to 7.75 GHz). The proposed antenna meets UWB design requirements.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u6307\u6570\u9525\u5f62\u4e09\u9677\u6ce2\u8d85\u5bbd\u5e26\u5929\u7ebf\u8bbe\u8ba1\u7b56\u7565\uff0c\u901a\u8fc7\u5728\u8f90\u5c04\u8d34\u7247\u4e0a\u5207\u5272\u4e24\u4e2a\u5012C\u5f62\u69fd\u548c\u5728\u9988\u7ebf\u4e0a\u8680\u523bU\u5f62\u69fd\uff0c\u5b9e\u73b0Wi-MAX\u3001WLAN\u548cX\u6ce2\u6bb5\u7684\u9891\u7387\u6291\u5236\u3002", "motivation": "\u8bbe\u8ba1\u80fd\u591f\u6291\u5236\u7279\u5b9a\u9891\u6bb5\u5e72\u6270\u7684\u8d85\u5bbd\u5e26\u5929\u7ebf\uff0c\u6ee1\u8db3Wi-MAX\u3001WLAN\u548cX\u6ce2\u6bb5\u536b\u661f\u4e0b\u884c\u94fe\u8def\u7684\u9891\u7387\u6291\u5236\u9700\u6c42\u3002", "method": "\u91c7\u7528\u6307\u6570\u9525\u5f62\u53d8\u6362\u5668\u5339\u914d\u5fae\u5e26\u7ebf\u9988\u7535\u548c\u8f90\u5c04\u8d34\u7247\uff0c\u5728\u8f90\u5c04\u8d34\u7247\u4e0a\u5207\u5272\u4e24\u4e2a\u5012C\u5f62\u69fd\u6291\u5236Wi-MAX\u548cWLAN\u9891\u6bb5\uff0c\u5728\u9988\u7ebf\u4e0a\u8680\u523bU\u5f62\u69fd\u6291\u5236X\u6ce2\u6bb5\uff0c\u5e76\u6700\u5c0f\u5316\u4ea4\u53c9\u8026\u5408\u3002", "result": "\u5929\u7ebf\u6210\u529f\u6291\u5236\u4e863.5GHz(Wi-MAX)\u30015.5GHz(WLAN)\u548c7.5GHz(X\u6ce2\u6bb5)\u4e09\u4e2a\u9891\u6bb5\uff0c\u6d4b\u91cf\u7ed3\u679c\u9a8c\u8bc1\u4e86\u8bbe\u8ba1\u7684\u53ef\u9760\u6027\uff0c\u6ee1\u8db3\u8d85\u5bbd\u5e26\u8bbe\u8ba1\u8981\u6c42\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u4e09\u9677\u6ce2\u8d85\u5bbd\u5e26\u5929\u7ebf\u8bbe\u8ba1\u7b56\u7565\u6709\u6548\uff0c\u80fd\u591f\u53ef\u9760\u5730\u6291\u5236\u7279\u5b9a\u9891\u6bb5\u5e72\u6270\uff0c\u6ee1\u8db3\u8d85\u5bbd\u5e26\u5e94\u7528\u9700\u6c42\u3002"}}
{"id": "2511.13219", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2511.13219", "abs": "https://arxiv.org/abs/2511.13219", "authors": ["Satvik Dixit", "Koichi Saito", "Zhi Zhong", "Yuki Mitsufuji", "Chris Donahue"], "title": "FoleyBench: A Benchmark For Video-to-Audio Models", "comment": null, "summary": "Video-to-audio generation (V2A) is of increasing importance in domains such as film post-production, AR/VR, and sound design, particularly for the creation of Foley sound effects synchronized with on-screen actions. Foley requires generating audio that is both semantically aligned with visible events and temporally aligned with their timing. Yet, there is a mismatch between evaluation and downstream applications due to the absence of a benchmark tailored to Foley-style scenarios. We find that 74% of videos from past evaluation datasets have poor audio-visual correspondence. Moreover, they are dominated by speech and music, domains that lie outside the use case for Foley. To address this gap, we introduce FoleyBench, the first large-scale benchmark explicitly designed for Foley-style V2A evaluation. FoleyBench contains 5,000 (video, ground-truth audio, text caption) triplets, each featuring visible sound sources with audio causally tied to on-screen events. The dataset is built using an automated, scalable pipeline applied to in-the-wild internet videos from YouTube-based and Vimeo-based sources. Compared to past datasets, we show that videos from FoleyBench have stronger coverage of sound categories from a taxonomy specifically designed for Foley sound. Each clip is further labeled with metadata capturing source complexity, UCS/AudioSet category, and video length, enabling fine-grained analysis of model performance and failure modes. We benchmark several state-of-the-art V2A models, evaluating them on audio quality, audio-video alignment, temporal synchronization, and audio-text consistency. Samples are available at: https://gclef-cmu.org/foleybench", "AI": {"tldr": "\u63d0\u51fa\u4e86FoleyBench\uff0c\u8fd9\u662f\u7b2c\u4e00\u4e2a\u4e13\u95e8\u4e3aFoley\u98ce\u683c\u89c6\u9891\u5230\u97f3\u9891\u751f\u6210\u8bc4\u4f30\u8bbe\u8ba1\u7684\u5927\u89c4\u6a21\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5305\u542b5000\u4e2a\u89c6\u9891-\u97f3\u9891-\u6587\u672c\u4e09\u5143\u7ec4\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6570\u636e\u96c6\u5728\u97f3\u9891-\u89c6\u89c9\u5bf9\u5e94\u6027\u548cFoley\u58f0\u97f3\u7c7b\u522b\u8986\u76d6\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u5230\u97f3\u9891\u751f\u6210\u8bc4\u4f30\u6570\u636e\u96c6\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a74%\u7684\u89c6\u9891\u97f3\u9891-\u89c6\u89c9\u5bf9\u5e94\u6027\u5dee\uff0c\u4e14\u4e3b\u8981\u88ab\u8bed\u97f3\u548c\u97f3\u4e50\u4e3b\u5bfc\uff0c\u4e0d\u9002\u5408Foley\u58f0\u97f3\u6548\u679c\u8bc4\u4f30\u3002\u9700\u8981\u4e13\u95e8\u9488\u5bf9Foley\u5e94\u7528\u573a\u666f\u7684\u57fa\u51c6\u6570\u636e\u96c6\u3002", "method": "\u901a\u8fc7\u81ea\u52a8\u5316\u3001\u53ef\u6269\u5c55\u7684\u6d41\u7a0b\u4eceYouTube\u548cVimeo\u7b49\u4e92\u8054\u7f51\u89c6\u9891\u6784\u5efa\u6570\u636e\u96c6\uff0c\u5305\u542b5000\u4e2a\u89c6\u9891-\u97f3\u9891-\u6587\u672c\u4e09\u5143\u7ec4\uff0c\u6bcf\u4e2a\u89c6\u9891\u90fd\u5305\u542b\u53ef\u89c1\u58f0\u6e90\u4e14\u97f3\u9891\u4e0e\u5c4f\u5e55\u4e8b\u4ef6\u56e0\u679c\u76f8\u5173\u3002", "result": "\u4e0e\u8fc7\u53bb\u6570\u636e\u96c6\u76f8\u6bd4\uff0cFoleyBench\u5728\u4e13\u95e8\u4e3aFoley\u58f0\u97f3\u8bbe\u8ba1\u7684\u5206\u7c7b\u6cd5\u4e2d\u5177\u6709\u66f4\u5f3a\u7684\u58f0\u97f3\u7c7b\u522b\u8986\u76d6\u3002\u6bcf\u4e2a\u7247\u6bb5\u90fd\u6807\u6ce8\u4e86\u6e90\u590d\u6742\u6027\u3001UCS/AudioSet\u7c7b\u522b\u548c\u89c6\u9891\u957f\u5ea6\u7b49\u5143\u6570\u636e\u3002", "conclusion": "FoleyBench\u586b\u8865\u4e86Foley\u98ce\u683c\u89c6\u9891\u5230\u97f3\u9891\u751f\u6210\u8bc4\u4f30\u7684\u7a7a\u767d\uff0c\u4e3a\u6a21\u578b\u6027\u80fd\u5206\u6790\u548c\u5931\u8d25\u6a21\u5f0f\u8bc6\u522b\u63d0\u4f9b\u4e86\u7ec6\u7c92\u5ea6\u5206\u6790\u80fd\u529b\uff0c\u5e76\u8bc4\u4f30\u4e86\u591a\u4e2a\u6700\u5148\u8fdb\u7684V2A\u6a21\u578b\u5728\u97f3\u9891\u8d28\u91cf\u3001\u97f3\u9891-\u89c6\u9891\u5bf9\u9f50\u3001\u65f6\u95f4\u540c\u6b65\u548c\u97f3\u9891-\u6587\u672c\u4e00\u81f4\u6027\u7b49\u65b9\u9762\u7684\u8868\u73b0\u3002"}}
{"id": "2511.12347", "categories": ["eess.AS", "cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2511.12347", "abs": "https://arxiv.org/abs/2511.12347", "authors": ["Zhisheng Zheng", "Puyuan Peng", "Anuj Diwan", "Cong Phuoc Huynh", "Xiaohang Sun", "Zhu Liu", "Vimal Bhat", "David Harwath"], "title": "VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing", "comment": "EMNLP 2025. Demo and code are available at https://zhishengzheng.com/voicecraft-x/", "summary": "We introduce VoiceCraft-X, an autoregressive neural codec language model which unifies multilingual speech editing and zero-shot Text-to-Speech (TTS) synthesis across 11 languages: English, Mandarin, Korean, Japanese, Spanish, French, German, Dutch, Italian, Portuguese, and Polish. VoiceCraft-X utilizes the Qwen3 large language model for phoneme-free cross-lingual text processing and a novel token reordering mechanism with time-aligned text and speech tokens to handle both tasks as a single sequence generation problem. The model generates high-quality, natural-sounding speech, seamlessly creating new audio or editing existing recordings within one framework. VoiceCraft-X shows robust performance in diverse linguistic settings, even with limited per-language data, underscoring the power of unified autoregressive approaches for advancing complex, real-world multilingual speech applications. Audio samples are available at https://zhishengzheng.com/voicecraft-x/.", "AI": {"tldr": "VoiceCraft-X\u662f\u4e00\u4e2a\u81ea\u56de\u5f52\u795e\u7ecf\u7f16\u89e3\u7801\u8bed\u8a00\u6a21\u578b\uff0c\u7edf\u4e00\u4e8611\u79cd\u8bed\u8a00\u7684\u8bed\u97f3\u7f16\u8f91\u548c\u96f6\u6837\u672c\u6587\u672c\u5230\u8bed\u97f3\u5408\u6210\u4efb\u52a1\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\u6765\u5904\u7406\u591a\u8bed\u8a00\u8bed\u97f3\u7f16\u8f91\u548cTTS\u5408\u6210\uff0c\u514b\u670d\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u5355\u72ec\u5904\u7406\u4e0d\u540c\u8bed\u8a00\u548c\u4efb\u52a1\u7684\u9650\u5236\u3002", "method": "\u4f7f\u7528Qwen3\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u65e0\u97f3\u7d20\u8de8\u8bed\u8a00\u6587\u672c\u5904\u7406\uff0c\u5e76\u91c7\u7528\u65b0\u9896\u7684\u4ee4\u724c\u91cd\u6392\u5e8f\u673a\u5236\uff0c\u5c06\u65f6\u95f4\u5bf9\u9f50\u7684\u6587\u672c\u548c\u8bed\u97f3\u4ee4\u724c\u4f5c\u4e3a\u5355\u4e00\u5e8f\u5217\u751f\u6210\u95ee\u9898\u5904\u7406\u3002", "result": "\u6a21\u578b\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u81ea\u7136\u7684\u58f0\u97f3\uff0c\u5728\u4e00\u4e2a\u6846\u67b6\u5185\u65e0\u7f1d\u521b\u5efa\u65b0\u97f3\u9891\u6216\u7f16\u8f91\u73b0\u6709\u5f55\u97f3\uff0c\u5728\u591a\u6837\u8bed\u8a00\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u7a33\u5065\u6027\u80fd\u3002", "conclusion": "\u7edf\u4e00\u7684\u81ea\u56de\u5f52\u65b9\u6cd5\u5728\u63a8\u8fdb\u590d\u6742\u3001\u73b0\u5b9e\u4e16\u754c\u7684\u591a\u8bed\u8a00\u8bed\u97f3\u5e94\u7528\u65b9\u9762\u5177\u6709\u5f3a\u5927\u6f5c\u529b\uff0c\u5373\u4f7f\u5728\u6bcf\u79cd\u8bed\u8a00\u6570\u636e\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u4e5f\u80fd\u6709\u6548\u5de5\u4f5c\u3002"}}
{"id": "2511.11947", "categories": ["eess.SP", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11947", "abs": "https://arxiv.org/abs/2511.11947", "authors": ["Tri Nhu Do"], "title": "AI-Open-RAN for Non-Terrestrial Networks", "comment": null, "summary": "In this paper, we propose the concept of AIO-RAN-NTN, a unified all-in-one Radio Access Network (RAN) for Non-Terrestrial Networks (NTNs), built on an open architecture that leverages open interfaces and artificial intelligence (AI)-based functionalities. This approach advances interoperability, flexibility, and intelligence in next-generation telecommunications. First, we provide a concise overview of the state-of-the-art architectures for Open-RAN and AI-RAN, highlighting key network functions and infrastructure elements. Next, we introduce our integrated AIO-RAN-NTN blueprint, emphasizing how internal and air interfaces from AIO-RAN and the 3rd Generation Partnership Project (3GPP) can be applied to emerging environments such as NTNs. To examine the impact of mobility on AIO-RAN, we implement a testbed transmission using the OpenAirInterface platform for a standalone (SA) New Radio (NR) 5G system. We then train an AI model on realistic data to forecast key performance indicators (KPIs). Our experiments demonstrate that the AIO-based SA architecture is sensitive to mobility, even at low speeds, but this limitation can be mitigated through AI-driven KPI forecasting.", "AI": {"tldr": "\u63d0\u51faAIO-RAN-NTN\u6982\u5ff5\uff0c\u8fd9\u662f\u4e00\u79cd\u57fa\u4e8e\u5f00\u653e\u67b6\u6784\u548cAI\u529f\u80fd\u7684\u5168\u5408\u4e00\u65e0\u7ebf\u63a5\u5165\u7f51\u7edc\uff0c\u7528\u4e8e\u975e\u5730\u9762\u7f51\u7edc\uff0c\u901a\u8fc7AI\u9a71\u52a8\u7684KPI\u9884\u6d4b\u6765\u7f13\u89e3\u79fb\u52a8\u6027\u5f71\u54cd\u3002", "motivation": "\u63a8\u8fdb\u4e0b\u4e00\u4ee3\u7535\u4fe1\u7f51\u7edc\u7684\u4e92\u64cd\u4f5c\u6027\u3001\u7075\u6d3b\u6027\u548c\u667a\u80fd\u5316\uff0c\u7279\u522b\u662f\u5728\u975e\u5730\u9762\u7f51\u7edc\u73af\u5883\u4e2d\u3002", "method": "\u57fa\u4e8eOpen-RAN\u548cAI-RAN\u67b6\u6784\uff0c\u63d0\u51fa\u96c6\u6210\u84dd\u56fe\uff0c\u4f7f\u7528OpenAirInterface\u5e73\u53f0\u5b9e\u73b05G\u72ec\u7acb\u7ec4\u7f51\u7cfb\u7edf\u6d4b\u8bd5\uff0c\u5e76\u8bad\u7ec3AI\u6a21\u578b\u9884\u6d4b\u5173\u952e\u6027\u80fd\u6307\u6807\u3002", "result": "\u5b9e\u9a8c\u8868\u660eAIO-RAN\u67b6\u6784\u5bf9\u79fb\u52a8\u6027\u654f\u611f\uff0c\u5373\u4f7f\u5728\u4f4e\u901f\u4e0b\u4e5f\u4f1a\u53d7\u5f71\u54cd\uff0c\u4f46\u901a\u8fc7AI\u9a71\u52a8\u7684KPI\u9884\u6d4b\u53ef\u4ee5\u6709\u6548\u7f13\u89e3\u8fd9\u4e00\u9650\u5236\u3002", "conclusion": "AIO-RAN-NTN\u67b6\u6784\u4e3a\u4e0b\u4e00\u4ee3\u975e\u5730\u9762\u7f51\u7edc\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\uff0cAI\u6280\u672f\u80fd\u591f\u6709\u6548\u5e94\u5bf9\u79fb\u52a8\u6027\u5e26\u6765\u7684\u6311\u6218\u3002"}}
{"id": "2511.13273", "categories": ["cs.SD", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13273", "abs": "https://arxiv.org/abs/2511.13273", "authors": ["Zhe Sun", "Yujun Cai", "Jiayu Yao", "Yiwei Wang"], "title": "Spatial Blind Spot: Auditory Motion Perception Deficits in Audio LLMs", "comment": null, "summary": "Large Audio-Language Models (LALMs) have recently shown impressive progress in speech recognition, audio captioning, and auditory question answering. Yet, whether these models can perceive spatial dynamics, particularly the motion of sound sources, remains unclear. In this work, we uncover a systematic motion perception deficit in current ALLMs. To investigate this issue, we introduce AMPBench, the first benchmark explicitly designed to evaluate auditory motion understanding. AMPBench introduces a controlled question-answering benchmark designed to evaluate whether Audio-Language Models (LALMs) can infer the direction and trajectory of moving sound sources from binaural audio. Comprehensive quantitative and qualitative analyses reveal that current models struggle to reliably recognize motion cues or distinguish directional patterns. The average accuracy remains below 50%, underscoring a fundamental limitation in auditory spatial reasoning. Our study highlights a fundamental gap between human and model auditory spatial reasoning, providing both a diagnostic tool and new insight for enhancing spatial cognition in future Audio-Language Models.", "AI": {"tldr": "\u5f53\u524d\u97f3\u9891\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u7cfb\u7edf\u6027\u7684\u8fd0\u52a8\u611f\u77e5\u7f3a\u9677\uff0c\u65e0\u6cd5\u53ef\u9760\u8bc6\u522b\u58f0\u97f3\u6e90\u7684\u8fd0\u52a8\u65b9\u5411\u548c\u8f68\u8ff9\uff0c\u51c6\u786e\u7387\u4f4e\u4e8e50%\u3002", "motivation": "\u7814\u7a76\u5927\u578b\u97f3\u9891\u8bed\u8a00\u6a21\u578b\u662f\u5426\u80fd\u591f\u611f\u77e5\u58f0\u97f3\u6e90\u7684\u7a7a\u95f4\u52a8\u6001\u548c\u8fd0\u52a8\u7279\u6027\uff0c\u586b\u8865\u73b0\u6709\u6a21\u578b\u5728\u542c\u89c9\u7a7a\u95f4\u63a8\u7406\u65b9\u9762\u7684\u7a7a\u767d\u3002", "method": "\u5f15\u5165AMPBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u901a\u8fc7\u53cc\u8033\u97f3\u9891\u7684\u95ee\u7b54\u4efb\u52a1\u8bc4\u4f30\u6a21\u578b\u5bf9\u79fb\u52a8\u58f0\u97f3\u6e90\u65b9\u5411\u548c\u8f68\u8ff9\u7684\u63a8\u7406\u80fd\u529b\u3002", "result": "\u5f53\u524d\u6a21\u578b\u5728\u8fd0\u52a8\u611f\u77e5\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u65e0\u6cd5\u53ef\u9760\u8bc6\u522b\u8fd0\u52a8\u7ebf\u7d22\u6216\u533a\u5206\u65b9\u5411\u6a21\u5f0f\uff0c\u5e73\u5747\u51c6\u786e\u7387\u4f4e\u4e8e50%\u3002", "conclusion": "\u63ed\u793a\u4e86\u4eba\u7c7b\u4e0e\u6a21\u578b\u5728\u542c\u89c9\u7a7a\u95f4\u63a8\u7406\u65b9\u9762\u7684\u6839\u672c\u5dee\u8ddd\uff0c\u4e3a\u672a\u6765\u97f3\u9891\u8bed\u8a00\u6a21\u578b\u7684\u7a7a\u95f4\u8ba4\u77e5\u589e\u5f3a\u63d0\u4f9b\u4e86\u8bca\u65ad\u5de5\u5177\u548c\u65b0\u89c1\u89e3\u3002"}}
{"id": "2511.12552", "categories": ["eess.AS", "physics.app-ph"], "pdf": "https://arxiv.org/pdf/2511.12552", "abs": "https://arxiv.org/abs/2511.12552", "authors": ["Reinhild Roden", "Tobias Sankowsky-Rothe", "Nick Wulbusch", "Alexey Chernov", "Matthias Blau"], "title": "Eardrum sound pressure prediction from ear canal reflectance based on the inverse solution of Webster's horn equation", "comment": "Manuscript submitted to the Journal of the Acoustical Society of America (under minor revision)", "summary": "To derive ear canal transfer functions for individualized equalization algorithms of in-ear hearing systems, individual ear canal models are needed. In a one-dimensional approach, this requires the estimation of the individual area function of the ear canal. The area function can be effectively and reproducibly calculated as the inverse solution of Webster's horn equation by finite difference approximation of the time domain reflectance. Building upon previous research, the present study further investigates the termination of the approximation at an optimal spatial resolution, addressing the absence of higher frequencies in typical ear canal measurements and enhancing the accuracy of the inverse solution. Compared to the geometric reference, more precise area functions were achieved by extrapolating simulated input impedances of ear canal geometries up to a frequency of 3.5 MHz, corresponding to 0.1 mm spatial resolution. The low pass of the previous work was adopted but adjusted for its cut-off frequency depending on the highest frequency of the band-limited input impedance. Robust criteria for terminating the area function at the approximated ear canal length were found. Finally, three-dimensional simulated and measured ear canal transfer impedances were replicated well employing the previously introduced and herein validated one-dimensional electro-acoustic model fed by the area functions.", "AI": {"tldr": "\u8be5\u7814\u7a76\u6539\u8fdb\u4e86\u901a\u8fc7Webster\u5587\u53ed\u65b9\u7a0b\u53cd\u6f14\u8ba1\u7b97\u8033\u9053\u9762\u79ef\u51fd\u6570\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316\u7a7a\u95f4\u5206\u8fa8\u7387\u7ec8\u6b62\u6761\u4ef6\u548c\u8c03\u6574\u622a\u6b62\u9891\u7387\uff0c\u63d0\u9ad8\u4e86\u8033\u9053\u6a21\u578b\u5728\u4e2a\u4f53\u5316\u542c\u529b\u7cfb\u7edf\u5747\u8861\u7b97\u6cd5\u4e2d\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u4e3a\u4e2a\u4f53\u5316\u8033\u5185\u542c\u529b\u7cfb\u7edf\u5747\u8861\u7b97\u6cd5\u5f00\u53d1\u51c6\u786e\u7684\u8033\u9053\u4f20\u8f93\u51fd\u6570\uff0c\u9700\u8981\u5efa\u7acb\u4e2a\u4f53\u5316\u7684\u8033\u9053\u6a21\u578b\uff0c\u8fd9\u8981\u6c42\u51c6\u786e\u4f30\u8ba1\u8033\u9053\u7684\u9762\u79ef\u51fd\u6570\u3002", "method": "\u91c7\u7528Webster\u5587\u53ed\u65b9\u7a0b\u7684\u4e00\u7ef4\u6709\u9650\u5dee\u5206\u8fd1\u4f3c\u65b9\u6cd5\uff0c\u901a\u8fc7\u65f6\u95f4\u57df\u53cd\u5c04\u7387\u53cd\u6f14\u8ba1\u7b97\u8033\u9053\u9762\u79ef\u51fd\u6570\uff0c\u4f18\u5316\u4e86\u7a7a\u95f4\u5206\u8fa8\u7387\u7ec8\u6b62\u6761\u4ef6\uff0c\u5e76\u8c03\u6574\u4e86\u622a\u6b62\u9891\u7387\u4ee5\u9002\u5e94\u9891\u5e26\u53d7\u9650\u7684\u8f93\u5165\u963b\u6297\u3002", "result": "\u4e0e\u51e0\u4f55\u53c2\u8003\u76f8\u6bd4\uff0c\u901a\u8fc7\u5c06\u6a21\u62df\u8f93\u5165\u963b\u6297\u5916\u63a8\u81f33.5MHz\u9891\u7387\uff08\u5bf9\u5e940.1mm\u7a7a\u95f4\u5206\u8fa8\u7387\uff09\uff0c\u83b7\u5f97\u4e86\u66f4\u7cbe\u786e\u7684\u9762\u79ef\u51fd\u6570\uff0c\u5e76\u5efa\u7acb\u4e86\u7a33\u5065\u7684\u8033\u9053\u957f\u5ea6\u7ec8\u6b62\u6807\u51c6\u3002", "conclusion": "\u9a8c\u8bc1\u4e86\u4e00\u7ef4\u7535\u58f0\u6a21\u578b\u7ed3\u5408\u6539\u8fdb\u7684\u9762\u79ef\u51fd\u6570\u8ba1\u7b97\u65b9\u6cd5\uff0c\u80fd\u591f\u5f88\u597d\u5730\u590d\u73b0\u4e09\u7ef4\u6a21\u62df\u548c\u6d4b\u91cf\u7684\u8033\u9053\u4f20\u8f93\u963b\u6297\uff0c\u4e3a\u4e2a\u4f53\u5316\u542c\u529b\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u57fa\u7840\u3002"}}
{"id": "2511.11951", "categories": ["eess.SP", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.11951", "abs": "https://arxiv.org/abs/2511.11951", "authors": ["Nghia Thinh Nguyen", "Tri Nhu Do"], "title": "Temporal Micro-Doppler Spectrogram-based ViT Multiclass Target Classification", "comment": null, "summary": "In this paper, we propose a new Temporal MDS-Vision Transformer (T-MDS-ViT) for multiclass target classification using millimeter-wave FMCW radar micro-Doppler spectrograms. Specifically, we design a transformer-based architecture that processes stacked range-velocity-angle (RVA) spatiotemporal tensors via patch embeddings and cross-axis attention mechanisms to explicitly model the sequential nature of MDS data across multiple frames. The T-MDS-ViT exploits mobility-aware constraints in its attention layer correspondences to maintain separability under target overlaps and partial occlusions. Next, we apply an explainable mechanism to examine how the attention layers focus on characteristic high-energy regions of the MDS representations and their effect on class-specific kinematic features. We also demonstrate that our proposed framework is superior to existing CNN-based methods in terms of classification accuracy while achieving better data efficiency and real-time deployability.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eTransformer\u7684T-MDS-ViT\u67b6\u6784\uff0c\u7528\u4e8e\u6beb\u7c73\u6ce2FMCW\u96f7\u8fbe\u5fae\u591a\u666e\u52d2\u8c31\u56fe\u7684\u591a\u7c7b\u76ee\u6807\u5206\u7c7b\uff0c\u901a\u8fc7\u8de8\u8f74\u6ce8\u610f\u529b\u673a\u5236\u5904\u7406\u65f6\u7a7a\u5f20\u91cf\uff0c\u5728\u76ee\u6807\u91cd\u53e0\u548c\u90e8\u5206\u906e\u6321\u60c5\u51b5\u4e0b\u4fdd\u6301\u53ef\u5206\u79bb\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eCNN\u7684\u65b9\u6cd5\u5728\u5904\u7406\u6beb\u7c73\u6ce2\u96f7\u8fbe\u5fae\u591a\u666e\u52d2\u8c31\u56fe\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f\u5728\u76ee\u6807\u91cd\u53e0\u548c\u906e\u6321\u60c5\u51b5\u4e0b\u5206\u7c7b\u6027\u80fd\u4e0b\u964d\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u65f6\u7a7a\u5efa\u6a21\u65b9\u6cd5\u3002", "method": "\u8bbe\u8ba1Transformer\u67b6\u6784\u5904\u7406\u5806\u53e0\u7684RVA\u65f6\u7a7a\u5f20\u91cf\uff0c\u4f7f\u7528\u8865\u4e01\u5d4c\u5165\u548c\u8de8\u8f74\u6ce8\u610f\u529b\u673a\u5236\u663e\u5f0f\u5efa\u6a21MDS\u6570\u636e\u7684\u5e8f\u5217\u7279\u6027\uff0c\u5e76\u5229\u7528\u79fb\u52a8\u611f\u77e5\u7ea6\u675f\u4fdd\u6301\u6ce8\u610f\u529b\u5c42\u5bf9\u5e94\u5173\u7cfb\u3002", "result": "\u6240\u63d0\u6846\u67b6\u5728\u5206\u7c7b\u7cbe\u5ea6\u4e0a\u4f18\u4e8e\u73b0\u6709CNN\u65b9\u6cd5\uff0c\u540c\u65f6\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u6570\u636e\u6548\u7387\u548c\u5b9e\u65f6\u90e8\u7f72\u80fd\u529b\u3002", "conclusion": "T-MDS-ViT\u901a\u8fc7Transformer\u67b6\u6784\u548c\u6ce8\u610f\u529b\u673a\u5236\u6709\u6548\u63d0\u5347\u4e86\u6beb\u7c73\u6ce2\u96f7\u8fbe\u76ee\u6807\u5206\u7c7b\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u573a\u666f\u4e0b\u5177\u6709\u4f18\u52bf\u3002"}}
{"id": "2511.13300", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2511.13300", "abs": "https://arxiv.org/abs/2511.13300", "authors": ["Xiaobin Rong", "Qinwen Hu", "Mansur Yesilbursa", "Kamil Wojcicki", "Jing Lu"], "title": "PASE: Leveraging the Phonological Prior of WavLM for Low-Hallucination Generative Speech Enhancement", "comment": "Accepted by AAAI 2026", "summary": "Generative models have shown remarkable performance in speech enhancement (SE), achieving superior perceptual quality over traditional discriminative approaches. However, existing generative SE approaches often overlook the risk of hallucination under severe noise, leading to incorrect spoken content or inconsistent speaker characteristics, which we term linguistic and acoustic hallucinations, respectively. We argue that linguistic hallucination stems from models' failure to constrain valid phonological structures and it is a more fundamental challenge. While language models (LMs) are well-suited for capturing the underlying speech structure through modeling the distribution of discrete tokens, existing approaches are limited in learning from noise-corrupted representations, which can lead to contaminated priors and hallucinations. To overcome these limitations, we propose the Phonologically Anchored Speech Enhancer (PASE), a generative SE framework that leverages the robust phonological prior embedded in the pre-trained WavLM model to mitigate hallucinations. First, we adapt WavLM into a denoising expert via representation distillation to clean its final-layer features. Guided by the model's intrinsic phonological prior, this process enables robust denoising while minimizing linguistic hallucinations. To further reduce acoustic hallucinations, we train the vocoder with a dual-stream representation: the high-level phonetic representation provides clean linguistic content, while a low-level acoustic representation retains speaker identity and prosody. Experimental results demonstrate that PASE not only surpasses state-of-the-art discriminative models in perceptual quality, but also significantly outperforms prior generative models with substantially lower linguistic and acoustic hallucinations.", "AI": {"tldr": "PASE\u662f\u4e00\u4e2a\u751f\u6210\u5f0f\u8bed\u97f3\u589e\u5f3a\u6846\u67b6\uff0c\u5229\u7528\u9884\u8bad\u7ec3WavLM\u6a21\u578b\u7684\u9c81\u68d2\u97f3\u97f5\u5148\u9a8c\u6765\u51cf\u8f7b\u5e7b\u89c9\u95ee\u9898\uff0c\u5728\u611f\u77e5\u8d28\u91cf\u4e0a\u8d85\u8d8a\u5224\u522b\u5f0f\u6a21\u578b\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8bed\u8a00\u548c\u58f0\u5b66\u5e7b\u89c9\u3002", "motivation": "\u73b0\u6709\u751f\u6210\u5f0f\u8bed\u97f3\u589e\u5f3a\u65b9\u6cd5\u5728\u4e25\u91cd\u566a\u58f0\u4e0b\u5bb9\u6613\u4ea7\u751f\u8bed\u8a00\u5e7b\u89c9\uff08\u9519\u8bef\u8bed\u97f3\u5185\u5bb9\uff09\u548c\u58f0\u5b66\u5e7b\u89c9\uff08\u4e0d\u4e00\u81f4\u7684\u8bf4\u8bdd\u4eba\u7279\u5f81\uff09\uff0c\u7279\u522b\u662f\u8bed\u8a00\u5e7b\u89c9\u6e90\u4e8e\u6a21\u578b\u672a\u80fd\u7ea6\u675f\u6709\u6548\u7684\u97f3\u97f5\u7ed3\u6784\uff0c\u8fd9\u662f\u66f4\u6839\u672c\u7684\u6311\u6218\u3002", "method": "1. \u901a\u8fc7\u8868\u793a\u84b8\u998f\u5c06WavLM\u9002\u914d\u4e3a\u53bb\u566a\u4e13\u5bb6\uff0c\u6e05\u7406\u5176\u6700\u7ec8\u5c42\u7279\u5f81\uff1b2. \u4f7f\u7528\u53cc\u6d41\u8868\u793a\u8bad\u7ec3\u58f0\u7801\u5668\uff1a\u9ad8\u5c42\u97f3\u97f5\u8868\u793a\u63d0\u4f9b\u5e72\u51c0\u8bed\u8a00\u5185\u5bb9\uff0c\u4f4e\u5c42\u58f0\u5b66\u8868\u793a\u4fdd\u7559\u8bf4\u8bdd\u4eba\u8eab\u4efd\u548c\u97f5\u5f8b\u3002", "result": "PASE\u4e0d\u4ec5\u5728\u611f\u77e5\u8d28\u91cf\u4e0a\u8d85\u8d8a\u4e86\u6700\u5148\u8fdb\u7684\u5224\u522b\u5f0f\u6a21\u578b\uff0c\u8fd8\u663e\u8457\u4f18\u4e8e\u5148\u524d\u7684\u751f\u6210\u5f0f\u6a21\u578b\uff0c\u8bed\u8a00\u548c\u58f0\u5b66\u5e7b\u89c9\u5927\u5e45\u964d\u4f4e\u3002", "conclusion": "\u901a\u8fc7\u5229\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u97f3\u97f5\u5148\u9a8c\u548c\u53cc\u6d41\u8868\u793a\u7b56\u7565\uff0cPASE\u6709\u6548\u89e3\u51b3\u4e86\u751f\u6210\u5f0f\u8bed\u97f3\u589e\u5f3a\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u8bed\u97f3\u589e\u5f3a\u6548\u679c\u3002"}}
{"id": "2511.11980", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.11980", "abs": "https://arxiv.org/abs/2511.11980", "authors": ["Yuan Guo", "Wen Chen", "Xudong Bai", "Chong He", "Qiong Wu"], "title": "Resource Allocation for Transmissive RIS Transceiver Enabled SWIPT Systems", "comment": null, "summary": "A novel transmissive reconfigurable intelligent surface (TRIS) transceiver-empowered simultaneous wireless information and power transfer (SWIPT) framework is proposed. The sum-rate of the information decoding (ID) users is maximized by optimizing the TRIS transceiver's beamforming, subject to the energy harvesting (EH) users' quality-of-harvest and the per-antenna power constraints. To solve this non-convex problem, we develop an efficient optimization algorithm. First, the original problem is reformulated as a semi-definite programming (SDP) problem. The resulting SDP problem is then addressed using successive convex approximation (SCA) combined with a penalty-based method. Numerical results demonstrate the effectiveness of the algorithm.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u900f\u5c04\u5f0f\u53ef\u91cd\u6784\u667a\u80fd\u8868\u9762(TRIS)\u6536\u53d1\u5668\u7684SWIPT\u6846\u67b6\uff0c\u901a\u8fc7\u4f18\u5316TRIS\u6ce2\u675f\u6210\u5f62\u6765\u6700\u5927\u5316\u4fe1\u606f\u89e3\u7801\u7528\u6237\u7684\u548c\u901f\u7387\uff0c\u540c\u65f6\u6ee1\u8db3\u80fd\u91cf\u6536\u96c6\u7528\u6237\u7684\u8d28\u91cf\u8981\u6c42\u548c\u6bcf\u5929\u7ebf\u529f\u7387\u7ea6\u675f\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u4f20\u7edfSWIPT\u7cfb\u7edf\u4e2d\u4fe1\u606f\u4f20\u8f93\u548c\u80fd\u91cf\u6536\u96c6\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\uff0c\u5229\u7528TRIS\u6280\u672f\u5b9e\u73b0\u66f4\u9ad8\u6548\u7684\u65e0\u7ebf\u4fe1\u606f\u4e0e\u80fd\u91cf\u540c\u65f6\u4f20\u8f93\u3002", "method": "\u5c06\u539f\u59cb\u975e\u51f8\u95ee\u9898\u91cd\u6784\u4e3a\u534a\u5b9a\u89c4\u5212\u95ee\u9898\uff0c\u91c7\u7528\u9010\u6b21\u51f8\u903c\u8fd1\u4e0e\u57fa\u4e8e\u60e9\u7f5a\u7684\u65b9\u6cd5\u76f8\u7ed3\u5408\u7684\u4f18\u5316\u7b97\u6cd5\u3002", "result": "\u6570\u503c\u7ed3\u679c\u8868\u660e\u6240\u63d0\u7b97\u6cd5\u5177\u6709\u6709\u6548\u6027\u3002", "conclusion": "TRIS\u6536\u53d1\u5668\u8d4b\u80fd\u7684SWIPT\u6846\u67b6\u80fd\u591f\u6709\u6548\u63d0\u5347\u4fe1\u606f\u89e3\u7801\u7528\u6237\u7684\u548c\u901f\u7387\uff0c\u540c\u65f6\u4fdd\u8bc1\u80fd\u91cf\u6536\u96c6\u7528\u6237\u7684\u8d28\u91cf\u8981\u6c42\u3002"}}
{"id": "2511.13487", "categories": ["eess.AS", "cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2511.13487", "abs": "https://arxiv.org/abs/2511.13487", "authors": ["Davoud Shariat Panah", "Alessandro Ragano", "Dan Barry", "Jan Skoglund", "Andrew Hines"], "title": "Systematic evaluation of time-frequency features for binaural sound source localization", "comment": "Submitted to ICASSP 2026", "summary": "This study presents a systematic evaluation of time-frequency feature design for binaural sound source localization (SSL), focusing on how feature selection influences model performance across diverse conditions. We investigate the performance of a convolutional neural network (CNN) model using various combinations of amplitude-based features (magnitude spectrogram, interaural level difference - ILD) and phase-based features (phase spectrogram, interaural phase difference - IPD). Evaluations on in-domain and out-of-domain data with mismatched head-related transfer functions (HRTFs) reveal that carefully chosen feature combinations often outperform increases in model complexity. While two-feature sets such as ILD + IPD are sufficient for in-domain SSL, generalization to diverse content requires richer inputs combining channel spectrograms with both ILD and IPD. Using the optimal feature sets, our low-complexity CNN model achieves competitive performance. Our findings underscore the importance of feature design in binaural SSL and provide practical guidance for both domain-specific and general-purpose localization.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u8bc4\u4f30\u4e86\u53cc\u8033\u58f0\u6e90\u5b9a\u4f4d\u4e2d\u7684\u65f6\u9891\u7279\u5f81\u8bbe\u8ba1\uff0c\u53d1\u73b0\u7cbe\u5fc3\u9009\u62e9\u7684\u7279\u5f81\u7ec4\u5408\u901a\u5e38\u4f18\u4e8e\u589e\u52a0\u6a21\u578b\u590d\u6742\u5ea6\u3002ILD+IPD\u53cc\u7279\u5f81\u7ec4\u5408\u8db3\u4ee5\u7528\u4e8e\u57df\u5185\u5b9a\u4f4d\uff0c\u800c\u6cdb\u5316\u5230\u591a\u6837\u5316\u5185\u5bb9\u9700\u8981\u7ed3\u5408\u901a\u9053\u8c31\u56fe\u3001ILD\u548cIPD\u7684\u66f4\u4e30\u5bcc\u8f93\u5165\u3002", "motivation": "\u7814\u7a76\u7279\u5f81\u9009\u62e9\u5982\u4f55\u5f71\u54cd\u53cc\u8033\u58f0\u6e90\u5b9a\u4f4d\u6a21\u578b\u5728\u4e0d\u540c\u6761\u4ef6\u4e0b\u7684\u6027\u80fd\uff0c\u63a2\u7d22\u632f\u5e45\u7279\u5f81\u548c\u76f8\u4f4d\u7279\u5f81\u7684\u6700\u4f73\u7ec4\u5408\u65b9\u5f0f\u3002", "method": "\u4f7f\u7528\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff0c\u8bc4\u4f30\u4e0d\u540c\u632f\u5e45\u7279\u5f81\uff08\u5e45\u5ea6\u8c31\u56fe\u3001ILD\uff09\u548c\u76f8\u4f4d\u7279\u5f81\uff08\u76f8\u4f4d\u8c31\u56fe\u3001IPD\uff09\u7ec4\u5408\u7684\u6027\u80fd\uff0c\u5728\u57df\u5185\u548c\u57df\u5916\u6570\u636e\u4e0a\u8fdb\u884c\u6d4b\u8bd5\u3002", "result": "\u7cbe\u5fc3\u9009\u62e9\u7684\u7279\u5f81\u7ec4\u5408\u5f80\u5f80\u4f18\u4e8e\u589e\u52a0\u6a21\u578b\u590d\u6742\u5ea6\u3002\u4f4e\u590d\u6742\u5ea6CNN\u6a21\u578b\u4f7f\u7528\u6700\u4f18\u7279\u5f81\u96c6\u80fd\u8fbe\u5230\u7ade\u4e89\u6027\u6027\u80fd\u3002", "conclusion": "\u7279\u5f81\u8bbe\u8ba1\u5728\u53cc\u8033\u58f0\u6e90\u5b9a\u4f4d\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4e3a\u7279\u5b9a\u9886\u57df\u548c\u901a\u7528\u5b9a\u4f4d\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\u3002"}}
{"id": "2511.11985", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.11985", "abs": "https://arxiv.org/abs/2511.11985", "authors": ["Yuan Guo", "Wen Chen", "Yanze Zhu", "Zhendong Li", "Qiong Wu", "Kunlun Wang"], "title": "Beamforming for Transmissive RIS Transmitter Enabled Simultaneous Wireless Information and Power Transfer Systems", "comment": null, "summary": "This paper investigates a novel transmissive reconfigurable intelligent surface (TRIS) transceiver-empowered simultaneous wireless information and power transfer (SWIPT) system with multiple information decoding (ID) and energy harvesting (EH) users. Under the considered system model, we formulate an optimization problem that maximizes the sum-rate of all ID users via the design of the TRIS transceiver's active beamforming. The design is constrained by per-antenna power limits at the TRIS transceiver and by the minimum harvested energy demand of all EH users. Due to the non-convexity of the objective function and the energy harvesting constraint, the sum-rate problem is difficult to tackle. To solve this challenging optimization problem, by leveraging the weighted minimum mean squared error (WMMSE) framework and the majorization-minimization (MM) method, we propose a second-order cone programming (SOCP)-based algorithm. Per-element power constraints introduce a large number of constraints, making the problem considerably more difficult. By applying the alternating direction method of multipliers (ADMM) method, we successfully develop an analytical, computationally efficient, and highly parallelizable algorithm to address this challenge. Numerical results are provided to validate the convergence and effectiveness of the proposed algorithms. Furthermore, the low-complexity algorithm significantly reduces computational complexity without performance degradation.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u57fa\u4e8e\u900f\u5c04\u5f0f\u53ef\u91cd\u6784\u667a\u80fd\u8868\u9762(TRIS)\u6536\u53d1\u5668\u7684\u65e0\u7ebf\u4fe1\u606f\u548c\u80fd\u91cf\u540c\u65f6\u4f20\u8f93(SWIPT)\u7cfb\u7edf\uff0c\u63d0\u51fa\u4e86\u5728\u6ee1\u8db3\u80fd\u91cf\u6536\u96c6\u7528\u6237\u9700\u6c42\u548c\u5929\u7ebf\u529f\u7387\u7ea6\u675f\u4e0b\u6700\u5927\u5316\u4fe1\u606f\u89e3\u7801\u7528\u6237\u603b\u901f\u7387\u7684\u4f18\u5316\u7b97\u6cd5\u3002", "motivation": "\u4f20\u7edfSWIPT\u7cfb\u7edf\u9762\u4e34\u4fe1\u606f\u4f20\u8f93\u548c\u80fd\u91cf\u6536\u96c6\u4e4b\u95f4\u7684\u6743\u8861\u6311\u6218\uff0cTRIS\u6280\u672f\u80fd\u591f\u7075\u6d3b\u8c03\u63a7\u7535\u78c1\u6ce2\uff0c\u4e3a\u63d0\u5347SWIPT\u7cfb\u7edf\u6027\u80fd\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002", "method": "\u91c7\u7528\u52a0\u6743\u6700\u5c0f\u5747\u65b9\u8bef\u5dee(WMMSE)\u6846\u67b6\u548c\u4e3b\u4f18\u5316-\u6700\u5c0f\u5316(MM)\u65b9\u6cd5\uff0c\u63d0\u51fa\u57fa\u4e8e\u4e8c\u9636\u9525\u89c4\u5212(SOCP)\u7684\u7b97\u6cd5\uff0c\u5e76\u5e94\u7528\u4ea4\u66ff\u65b9\u5411\u4e58\u5b50\u6cd5(ADMM)\u5904\u7406\u5929\u7ebf\u529f\u7387\u7ea6\u675f\u95ee\u9898\u3002", "result": "\u6570\u503c\u7ed3\u679c\u8868\u660e\u6240\u63d0\u7b97\u6cd5\u5177\u6709\u826f\u597d\u7684\u6536\u655b\u6027\u548c\u6709\u6548\u6027\uff0c\u4f4e\u590d\u6742\u5ea6\u7b97\u6cd5\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "conclusion": "\u63d0\u51fa\u7684TRIS\u6536\u53d1\u5668\u8d4b\u80fdSWIPT\u7cfb\u7edf\u80fd\u591f\u6709\u6548\u63d0\u5347\u4fe1\u606f\u4f20\u8f93\u901f\u7387\uff0c\u540c\u65f6\u6ee1\u8db3\u80fd\u91cf\u6536\u96c6\u9700\u6c42\uff0c\u4e3a\u672a\u6765\u65e0\u7ebf\u901a\u4fe1\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.12045", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.12045", "abs": "https://arxiv.org/abs/2511.12045", "authors": ["Paloma Sette", "Maria Werneck", "William Barbosa", "Ana Loubacker"], "title": "MUSTEM: A Dual-Modality System for Vibrotactile and Visual Translation of Music as an Assistive Technology", "comment": null, "summary": "The emotional and structural experience of music remains a significant accessibility challenge for the deaf and hard of hearing community. This paper introduces MUSTEM (Multisensorial Emotional Translation), a novel system designed to translate music into a rich, coherent, and scientifically-grounded sensory experience. We present a dual-modality approach addressing this challenge through two interconnected components. First, a low-cost, portable hardware prototype that performs real-time audio analysis, mapping distinct frequency bands (sub-bass, bass, mid-range, treble) to a four-channel vibrotactile system, allowing users to feel the music's rhythmic and foundational structure. Second, to overcome the processing limitations of embedded hardware, we developed a high-fidelity software simulation that demonstrates the full potential of the visual translation. This assistive dashboard decodes musical components - such as rhythm, harmony, and frequency spectrum - into an intuitive and educational visual interface. MUSTEM offers a comprehensive framework for sensory substitution, presenting a viable and accessible pathway for the deaf community to experience music not just as vibration, but as a structured, substantiated and emotionally resonant visual and tactile language. Preliminary feedback from seven deaf users suggests the system's spatial vibrotactile mapping is perceptible and engaging. All source code and hardware designs are released as open-source. Video demonstrations and open-source code are available on the project's official channel.", "AI": {"tldr": "MUSTEM\u7cfb\u7edf\u5c06\u97f3\u4e50\u8f6c\u5316\u4e3a\u591a\u611f\u5b98\u4f53\u9a8c\uff0c\u901a\u8fc7\u632f\u52a8\u89e6\u89c9\u548c\u89c6\u89c9\u754c\u9762\u8ba9\u542c\u969c\u4eba\u58eb\u611f\u53d7\u97f3\u4e50\u7684\u7ed3\u6784\u548c\u60c5\u611f", "motivation": "\u89e3\u51b3\u542c\u969c\u7fa4\u4f53\u5728\u97f3\u4e50\u60c5\u611f\u548c\u7ed3\u6784\u4f53\u9a8c\u65b9\u9762\u7684\u53ef\u8bbf\u95ee\u6027\u6311\u6218\uff0c\u63d0\u4f9b\u79d1\u5b66\u57fa\u7840\u7684\u591a\u611f\u5b98\u97f3\u4e50\u4f53\u9a8c", "method": "\u53cc\u6a21\u6001\u65b9\u6cd5\uff1a\u4f4e\u6210\u672c\u4fbf\u643a\u786c\u4ef6\u539f\u578b\uff08\u5b9e\u65f6\u97f3\u9891\u5206\u6790+\u56db\u901a\u9053\u632f\u52a8\u89e6\u89c9\u7cfb\u7edf\uff09+\u9ad8\u4fdd\u771f\u8f6f\u4ef6\u6a21\u62df\uff08\u89c6\u89c9\u7ffb\u8bd1\u8f85\u52a9\u4eea\u8868\u677f\uff09", "result": "\u521d\u6b65\u7528\u6237\u53cd\u9988\u663e\u793a\u7a7a\u95f4\u632f\u52a8\u89e6\u89c9\u6620\u5c04\u53ef\u611f\u77e5\u4e14\u5f15\u4eba\u5165\u80dc\uff0c\u7cfb\u7edf\u63d0\u4f9b\u4e86\u7ed3\u6784\u5316\u548c\u60c5\u611f\u5171\u9e23\u7684\u89c6\u89c9\u89e6\u89c9\u8bed\u8a00", "conclusion": "MUSTEM\u4e3a\u542c\u969c\u793e\u533a\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u97f3\u4e50\u4f53\u9a8c\u9014\u5f84\uff0c\u4e0d\u4ec5\u662f\u632f\u52a8\uff0c\u66f4\u662f\u7ed3\u6784\u5316\u548c\u60c5\u611f\u5171\u9e23\u7684\u591a\u611f\u5b98\u4f53\u9a8c"}}
{"id": "2511.12051", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.12051", "abs": "https://arxiv.org/abs/2511.12051", "authors": ["Scott Staniewicz", "Sara Mirzaee", "Heresh Fattahi", "Talib Oliver-Cabrera", "Emre Havazli", "Geoffrey Gunter", "Se-Yeon Jeon", "Mary Grace Bato", "Jinwoo Kim", "Simran S. Sangha", "Bruce Chapman", "Alexander L. Handwerger", "Marin Govorcin", "Piyush Agram", "David Bekaert"], "title": "Near-Real-Time InSAR Phase Estimation for Large-Scale Surface Displacement Monitoring", "comment": "14 pages, 11 figures, plus supplementary material", "summary": "Operational near-real-time monitoring of Earth's surface deformation using Interferometric Synthetic Aperture Radar (InSAR) requires processing algorithms that efficiently incorporate new acquisitions without reprocessing historical archives. We present sequential phase linking approach using compressed single-look-complex images (SLCs) capable of producing surface displacement estimates within hours of the time of a new acquisition. Our key algorithmic contribution is a mini-stack reference scheme that maintains phase consistency across processing batches without adjusting or re-estimating previous time steps, enabling straightforward operational deployment. We introduce online methods for persistent and distributed scatterer identification that adapt to temporal changes in surface properties through incremental amplitude statistics updates. The processing chain incorporates multiple complementary metrics for pixel quality that are reliable for small SLC stack sizes, and an L1-norm network inversion to limit propagation of unwrapping errors across the time series. We use our algorithm to produce OPERA Surface Displacement from Sentinel-1 product, the first continental-scale surface displacement product over North America. Validation against GPS measurements and InSAR residual analysis demonstrates millimeter-level agreement in velocity estimates in varying environmental conditions. We demonstrate our algorithm's capabilities with a successful recovery of meter-scale co-eruptive displacement at Kilauea volcano during the 2018 eruption, as well as detection of subtle uplift at Three Sisters volcano, Oregon- a challenging environment for C-band InSAR due to dense vegetation and seasonal snow. We have made all software available as open source libraries, providing a significant advancement to the open scientific community's ability to process large InSAR data sets in a cloud environment.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u8fd1\u5b9e\u65f6InSAR\u76d1\u6d4b\u7684\u5e8f\u5217\u76f8\u4f4d\u8fde\u63a5\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u6570\u5c0f\u65f6\u5185\u5904\u7406\u65b0\u83b7\u53d6\u7684\u6570\u636e\uff0c\u65e0\u9700\u91cd\u65b0\u5904\u7406\u5386\u53f2\u6863\u6848\u3002", "motivation": "\u5b9e\u73b0\u5730\u7403\u8868\u9762\u5f62\u53d8\u7684\u8fd1\u5b9e\u65f6\u76d1\u6d4b\u9700\u8981\u80fd\u591f\u9ad8\u6548\u6574\u5408\u65b0\u83b7\u53d6\u6570\u636e\u7684\u5904\u7406\u7b97\u6cd5\uff0c\u907f\u514d\u91cd\u65b0\u5904\u7406\u5386\u53f2\u6570\u636e\u3002", "method": "\u4f7f\u7528\u538b\u7f29\u5355\u89c6\u590d\u56fe\u50cf\uff0c\u91c7\u7528\u5c0f\u5806\u6808\u53c2\u8003\u65b9\u6848\u4fdd\u6301\u76f8\u4f4d\u4e00\u81f4\u6027\uff0c\u5f15\u5165\u5728\u7ebf\u65b9\u6cd5\u8bc6\u522b\u6301\u4e45\u548c\u5206\u5e03\u5f0f\u6563\u5c04\u4f53\uff0c\u7ed3\u5408\u591a\u79cd\u50cf\u7d20\u8d28\u91cf\u6307\u6807\u548cL1\u8303\u6570\u7f51\u7edc\u53cd\u6f14\u3002", "result": "\u6210\u529f\u751f\u6210\u4e86\u5317\u7f8e\u5927\u9646\u5c3a\u5ea6\u7684\u8868\u9762\u4f4d\u79fb\u4ea7\u54c1\uff0c\u4e0eGPS\u6d4b\u91cf\u7ed3\u679c\u5728\u6beb\u7c73\u7ea7\u522b\u4e00\u81f4\uff0c\u6210\u529f\u68c0\u6d4b\u5230\u57fa\u62c9\u97e6\u5384\u706b\u5c71\u55b7\u53d1\u671f\u95f4\u7c73\u7ea7\u4f4d\u79fb\u548c\u4e09\u59d0\u59b9\u706b\u5c71\u7684\u7ec6\u5fae\u62ac\u5347\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5728\u4e91\u73af\u5883\u4e2d\u5904\u7406\u5927\u578bInSAR\u6570\u636e\u96c6\u63d0\u4f9b\u4e86\u91cd\u8981\u8fdb\u5c55\uff0c\u6240\u6709\u8f6f\u4ef6\u5df2\u4f5c\u4e3a\u5f00\u6e90\u5e93\u63d0\u4f9b\u3002"}}
{"id": "2511.12073", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12073", "abs": "https://arxiv.org/abs/2511.12073", "authors": ["Woojae Jeong", "Wenhui Cui", "Kleanthis Avramidis", "Takfarinas Medani", "Shrikanth Narayanan", "Richard Leahy"], "title": "Informed Bootstrap Augmentation Improves EEG Decoding", "comment": null, "summary": "Electroencephalography (EEG) offers detailed access to neural dynamics but remains constrained by noise and trial-by-trial variability, limiting decoding performance in data-restricted or complex paradigms. Data augmentation is often employed to enhance feature representations, yet conventional uniform averaging overlooks differences in trial informativeness and can degrade representational quality. We introduce a weighted bootstrapping approach that prioritizes more reliable trials to generate higher-quality augmented samples. In a Sentence Evaluation paradigm, weights were computed from relative ERP differences and applied during probabilistic sampling and averaging. Across conditions, weighted bootstrapping improved decoding accuracy relative to unweighted (from 68.35% to 71.25% at best), demonstrating that emphasizing reliable trials strengthens representational quality. The results demonstrate that reliability-based augmentation yields more robust and discriminative EEG representations. The code is publicly available at https://github.com/lyricists/NeuroBootstrap.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53ef\u9760\u6027\u7684\u52a0\u6743\u81ea\u52a9\u6cd5\u6765\u589e\u5f3aEEG\u6570\u636e\u8868\u793a\uff0c\u901a\u8fc7\u4f18\u5148\u9009\u62e9\u4fe1\u606f\u91cf\u66f4\u5927\u7684\u8bd5\u9a8c\u6837\u672c\u6765\u63d0\u9ad8\u89e3\u7801\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u5747\u5300\u5e73\u5747\u65b9\u6cd5\u5ffd\u7565\u4e86\u8bd5\u9a8c\u6837\u672c\u7684\u4fe1\u606f\u91cf\u5dee\u5f02\uff0c\u53ef\u80fd\u964d\u4f4e\u8868\u793a\u8d28\u91cf\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u4f18\u5148\u8003\u8651\u53ef\u9760\u8bd5\u9a8c\u7684\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u52a0\u6743\u81ea\u52a9\u6cd5\uff0c\u57fa\u4e8e\u76f8\u5bf9ERP\u5dee\u5f02\u8ba1\u7b97\u6743\u91cd\uff0c\u5728\u6982\u7387\u91c7\u6837\u548c\u5e73\u5747\u8fc7\u7a0b\u4e2d\u4f18\u5148\u9009\u62e9\u66f4\u53ef\u9760\u7684\u8bd5\u9a8c\u6837\u672c\u3002", "result": "\u5728\u53e5\u5b50\u8bc4\u4f30\u8303\u5f0f\u4e2d\uff0c\u52a0\u6743\u81ea\u52a9\u6cd5\u5c06\u89e3\u7801\u51c6\u786e\u7387\u4ece68.35%\u63d0\u5347\u81f371.25%\uff0c\u8868\u660e\u5f3a\u8c03\u53ef\u9760\u8bd5\u9a8c\u80fd\u589e\u5f3a\u8868\u793a\u8d28\u91cf\u3002", "conclusion": "\u57fa\u4e8e\u53ef\u9760\u6027\u7684\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u80fd\u4ea7\u751f\u66f4\u9c81\u68d2\u548c\u66f4\u5177\u533a\u5206\u5ea6\u7684EEG\u8868\u793a\uff0c\u4ee3\u7801\u5df2\u516c\u5f00\u3002"}}
{"id": "2511.12102", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.12102", "abs": "https://arxiv.org/abs/2511.12102", "authors": ["Abhisha Garg", "Akash Kumar", "Suraj Srivastava", "Nimish Yadav", "Aditya K. Jagannatham", "Lajos Hanzo"], "title": "Bayesian Learning Aided Simultaneous Sparse Estimation of Dual-Wideband THz Channels in Multi-User Hybrid MIMO Systems", "comment": null, "summary": "This work conceives the Bayesian Group-Sparse Regression (BGSR) for the estimation of a spatial and frequency wideband, i.e., a dual wideband channel in Multi-User (MU) THz hybrid MIMO scenarios. We develop a practical dual wideband THz channel model that incorporates absorption losses, reflection losses, diffused ray modeling and angles of arrival/departure (AoAs/AoDs) using a Gaussian Mixture Model (GMM). Furthermore, a low-resolution analog-to-digital converter (ADC) is employed at each RF chain, which is crucial for wideband THz massive MIMO systems to reduce power consumption and hardware complexity, given the high sampling rates and large number of antennas involved. The quantized MU THz MIMO model is linearized using the popular Bussgang decomposition followed by BGSR based channel learning framework that results in sparsity across different subcarriers, where each subcarrier has its unique dictionary matrix. Next, the Bayesian Cram\u00e9r Rao Bound (BCRB) is devised for bounding the normalized mean square error (NMSE) performance. Extensive simulations were performed to assess the performance improvements achieved by the proposed BGSR method compared to other sparse estimation techniques. The metrics considered for quantifying the performance improvements include the NMSE and bit error rate (BER).", "AI": {"tldr": "\u63d0\u51fa\u8d1d\u53f6\u65af\u7fa4\u7a00\u758f\u56de\u5f52\u65b9\u6cd5\u7528\u4e8e\u592a\u8d6b\u5179\u6df7\u5408MIMO\u7cfb\u7edf\u4e2d\u7684\u53cc\u5bbd\u5e26\u4fe1\u9053\u4f30\u8ba1\uff0c\u7ed3\u5408\u4f4e\u5206\u8fa8\u7387ADC\u548c\u8d1d\u53f6\u65af\u514b\u62c9\u7f8e\u7f57\u754c\u5206\u6790\u3002", "motivation": "\u592a\u8d6b\u5179\u5927\u89c4\u6a21MIMO\u7cfb\u7edf\u9700\u8981\u89e3\u51b3\u9ad8\u91c7\u6837\u7387\u548c\u5927\u5929\u7ebf\u6570\u91cf\u5e26\u6765\u7684\u529f\u8017\u548c\u786c\u4ef6\u590d\u6742\u5ea6\u95ee\u9898\uff0c\u540c\u65f6\u9700\u8981\u51c6\u786e\u4f30\u8ba1\u7a7a\u95f4\u548c\u9891\u7387\u53cc\u5bbd\u5e26\u4fe1\u9053\u3002", "method": "\u5f00\u53d1\u5b9e\u7528\u7684\u53cc\u5bbd\u5e26\u592a\u8d6b\u5179\u4fe1\u9053\u6a21\u578b\uff0c\u91c7\u7528\u4f4e\u5206\u8fa8\u7387ADC\uff0c\u4f7f\u7528Bussgang\u5206\u89e3\u7ebf\u6027\u5316\u91cf\u5316\u6a21\u578b\uff0c\u63d0\u51fa\u8d1d\u53f6\u65af\u7fa4\u7a00\u758f\u56de\u5f52\u6846\u67b6\u8fdb\u884c\u4fe1\u9053\u5b66\u4e60\u3002", "result": "\u901a\u8fc7\u5e7f\u6cdb\u4eff\u771f\u9a8c\u8bc1\uff0c\u6240\u63d0\u51fa\u7684BGSR\u65b9\u6cd5\u5728\u5f52\u4e00\u5316\u5747\u65b9\u8bef\u5dee\u548c\u8bef\u7801\u7387\u65b9\u9762\u76f8\u6bd4\u5176\u4ed6\u7a00\u758f\u4f30\u8ba1\u6280\u672f\u6709\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "BGSR\u65b9\u6cd5\u80fd\u6709\u6548\u89e3\u51b3\u592a\u8d6b\u5179\u5927\u89c4\u6a21MIMO\u7cfb\u7edf\u4e2d\u7684\u53cc\u5bbd\u5e26\u4fe1\u9053\u4f30\u8ba1\u95ee\u9898\uff0c\u4e3a\u4f4e\u529f\u8017\u9ad8\u7cbe\u5ea6\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2511.12137", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.12137", "abs": "https://arxiv.org/abs/2511.12137", "authors": ["Zheng Wang", "Yifu Li", "Yuchao Mei", "Xinyu Sui", "Qingbin Li", "Xu Luo", "Rui Wang", "Dongxin Ni", "Jian Pang"], "title": "A 24-GHz CMOS Transformer-Based Three-Tline Series Doherty Power Amplifier Achieving 39% PAE", "comment": "This paper has been accepted by ICTA2025", "summary": "This paper presents a transformer-based three- transmission-line (Tline) series Doherty power amplifier (PA) implemented in 65-nm CMOS, targeting broadband K/Ka-band applications. By integrating an impedance-scaling network into the output matching structure, the design enables effective load modulation and reduced impedance transformation ratio (ITR) at power back-off when employing stacked cascode transistors. The PA demonstrates a -3-dB small-signal gain bandwidth from 22 to 32.5 GHz, a saturated output power (Psat) of 21.6 dBm, and a peak power-added efficiency (PAE) of 39%. At 6dB back-off, the PAE remains above 24%, validating its suitability for high- efficiency mm-wave phased-array transmitters in next-generation wireless systems.", "AI": {"tldr": "\u57fa\u4e8e\u53d8\u538b\u5668\u7684\u4e09\u4f20\u8f93\u7ebf\u7ea7\u8054Doherty\u529f\u7387\u653e\u5927\u5668\uff0c\u572865nm CMOS\u5de5\u827a\u4e2d\u5b9e\u73b0\uff0c\u9488\u5bf9K/Ka\u6ce2\u6bb5\u5bbd\u5e26\u5e94\u7528\uff0c\u901a\u8fc7\u963b\u6297\u7f29\u653e\u7f51\u7edc\u5b9e\u73b0\u6709\u6548\u8d1f\u8f7d\u8c03\u5236\u548c\u964d\u4f4e\u963b\u6297\u53d8\u6362\u6bd4\u3002", "motivation": "\u9488\u5bf9\u4e0b\u4e00\u4ee3\u65e0\u7ebf\u7cfb\u7edf\u4e2d\u6beb\u7c73\u6ce2\u76f8\u63a7\u9635\u53d1\u5c04\u673a\u7684\u9ad8\u6548\u7387\u9700\u6c42\uff0c\u5f00\u53d1\u5bbd\u5e26K/Ka\u6ce2\u6bb5Doherty\u529f\u7387\u653e\u5927\u5668\u3002", "method": "\u5728\u8f93\u51fa\u5339\u914d\u7ed3\u6784\u4e2d\u96c6\u6210\u963b\u6297\u7f29\u653e\u7f51\u7edc\uff0c\u91c7\u7528\u5806\u53e0\u5171\u6e90\u5171\u6805\u6676\u4f53\u7ba1\uff0c\u5b9e\u73b0\u6709\u6548\u8d1f\u8f7d\u8c03\u5236\u548c\u964d\u4f4e\u529f\u7387\u56de\u9000\u65f6\u7684\u963b\u6297\u53d8\u6362\u6bd4\u3002", "result": "-3dB\u5c0f\u4fe1\u53f7\u589e\u76ca\u5e26\u5bbd22-32.5GHz\uff0c\u9971\u548c\u8f93\u51fa\u529f\u738721.6dBm\uff0c\u5cf0\u503c\u529f\u7387\u9644\u52a0\u6548\u738739%\uff0c6dB\u56de\u9000\u65f6\u6548\u7387\u4ecd\u9ad8\u4e8e24%\u3002", "conclusion": "\u8be5\u8bbe\u8ba1\u9a8c\u8bc1\u4e86\u5176\u5728\u4e0b\u4e00\u4ee3\u65e0\u7ebf\u7cfb\u7edf\u4e2d\u6beb\u7c73\u6ce2\u76f8\u63a7\u9635\u53d1\u5c04\u673a\u7684\u9ad8\u6548\u7387\u9002\u7528\u6027\u3002"}}
{"id": "2511.12297", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.12297", "abs": "https://arxiv.org/abs/2511.12297", "authors": ["Angqi Liu", "Filippo Moro", "Sebastian Billaudelle", "Melika Payvand"], "title": "A Linear Implementation of an Analog Resonate-and-Fire Neuron", "comment": null, "summary": "Oscillatory dynamics have recently proven highly effective in machine learning (ML), particularly through State-Space-Models (SSM) that leverage structured linear recurrences for long-range temporal processing. Resonate-and-Fire neurons capture such oscillatory behavior in a spiking framework, offering strong expressivity with sparse event-based communication. While early analog RAF circuits employed nonlinear coupling and suffered from process sensitivity, modern ML practice favors linear recurrence. In this work, we introduce a resonate-and-fire (RAF) neuron, built in 22nm Fully-Depleted Silicon-on-Insulator technology, that aligns with SSM principles while retaining the efficiency of spike-based communication. We analyze its dynamics, linearity, and resilience to Process, Voltage, and Temperature variations, and evaluate its power, performance, and area trade-offs. We map the characteristics of our circuit into a system-level simulation where our RAF neuron is utilized in a keyword-spotting task, showing that its non-idealities do not hinder performance. Our results establish RAF neurons as robust, energy-efficient computational primitives for neuromorphic hardware.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e22nm FD-SOI\u6280\u672f\u7684\u5171\u632f-\u53d1\u653e\u795e\u7ecf\u5143\uff0c\u8be5\u795e\u7ecf\u5143\u4e0e\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u539f\u7406\u4e00\u81f4\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u57fa\u4e8e\u8109\u51b2\u901a\u4fe1\u7684\u6548\u7387\uff0c\u5728\u5173\u952e\u8bcd\u68c0\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u826f\u597d\u7684\u6027\u80fd\u548c\u80fd\u6548\u3002", "motivation": "\u632f\u8361\u52a8\u529b\u5b66\u5728\u673a\u5668\u5b66\u4e60\u4e2d\u5f88\u6709\u6548\uff0c\u7279\u522b\u662f\u901a\u8fc7\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u8fdb\u884c\u957f\u7a0b\u65f6\u95f4\u5904\u7406\u3002\u5171\u632f-\u53d1\u653e\u795e\u7ecf\u5143\u5728\u8109\u51b2\u6846\u67b6\u4e2d\u6355\u83b7\u8fd9\u79cd\u632f\u8361\u884c\u4e3a\uff0c\u63d0\u4f9b\u5f3a\u8868\u8fbe\u80fd\u529b\u548c\u7a00\u758f\u4e8b\u4ef6\u901a\u4fe1\u3002", "method": "\u572822nm\u5168\u8017\u5c3d\u7edd\u7f18\u4f53\u4e0a\u7845\u6280\u672f\u4e2d\u6784\u5efa\u5171\u632f-\u53d1\u653e\u795e\u7ecf\u5143\uff0c\u5206\u6790\u5176\u52a8\u529b\u5b66\u3001\u7ebf\u6027\u5ea6\u4ee5\u53ca\u5bf9\u5de5\u827a\u3001\u7535\u538b\u548c\u6e29\u5ea6\u53d8\u5316\u7684\u9c81\u68d2\u6027\uff0c\u8bc4\u4f30\u529f\u7387\u3001\u6027\u80fd\u548c\u9762\u79ef\u6743\u8861\u3002", "result": "\u7535\u8def\u7684\u975e\u7406\u60f3\u6027\u4e0d\u4f1a\u5f71\u54cd\u6027\u80fd\uff0c\u5728\u7cfb\u7edf\u7ea7\u6a21\u62df\u4e2d\u7528\u4e8e\u5173\u952e\u8bcd\u68c0\u6d4b\u4efb\u52a1\u8868\u73b0\u826f\u597d\u3002\u5171\u632f-\u53d1\u653e\u795e\u7ecf\u5143\u88ab\u8bc1\u660e\u662f\u795e\u7ecf\u5f62\u6001\u786c\u4ef6\u7684\u7a33\u5065\u3001\u9ad8\u80fd\u6548\u8ba1\u7b97\u539f\u8bed\u3002", "conclusion": "\u5171\u632f-\u53d1\u653e\u795e\u7ecf\u5143\u662f\u7a33\u5065\u3001\u80fd\u91cf\u9ad8\u6548\u7684\u8ba1\u7b97\u539f\u8bed\uff0c\u9002\u5408\u795e\u7ecf\u5f62\u6001\u786c\u4ef6\u5e94\u7528\u3002"}}
{"id": "2511.12308", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.12308", "abs": "https://arxiv.org/abs/2511.12308", "authors": ["Jiajun Zhu", "Yanqun Tang", "Cong Yi", "Haoran Yin", "Yuanhan Ni", "Fan Liu", "Zhiqiang Wei", "Huseyin Arslan"], "title": "ISAC with Affine Frequency Division Multiplexing: An FMCW-Based Signal Processing Perspective", "comment": "Submitted to IEEE for possible publication", "summary": "This paper investigates the sensing potential of affine frequency division multiplexing (AFDM) in high-mobility integrated sensing and communication (ISAC) from the perspective of radar waveforms. We introduce an innovative parameter selection criterion that establishes a precise mathematical equivalence between AFDM subcarriers and Nyquist-sampled frequency-modulated continuous-wave (FMCW). This connection not only provides a clear physical insight into AFDM's sensing mechanism but also enables a direct mapping from the DAFT index to delay-Doppler (DD) parameters of wireless channels. Building on this, we develop a novel input-output model in a DD-parameterized DAFT (DD-DAFT) domain for AFDM, which explicitly reveals the inherent DD coupling effect arising from the chirp-channel interaction. Subsequently, we design two matched-filtering sensing algorithms. The first is performed in the time-frequency domain with low complexity, while the second is operated in the DD-DAFT domain to precisely resolve the DD coupling. Simulations show that our algorithms achieve effective pilot-free sensing and demonstrate a fundamental trade-off between sensing performance, communication overhead, and computational complexity. The proposed AFDM outperforms classical AFDM and other variants in most scenarios.", "AI": {"tldr": "\u672c\u6587\u4ece\u96f7\u8fbe\u6ce2\u5f62\u89d2\u5ea6\u7814\u7a76\u4e86AFDM\u5728\u9ad8\u79fb\u52a8\u6027ISAC\u4e2d\u7684\u611f\u77e5\u6f5c\u529b\uff0c\u63d0\u51fa\u4e86\u53c2\u6570\u9009\u62e9\u51c6\u5219\u5c06AFDM\u5b50\u8f7d\u6ce2\u4e0eFMCW\u7b49\u4ef7\uff0c\u5f00\u53d1\u4e86DD-DAFT\u57df\u8f93\u5165\u8f93\u51fa\u6a21\u578b\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e24\u79cd\u5339\u914d\u6ee4\u6ce2\u611f\u77e5\u7b97\u6cd5\u3002", "motivation": "\u7814\u7a76AFDM\u5728\u9ad8\u901f\u79fb\u52a8\u96c6\u6210\u611f\u77e5\u4e0e\u901a\u4fe1\u4e2d\u7684\u611f\u77e5\u80fd\u529b\uff0c\u63a2\u7d22\u5176\u4f5c\u4e3a\u96f7\u8fbe\u6ce2\u5f62\u7684\u6f5c\u529b\uff0c\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u5728\u9ad8\u901f\u79fb\u52a8\u573a\u666f\u4e0b\u7684\u6027\u80fd\u9650\u5236\u3002", "method": "\u63d0\u51fa\u53c2\u6570\u9009\u62e9\u51c6\u5219\u5efa\u7acbAFDM\u5b50\u8f7d\u6ce2\u4e0eFMCW\u7684\u6570\u5b66\u7b49\u4ef7\u6027\uff0c\u5f00\u53d1DD-DAFT\u57df\u8f93\u5165\u8f93\u51fa\u6a21\u578b\u63ed\u793a\u5541\u557e-\u4fe1\u9053\u4ea4\u4e92\u4ea7\u751f\u7684DD\u8026\u5408\u6548\u5e94\uff0c\u8bbe\u8ba1\u4e24\u79cd\u5339\u914d\u6ee4\u6ce2\u611f\u77e5\u7b97\u6cd5\uff08\u65f6\u9891\u57df\u548cDD-DAFT\u57df\uff09\u3002", "result": "\u4eff\u771f\u663e\u793a\u7b97\u6cd5\u5b9e\u73b0\u4e86\u6709\u6548\u7684\u65e0\u5bfc\u9891\u611f\u77e5\uff0c\u5728\u5927\u591a\u6570\u573a\u666f\u4e0b\u4f18\u4e8e\u7ecf\u5178AFDM\u548c\u5176\u4ed6\u53d8\u4f53\uff0c\u63ed\u793a\u4e86\u611f\u77e5\u6027\u80fd\u3001\u901a\u4fe1\u5f00\u9500\u548c\u8ba1\u7b97\u590d\u6742\u5ea6\u4e4b\u95f4\u7684\u57fa\u672c\u6743\u8861\u5173\u7cfb\u3002", "conclusion": "AFDM\u5728\u9ad8\u79fb\u52a8\u6027ISAC\u4e2d\u5177\u6709\u4f18\u79c0\u7684\u611f\u77e5\u6f5c\u529b\uff0c\u63d0\u51fa\u7684\u65b9\u6cd5\u4e3aAFDM\u611f\u77e5\u63d0\u4f9b\u4e86\u6e05\u6670\u7684\u7269\u7406\u89e3\u91ca\u548c\u6709\u6548\u7684\u5b9e\u73b0\u65b9\u6848\u3002"}}
{"id": "2511.12348", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.12348", "abs": "https://arxiv.org/abs/2511.12348", "authors": ["Mostafa Nozari", "Israel Leyva-Mayorga", "Fabio Saggese", "Gilberto Berardinelli"], "title": "Toward ISAC-empowered subnetworks: Cooperative localization and iterative node selection", "comment": null, "summary": "This paper tackles the sensing-communication trade-off in integrated sensing and communication (ISAC)-empowered subnetworks for mono-static target localization. We propose a low-complexity iterative node selection algorithm that exploits the spatial diversity of subnetwork deployments and dynamically refines the set of sensing subnetworks to maximize localization accuracy under tight resource constraints. Simulation results show that our method achieves sub-7 cm accuracy in additive white Gaussian noise (AWGN) channels within only three iterations, yielding over 97% improvement compared to the best-performing benchmark under the same sensing budget. We further demonstrate that increasing spatial diversity through additional antennas and subnetworks enhances sensing robustness, especially in fading channels. Finally, we quantify the sensing-communication trade-off, showing that reducing sensing iterations and the number of sensing subnetworks improves throughput at the cost of reduced localization precision.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u4f4e\u590d\u6742\u5ea6\u8fed\u4ee3\u8282\u70b9\u9009\u62e9\u7b97\u6cd5\uff0c\u7528\u4e8eISAC\u5b50\u7f51\u7edc\u4e2d\u7684\u5355\u7ad9\u76ee\u6807\u5b9a\u4f4d\uff0c\u5728\u8d44\u6e90\u53d7\u9650\u6761\u4ef6\u4e0b\u6700\u5927\u5316\u5b9a\u4f4d\u7cbe\u5ea6\uff0c\u5728AWGN\u4fe1\u9053\u4e2d\u5b9e\u73b0\u4e9a7\u5398\u7c73\u7cbe\u5ea6", "motivation": "\u89e3\u51b3\u96c6\u6210\u611f\u77e5\u4e0e\u901a\u4fe1(ISAC)\u8d4b\u80fd\u5b50\u7f51\u7edc\u4e2d\u611f\u77e5\u4e0e\u901a\u4fe1\u7684\u6743\u8861\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u5355\u7ad9\u76ee\u6807\u5b9a\u4f4d\u573a\u666f\u4e0b\uff0c\u5982\u4f55\u5728\u4e25\u683c\u8d44\u6e90\u7ea6\u675f\u4e0b\u4f18\u5316\u5b9a\u4f4d\u6027\u80fd", "method": "\u5229\u7528\u5b50\u7f51\u7edc\u90e8\u7f72\u7684\u7a7a\u95f4\u591a\u6837\u6027\uff0c\u63d0\u51fa\u4f4e\u590d\u6742\u5ea6\u8fed\u4ee3\u8282\u70b9\u9009\u62e9\u7b97\u6cd5\uff0c\u52a8\u6001\u4f18\u5316\u611f\u77e5\u5b50\u7f51\u7edc\u96c6\u5408", "result": "\u5728AWGN\u4fe1\u9053\u4e2d\u4ec5\u97003\u6b21\u8fed\u4ee3\u5373\u53ef\u5b9e\u73b0\u4e9a7\u5398\u7c73\u5b9a\u4f4d\u7cbe\u5ea6\uff0c\u76f8\u6bd4\u6700\u4f73\u57fa\u51c6\u65b9\u6cd5\u5728\u76f8\u540c\u611f\u77e5\u9884\u7b97\u4e0b\u63d0\u5347\u8d85\u8fc797%\uff1b\u589e\u52a0\u7a7a\u95f4\u591a\u6837\u6027\uff08\u66f4\u591a\u5929\u7ebf\u548c\u5b50\u7f51\u7edc\uff09\u53ef\u589e\u5f3a\u611f\u77e5\u9c81\u68d2\u6027\uff0c\u7279\u522b\u662f\u5728\u8870\u843d\u4fe1\u9053\u4e2d", "conclusion": "\u91cf\u5316\u4e86\u611f\u77e5-\u901a\u4fe1\u6743\u8861\uff1a\u51cf\u5c11\u611f\u77e5\u8fed\u4ee3\u6b21\u6570\u548c\u611f\u77e5\u5b50\u7f51\u7edc\u6570\u91cf\u53ef\u63d0\u9ad8\u541e\u5410\u91cf\uff0c\u4f46\u4f1a\u964d\u4f4e\u5b9a\u4f4d\u7cbe\u5ea6"}}
{"id": "2511.12470", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.12470", "abs": "https://arxiv.org/abs/2511.12470", "authors": ["Zijun Wang", "Anjali Omer", "Jacob Chakareski", "Nicholas Mastronarde", "Rui Zhang"], "title": "Cross-Layer Design for Near-Field mmWave Beam Management and Scheduling under Delay-Sensitive Traffic", "comment": "Workshop paper submitted to the AI4NextG Workshop at NeurIPS 2025", "summary": "Next-generation wireless networks will rely on mmWave/sub-THz spectrum and extremely large antenna arrays (ELAAs). This will push their operation into the near field where far-field beam management degrades and beam training becomes more costly and must be done more frequently. Because ELAA training and data transmission consume energy and training trades off with service time, we pose a cross-layer control problem that couples PHY-layer beam management with MAC-layer service under delay-sensitive traffic. The controller decides when to retrain and how aggressively to train (pilot count and sparsity) while allocating transmit power, explicitly balancing pilot overhead, data-phase rate, and energy to reduce the queueing delay of MAC-layer frames/packets to be transmitted. We model the problem as a partially observable Markov decision process and solve it with deep reinforcement learning. In simulations with a realistic near-field channel and varying mobility and traffic load, the learned policy outperforms strong 5G-NR--style baselines at a comparable energy: it achieves 85.5% higher throughput than DFT sweeping and reduces the overflow rate by 78%. These results indicate a practical path to overhead-aware, traffic-adaptive near-field beam management with implications for emerging low-latency, high-rate next-generation applications such as digital twin, spatial computing, and immersive communication.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8de8\u5c42\u63a7\u5236\u65b9\u6cd5\uff0c\u5c06\u7269\u7406\u5c42\u6ce2\u675f\u7ba1\u7406\u4e0eMAC\u5c42\u670d\u52a1\u76f8\u7ed3\u5408\uff0c\u901a\u8fc7\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u8fd1\u573a\u6ce2\u675f\u8bad\u7ec3\u7b56\u7565\uff0c\u5728\u4fdd\u8bc1\u670d\u52a1\u8d28\u91cf\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u80fd\u8017\u548c\u5ef6\u8fdf\u3002", "motivation": "\u4e0b\u4e00\u4ee3\u65e0\u7ebf\u7f51\u7edc\u4f7f\u7528\u6beb\u7c73\u6ce2/\u592a\u8d6b\u5179\u9891\u8c31\u548c\u6781\u5927\u89c4\u6a21\u5929\u7ebf\u9635\u5217\uff0c\u4f7f\u7cfb\u7edf\u5de5\u4f5c\u5728\u8fd1\u573a\u533a\u57df\uff0c\u4f20\u7edf\u8fdc\u573a\u6ce2\u675f\u7ba1\u7406\u6027\u80fd\u4e0b\u964d\uff0c\u6ce2\u675f\u8bad\u7ec3\u6210\u672c\u589e\u52a0\u4e14\u9700\u8981\u66f4\u9891\u7e41\u8fdb\u884c\u3002\u6ce2\u675f\u8bad\u7ec3\u4e0e\u6570\u636e\u4f20\u8f93\u5b58\u5728\u80fd\u8017\u548c\u65f6\u9699\u5206\u914d\u7684\u6743\u8861\u3002", "method": "\u5c06\u95ee\u9898\u5efa\u6a21\u4e3a\u90e8\u5206\u53ef\u89c2\u6d4b\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff0c\u4f7f\u7528\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u6c42\u89e3\u3002\u63a7\u5236\u5668\u51b3\u7b56\u4f55\u65f6\u91cd\u65b0\u8bad\u7ec3\u3001\u8bad\u7ec3\u5f3a\u5ea6\uff08\u5bfc\u9891\u6570\u91cf\u548c\u7a00\u758f\u5ea6\uff09\uff0c\u540c\u65f6\u5206\u914d\u53d1\u5c04\u529f\u7387\uff0c\u5e73\u8861\u5bfc\u9891\u5f00\u9500\u3001\u6570\u636e\u901f\u7387\u548c\u80fd\u8017\u3002", "result": "\u5728\u771f\u5b9e\u8fd1\u573a\u4fe1\u9053\u548c\u4e0d\u540c\u79fb\u52a8\u6027\u3001\u6d41\u91cf\u8d1f\u8f7d\u7684\u4eff\u771f\u4e2d\uff0c\u5b66\u4e60\u5230\u7684\u7b56\u7565\u5728\u53ef\u6bd4\u80fd\u8017\u4e0b\u4f18\u4e8e5G-NR\u57fa\u51c6\u65b9\u6cd5\uff1a\u541e\u5410\u91cf\u6bd4DFT\u626b\u63cf\u63d0\u9ad885.5%\uff0c\u6ea2\u51fa\u7387\u964d\u4f4e78%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5f00\u9500\u611f\u77e5\u3001\u6d41\u91cf\u81ea\u9002\u5e94\u7684\u8fd1\u573a\u6ce2\u675f\u7ba1\u7406\u63d0\u4f9b\u4e86\u5b9e\u7528\u8def\u5f84\uff0c\u5bf9\u6570\u5b57\u5b6a\u751f\u3001\u7a7a\u95f4\u8ba1\u7b97\u548c\u6c89\u6d78\u5f0f\u901a\u4fe1\u7b49\u65b0\u5174\u4f4e\u5ef6\u8fdf\u3001\u9ad8\u901f\u7387\u5e94\u7528\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2511.12478", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.12478", "abs": "https://arxiv.org/abs/2511.12478", "authors": ["Mahdi Pirayesh Shirazi Nejad", "David Hicks", "Matt Valentine", "Ki H. Chon"], "title": "Lightweight Deep Autoencoder for ECG Denoising with Morphology Preservation and Near Real-Time Hardware Deployment", "comment": null, "summary": "Electrocardiogram (ECG) signals are often degraded by various noise sources such as baseline wander, motion artifacts, and electromyographic interference, posing a major challenge in clinical settings. This paper presents a lightweight deep learning-based denoising framework, forming a compact autoencoder architecture. The model was trained under severe noise conditions (-5 dB signal-to-noise ratio (SNR)) using a rigorously partitioned dataset to ensure no data leakage and robust generalization. Extensive evaluations were conducted across seven noise configurations and three SNR levels (-5 dB, 0 dB, and +5 dB), showing consistent denoising performance with minimal morphological distortion, critical for maintaining diagnostic integrity. In particular, tests on clinically vital rhythms such as ventricular tachycardia (VT) and ventricular fibrillation (VF) confirm that the proposed model effectively suppresses noise without altering arrhythmic features essential for diagnosis. Visual and quantitative assessments, including SNR improvement, RMSE, and correlation metrics, validate the model's efficacy in preserving waveform fidelity. To demonstrate real-world applicability, the model was deployed on a Raspberry Pi 4 using TensorFlow Lite with float16 precision. Inference latency was measured at just 1.41 seconds per 14-second ECG segment, indicating feasibility for near-real-time use in edge devices. Overall, this study introduces a lightweight, hardware-validated, and morphologically reliable ECG denoising solution suitable for integration into portable or wearable healthcare systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u6df1\u5ea6\u5b66\u4e60\u53bb\u566a\u6846\u67b6\uff0c\u91c7\u7528\u7d27\u51d1\u81ea\u7f16\u7801\u5668\u67b6\u6784\uff0c\u5728-5 dB\u5f3a\u566a\u58f0\u6761\u4ef6\u4e0b\u8bad\u7ec3\uff0c\u5728\u591a\u79cd\u566a\u58f0\u914d\u7f6e\u548cSNR\u6c34\u5e73\u4e0b\u5747\u8868\u73b0\u7a33\u5b9a\uff0c\u80fd\u6709\u6548\u6291\u5236\u566a\u58f0\u540c\u65f6\u4fdd\u6301\u5fc3\u7535\u56fe\u5f62\u6001\u5b8c\u6574\u6027\uff0c\u5e76\u5728\u6811\u8393\u6d3e4\u4e0a\u9a8c\u8bc1\u4e86\u8fb9\u7f18\u8bbe\u5907\u90e8\u7f72\u53ef\u884c\u6027\u3002", "motivation": "\u5fc3\u7535\u56fe\u4fe1\u53f7\u5e38\u53d7\u57fa\u7ebf\u6f02\u79fb\u3001\u8fd0\u52a8\u4f2a\u5f71\u548c\u808c\u7535\u5e72\u6270\u7b49\u591a\u79cd\u566a\u58f0\u5f71\u54cd\uff0c\u8fd9\u5728\u4e34\u5e8a\u73af\u5883\u4e2d\u6784\u6210\u91cd\u5927\u6311\u6218\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u6709\u6548\u53bb\u566a\u540c\u65f6\u4fdd\u6301\u8bca\u65ad\u5173\u952e\u5f62\u6001\u7279\u5f81\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u6784\u5efa\u8f7b\u91cf\u7ea7\u6df1\u5ea6\u5b66\u4e60\u53bb\u566a\u6846\u67b6\uff0c\u91c7\u7528\u7d27\u51d1\u81ea\u7f16\u7801\u5668\u67b6\u6784\uff0c\u5728-5 dB\u5f3a\u566a\u58f0\u6761\u4ef6\u4e0b\u8bad\u7ec3\uff0c\u4f7f\u7528\u4e25\u683c\u5212\u5206\u7684\u6570\u636e\u96c6\u907f\u514d\u6570\u636e\u6cc4\u9732\uff0c\u786e\u4fdd\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u5728\u4e03\u79cd\u566a\u58f0\u914d\u7f6e\u548c\u4e09\u4e2aSNR\u6c34\u5e73\uff08-5 dB\u30010 dB\u3001+5 dB\uff09\u4e0b\u5747\u8868\u73b0\u4e00\u81f4\u7684\u53bb\u566a\u6027\u80fd\uff0c\u5f62\u6001\u5931\u771f\u6700\u5c0f\uff1b\u5bf9\u5fc3\u5ba4\u5fc3\u52a8\u8fc7\u901f\u548c\u5fc3\u5ba4\u98a4\u52a8\u7b49\u5173\u952e\u5fc3\u5f8b\u6d4b\u8bd5\u8bc1\u5b9e\u80fd\u6709\u6548\u6291\u5236\u566a\u58f0\u800c\u4e0d\u6539\u53d8\u5fc3\u5f8b\u5931\u5e38\u7279\u5f81\uff1b\u6811\u8393\u6d3e4\u4e0a\u90e8\u7f72\u663e\u793a\u6bcf14\u79d2ECG\u6bb5\u63a8\u7406\u5ef6\u8fdf\u4ec51.41\u79d2\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u786c\u4ef6\u9a8c\u8bc1\u4e14\u5f62\u6001\u53ef\u9760\u7684ECG\u53bb\u566a\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u5408\u96c6\u6210\u5230\u4fbf\u643a\u6216\u53ef\u7a7f\u6234\u533b\u7597\u7cfb\u7edf\u4e2d\uff0c\u5177\u6709\u8fd1\u5b9e\u65f6\u8fb9\u7f18\u8bbe\u5907\u5e94\u7528\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2511.12508", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.12508", "abs": "https://arxiv.org/abs/2511.12508", "authors": ["Yanhao Wang", "Lei Wang", "Jie Wang", "Yimin Liu"], "title": "Robust Radar HRRP Recognition under Non-uniform Jamming Based on Complex-valued Frequency Attention Network", "comment": null, "summary": "Complex electromagnetic environments, often containing multiple jammers with different jamming patterns, produce non-uniform jamming power across the frequency spectrum. This spectral non-uniformity directly induces severe distortion in the target's HRRP, consequently compromising the performance and reliability of conventional HRRP-based target recognition methods. This paper proposes a novel, end-to-end trained network for robust radar target recognition. The core of our model is a CFA module that operates directly on the complex spectrum of the received echo. The CFA module learns to generate an adaptive frequency-domain filter, assigning lower weights to bands corrupted by strong jamming while preserving critical target information in cleaner bands. The filtered spectrum is then fed into a classifier backbone for recognition. Experimental results on simulated HRRP data with various jamming combinations demonstrate our method's superiority. Notably, under severe jamming conditions, our model achieves a recognition accuracy nearly 9% higher than traditional model-based approaches, all while introducing negligible computational overhead. This highlights its exceptional performance and robustness in challenging jamming environments.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7aef\u5230\u7aef\u8bad\u7ec3\u7684\u96f7\u8fbe\u76ee\u6807\u8bc6\u522b\u7f51\u7edc\uff0c\u901a\u8fc7CFA\u6a21\u5757\u5728\u590d\u6570\u9891\u8c31\u4e0a\u751f\u6210\u81ea\u9002\u5e94\u6ee4\u6ce2\u5668\uff0c\u6291\u5236\u5f3a\u5e72\u6270\u9891\u5e26\u5e76\u4fdd\u7559\u5e72\u51c0\u9891\u5e26\u7684\u76ee\u6807\u4fe1\u606f\uff0c\u5728\u4e25\u91cd\u5e72\u6270\u73af\u5883\u4e0b\u6bd4\u4f20\u7edf\u65b9\u6cd5\u8bc6\u522b\u51c6\u786e\u7387\u63d0\u9ad8\u8fd19%\u3002", "motivation": "\u590d\u6742\u7535\u78c1\u73af\u5883\u4e2d\u591a\u5e72\u6270\u6e90\u5bfc\u81f4\u9891\u8c31\u529f\u7387\u4e0d\u5747\u5300\uff0c\u4e25\u91cd\u626d\u66f2\u76ee\u6807\u9ad8\u5206\u8fa8\u7387\u8ddd\u79bb\u50cf(HRRP)\uff0c\u5f71\u54cd\u4f20\u7edfHRRP\u76ee\u6807\u8bc6\u522b\u65b9\u6cd5\u7684\u6027\u80fd\u548c\u53ef\u9760\u6027\u3002", "method": "\u5f00\u53d1CFA\u6a21\u5757\u76f4\u63a5\u5728\u63a5\u6536\u56de\u6ce2\u7684\u590d\u6570\u9891\u8c31\u4e0a\u64cd\u4f5c\uff0c\u5b66\u4e60\u751f\u6210\u81ea\u9002\u5e94\u9891\u57df\u6ee4\u6ce2\u5668\uff0c\u5bf9\u5f3a\u5e72\u6270\u9891\u5e26\u8d4b\u4e88\u8f83\u4f4e\u6743\u91cd\uff0c\u540c\u65f6\u4fdd\u7559\u5e72\u51c0\u9891\u5e26\u4e2d\u7684\u5173\u952e\u76ee\u6807\u4fe1\u606f\uff0c\u7136\u540e\u5c06\u6ee4\u6ce2\u540e\u7684\u9891\u8c31\u8f93\u5165\u5206\u7c7b\u5668\u8fdb\u884c\u8bc6\u522b\u3002", "result": "\u5728\u6a21\u62dfHRRP\u6570\u636e\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4e25\u91cd\u5e72\u6270\u6761\u4ef6\u4e0b\u6bd4\u4f20\u7edf\u57fa\u4e8e\u6a21\u578b\u7684\u65b9\u6cd5\u8bc6\u522b\u51c6\u786e\u7387\u63d0\u9ad8\u8fd19%\uff0c\u4e14\u8ba1\u7b97\u5f00\u9500\u53ef\u5ffd\u7565\u4e0d\u8ba1\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u6311\u6218\u6027\u5e72\u6270\u73af\u5883\u4e2d\u5c55\u73b0\u51fa\u5353\u8d8a\u7684\u6027\u80fd\u548c\u9c81\u68d2\u6027\uff0c\u4e3a\u89e3\u51b3\u590d\u6742\u7535\u78c1\u73af\u5883\u4e0b\u7684\u96f7\u8fbe\u76ee\u6807\u8bc6\u522b\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u3002"}}
{"id": "2511.12540", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.12540", "abs": "https://arxiv.org/abs/2511.12540", "authors": ["Dimitris Antoniadis", "Timothy G. Constandinou"], "title": "A mixed-signal analogue front-end for brain-implantable neural interfaces using a digital fixed-point IIR filter and bulk offset cancellation", "comment": "4 pages plus 1 references IEEE conference style", "summary": "Advances in miniaturised implantable neural electronics have paved the way for therapeutic brain-computer interfaces with clinical potential for movement disorders, epilepsy, and broader neurological applications. This paper presents a mixed-signal analogue front end (AFE) designed to record both extracellular action potentials (EAPs) and local field potentials (LFPs). The feedforward path integrates a low-noise amplifier (LNA) and a successive-approximation-register (SAR) analogue-to-digital converter (ADC), while the feedback path employs a fixed-point infinite-impulse-response (IIR) Chebyshev Type II low-pass filter to suppress sub-mHz components via bulk-voltage control of the LNA input differential pair using two R-2R pseudo-resistor digital-to-analogue converters (DACs). The proposed AFE achieves up to 41.42dB gain, consumes 2.178uA per channel, occupies 0.198mm2 per channel, and supports neural signal monitoring from 0.1Hz to 10kHz with 3.59uVrms input-referred integrated noise.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u8bb0\u5f55\u7ec6\u80de\u5916\u52a8\u4f5c\u7535\u4f4d\u548c\u5c40\u90e8\u573a\u7535\u4f4d\u7684\u6df7\u5408\u4fe1\u53f7\u6a21\u62df\u524d\u7aef\uff0c\u96c6\u6210\u4e86\u4f4e\u566a\u58f0\u653e\u5927\u5668\u548cSAR ADC\uff0c\u901a\u8fc7IIR\u6ee4\u6ce2\u5668\u6291\u5236\u4e9a\u6beb\u8d6b\u5179\u5206\u91cf\uff0c\u5b9e\u73b0\u4e86\u9ad8\u589e\u76ca\u3001\u4f4e\u529f\u8017\u7684\u795e\u7ecf\u4fe1\u53f7\u76d1\u6d4b\u3002", "motivation": "\u968f\u7740\u5fae\u578b\u690d\u5165\u5f0f\u795e\u7ecf\u7535\u5b50\u5b66\u7684\u53d1\u5c55\uff0c\u9700\u8981\u8bbe\u8ba1\u80fd\u591f\u540c\u65f6\u8bb0\u5f55EAPs\u548cLFPs\u7684\u9ad8\u6027\u80fd\u6a21\u62df\u524d\u7aef\uff0c\u4ee5\u652f\u6301\u6cbb\u7597\u6027\u8111\u673a\u63a5\u53e3\u5728\u8fd0\u52a8\u969c\u788d\u3001\u766b\u75eb\u7b49\u795e\u7ecf\u7cfb\u7edf\u75be\u75c5\u4e2d\u7684\u4e34\u5e8a\u5e94\u7528\u3002", "method": "\u91c7\u7528\u6df7\u5408\u4fe1\u53f7\u8bbe\u8ba1\uff0c\u524d\u5411\u8def\u5f84\u96c6\u6210LNA\u548cSAR ADC\uff0c\u53cd\u9988\u8def\u5f84\u4f7f\u7528\u5b9a\u70b9IIR\u5207\u6bd4\u96ea\u592bII\u578b\u4f4e\u901a\u6ee4\u6ce2\u5668\uff0c\u901a\u8fc7R-2R\u4f2a\u7535\u963bDACs\u8fdb\u884cLNA\u8f93\u5165\u5dee\u5206\u5bf9\u7684\u4f53\u7535\u538b\u63a7\u5236\uff0c\u6291\u5236\u4e9a\u6beb\u8d6b\u5179\u5206\u91cf\u3002", "result": "AFE\u5b9e\u73b0\u4e8641.42dB\u589e\u76ca\uff0c\u6bcf\u901a\u9053\u529f\u80172.178\u03bcA\uff0c\u9762\u79ef0.198mm\u00b2\uff0c\u652f\u63010.1Hz\u81f310kHz\u7684\u795e\u7ecf\u4fe1\u53f7\u76d1\u6d4b\uff0c\u8f93\u5165\u53c2\u8003\u96c6\u6210\u566a\u58f0\u4e3a3.59\u03bcVrms\u3002", "conclusion": "\u8be5\u6a21\u62df\u524d\u7aef\u8bbe\u8ba1\u6210\u529f\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fd\u7684\u795e\u7ecf\u4fe1\u53f7\u8bb0\u5f55\u80fd\u529b\uff0c\u4e3a\u690d\u5165\u5f0f\u8111\u673a\u63a5\u53e3\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u4fe1\u53f7\u91c7\u96c6\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.12733", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.12733", "abs": "https://arxiv.org/abs/2511.12733", "authors": ["Ahmed Hussain", "Ahmed Sultan", "Asmaa Abdallah", "Abdulkadir Celik", "Ahmed M. Eltawil"], "title": "Near Field Tapering with Slepian Window: Balancing the Range Angle Sidelobe Trade off", "comment": null, "summary": "Near-field beamforming enables target discrimination in both range (axial) and angle (lateral) dimensions. Elevated sidelobes along either dimension, however, increase susceptibility to interference and degrade detection performance. Conventional amplitude tapering techniques, designed for far-field scenarios, cannot simultaneously suppress axial and lateral sidelobes in near-field. In this letter, we propose a Slepian-based amplitude tapering approach that maximizes mainlobe energy concentration, achieving significant sidelobe reduction in both dimensions. Numerical results show that the proposed taper improves peak sidelobe suppression by approximately 24 dB in the lateral domain and 10 dB in the axial domain compared to a conventional uniform window.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eSlepian\u7684\u5e45\u5ea6\u9525\u524a\u65b9\u6cd5\uff0c\u7528\u4e8e\u8fd1\u573a\u6ce2\u675f\u6210\u5f62\uff0c\u80fd\u540c\u65f6\u5728\u8f74\u5411\u548c\u6a2a\u5411\u7ef4\u5ea6\u663e\u8457\u964d\u4f4e\u65c1\u74e3\u6c34\u5e73\u3002", "motivation": "\u4f20\u7edf\u5e45\u5ea6\u9525\u524a\u6280\u672f\u8bbe\u8ba1\u7528\u4e8e\u8fdc\u573a\u573a\u666f\uff0c\u65e0\u6cd5\u540c\u65f6\u6291\u5236\u8fd1\u573a\u4e2d\u7684\u8f74\u5411\u548c\u6a2a\u5411\u65c1\u74e3\uff0c\u800c\u65c1\u74e3\u5347\u9ad8\u4f1a\u589e\u52a0\u5e72\u6270\u654f\u611f\u6027\u5e76\u964d\u4f4e\u68c0\u6d4b\u6027\u80fd\u3002", "method": "\u91c7\u7528Slepian\u57fa\u7684\u5e45\u5ea6\u9525\u524a\u65b9\u6cd5\uff0c\u6700\u5927\u5316\u4e3b\u74e3\u80fd\u91cf\u96c6\u4e2d\u5ea6\u3002", "result": "\u6570\u503c\u7ed3\u679c\u663e\u793a\uff0c\u4e0e\u4f20\u7edf\u5747\u5300\u7a97\u53e3\u76f8\u6bd4\uff0c\u6240\u63d0\u51fa\u7684\u9525\u524a\u65b9\u6cd5\u5728\u6a2a\u5411\u57df\u5c06\u5cf0\u503c\u65c1\u74e3\u6291\u5236\u63d0\u9ad8\u4e86\u7ea624 dB\uff0c\u5728\u8f74\u5411\u57df\u63d0\u9ad8\u4e86\u7ea610 dB\u3002", "conclusion": "Slepian\u57fa\u5e45\u5ea6\u9525\u524a\u65b9\u6cd5\u80fd\u6709\u6548\u540c\u65f6\u6291\u5236\u8fd1\u573a\u6ce2\u675f\u6210\u5f62\u4e2d\u7684\u8f74\u5411\u548c\u6a2a\u5411\u65c1\u74e3\uff0c\u663e\u8457\u63d0\u5347\u68c0\u6d4b\u6027\u80fd\u3002"}}
{"id": "2511.12750", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.12750", "abs": "https://arxiv.org/abs/2511.12750", "authors": ["Ahmed Hussain", "Asmaa Abdallah", "Abdulkadir Celik", "Ahmed M. Eltawil"], "title": "Uniform Circular Arrays in Near-Field: Omnidirectional Coverage with Limited Capacity", "comment": null, "summary": "Recent studies suggest that uniform circular arrays (UCAs) can extend the angular coverage of the radiative near field region. This work investigates whether such enhanced angular coverage translates into improved spatial multiplexing performance when compared to uniform linear arrays (ULAs). To more accurately delineate the effective near field region, we introduce the effective beamfocusing Rayleigh distance (EBRD), an angle dependent metric that bounds the spatial region where beamfocusing remains effective. Closed form expressions for both beamdepth and EBRD are derived for UCAs. Our analysis shows that, under a fixed antenna element count, ULAs achieve narrower beamdepth and a longer EBRD than UCAs. Conversely, under a fixed aperture length, UCAs provide slightly narrower beamdepth and a marginally longer EBRD. Simulation results further confirm that ULAs achieve higher sum rate under the fixed element constraint, while UCAs offer marginal performance gain under the fixed aperture constraint.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5747\u5300\u5706\u5f62\u9635\u5217(UCA)\u4e0e\u5747\u5300\u7ebf\u6027\u9635\u5217(ULA)\u5728\u8fd1\u573a\u7a7a\u95f4\u590d\u7528\u6027\u80fd\u65b9\u9762\u7684\u6bd4\u8f83\uff0c\u5f15\u5165\u4e86\u6709\u6548\u6ce2\u675f\u805a\u7126\u745e\u5229\u8ddd\u79bb(EBRD)\u6765\u66f4\u51c6\u786e\u754c\u5b9a\u6709\u6548\u8fd1\u573a\u533a\u57df\u3002", "motivation": "\u8fd1\u671f\u7814\u7a76\u8868\u660eUCA\u53ef\u4ee5\u6269\u5c55\u8f90\u5c04\u8fd1\u573a\u533a\u57df\u7684\u89d2\u8986\u76d6\u8303\u56f4\uff0c\u672c\u6587\u65e8\u5728\u63a2\u7a76\u8fd9\u79cd\u589e\u5f3a\u7684\u89d2\u8986\u76d6\u662f\u5426\u80fd\u8f6c\u5316\u4e3a\u6bd4ULA\u66f4\u597d\u7684\u7a7a\u95f4\u590d\u7528\u6027\u80fd\u3002", "method": "\u63a8\u5bfc\u4e86UCA\u7684\u6ce2\u675f\u6df1\u5ea6\u548cEBRD\u7684\u95ed\u5f0f\u8868\u8fbe\u5f0f\uff0c\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u4eff\u771f\u6bd4\u8f83\u4e86\u5728\u56fa\u5b9a\u5929\u7ebf\u6570\u91cf\u548c\u56fa\u5b9a\u5b54\u5f84\u957f\u5ea6\u4e24\u79cd\u7ea6\u675f\u6761\u4ef6\u4e0bUCA\u548cULA\u7684\u6027\u80fd\u3002", "result": "\u5728\u56fa\u5b9a\u5929\u7ebf\u6570\u91cf\u6761\u4ef6\u4e0b\uff0cULA\u5b9e\u73b0\u66f4\u7a84\u7684\u6ce2\u675f\u6df1\u5ea6\u548c\u66f4\u957f\u7684EBRD\uff1b\u5728\u56fa\u5b9a\u5b54\u5f84\u957f\u5ea6\u6761\u4ef6\u4e0b\uff0cUCA\u63d0\u4f9b\u7a0d\u7a84\u7684\u6ce2\u675f\u6df1\u5ea6\u548c\u7a0d\u957f\u7684EBRD\u3002\u4eff\u771f\u7ed3\u679c\u663e\u793aULA\u5728\u56fa\u5b9a\u5143\u7d20\u7ea6\u675f\u4e0b\u83b7\u5f97\u66f4\u9ad8\u548c\u901f\u7387\uff0c\u800cUCA\u5728\u56fa\u5b9a\u5b54\u5f84\u7ea6\u675f\u4e0b\u4ec5\u6709\u8fb9\u9645\u6027\u80fd\u589e\u76ca\u3002", "conclusion": "UCA\u5728\u6269\u5c55\u89d2\u8986\u76d6\u65b9\u9762\u7684\u4f18\u52bf\u5e76\u4e0d\u76f4\u63a5\u8f6c\u5316\u4e3a\u663e\u8457\u7684\u7a7a\u95f4\u590d\u7528\u6027\u80fd\u63d0\u5347\uff0c\u9635\u5217\u51e0\u4f55\u5f62\u72b6\u7684\u9009\u62e9\u5e94\u57fa\u4e8e\u5177\u4f53\u5e94\u7528\u7ea6\u675f\u6761\u4ef6\u3002"}}
{"id": "2511.13104", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.13104", "abs": "https://arxiv.org/abs/2511.13104", "authors": ["Reiner Thom\u00e4", "Michael D\u00f6bereiner", "Reza Faramarzahangari", "Jonas Gedschold. Marc Francisco Colaco Miranda", "Saw James Myint", "Steffen Schieler", "Christian Schneider", "Sebastian Semper", "Carsten Smeenk", "Gerd Sommerkorn", "Zhixiang Zhao"], "title": "Distributed Multisensor ISAC", "comment": null, "summary": "Integrated Sensing and Communications (ISAC) will become a service in future mobile communication networks. It enables the detection and recognition of passive objects and environments using radar-like sensing. The ultimate advantage is the reuse of the mobile network and radio access resources for scene illumination, sensing, data transportation, computation, and fusion. It enables building a distributed, ubiquitous sensing network that can be adapted for a variety of radio sensing tasks and services.\n  In this article, we develop the principles of multi-sensor ISAC (MS-ISAC). MS-ISAC corresponds to multi-user MIMO communication, which in radar terminology is known as distributed MIMO radar. \\ First, we develop basic architectural principles for MS-ISAC and link them to example use cases. We then propose a generic MS-ISAC architecture. After a brief reference to multipath propagation and multistatic target reflectivity issues, we outline multilink access, coordination, precoding and link adaptation schemes for MS-ISAC. Moreover, we review model-based estimation and tracking of delay~/~Doppler from sparse OFDMA~/~TDMA frames. We emphasize Cooperative Passive Coherent Location (CPCL) for bistatic correlation and synchronization. Finally, issues of multisensor node synchronization and distributed data fusion are addressed.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u591a\u4f20\u611f\u5668\u96c6\u6210\u611f\u77e5\u4e0e\u901a\u4fe1(MS-ISAC)\u7684\u57fa\u672c\u539f\u7406\u548c\u67b6\u6784\uff0c\u5c06\u591a\u7528\u6237MIMO\u901a\u4fe1\u4e0e\u5206\u5e03\u5f0fMIMO\u96f7\u8fbe\u76f8\u7ed3\u5408\uff0c\u63a2\u8ba8\u4e86\u591a\u94fe\u8def\u63a5\u5165\u3001\u534f\u8c03\u3001\u9884\u7f16\u7801\u7b49\u5173\u952e\u6280\u672f\uff0c\u4ee5\u53ca\u57fa\u4e8e\u6a21\u578b\u7684\u5ef6\u8fdf/\u591a\u666e\u52d2\u4f30\u8ba1\u548c\u5206\u5e03\u5f0f\u6570\u636e\u878d\u5408\u95ee\u9898\u3002", "motivation": "\u96c6\u6210\u611f\u77e5\u4e0e\u901a\u4fe1(ISAC)\u5c06\u6210\u4e3a\u672a\u6765\u79fb\u52a8\u901a\u4fe1\u7f51\u7edc\u7684\u91cd\u8981\u670d\u52a1\uff0c\u80fd\u591f\u5229\u7528\u79fb\u52a8\u7f51\u7edc\u548c\u65e0\u7ebf\u63a5\u5165\u8d44\u6e90\u5b9e\u73b0\u88ab\u52a8\u5bf9\u8c61\u548c\u73af\u5883\u7684\u68c0\u6d4b\u8bc6\u522b\uff0c\u6784\u5efa\u5206\u5e03\u5f0f\u3001\u65e0\u5904\u4e0d\u5728\u7684\u611f\u77e5\u7f51\u7edc\u3002", "method": "\u5f00\u53d1\u4e86MS-ISAC\u7684\u57fa\u672c\u67b6\u6784\u539f\u5219\uff0c\u63d0\u51fa\u901a\u7528MS-ISAC\u67b6\u6784\uff0c\u6db5\u76d6\u591a\u94fe\u8def\u63a5\u5165\u3001\u534f\u8c03\u3001\u9884\u7f16\u7801\u548c\u94fe\u8def\u81ea\u9002\u5e94\u65b9\u6848\uff0c\u91c7\u7528\u57fa\u4e8e\u6a21\u578b\u7684\u7a00\u758fOFDMA/TDMA\u5e27\u5ef6\u8fdf/\u591a\u666e\u52d2\u4f30\u8ba1\u548c\u8ddf\u8e2a\uff0c\u5f3a\u8c03\u534f\u4f5c\u88ab\u52a8\u76f8\u5e72\u5b9a\u4f4d\u7528\u4e8e\u53cc\u57fa\u5730\u76f8\u5173\u548c\u540c\u6b65\u3002", "result": "\u5efa\u7acb\u4e86\u5c06\u591a\u7528\u6237MIMO\u901a\u4fe1\u4e0e\u5206\u5e03\u5f0fMIMO\u96f7\u8fbe\u76f8\u7ed3\u5408\u7684MS-ISAC\u7406\u8bba\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u591a\u4f20\u611f\u5668\u8282\u70b9\u540c\u6b65\u548c\u5206\u5e03\u5f0f\u6570\u636e\u878d\u5408\u7b49\u5173\u952e\u95ee\u9898\u3002", "conclusion": "MS-ISAC\u4e3a\u5b9e\u73b0\u5206\u5e03\u5f0f\u3001\u81ea\u9002\u5e94\u7684\u65e0\u7ebf\u611f\u77e5\u7f51\u7edc\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u6280\u672f\u8def\u5f84\uff0c\u80fd\u591f\u652f\u6301\u591a\u79cd\u65e0\u7ebf\u611f\u77e5\u4efb\u52a1\u548c\u670d\u52a1\u3002"}}
{"id": "2511.13171", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.13171", "abs": "https://arxiv.org/abs/2511.13171", "authors": ["Niccol\u00f2 Paglierani", "Francesco Linsalata", "Vineeth Teeda", "Davide Scazzoli", "Maurizio Magarini"], "title": "Autonomous Sensing UAV for Accurate Multi-User Identification and Localization in Cellular Networks", "comment": null, "summary": "This paper presents an autonomous sensing frame- work for identifying and localizing multiple users in Fifth Generation (5G) networks using an Unmanned Aerial Vehicle (UAV) that is not part of the serving access network. Unlike conventional aerial serving nodes, the proposed UAV operates passively and is dedicated solely to sensing. It captures Uplink (UL) Sounding Reference Signals (SRS), and requires virtually no coordination with the network infrastructure. A complete signal processing chain is proposed and developed, encompassing synchronization, user identification, and localization, all executed onboard UAV during flight. The system autonomously plans and adapts its mission workflow to estimate multiple user positions within a single deployment, integrating flight control with real-time sensing. Extensive simulations and a full-scale low- altitude experimental campaign validate the approach, showing localization errors below 3 m in rural field tests and below 8 m in urban simulation scenarios, while reliably identifying each user. The results confirm the feasibility of infrastructure-independent sensing UAVs as a core element of the emerging Low Altitude Economy (LAE), supporting situational awareness and rapid deployment in emergency or connectivity-limited environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u65e0\u4eba\u673a\u7684\u81ea\u4e3b\u611f\u77e5\u6846\u67b6\uff0c\u5229\u75285G\u7f51\u7edc\u7684\u4e0a\u884c\u63a2\u6d4b\u53c2\u8003\u4fe1\u53f7\u6765\u8bc6\u522b\u548c\u5b9a\u4f4d\u591a\u4e2a\u7528\u6237\uff0c\u65e0\u9700\u4e0e\u7f51\u7edc\u57fa\u7840\u8bbe\u65bd\u534f\u8c03\uff0c\u5b9e\u73b0\u4e86\u57fa\u7840\u8bbe\u65bd\u72ec\u7acb\u7684\u611f\u77e5\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u7a7a\u4e2d\u670d\u52a1\u8282\u70b9\u9700\u8981\u4e0e\u7f51\u7edc\u57fa\u7840\u8bbe\u65bd\u534f\u8c03\uff0c\u800c\u672c\u6587\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u5b8c\u5168\u88ab\u52a8\u3001\u4ec5\u7528\u4e8e\u611f\u77e5\u7684\u65e0\u4eba\u673a\u7cfb\u7edf\uff0c\u652f\u6301\u5728\u7d27\u6025\u60c5\u51b5\u6216\u8fde\u63a5\u53d7\u9650\u73af\u5883\u4e2d\u7684\u5feb\u901f\u90e8\u7f72\u548c\u6001\u52bf\u611f\u77e5\u3002", "method": "\u5f00\u53d1\u4e86\u5b8c\u6574\u7684\u4fe1\u53f7\u5904\u7406\u94fe\uff0c\u5305\u62ec\u540c\u6b65\u3001\u7528\u6237\u8bc6\u522b\u548c\u5b9a\u4f4d\uff0c\u6240\u6709\u5904\u7406\u90fd\u5728\u65e0\u4eba\u673a\u98de\u884c\u671f\u95f4\u673a\u8f7d\u6267\u884c\u3002\u7cfb\u7edf\u81ea\u4e3b\u89c4\u5212\u548c\u8c03\u6574\u4efb\u52a1\u5de5\u4f5c\u6d41\u7a0b\uff0c\u5c06\u98de\u884c\u63a7\u5236\u4e0e\u5b9e\u65f6\u611f\u77e5\u96c6\u6210\u3002", "result": "\u901a\u8fc7\u5e7f\u6cdb\u4eff\u771f\u548c\u5168\u5c3a\u5ea6\u4f4e\u7a7a\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u5728\u519c\u6751\u5b9e\u5730\u6d4b\u8bd5\u4e2d\u5b9a\u4f4d\u8bef\u5dee\u4f4e\u4e8e3\u7c73\uff0c\u5728\u57ce\u5e02\u4eff\u771f\u573a\u666f\u4e2d\u4f4e\u4e8e8\u7c73\uff0c\u540c\u65f6\u53ef\u9760\u8bc6\u522b\u6bcf\u4e2a\u7528\u6237\u3002", "conclusion": "\u7ed3\u679c\u8bc1\u5b9e\u4e86\u57fa\u7840\u8bbe\u65bd\u72ec\u7acb\u611f\u77e5\u65e0\u4eba\u673a\u4f5c\u4e3a\u65b0\u5174\u4f4e\u7a7a\u7ecf\u6d4e\u6838\u5fc3\u5143\u7d20\u7684\u53ef\u884c\u6027\uff0c\u652f\u6301\u5728\u7d27\u6025\u6216\u8fde\u63a5\u53d7\u9650\u73af\u5883\u4e2d\u7684\u6001\u52bf\u611f\u77e5\u548c\u5feb\u901f\u90e8\u7f72\u3002"}}
{"id": "2511.13272", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.13272", "abs": "https://arxiv.org/abs/2511.13272", "authors": ["Zeyang Sun", "Xidong Mu", "Shuai Han", "Sai Xu", "Michail Matthaiou"], "title": "Pinching-Antenna-Enabled Cognitive Radio Networks", "comment": "13 pages, 7 figures", "summary": "This paper investigates a pinching-antenna (PA)-enabled cognitive radio network, where both the primary transmitter (PT) and secondary transmitter (ST) are equipped with a single waveguide and multiple PAs to facilitate simultaneous spectrum sharing. Under a general Ricean fading channel model, a closed-form analytical expression for the average spectral efficiency (SE) achieved by PAs is first derived. Based on this, a sum-SE maximization problem is formulated to jointly optimize the primary and secondary pinching beamforming, subject to system constraints on the transmission power budgets, minimum antenna separation requirements, and feasible PA deployment regions. To address this non-convex problem, a three-stage optimization algorithm is developed to sequentially optimize both the PT and ST pinching beamforming, and the ST power control. For the PT and ST pinching beamforming optimization, the coarse positions of PA are first determined at the waveguide-level. Then, wavelength-level refinements achieve constructive signal combination at the intended user and destructive superposition at the unintended user. For the ST power control, a closed-form solution is derived. Simulation results demonstrate that i) PAs can achieve significant SE improvements over conventional fixed-position antennas; ii) the proposed pinching beamforming design achieves effective interference suppression and superior performance for both even and odd numbers of PAs; and iii) the developed three-stage optimization algorithm enables nearly orthogonal transmission between the primary and secondary networks.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u57fa\u4e8e\u5939\u6301\u5929\u7ebf\u7684\u8ba4\u77e5\u65e0\u7ebf\u7535\u7f51\u7edc\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u4e3b\u6b21\u7528\u6237\u7684\u5939\u6301\u6ce2\u675f\u6210\u5f62\u548c\u529f\u7387\u63a7\u5236\uff0c\u5b9e\u73b0\u4e86\u663e\u8457\u9891\u8c31\u6548\u7387\u63d0\u5347\u548c\u6709\u6548\u5e72\u6270\u6291\u5236\u3002", "motivation": "\u4f20\u7edf\u56fa\u5b9a\u4f4d\u7f6e\u5929\u7ebf\u5728\u8ba4\u77e5\u65e0\u7ebf\u7535\u7f51\u7edc\u4e2d\u9891\u8c31\u6548\u7387\u6709\u9650\uff0c\u9700\u8981\u5f00\u53d1\u65b0\u7684\u5929\u7ebf\u6280\u672f\u6765\u6539\u5584\u540c\u65f6\u9891\u8c31\u5171\u4eab\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e09\u9636\u6bb5\u4f18\u5316\u7b97\u6cd5\uff1a\u9996\u5148\u5728\u6ce2\u5bfc\u7ea7\u786e\u5b9aPA\u7c97\u4f4d\u7f6e\uff0c\u7136\u540e\u5728\u6ce2\u957f\u7ea7\u8fdb\u884c\u7cbe\u7ec6\u8c03\u6574\u5b9e\u73b0\u4fe1\u53f7\u5efa\u8bbe\u6027\u7ec4\u5408\uff0c\u6700\u540e\u63a8\u5bfcST\u529f\u7387\u63a7\u5236\u7684\u95ed\u5f0f\u89e3\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff1aPA\u76f8\u6bd4\u4f20\u7edf\u5929\u7ebf\u663e\u8457\u63d0\u5347\u9891\u8c31\u6548\u7387\uff1b\u6240\u63d0\u6ce2\u675f\u6210\u5f62\u8bbe\u8ba1\u6709\u6548\u6291\u5236\u5e72\u6270\uff1b\u4e09\u9636\u6bb5\u7b97\u6cd5\u4f7f\u4e3b\u6b21\u7f51\u7edc\u4f20\u8f93\u8fd1\u4e4e\u6b63\u4ea4\u3002", "conclusion": "\u5939\u6301\u5929\u7ebf\u6280\u672f\u7ed3\u5408\u6240\u63d0\u4f18\u5316\u7b97\u6cd5\u80fd\u591f\u6709\u6548\u63d0\u5347\u8ba4\u77e5\u65e0\u7ebf\u7535\u7f51\u7edc\u7684\u9891\u8c31\u6548\u7387\uff0c\u5b9e\u73b0\u4e3b\u6b21\u7528\u6237\u95f4\u7684\u6709\u6548\u5171\u5b58\u3002"}}
{"id": "2511.13336", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.13336", "abs": "https://arxiv.org/abs/2511.13336", "authors": ["Maolin Li", "Feng Shu", "Minghao Chen", "Cunhua Pan", "Fuhui Zhou", "Yongpeng Wu", "Liang Yang"], "title": "Sensing-enabled Secure Rotatable Array System Enhanced by Multi-Layer Transmitting RIS", "comment": null, "summary": "Programmable metasurfaces and adjustable antennas are promising technologies. The security of a rotatable array system is investigated in this paper. A dual-base-station (BS) architecture is adopted, in which the BSs collaboratively perform integrated sensing of the eavesdropper (the target) and communication tasks. To address the security challenge when the sensing target is located on the main communication link, the problem of maximizing the secrecy rate (SR) under sensing signal-to-interference-plus-noise ratio requirements and discrete constraints is formulated. This problem involves the joint optimization of the array pose, the antenna distribution on the array surface, the multi-layer transmitting RIS phase matrices, and the beamforming matrices, which is non-convex. To solve this challenge, an two-stage online algorithm based on the generalized Rayleigh quotient and an offline algorithm based on the Multi-Agent Deep Deterministic Policy Gradient are proposed. Simulation results validate the effectiveness of the proposed algorithms. Compared to conventional schemes without array pose adjustment, the proposed approach achieves approximately 22\\% improvement in SR. Furthermore, array rotation provides higher performance gains than position changes.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u53ef\u65cb\u8f6c\u9635\u5217\u7cfb\u7edf\u7684\u5b89\u5168\u6027\uff0c\u91c7\u7528\u53cc\u57fa\u7ad9\u67b6\u6784\u534f\u540c\u6267\u884c\u7a83\u542c\u8005\u611f\u77e5\u548c\u901a\u4fe1\u4efb\u52a1\uff0c\u63d0\u51fa\u4e86\u5728\u7ebf\u548c\u79bb\u7ebf\u7b97\u6cd5\u6765\u4f18\u5316\u9635\u5217\u59ff\u6001\u3001\u5929\u7ebf\u5206\u5e03\u548c\u6ce2\u675f\u6210\u5f62\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6848\u5b9e\u73b0\u4e86\u7ea622%\u7684\u4fdd\u5bc6\u7387\u63d0\u5347\u3002", "motivation": "\u53ef\u7f16\u7a0b\u8d85\u8868\u9762\u548c\u53ef\u8c03\u5929\u7ebf\u662f\u524d\u666f\u6280\u672f\uff0c\u4f46\u4f20\u7edf\u65b9\u6848\u5728\u7a83\u542c\u8005\u4f4d\u4e8e\u4e3b\u901a\u4fe1\u94fe\u8def\u4e0a\u65f6\u5b58\u5728\u5b89\u5168\u6311\u6218\uff0c\u9700\u8981\u7814\u7a76\u5982\u4f55\u901a\u8fc7\u9635\u5217\u59ff\u6001\u8c03\u6574\u6765\u589e\u5f3a\u901a\u4fe1\u5b89\u5168\u6027\u3002", "method": "\u91c7\u7528\u53cc\u57fa\u7ad9\u67b6\u6784\u534f\u540c\u6267\u884c\u611f\u77e5\u548c\u901a\u4fe1\u4efb\u52a1\uff0c\u63d0\u51fa\u57fa\u4e8e\u5e7f\u4e49\u745e\u5229\u5546\u7684\u5728\u7ebf\u4e24\u9636\u6bb5\u7b97\u6cd5\u548c\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u6df1\u5ea6\u786e\u5b9a\u6027\u7b56\u7565\u68af\u5ea6\u7684\u79bb\u7ebf\u7b97\u6cd5\uff0c\u8054\u5408\u4f18\u5316\u9635\u5217\u59ff\u6001\u3001\u5929\u7ebf\u5206\u5e03\u3001RIS\u76f8\u4f4d\u77e9\u9635\u548c\u6ce2\u675f\u6210\u5f62\u77e9\u9635\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\u6240\u63d0\u7b97\u6cd5\u6709\u6548\uff0c\u76f8\u6bd4\u65e0\u9635\u5217\u59ff\u6001\u8c03\u6574\u7684\u4f20\u7edf\u65b9\u6848\uff0c\u4fdd\u5bc6\u7387\u63d0\u5347\u7ea622%\uff0c\u4e14\u9635\u5217\u65cb\u8f6c\u6bd4\u4f4d\u7f6e\u53d8\u5316\u63d0\u4f9b\u66f4\u9ad8\u7684\u6027\u80fd\u589e\u76ca\u3002", "conclusion": "\u901a\u8fc7\u9635\u5217\u59ff\u6001\u8c03\u6574\u53ef\u4ee5\u6709\u6548\u63d0\u5347\u901a\u4fe1\u7cfb\u7edf\u7684\u5b89\u5168\u6027\uff0c\u65cb\u8f6c\u9635\u5217\u6bd4\u4f4d\u7f6e\u8c03\u6574\u5177\u6709\u66f4\u597d\u7684\u6027\u80fd\u589e\u76ca\uff0c\u6240\u63d0\u7b97\u6cd5\u4e3a\u89e3\u51b3\u590d\u6742\u975e\u51f8\u4f18\u5316\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}

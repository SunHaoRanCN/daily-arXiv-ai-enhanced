{"id": "2511.13729", "categories": ["eess.SP", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.13729", "abs": "https://arxiv.org/abs/2511.13729", "authors": ["Huseyin Goksu"], "title": "DualLaguerreNet: A Decoupled Spectral Filter GNN and the Uncovering of the Flexibility-Stability Trade-off", "comment": null, "summary": "Graph Neural Networks (GNNs) based on spectral filters, such as the Adaptive Orthogonal Polynomial Filter (AOPF) class (e.g., LaguerreNet), have shown promise in unifying the solutions for heterophily and over-smoothing. However, these single-filter models suffer from a \"compromise\" problem, as their single adaptive parameter (e.g., alpha) must learn a suboptimal, averaged response across the entire graph spectrum. In this paper, we propose DualLaguerreNet, a novel GNN architecture that solves this by introducing \"Decoupled Spectral Flexibility.\" DualLaguerreNet splits the graph Laplacian into two operators, L_low (low-frequency) and L_high (high-frequency), and learns two independent, adaptive Laguerre polynomial filters, parameterized by alpha_1 and alpha_2, respectively. This work, however, uncovers a deeper finding. While our experiments show DualLaguerreNet's flexibility allows it to achieve state-of-the-art results on complex heterophilic tasks (outperforming LaguerreNet), it simultaneously underperforms on simpler, homophilic tasks. We identify this as a fundamental \"Flexibility-Stability Trade-off\". The increased parameterization (2x filter parameters and 2x model parameters) leads to overfitting on simple tasks, demonstrating that the \"compromise\" of simpler models acts as a crucial regularizer. This paper presents a new SOTA architecture for heterophily while providing a critical analysis of the bias-variance trade-off inherent in adaptive GNN filter design.", "AI": {"tldr": "DualLaguerreNet\u901a\u8fc7\u89e3\u8026\u9891\u8c31\u7075\u6d3b\u6027\u89e3\u51b3\u4e86\u5355\u6ee4\u6ce2\u5668GNN\u7684\u6298\u8877\u95ee\u9898\uff0c\u5728\u5f02\u914d\u6027\u4efb\u52a1\u4e2d\u8fbe\u5230SOTA\uff0c\u4f46\u5728\u540c\u914d\u6027\u4efb\u52a1\u4e2d\u56e0\u8fc7\u5ea6\u53c2\u6570\u5316\u800c\u8868\u73b0\u4e0d\u4f73\uff0c\u63ed\u793a\u4e86GNN\u6ee4\u6ce2\u5668\u8bbe\u8ba1\u4e2d\u7684\u7075\u6d3b\u6027-\u7a33\u5b9a\u6027\u6743\u8861\u3002", "motivation": "\u57fa\u4e8e\u9891\u8c31\u6ee4\u6ce2\u5668\u7684GNN\uff08\u5982LaguerreNet\uff09\u5728\u7edf\u4e00\u89e3\u51b3\u5f02\u914d\u6027\u548c\u8fc7\u5e73\u6ed1\u95ee\u9898\u65f6\u5b58\u5728\"\u6298\u8877\"\u95ee\u9898\uff0c\u5355\u4e00\u81ea\u9002\u5e94\u53c2\u6570\u5fc5\u987b\u5728\u6574\u4e2a\u56fe\u9891\u8c31\u4e0a\u5b66\u4e60\u6b21\u4f18\u7684\u5e73\u5747\u54cd\u5e94\u3002", "method": "\u63d0\u51faDualLaguerreNet\u67b6\u6784\uff0c\u5c06\u56fe\u62c9\u666e\u62c9\u65af\u7b97\u5b50\u5206\u89e3\u4e3a\u4f4e\u9891\u548c\u9ad8\u9891\u4e24\u4e2a\u7b97\u5b50\uff0c\u5206\u522b\u5b66\u4e60\u4e24\u4e2a\u72ec\u7acb\u7684\u81ea\u9002\u5e94Laguerre\u591a\u9879\u5f0f\u6ee4\u6ce2\u5668\uff0c\u53c2\u6570\u5316\u4e3aalpha_1\u548calpha_2\u3002", "result": "DualLaguerreNet\u5728\u590d\u6742\u5f02\u914d\u6027\u4efb\u52a1\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\uff08\u4f18\u4e8eLaguerreNet\uff09\uff0c\u4f46\u5728\u7b80\u5355\u540c\u914d\u6027\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u63ed\u793a\u4e86\"\u7075\u6d3b\u6027-\u7a33\u5b9a\u6027\u6743\u8861\"\u3002", "conclusion": "\u589e\u52a0\u53c2\u6570\u5316\uff082\u500d\u6ee4\u6ce2\u5668\u53c2\u6570\u548c2\u500d\u6a21\u578b\u53c2\u6570\uff09\u4f1a\u5bfc\u81f4\u7b80\u5355\u4efb\u52a1\u4e0a\u7684\u8fc7\u62df\u5408\uff0c\u8868\u660e\u7b80\u5355\u6a21\u578b\u7684\"\u6298\u8877\"\u8d77\u5230\u4e86\u5173\u952e\u7684\u6b63\u5219\u5316\u4f5c\u7528\uff0c\u4e3a\u81ea\u9002\u5e94GNN\u6ee4\u6ce2\u5668\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u504f\u5dee-\u65b9\u5dee\u6743\u8861\u7684\u91cd\u8981\u5206\u6790\u3002"}}
{"id": "2511.13730", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.13730", "abs": "https://arxiv.org/abs/2511.13730", "authors": ["Huseyin Goksu"], "title": "GegenbauerNet: Finding the Optimal Compromise in the GNN Flexibility-Stability Trade-off", "comment": null, "summary": "Spectral Graph Neural Networks (GNNs) operating in the canonical [-1, 1] domain (like ChebyNet and its adaptive generalization, L-JacobiNet) face a fundamental Flexibility-Stability Trade-off. Our previous work revealed a critical puzzle: the 2-parameter adaptive L-JacobiNet often suffered from high variance and was surprisingly outperformed by the 0-parameter, stabilized-static S-JacobiNet. This suggested that stabilization was more critical than adaptation in this domain. In this paper, we propose \\textbf{GegenbauerNet}, a novel GNN filter based on the Gegenbauer polynomials, to find the Optimal Compromise in this trade-off. By enforcing symmetry (alpha=beta) but allowing a single shape parameter (lambda) to be learned, GegenbauerNet limits flexibility (variance) while escaping the fixed bias of S-JacobiNet. We demonstrate that GegenbauerNet (1-parameter) achieves superior performance in the key local filtering regime (K=2 on heterophilic graphs) where overfitting is minimal, validating the hypothesis that a controlled, symmetric degree of freedom is optimal. Furthermore, our comprehensive K-ablation study across homophilic and heterophilic graphs, using 7 diverse datasets, clarifies the domain's behavior: the fully adaptive L-JacobiNet maintains the highest performance on high-K filtering tasks, showing the value of maximum flexibility when regularization is managed. This study provides crucial design principles for GNN developers, showing that in the [-1, 1] spectral domain, the optimal filter depends critically on the target locality (K) and the acceptable level of design bias.", "AI": {"tldr": "\u63d0\u51faGegenbauerNet\u4f5c\u4e3a\u8c31\u56fe\u795e\u7ecf\u7f51\u7edc\u5728[-1,1]\u57df\u4e2d\u7684\u6700\u4f18\u6298\u8877\u65b9\u6848\uff0c\u901a\u8fc7\u5355\u4e00\u5f62\u72b6\u53c2\u6570\u5e73\u8861\u7075\u6d3b\u6027\u4e0e\u7a33\u5b9a\u6027\uff0c\u5728\u5c40\u90e8\u6ee4\u6ce2\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u8d8a\u3002", "motivation": "\u89e3\u51b3\u8c31\u56fe\u795e\u7ecf\u7f51\u7edc\u5728[-1,1]\u57df\u4e2d\u9762\u4e34\u7684\u7075\u6d3b\u6027-\u7a33\u5b9a\u6027\u6743\u8861\u95ee\u9898\uff0c\u7279\u522b\u662f\u53d1\u73b02\u53c2\u6570\u81ea\u9002\u5e94L-JacobiNet\u5b58\u5728\u9ad8\u65b9\u5dee\u95ee\u9898\uff0c\u800c0\u53c2\u6570S-JacobiNet\u867d\u7136\u7a33\u5b9a\u4f46\u5b58\u5728\u56fa\u5b9a\u504f\u5dee\u3002", "method": "\u57fa\u4e8eGegenbauer\u591a\u9879\u5f0f\u6784\u5efaGNN\u6ee4\u6ce2\u5668\uff0c\u901a\u8fc7\u5f3a\u5236\u5bf9\u79f0\u6027\u4f46\u5141\u8bb8\u5b66\u4e60\u5355\u4e00\u5f62\u72b6\u53c2\u6570\u03bb\uff0c\u9650\u5236\u7075\u6d3b\u6027\uff08\u65b9\u5dee\uff09\u540c\u65f6\u907f\u514dS-JacobiNet\u7684\u56fa\u5b9a\u504f\u5dee\u3002", "result": "GegenbauerNet\uff081\u53c2\u6570\uff09\u5728\u5173\u952e\u5c40\u90e8\u6ee4\u6ce2\u673a\u5236\uff08K=2\uff0c\u5f02\u914d\u6027\u56fe\uff09\u4e2d\u8868\u73b0\u6700\u4f18\uff0c\u6b64\u65f6\u8fc7\u62df\u5408\u6700\u5c0f\uff1b\u800c\u5b8c\u5168\u81ea\u9002\u5e94\u7684L-JacobiNet\u5728\u9ad8K\u6ee4\u6ce2\u4efb\u52a1\u4e2d\u4fdd\u6301\u6700\u9ad8\u6027\u80fd\u3002", "conclusion": "\u5728[-1,1]\u8c31\u57df\u4e2d\uff0c\u6700\u4f18\u6ee4\u6ce2\u5668\u8bbe\u8ba1\u53d6\u51b3\u4e8e\u76ee\u6807\u5c40\u90e8\u6027\uff08K\uff09\u548c\u53ef\u63a5\u53d7\u7684\u8bbe\u8ba1\u504f\u5dee\u6c34\u5e73\uff0c\u4e3aGNN\u5f00\u53d1\u8005\u63d0\u4f9b\u4e86\u5173\u952e\u8bbe\u8ba1\u539f\u5219\u3002"}}
{"id": "2511.13733", "categories": ["eess.SP", "cs.LG", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2511.13733", "abs": "https://arxiv.org/abs/2511.13733", "authors": ["Wenchao Yang", "Weidong Yan", "Wenkang Liu", "Yulan Ma", "Yang Li"], "title": "THD-BAR: Topology Hierarchical Derived Brain Autoregressive Modeling for EEG Generic Representations", "comment": null, "summary": "Large-scale pre-trained models hold significant potential for learning universal EEG representations. However, most existing methods, particularly autoregressive (AR) frameworks, primarily rely on straightforward temporal sequencing of multi-channel EEG data, which fails to capture the rich physiological characteristics inherent to EEG signals. Moreover, their time-centered modeling approach also limits the effective representation of the dynamic spatial topology of brain activity. To address these challenges and fully exploit the potential of large-scale EEG models, we propose a novel Topology Hierarchical Derived Brain Autoregressive Modeling (THD-BAR) for EEG generic representations. The core innovation of THD-BAR lies in the introduction of the Brain Topology Hierarchy (BTH), which establishes a multi-scale spatial order for EEG channels. This hierarchical structure enables a redefinition of autoregressive learning as a \"next-scale-time prediction\" problem, effectively capturing both spatial and temporal dynamics. Based on BTH, we design a Topology-Hierarchical Vector Quantized-Variational Autoencoder (THVQ-VAE) for multi-scale tokenization and develop an enhanced Brain Autoregressive (BAR) module with specialized masking strategies for prediction. Through extensive large-scale pre-training on 17 datasets, followed by rigorous validation on 10 downstream datasets spanning 5 distinct tasks, THD-BAR consistently outperforms existing methods. These results highlight the superior generalization and modeling capabilities of our proposed approach.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u62d3\u6251\u5c42\u6b21\u884d\u751f\u8111\u81ea\u56de\u5f52\u5efa\u6a21(THD-BAR)\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u8111\u62d3\u6251\u5c42\u6b21(BTH)\u91cd\u65b0\u5b9a\u4e49\u81ea\u56de\u5f52\u5b66\u4e60\u4e3a\"\u4e0b\u4e00\u5c3a\u5ea6\u65f6\u95f4\u9884\u6d4b\"\u95ee\u9898\uff0c\u6709\u6548\u6355\u6349EEG\u4fe1\u53f7\u7684\u65f6\u7a7a\u52a8\u6001\u7279\u6027\u3002", "motivation": "\u73b0\u6709\u81ea\u56de\u5f52\u6846\u67b6\u4e3b\u8981\u4f9d\u8d56\u591a\u901a\u9053EEG\u6570\u636e\u7684\u7b80\u5355\u65f6\u95f4\u5e8f\u5217\uff0c\u65e0\u6cd5\u6355\u6349EEG\u4fe1\u53f7\u7684\u4e30\u5bcc\u751f\u7406\u7279\u5f81\uff0c\u4e14\u65f6\u95f4\u4e2d\u5fc3\u5efa\u6a21\u65b9\u6cd5\u9650\u5236\u4e86\u8111\u6d3b\u52a8\u52a8\u6001\u7a7a\u95f4\u62d3\u6251\u7684\u6709\u6548\u8868\u793a\u3002", "method": "\u5f15\u5165\u8111\u62d3\u6251\u5c42\u6b21(BTH)\u5efa\u7acbEEG\u901a\u9053\u7684\u591a\u5c3a\u5ea6\u7a7a\u95f4\u987a\u5e8f\uff0c\u8bbe\u8ba1\u62d3\u6251\u5c42\u6b21\u5411\u91cf\u91cf\u5316\u53d8\u5206\u81ea\u7f16\u7801\u5668(THVQ-VAE)\u8fdb\u884c\u591a\u5c3a\u5ea6\u6807\u8bb0\u5316\uff0c\u5f00\u53d1\u589e\u5f3a\u7684\u8111\u81ea\u56de\u5f52(BAR)\u6a21\u5757\u548c\u4e13\u7528\u63a9\u7801\u7b56\u7565\u8fdb\u884c\u9884\u6d4b\u3002", "result": "\u572817\u4e2a\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\uff0c\u572810\u4e2a\u4e0b\u6e38\u6570\u636e\u96c65\u4e2a\u4e0d\u540c\u4efb\u52a1\u4e0a\u9a8c\u8bc1\uff0cTHD-BAR\u59cb\u7ec8\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5177\u6709\u4f18\u8d8a\u7684\u6cdb\u5316\u80fd\u529b\u548c\u5efa\u6a21\u80fd\u529b\uff0c\u4e3aEEG\u901a\u7528\u8868\u793a\u5b66\u4e60\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.13912", "categories": ["eess.SP", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.13912", "abs": "https://arxiv.org/abs/2511.13912", "authors": ["Xiaoyu Zhang", "Mingtao Hu", "Sen Lu", "Soohyeon Kim", "Eric Yeu-Jer Lee", "Yuyang Liu", "Wei D. Lu"], "title": "Compute-in-Memory Implementation of State Space Models for Event Sequence Processing", "comment": "Xiaoyu Zhang and Mingtao Hu contributed equally to this work", "summary": "State space models (SSMs) have recently emerged as a powerful framework for long sequence processing, outperforming traditional methods on diverse benchmarks. Fundamentally, SSMs can generalize both recurrent and convolutional networks and have been shown to even capture key functions of biological systems. Here we report an approach to implement SSMs in energy-efficient compute-in-memory (CIM) hardware to achieve real-time, event-driven processing. Our work re-parameterizes the model to function with real-valued coefficients and shared decay constants, reducing the complexity of model mapping onto practical hardware systems. By leveraging device dynamics and diagonalized state transition parameters, the state evolution can be natively implemented in crossbar-based CIM systems combined with memristors exhibiting short-term memory effects. Through this algorithm and hardware co-design, we show the proposed system offers both high accuracy and high energy efficiency while supporting fully asynchronous processing for event-based vision and audio tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u5b58\u5185\u8ba1\u7b97\u786c\u4ef6\u4e2d\u5b9e\u73b0\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u91cd\u65b0\u53c2\u6570\u5316\u6a21\u578b\u4ee5\u4f7f\u7528\u5b9e\u503c\u7cfb\u6570\u548c\u5171\u4eab\u8870\u51cf\u5e38\u6570\uff0c\u964d\u4f4e\u4e86\u786c\u4ef6\u6620\u5c04\u590d\u6742\u5ea6\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u548c\u9ad8\u80fd\u6548\u7684\u5f02\u6b65\u4e8b\u4ef6\u9a71\u52a8\u5904\u7406\u3002", "motivation": "\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u5728\u957f\u5e8f\u5217\u5904\u7406\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u9700\u8981\u9ad8\u6548\u7684\u786c\u4ef6\u5b9e\u73b0\u6765\u652f\u6301\u5b9e\u65f6\u4e8b\u4ef6\u9a71\u52a8\u5904\u7406\u3002\u5b58\u5185\u8ba1\u7b97\u786c\u4ef6\u7ed3\u5408\u5fc6\u963b\u5668\u7684\u77ed\u671f\u8bb0\u5fc6\u6548\u5e94\u4e3a\u5b9e\u73b0\u8fd9\u79cd\u6a21\u578b\u63d0\u4f9b\u4e86\u53ef\u80fd\u3002", "method": "\u91cd\u65b0\u53c2\u6570\u5316\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff0c\u4f7f\u7528\u5b9e\u503c\u7cfb\u6570\u548c\u5171\u4eab\u8870\u51cf\u5e38\u6570\uff1b\u5229\u7528\u8bbe\u5907\u52a8\u6001\u7279\u6027\u548c\u5bf9\u89d2\u5316\u72b6\u6001\u8f6c\u79fb\u53c2\u6570\uff0c\u5728\u57fa\u4e8e\u4ea4\u53c9\u9635\u5217\u7684\u5b58\u5185\u8ba1\u7b97\u7cfb\u7edf\u4e2d\u539f\u751f\u5b9e\u73b0\u72b6\u6001\u6f14\u5316\u3002", "result": "\u63d0\u51fa\u7684\u7cfb\u7edf\u5728\u57fa\u4e8e\u4e8b\u4ef6\u7684\u89c6\u89c9\u548c\u97f3\u9891\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u548c\u9ad8\u80fd\u6548\uff0c\u652f\u6301\u5b8c\u5168\u5f02\u6b65\u5904\u7406\u3002", "conclusion": "\u901a\u8fc7\u7b97\u6cd5\u548c\u786c\u4ef6\u534f\u540c\u8bbe\u8ba1\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u5728\u5b58\u5185\u8ba1\u7b97\u786c\u4ef6\u4e2d\u7684\u9ad8\u6548\u5b9e\u73b0\uff0c\u4e3a\u5b9e\u65f6\u4e8b\u4ef6\u9a71\u52a8\u5904\u7406\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2511.13731", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2511.13731", "abs": "https://arxiv.org/abs/2511.13731", "authors": ["Xiao Li", "Kotaro Funakoshi", "Manabu Okumura"], "title": "Emotion Recognition in Multi-Speaker Conversations through Speaker Identification, Knowledge Distillation, and Hierarchical Fusion", "comment": null, "summary": "Emotion recognition in multi-speaker conversations faces significant challenges due to speaker ambiguity and severe class imbalance. We propose a novel framework that addresses these issues through three key innovations: (1) a speaker identification module that leverages audio-visual synchronization to accurately identify the active speaker, (2) a knowledge distillation strategy that transfers superior textual emotion understanding to audio and visual modalities, and (3) hierarchical attention fusion with composite loss functions to handle class imbalance. Comprehensive evaluations on MELD and IEMOCAP datasets demonstrate superior performance, achieving 67.75% and 72.44% weighted F1 scores respectively, with particularly notable improvements on minority emotion classes.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u89e3\u51b3\u591a\u8bf4\u8bdd\u4eba\u5bf9\u8bdd\u4e2d\u60c5\u7eea\u8bc6\u522b\u6311\u6218\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u8bf4\u8bdd\u4eba\u8bc6\u522b\u3001\u77e5\u8bc6\u84b8\u998f\u548c\u5206\u5c42\u6ce8\u610f\u529b\u878d\u5408\u4e09\u4e2a\u521b\u65b0\u70b9\uff0c\u5728MELD\u548cIEMOCAP\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e8667.75%\u548c72.44%\u7684\u52a0\u6743F1\u5206\u6570\u3002", "motivation": "\u591a\u8bf4\u8bdd\u4eba\u5bf9\u8bdd\u4e2d\u7684\u60c5\u7eea\u8bc6\u522b\u9762\u4e34\u8bf4\u8bdd\u4eba\u6a21\u7cca\u6027\u548c\u4e25\u91cd\u7c7b\u522b\u4e0d\u5e73\u8861\u7684\u6311\u6218\uff0c\u9700\u8981\u6709\u6548\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u4ee5\u63d0\u9ad8\u8bc6\u522b\u6027\u80fd\u3002", "method": "1) \u5229\u7528\u89c6\u542c\u540c\u6b65\u7684\u8bf4\u8bdd\u4eba\u8bc6\u522b\u6a21\u5757\uff1b2) \u5c06\u6587\u672c\u60c5\u7eea\u7406\u89e3\u77e5\u8bc6\u84b8\u998f\u5230\u97f3\u9891\u548c\u89c6\u89c9\u6a21\u6001\u7684\u7b56\u7565\uff1b3) \u5904\u7406\u7c7b\u522b\u4e0d\u5e73\u8861\u7684\u5206\u5c42\u6ce8\u610f\u529b\u878d\u5408\u4e0e\u590d\u5408\u635f\u5931\u51fd\u6570\u3002", "result": "\u5728MELD\u548cIEMOCAP\u6570\u636e\u96c6\u4e0a\u5206\u522b\u8fbe\u523067.75%\u548c72.44%\u7684\u52a0\u6743F1\u5206\u6570\uff0c\u5728\u5c11\u6570\u60c5\u7eea\u7c7b\u522b\u4e0a\u8868\u73b0\u5c24\u4e3a\u7a81\u51fa\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u4e09\u4e2a\u5173\u952e\u521b\u65b0\u6709\u6548\u89e3\u51b3\u4e86\u591a\u8bf4\u8bdd\u4eba\u5bf9\u8bdd\u4e2d\u7684\u60c5\u7eea\u8bc6\u522b\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u8bf4\u8bdd\u4eba\u6a21\u7cca\u6027\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u6210\u6548\u3002"}}
{"id": "2511.13732", "categories": ["eess.AS", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.13732", "abs": "https://arxiv.org/abs/2511.13732", "authors": ["Moran Yanuka", "Paul Dixon", "Eyal Finkelshtein", "Daniel Rotman", "Raja Giryes"], "title": "Principled Coarse-Grained Acceptance for Speculative Decoding in Speech", "comment": null, "summary": "Speculative decoding accelerates autoregressive speech generation by letting a fast draft model propose tokens that a larger target model verifies. However, for speech LLMs that generate acoustic tokens, exact token matching is overly restrictive: many discrete tokens are acoustically or semantically interchangeable, reducing acceptance rates and limiting speedups. We introduce Principled Coarse-Graining (PCG), which verifies proposals at the level of Acoustic Similarity Groups (ASGs) derived from the target model's embedding space. By splitting each token's probability mass across the overlapping groups that contain it, we define an overlap-aware coarse-grained distribution and perform rejection sampling on the resulting group variable. This yields an exactness guarantee at the group level while allowing the accepted draft token to stand in for any member of the group in practice. On LibriTTS, PCG increases acceptance and throughput relative to standard speculative decoding and prior speech-specific relaxations while maintaining intelligibility and speaker similarity. These results suggest acoustically aware, group-level acceptance as a simple and general way to accelerate speech token generation while maintaining speech quality.", "AI": {"tldr": "PCG\u901a\u8fc7\u58f0\u5b66\u76f8\u4f3c\u6027\u7ec4\u9a8c\u8bc1\u6765\u52a0\u901f\u8bed\u97f3LLM\u751f\u6210\uff0c\u76f8\u6bd4\u6807\u51c6\u63a8\u6d4b\u89e3\u7801\u63d0\u9ad8\u4e86\u63a5\u53d7\u7387\u548c\u541e\u5410\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u8bed\u97f3\u8d28\u91cf\u3002", "motivation": "\u6807\u51c6\u63a8\u6d4b\u89e3\u7801\u5728\u8bed\u97f3\u751f\u6210\u4e2d\u8fc7\u4e8e\u4e25\u683c\uff0c\u8bb8\u591a\u58f0\u5b66\u6216\u8bed\u4e49\u53ef\u4e92\u6362\u7684\u79bb\u6563token\u88ab\u9519\u8bef\u62d2\u7edd\uff0c\u9650\u5236\u4e86\u52a0\u901f\u6548\u679c\u3002", "method": "\u63d0\u51fa\u539f\u5219\u6027\u7c97\u7c92\u5ea6\u65b9\u6cd5\uff0c\u57fa\u4e8e\u76ee\u6807\u6a21\u578b\u7684\u5d4c\u5165\u7a7a\u95f4\u5b9a\u4e49\u58f0\u5b66\u76f8\u4f3c\u6027\u7ec4\uff0c\u5728\u7ec4\u7ea7\u522b\u8fdb\u884c\u62d2\u7edd\u91c7\u6837\u9a8c\u8bc1\u3002", "result": "\u5728LibriTTS\u6570\u636e\u96c6\u4e0a\uff0cPCG\u76f8\u6bd4\u6807\u51c6\u63a8\u6d4b\u89e3\u7801\u548c\u5148\u524d\u8bed\u97f3\u7279\u5b9a\u677e\u5f1b\u65b9\u6cd5\uff0c\u63d0\u9ad8\u4e86\u63a5\u53d7\u7387\u548c\u541e\u5410\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u53ef\u61c2\u5ea6\u548c\u8bf4\u8bdd\u4eba\u76f8\u4f3c\u6027\u3002", "conclusion": "\u58f0\u5b66\u611f\u77e5\u7684\u7ec4\u7ea7\u522b\u63a5\u53d7\u662f\u52a0\u901f\u8bed\u97f3token\u751f\u6210\u540c\u65f6\u4fdd\u6301\u8bed\u97f3\u8d28\u91cf\u7684\u7b80\u5355\u901a\u7528\u65b9\u6cd5\u3002"}}
{"id": "2511.13933", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.13933", "abs": "https://arxiv.org/abs/2511.13933", "authors": ["Alireza Fadakar", "Andreas F. Molisch"], "title": "Stacked Intelligent Metasurfaces for Multicarrier Cognitive Radio ISAC", "comment": "13 pages, 14 figures", "summary": "The fusion of cognitive radio (CR) and integrated sensing and communication (ISAC), enabled by stacked intelligent metasurfaces (SIMs), offers a promising path for multi-functional programmable front ends in 6G and beyond. In this paper we propose a novel CR-ISAC framework that leverages an SIM integrated with the secondary base station (SB) to learn and realize optimal beampatterns that simultaneously (i) minimize the Bayesian Cram\u00e9r-Rao bound (BCRB) for localizing a secondary user equipment (SU) and (ii) limit averaged interference at primary user equipments (PUs) so that spectral efficiency loss is constrained, with the target of at most a few percent degradation. We propose an efficient alternating optimization-based algorithm to obtain the optimal end-to-end transmission response of the SIM for all orthogonal frequency division multiplexing (OFDM) subcarriers. Drawing an analogy between the layered SIM architecture and deep neural networks, we define a beampattern- matching loss, derive analytical gradients for backpropagation, and implement a learning-based optimization of the SIM coefficients using a mini-batch Adam optimizer. A complexity analysis is provided, and extensive numerical experiments are performed to evaluate the proposed CR-ISAC framework. The results show that the proposed SIM coefficient optimization methods attain near-optimal performance in terms of both the SU BCRB localization metric and the PUs average spectral efficiency when the SIM has a sufficient number of layers, and they substantially outperform traditional single-layer reconfigurable intelligent surface (RIS) designs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5806\u53e0\u667a\u80fd\u8d85\u8868\u9762(SIM)\u7684\u8ba4\u77e5\u65e0\u7ebf\u7535-\u96c6\u6210\u611f\u77e5\u901a\u4fe1(CR-ISAC)\u6846\u67b6\uff0c\u901a\u8fc7\u4f18\u5316SIM\u7cfb\u6570\u5b9e\u73b0\u540c\u65f6\u5b9a\u4f4d\u6b21\u7ea7\u7528\u6237\u8bbe\u5907(SU)\u548c\u9650\u5236\u5bf9\u4e3b\u7528\u6237\u8bbe\u5907(PU)\u7684\u5e72\u6270\u3002", "motivation": "\u878d\u5408\u8ba4\u77e5\u65e0\u7ebf\u7535\u548c\u96c6\u6210\u611f\u77e5\u901a\u4fe1\u6280\u672f\uff0c\u5229\u7528SIM\u5b9e\u73b06G\u53ca\u4ee5\u540e\u7684\u591a\u529f\u80fd\u53ef\u7f16\u7a0b\u524d\u7aef\uff0c\u540c\u65f6\u89e3\u51b3\u5b9a\u4f4d\u7cbe\u5ea6\u548c\u9891\u8c31\u6548\u7387\u95ee\u9898\u3002", "method": "\u91c7\u7528\u4ea4\u66ff\u4f18\u5316\u7b97\u6cd5\u83b7\u53d6SIM\u7684\u6700\u4f18\u7aef\u5230\u7aef\u4f20\u8f93\u54cd\u5e94\uff0c\u501f\u9274\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u601d\u60f3\u5b9a\u4e49\u6ce2\u675f\u6a21\u5f0f\u5339\u914d\u635f\u5931\uff0c\u4f7f\u7528\u5c0f\u6279\u91cfAdam\u4f18\u5316\u5668\u5b66\u4e60\u4f18\u5316SIM\u7cfb\u6570\u3002", "result": "\u5f53SIM\u5177\u6709\u8db3\u591f\u5c42\u6570\u65f6\uff0c\u6240\u63d0\u65b9\u6cd5\u5728SU\u5b9a\u4f4d\u7cbe\u5ea6\u548cPU\u9891\u8c31\u6548\u7387\u65b9\u9762\u8fbe\u5230\u63a5\u8fd1\u6700\u4f18\u6027\u80fd\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u5355\u5c42\u53ef\u91cd\u6784\u667a\u80fd\u8868\u9762\u8bbe\u8ba1\u3002", "conclusion": "\u57fa\u4e8eSIM\u7684CR-ISAC\u6846\u67b6\u80fd\u591f\u6709\u6548\u5e73\u8861\u5b9a\u4f4d\u7cbe\u5ea6\u548c\u5e72\u6270\u63a7\u5236\uff0c\u4e3a6G\u53ca\u4ee5\u540e\u7684\u901a\u4fe1\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.13936", "categories": ["cs.SD", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.13936", "abs": "https://arxiv.org/abs/2511.13936", "authors": ["Aaron Broukhim", "Yiran Shen", "Prithviraj Ammanabrolu", "Nadir Weibel"], "title": "Preference-Based Learning in Audio Applications: A Systematic Analysis", "comment": null, "summary": "Despite the parallel challenges that audio and text domains face in evaluating generative model outputs, preference learning remains remarkably underexplored in audio applications. Through a PRISMA-guided systematic review of approximately 500 papers, we find that only 30 (6%) apply preference learning to audio tasks. Our analysis reveals a field in transition: pre-2021 works focused on emotion recognition using traditional ranking methods (rankSVM), while post-2021 studies have pivoted toward generation tasks employing modern RLHF frameworks. We identify three critical patterns: (1) the emergence of multi-dimensional evaluation strategies combining synthetic, automated, and human preferences; (2) inconsistent alignment between traditional metrics (WER, PESQ) and human judgments across different contexts; and (3) convergence on multi-stage training pipelines that combine reward signals. Our findings suggest that while preference learning shows promise for audio, particularly in capturing subjective qualities like naturalness and musicality, the field requires standardized benchmarks, higher-quality datasets, and systematic investigation of how temporal factors unique to audio impact preference learning frameworks.", "AI": {"tldr": "\u97f3\u9891\u9886\u57df\u4e2d\u7684\u504f\u597d\u5b66\u4e60\u5e94\u7528\u4e25\u91cd\u4e0d\u8db3\uff0c\u4ec56%\u76f8\u5173\u8bba\u6587\u6d89\u53ca\u8be5\u6280\u672f\uff0c\u7814\u7a76\u91cd\u70b9\u4ece\u4f20\u7edf\u60c5\u611f\u8bc6\u522b\u8f6c\u5411\u73b0\u4ee3RLHF\u751f\u6210\u4efb\u52a1\u3002", "motivation": "\u5c3d\u7ba1\u97f3\u9891\u548c\u6587\u672c\u9886\u57df\u5728\u8bc4\u4f30\u751f\u6210\u6a21\u578b\u8f93\u51fa\u65f6\u9762\u4e34\u76f8\u4f3c\u7684\u6311\u6218\uff0c\u4f46\u504f\u597d\u5b66\u4e60\u5728\u97f3\u9891\u5e94\u7528\u4e2d\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u7d22\uff0c\u9700\u8981\u7cfb\u7edf\u6027\u5206\u6790\u5176\u53d1\u5c55\u73b0\u72b6\u548c\u6a21\u5f0f\u3002", "method": "\u901a\u8fc7PRISMA\u6307\u5bfc\u7684\u7cfb\u7edf\u6027\u7efc\u8ff0\uff0c\u5206\u6790\u4e86\u7ea6500\u7bc7\u8bba\u6587\uff0c\u8bc6\u522b\u4e8630\u7bc7\u5e94\u7528\u504f\u597d\u5b66\u4e60\u7684\u97f3\u9891\u7814\u7a76\uff0c\u5bf9\u6bd4\u4e86\u4e0d\u540c\u65f6\u671f\u7684\u65b9\u6cd5\u6f14\u53d8\u3002", "result": "\u53d1\u73b0\u4e09\u4e2a\u5173\u952e\u6a21\u5f0f\uff1a\u591a\u7ef4\u8bc4\u4f30\u7b56\u7565\u7684\u51fa\u73b0\u3001\u4f20\u7edf\u6307\u6807\u4e0e\u4eba\u7c7b\u5224\u65ad\u7684\u4e0d\u4e00\u81f4\u3001\u591a\u9636\u6bb5\u8bad\u7ec3\u7ba1\u9053\u7684\u8d8b\u540c\u3002\u504f\u597d\u5b66\u4e60\u5728\u6355\u6349\u97f3\u9891\u4e3b\u89c2\u8d28\u91cf\u65b9\u9762\u663e\u793a\u6f5c\u529b\u3002", "conclusion": "\u97f3\u9891\u504f\u597d\u5b66\u4e60\u9886\u57df\u9700\u8981\u6807\u51c6\u5316\u57fa\u51c6\u3001\u66f4\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\uff0c\u5e76\u7cfb\u7edf\u7814\u7a76\u97f3\u9891\u7279\u6709\u7684\u65f6\u95f4\u56e0\u7d20\u5982\u4f55\u5f71\u54cd\u504f\u597d\u5b66\u4e60\u6846\u67b6\u3002"}}
{"id": "2511.14138", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2511.14138", "abs": "https://arxiv.org/abs/2511.14138", "authors": ["Hojoon Ki", "Jongsuk Kim", "Minchan Kwon", "Junmo Kim"], "title": "FxSearcher: gradient-free text-driven audio transformation", "comment": null, "summary": "Achieving diverse and high-quality audio transformations from text prompts remains challenging, as existing methods are fundamentally constrained by their reliance on a limited set of differentiable audio effects. This paper proposes \\textbf{FxSearcher}, a novel gradient-free framework that discovers the optimal configuration of audio effects (FX) to transform a source signal according to a text prompt. Our method employs Bayesian Optimization and CLAP-based score function to perform this search efficiently. Furthermore, a guiding prompt is introduced to prevent undesirable artifacts and enhance human preference. To objectively evaluate our method, we propose an AI-based evaluation framework. The results demonstrate that the highest scores achieved by our method on these metrics align closely with human preferences. Demos are available at https://hojoonki.github.io/FxSearcher/", "AI": {"tldr": "FxSearcher\u662f\u4e00\u4e2a\u65e0\u9700\u68af\u5ea6\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u8d1d\u53f6\u65af\u4f18\u5316\u548cCLAP\u8bc4\u5206\u51fd\u6570\u5bfb\u627e\u97f3\u9891\u6548\u679c\u7684\u6700\u4f73\u914d\u7f6e\uff0c\u4ee5\u5b9e\u73b0\u57fa\u4e8e\u6587\u672c\u63d0\u793a\u7684\u97f3\u9891\u8f6c\u6362\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u6709\u9650\u7684\u97f3\u9891\u6548\u679c\uff0c\u96be\u4ee5\u5b9e\u73b0\u591a\u6837\u5316\u548c\u9ad8\u8d28\u91cf\u7684\u97f3\u9891\u8f6c\u6362\u3002", "method": "\u4f7f\u7528\u8d1d\u53f6\u65af\u4f18\u5316\u548cCLAP\u8bc4\u5206\u51fd\u6570\u641c\u7d22\u97f3\u9891\u6548\u679c\u914d\u7f6e\uff0c\u5f15\u5165\u5f15\u5bfc\u63d0\u793a\u9632\u6b62\u4f2a\u5f71\u5e76\u63d0\u5347\u4eba\u7c7b\u504f\u597d\u3002", "result": "\u8be5\u65b9\u6cd5\u5728AI\u8bc4\u4f30\u6846\u67b6\u4e2d\u83b7\u5f97\u7684\u6700\u9ad8\u5206\u6570\u4e0e\u4eba\u7c7b\u504f\u597d\u9ad8\u5ea6\u4e00\u81f4\u3002", "conclusion": "FxSearcher\u80fd\u591f\u6709\u6548\u5b9e\u73b0\u57fa\u4e8e\u6587\u672c\u63d0\u793a\u7684\u97f3\u9891\u8f6c\u6362\uff0c\u4e14\u7ed3\u679c\u7b26\u5408\u4eba\u7c7b\u504f\u597d\u3002"}}
{"id": "2511.14051", "categories": ["eess.SP", "cs.IT"], "pdf": "https://arxiv.org/pdf/2511.14051", "abs": "https://arxiv.org/abs/2511.14051", "authors": ["Xiang Chen", "Ming-Min Zhao", "An Liu", "Min Li", "Qingjiang Shi", "Min-Jian Zhao"], "title": "Cross-Sparsity-Enabled Multipath Perception via Structured Bayesian Inference for Multi-Target Estimation", "comment": "13 pages, 9 figures", "summary": "In this paper, we investigate a multi-target sensing system in multipath environment, where inter-target scattering gives rise to first-order reflected paths whose angles of departure (AoDs) and angles of arrival (AoAs) coincide with the direct-path angles of different targets. Unlike other multipath components, these first-order paths carry structural information that can be exploited as additional prior knowledge for target direction estimation. To exploit this property, we construct a sparse representation of the multi-target sensing channel and propose a novel cross sparsity structure under a three-layer hierarchical structured (3LHS) prior model, which leverages the first-order paths to enhance the prior probability of the direct paths and thereby improve the estimation accuracy. Building on this model, we propose a structured fast turbo variational Bayesian inference (SF-TVBI) algorithm, which integrates an efficient message-passing strategy to enable tractable probabilistic exchange within the cross sparsity, and a two-timescale update scheme to reduce the update frequency of the high-dimensional sparse vector. Simulation results demonstrate that leveraging the proposed cross sparsity structure is able to improve the target angle estimation accuracy substantially, and the SF-TVBI algorithm achieves estimation performance comparable to that of the Turbo-VBI, but with lower computational complexity.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u591a\u5f84\u73af\u5883\u4e2d\u76ee\u6807\u95f4\u6563\u5c04\u4ea7\u751f\u7684\u4e00\u9636\u53cd\u5c04\u8def\u5f84\u6765\u589e\u5f3a\u591a\u76ee\u6807\u89d2\u5ea6\u4f30\u8ba1\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u6784\u5efa\u4ea4\u53c9\u7a00\u758f\u7ed3\u6784\u548c\u5206\u5c42\u5148\u9a8c\u6a21\u578b\uff0c\u5f00\u53d1\u4e86\u4f4e\u590d\u6742\u5ea6\u7684\u53d8\u5206\u8d1d\u53f6\u65af\u63a8\u7406\u7b97\u6cd5\u3002", "motivation": "\u5728\u591a\u76ee\u6807\u611f\u77e5\u7cfb\u7edf\u7684\u591a\u5f84\u73af\u5883\u4e2d\uff0c\u76ee\u6807\u95f4\u6563\u5c04\u4ea7\u751f\u7684\u4e00\u9636\u53cd\u5c04\u8def\u5f84\u5305\u542b\u7ed3\u6784\u4fe1\u606f\uff0c\u8fd9\u4e9b\u8def\u5f84\u7684\u51fa\u53d1\u89d2\u548c\u5230\u8fbe\u89d2\u4e0e\u4e0d\u540c\u76ee\u6807\u7684\u76f4\u63a5\u8def\u5f84\u89d2\u5ea6\u91cd\u5408\uff0c\u53ef\u4f5c\u4e3a\u989d\u5916\u7684\u5148\u9a8c\u77e5\u8bc6\u6765\u63d0\u5347\u76ee\u6807\u65b9\u5411\u4f30\u8ba1\u7cbe\u5ea6\u3002", "method": "\u6784\u5efa\u591a\u76ee\u6807\u611f\u77e5\u4fe1\u9053\u7684\u7a00\u758f\u8868\u793a\uff0c\u63d0\u51fa\u4e09\u5c42\u5206\u5c42\u7ed3\u6784\u5316\u5148\u9a8c\u6a21\u578b\u4e0b\u7684\u4ea4\u53c9\u7a00\u758f\u7ed3\u6784\uff0c\u5229\u7528\u4e00\u9636\u8def\u5f84\u589e\u5f3a\u76f4\u63a5\u8def\u5f84\u7684\u5148\u9a8c\u6982\u7387\u3002\u5f00\u53d1\u7ed3\u6784\u5316\u5feb\u901fTurbo\u53d8\u5206\u8d1d\u53f6\u65af\u63a8\u7406\u7b97\u6cd5\uff0c\u96c6\u6210\u9ad8\u6548\u6d88\u606f\u4f20\u9012\u7b56\u7565\u548c\u53cc\u65f6\u95f4\u5c3a\u5ea6\u66f4\u65b0\u65b9\u6848\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u5229\u7528\u6240\u63d0\u51fa\u7684\u4ea4\u53c9\u7a00\u758f\u7ed3\u6784\u80fd\u663e\u8457\u63d0\u9ad8\u76ee\u6807\u89d2\u5ea6\u4f30\u8ba1\u7cbe\u5ea6\uff0cSF-TVBI\u7b97\u6cd5\u5728\u8fbe\u5230\u4e0eTurbo-VBI\u76f8\u5f53\u4f30\u8ba1\u6027\u80fd\u7684\u540c\u65f6\uff0c\u5177\u6709\u66f4\u4f4e\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "conclusion": "\u901a\u8fc7\u5229\u7528\u591a\u5f84\u73af\u5883\u4e2d\u7684\u4e00\u9636\u53cd\u5c04\u8def\u5f84\u7ed3\u6784\u4fe1\u606f\u6784\u5efa\u4ea4\u53c9\u7a00\u758f\u6a21\u578b\uff0c\u53ef\u4ee5\u6709\u6548\u63d0\u5347\u591a\u76ee\u6807\u89d2\u5ea6\u4f30\u8ba1\u6027\u80fd\uff0c\u540c\u65f6\u63d0\u51fa\u7684SF-TVBI\u7b97\u6cd5\u5728\u4fdd\u6301\u7cbe\u5ea6\u7684\u524d\u63d0\u4e0b\u964d\u4f4e\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6\u3002"}}
{"id": "2511.14250", "categories": ["cs.SD", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.14250", "abs": "https://arxiv.org/abs/2511.14250", "authors": ["Jonathan Yaffe", "Ben Maman", "Meinard M\u00fcller", "Amit H. Bermano"], "title": "Count The Notes: Histogram-Based Supervision for Automatic Music Transcription", "comment": "ISMIR 2025", "summary": "Automatic Music Transcription (AMT) converts audio recordings into symbolic musical representations. Training deep neural networks (DNNs) for AMT typically requires strongly aligned training pairs with precise frame-level annotations. Since creating such datasets is costly and impractical for many musical contexts, weakly aligned approaches using segment-level annotations have gained traction. However, existing methods often rely on Dynamic Time Warping (DTW) or soft alignment loss functions, both of which still require local semantic correspondences, making them error-prone and computationally expensive. In this article, we introduce CountEM, a novel AMT framework that eliminates the need for explicit local alignment by leveraging note event histograms as supervision, enabling lighter computations and greater flexibility. Using an Expectation-Maximization (EM) approach, CountEM iteratively refines predictions based solely on note occurrence counts, significantly reducing annotation efforts while maintaining high transcription accuracy. Experiments on piano, guitar, and multi-instrument datasets demonstrate that CountEM matches or surpasses existing weakly supervised methods, improving AMT's robustness, scalability, and efficiency. Our project page is available at https://yoni-yaffe.github.io/count-the-notes.", "AI": {"tldr": "CountEM\u662f\u4e00\u4e2a\u65b0\u7684\u81ea\u52a8\u97f3\u4e50\u8f6c\u5f55\u6846\u67b6\uff0c\u901a\u8fc7\u4f7f\u7528\u97f3\u7b26\u4e8b\u4ef6\u76f4\u65b9\u56fe\u4f5c\u4e3a\u76d1\u7763\uff0c\u65e0\u9700\u663e\u5f0f\u5c40\u90e8\u5bf9\u9f50\uff0c\u91c7\u7528\u671f\u671b\u6700\u5927\u5316\u65b9\u6cd5\u8fed\u4ee3\u4f18\u5316\u9884\u6d4b\uff0c\u663e\u8457\u51cf\u5c11\u6807\u6ce8\u5de5\u4f5c\u91cf\u5e76\u4fdd\u6301\u9ad8\u8f6c\u5f55\u7cbe\u5ea6\u3002", "motivation": "\u4f20\u7edf\u81ea\u52a8\u97f3\u4e50\u8f6c\u5f55\u9700\u8981\u7cbe\u786e\u7684\u5e27\u7ea7\u6807\u6ce8\uff0c\u6210\u672c\u9ad8\u6602\u4e14\u4e0d\u5b9e\u7528\u3002\u73b0\u6709\u7684\u5f31\u5bf9\u9f50\u65b9\u6cd5\u4ecd\u9700\u8981\u5c40\u90e8\u8bed\u4e49\u5bf9\u5e94\uff0c\u5bb9\u6613\u51fa\u9519\u4e14\u8ba1\u7b97\u6602\u8d35\u3002", "method": "\u4f7f\u7528\u97f3\u7b26\u4e8b\u4ef6\u76f4\u65b9\u56fe\u4f5c\u4e3a\u76d1\u7763\uff0c\u65e0\u9700\u663e\u5f0f\u5c40\u90e8\u5bf9\u9f50\uff0c\u91c7\u7528\u671f\u671b\u6700\u5927\u5316\u65b9\u6cd5\u8fed\u4ee3\u4f18\u5316\u9884\u6d4b\uff0c\u4ec5\u57fa\u4e8e\u97f3\u7b26\u51fa\u73b0\u6b21\u6570\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u5728\u94a2\u7434\u3001\u5409\u4ed6\u548c\u591a\u4e50\u5668\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCountEM\u5339\u914d\u6216\u8d85\u8d8a\u4e86\u73b0\u6709\u7684\u5f31\u76d1\u7763\u65b9\u6cd5\uff0c\u63d0\u9ad8\u4e86\u81ea\u52a8\u97f3\u4e50\u8f6c\u5f55\u7684\u9c81\u68d2\u6027\u3001\u53ef\u6269\u5c55\u6027\u548c\u6548\u7387\u3002", "conclusion": "CountEM\u901a\u8fc7\u6d88\u9664\u663e\u5f0f\u5c40\u90e8\u5bf9\u9f50\u9700\u6c42\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u6807\u6ce8\u5de5\u4f5c\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u8f6c\u5f55\u7cbe\u5ea6\uff0c\u4e3a\u81ea\u52a8\u97f3\u4e50\u8f6c\u5f55\u63d0\u4f9b\u4e86\u66f4\u5b9e\u7528\u548c\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.14410", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2511.14410", "abs": "https://arxiv.org/abs/2511.14410", "authors": ["Wei Liu", "Jiahong Li", "Yiwen Shao", "Dong Yu"], "title": "TTA: Transcribe, Translate and Alignment for Cross-lingual Speech Representation", "comment": "Submitted to ICASSP2026", "summary": "Speech-LLM models have demonstrated great performance in multi-modal and multi-task speech understanding. A typical speech-LLM paradigm is integrating speech modality with a large language model (LLM). While the Whisper encoder was frequently adopted in previous studies for speech input, it shows limitations regarding input format, model scale, and semantic performance. To this end, we propose a lightweight TTA model specialized in speech semantics for more effective LLM integration. With large-scale training of 358k hours of speech data on multilingual speech recognition (ASR), speech translation (ST) and speech-text alignment tasks, TTA is capable of producing robust cross-lingual speech representations. Extensive evaluations across diverse benchmarks, including ASR/ST, speech retrieval, and ASR-LLM performance assessments, demonstrate TTA's superiority over Whisper. Furthermore, we rigorously validate the interplay between cross-lingual capabilities and ASR/ST performance. The model weights and training recipes of TTA will be released as part of an audio understanding toolkit Auden.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u8f7b\u91cf\u7ea7TTA\u6a21\u578b\uff0c\u4e13\u95e8\u7528\u4e8e\u8bed\u97f3\u8bed\u4e49\u7406\u89e3\uff0c\u4ee5\u66f4\u6709\u6548\u5730\u4e0eLLM\u96c6\u6210\uff0c\u5728\u591a\u9879\u4efb\u52a1\u4e0a\u8d85\u8d8aWhisper\u7f16\u7801\u5668\u3002", "motivation": "\u73b0\u6709\u7684Speech-LLM\u6a21\u578b\u901a\u5e38\u4f7f\u7528Whisper\u7f16\u7801\u5668\u5904\u7406\u8bed\u97f3\u8f93\u5165\uff0c\u4f46\u5b58\u5728\u8f93\u5165\u683c\u5f0f\u9650\u5236\u3001\u6a21\u578b\u89c4\u6a21\u95ee\u9898\u548c\u8bed\u4e49\u6027\u80fd\u4e0d\u8db3\u7b49\u5c40\u9650\u6027\u3002", "method": "\u901a\u8fc7358k\u5c0f\u65f6\u591a\u8bed\u8a00\u8bed\u97f3\u6570\u636e\u7684\u5927\u89c4\u6a21\u8bad\u7ec3\uff0c\u5728\u8bed\u97f3\u8bc6\u522b\u3001\u8bed\u97f3\u7ffb\u8bd1\u548c\u8bed\u97f3\u6587\u672c\u5bf9\u9f50\u4efb\u52a1\u4e0a\u8bad\u7ec3TTA\u6a21\u578b\uff0c\u751f\u6210\u9c81\u68d2\u7684\u8de8\u8bed\u8a00\u8bed\u97f3\u8868\u793a\u3002", "result": "\u5728ASR/ST\u3001\u8bed\u97f3\u68c0\u7d22\u548cASR-LLM\u6027\u80fd\u8bc4\u4f30\u7b49\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cTTA\u8868\u73b0\u51fa\u4f18\u4e8eWhisper\u7684\u6027\u80fd\uff0c\u5e76\u9a8c\u8bc1\u4e86\u8de8\u8bed\u8a00\u80fd\u529b\u4e0eASR/ST\u6027\u80fd\u7684\u76f8\u4e92\u4f5c\u7528\u3002", "conclusion": "TTA\u6a21\u578b\u4e3a\u8bed\u97f3\u7406\u89e3\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684LLM\u96c6\u6210\u65b9\u6848\uff0c\u6a21\u578b\u6743\u91cd\u548c\u8bad\u7ec3\u65b9\u6cd5\u5c06\u4f5c\u4e3aAuden\u97f3\u9891\u7406\u89e3\u5de5\u5177\u5305\u7684\u4e00\u90e8\u5206\u53d1\u5e03\u3002"}}
{"id": "2511.14104", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.14104", "abs": "https://arxiv.org/abs/2511.14104", "authors": ["Lehuai Xu", "Zirui Lu", "Haoran Yang", "Yina Zhou"], "title": "Lightweight Multi-task CNN for ECG Diagnosis with GRU-Diffusion", "comment": "15 pages, 5 figures", "summary": "With the increasing demand for real-time Electrocardiogram (ECG) classification on edge devices, existing models face challenges of high computational cost and limited accuracy on imbalanced datasets.This paper presents Multi-task DFNet, a lightweight multi-task framework for ECG classification across the MIT-BIH Arrhythmia Database and the PTB Diagnostic ECG Database, enabling efficient task collaboration by dynamically sharing knowledge across tasks, such as arrhythmia detection, myocardial infarction (MI) classification, and other cardiovascular abnormalities. The proposed method integrates GRU-augmented Diffusion, where the GRU is embedded within the diffusion model to capture temporal dependencies better and generate high-quality synthetic signals for imbalanced classes. The experimental results show that Multi-task DFNet achieves 99.72% and 99.89% accuracy on the MIT-BIH dataset and PTB dataset, respectively, with significantly fewer parameters compared to traditional models, making it suitable for deployment on wearable ECG monitors. This work offers a compact and efficient solution for multi-task ECG diagnosis, providing a promising potential for edge healthcare applications on resource-constrained devices.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u591a\u4efb\u52a1ECG\u5206\u7c7b\u6846\u67b6Multi-task DFNet\uff0c\u901a\u8fc7\u52a8\u6001\u5171\u4eab\u77e5\u8bc6\u5b9e\u73b0\u4efb\u52a1\u534f\u4f5c\uff0c\u7ed3\u5408GRU\u589e\u5f3a\u7684\u6269\u6563\u6a21\u578b\u751f\u6210\u9ad8\u8d28\u91cf\u5408\u6210\u4fe1\u53f7\uff0c\u5728MIT-BIH\u548cPTB\u6570\u636e\u96c6\u4e0a\u5206\u522b\u8fbe\u523099.72%\u548c99.89%\u7684\u51c6\u786e\u7387\uff0c\u53c2\u6570\u66f4\u5c11\u9002\u5408\u8fb9\u7f18\u8bbe\u5907\u90e8\u7f72\u3002", "motivation": "\u73b0\u6709ECG\u5206\u7c7b\u6a21\u578b\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u5b58\u5728\u8ba1\u7b97\u6210\u672c\u9ad8\u3001\u5728\u975e\u5e73\u8861\u6570\u636e\u96c6\u4e0a\u51c6\u786e\u7387\u6709\u9650\u7684\u95ee\u9898\uff0c\u9700\u8981\u5f00\u53d1\u8f7b\u91cf\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u591a\u4efb\u52a1\u6846\u67b6\u52a8\u6001\u5171\u4eab\u77e5\u8bc6\uff0c\u96c6\u6210GRU\u589e\u5f3a\u7684\u6269\u6563\u6a21\u578b\u6765\u6355\u83b7\u65f6\u95f4\u4f9d\u8d56\u5173\u7cfb\u5e76\u751f\u6210\u9ad8\u8d28\u91cf\u5408\u6210\u4fe1\u53f7\uff0c\u5904\u7406\u975e\u5e73\u8861\u7c7b\u522b\u3002", "result": "\u5728MIT-BIH\u6570\u636e\u96c6\u4e0a\u8fbe\u523099.72%\u51c6\u786e\u7387\uff0cPTB\u6570\u636e\u96c6\u4e0a\u8fbe\u523099.89%\u51c6\u786e\u7387\uff0c\u53c2\u6570\u663e\u8457\u5c11\u4e8e\u4f20\u7edf\u6a21\u578b\u3002", "conclusion": "\u4e3a\u591a\u4efb\u52a1ECG\u8bca\u65ad\u63d0\u4f9b\u4e86\u7d27\u51d1\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u4e0a\u5177\u6709\u5e7f\u9614\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2511.14293", "categories": ["cs.SD", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.14293", "abs": "https://arxiv.org/abs/2511.14293", "authors": ["Marcel Gibier", "Rapha\u00ebl Duroselle", "Pierre Serrano", "Olivier Boeffard", "Jean-Fran\u00e7ois Bonastre"], "title": "Segmentwise Pruning in Audio-Language Models", "comment": "Submitted to ICASSP 2026 (under review)", "summary": "Recent audio-language models have shown impressive performance across a wide range of audio tasks and are increasingly capable of handling long audio inputs. However, the computing costs in these models heavily depend on sequence length, which can become very large given the nature of audio data. In the vision-language domain, token pruning methods have proven effective in reducing token counts while preserving strong performance on standard benchmarks. In this work, we investigate the relevance and effectiveness of such token selection strategies in the context of audio-language models. We also improve them by proposing a lightweight strategy that takes the time dimension into account. While retaining only a quarter of the initial tokens, our approach results in a relative maximum decrease of 2% in CIDEr on Clotho v2 and a relative maximum decrease of 4% in accuracy on MMAU.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u97f3\u9891-\u8bed\u8a00\u6a21\u578b\u4e2d\u7684token\u526a\u679d\u7b56\u7565\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u8003\u8651\u65f6\u95f4\u7ef4\u5ea6\u7684\u8f7b\u91cf\u7ea7\u65b9\u6cd5\uff0c\u4ec5\u4fdd\u7559\u56db\u5206\u4e4b\u4e00\u521d\u59cbtoken\u65f6\u6027\u80fd\u4e0b\u964d\u5f88\u5c0f\u3002", "motivation": "\u97f3\u9891-\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u957f\u97f3\u9891\u8f93\u5165\u65f6\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\uff0c\u800c\u89c6\u89c9-\u8bed\u8a00\u9886\u57df\u4e2d\u7684token\u526a\u679d\u65b9\u6cd5\u5df2\u88ab\u8bc1\u660e\u6709\u6548\uff0c\u56e0\u6b64\u7814\u7a76\u5176\u5728\u97f3\u9891\u9886\u57df\u7684\u9002\u7528\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8003\u8651\u65f6\u95f4\u7ef4\u5ea6\u7684\u8f7b\u91cf\u7ea7token\u9009\u62e9\u7b56\u7565\uff0c\u6539\u8fdb\u73b0\u6709\u7684token\u526a\u679d\u65b9\u6cd5\u3002", "result": "\u4ec5\u4fdd\u7559\u56db\u5206\u4e4b\u4e00\u521d\u59cbtoken\u65f6\uff0c\u5728Clotho v2\u4e0a\u7684CIDEr\u6307\u6807\u76f8\u5bf9\u6700\u5927\u4e0b\u964d2%\uff0c\u5728MMAU\u4e0a\u7684\u51c6\u786e\u7387\u76f8\u5bf9\u6700\u5927\u4e0b\u964d4%\u3002", "conclusion": "token\u526a\u679d\u7b56\u7565\u5728\u97f3\u9891-\u8bed\u8a00\u6a21\u578b\u4e2d\u540c\u6837\u6709\u6548\uff0c\u63d0\u51fa\u7684\u65f6\u95f4\u611f\u77e5\u65b9\u6cd5\u80fd\u591f\u663e\u8457\u51cf\u5c11\u8ba1\u7b97\u6210\u672c\u540c\u65f6\u4fdd\u6301\u826f\u597d\u6027\u80fd\u3002"}}
{"id": "2511.14110", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.14110", "abs": "https://arxiv.org/abs/2511.14110", "authors": ["Sithmini Ranasingha", "Agasthi Haputhanthri", "Hansa Marasinghe", "Nima Wickramasinghe", "Kithmin Wickremasinghe", "Jithangi Wanigasinghe", "Chamira U. S. Edussooriya", "Joshua P. Kulasingham"], "title": "A Patient-Independent Neonatal Seizure Prediction Model Using Reduced Montage EEG and ECG", "comment": "10 pages, 4 figures", "summary": "Neonates are highly susceptible to seizures, often leading to short or long-term neurological impairments. However, clinical manifestations of neonatal seizures are subtle and often lead to misdiagnoses. This increases the risk of prolonged, untreated seizure activity and subsequent brain injury. Continuous video electroencephalogram (cEEG) monitoring is the gold standard for seizure detection. However, this is an expensive evaluation that requires expertise and time. In this study, we propose a convolutional neural network-based model for early prediction of neonatal seizures by distinguishing between interictal and preictal states of the EEG. Our model is patient-independent, enabling generalization across multiple subjects, and utilizes mel-frequency cepstral coefficient matrices extracted from multichannel EEG and electrocardiogram (ECG) signals as input features. Trained and validated on the Helsinki neonatal EEG dataset with 10-fold cross-validation, the proposed model achieved an average accuracy of 97.52%, sensitivity of 98.31%, specificity of 96.39%, and F1-score of 97.95%, enabling accurate seizure prediction up to 30 minutes before onset. The inclusion of ECG alongside EEG improved the F1-score by 1.42%, while the incorporation of an attention mechanism yielded an additional 0.5% improvement. To enhance transparency, we incorporated SHapley Additive exPlanations (SHAP) as an explainable artificial intelligence method to interpret the model and provided localization of seizure focus using scalp plots. The overall results demonstrate the model's potential for minimally supervised deployment in neonatal intensive care units, enabling timely and reliable prediction of neonatal seizures, while demonstrating strong generalization capability across unseen subjects through transfer learning.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u7684\u6a21\u578b\uff0c\u901a\u8fc7\u533a\u5206EEG\u7684\u53d1\u4f5c\u95f4\u671f\u548c\u53d1\u4f5c\u524d\u671f\u72b6\u6001\u6765\u65e9\u671f\u9884\u6d4b\u65b0\u751f\u513f\u766b\u75eb\u53d1\u4f5c\u3002\u8be5\u6a21\u578b\u4f7f\u7528\u4ece\u591a\u901a\u9053EEG\u548cECG\u4fe1\u53f7\u63d0\u53d6\u7684\u6885\u5c14\u9891\u7387\u5012\u8c31\u7cfb\u6570\u77e9\u9635\u4f5c\u4e3a\u8f93\u5165\u7279\u5f81\uff0c\u5728\u8d6b\u5c14\u8f9b\u57fa\u65b0\u751f\u513fEEG\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e8697.52%\u7684\u5e73\u5747\u51c6\u786e\u7387\uff0c\u80fd\u591f\u5728\u766b\u75eb\u53d1\u4f5c\u524d30\u5206\u949f\u8fdb\u884c\u51c6\u786e\u9884\u6d4b\u3002", "motivation": "\u65b0\u751f\u513f\u5bf9\u766b\u75eb\u53d1\u4f5c\u9ad8\u5ea6\u654f\u611f\uff0c\u5e38\u5bfc\u81f4\u795e\u7ecf\u529f\u80fd\u635f\u4f24\u3002\u4f46\u65b0\u751f\u513f\u766b\u75eb\u7684\u4e34\u5e8a\u8868\u73b0\u5fae\u5999\uff0c\u5bb9\u6613\u8bef\u8bca\uff0c\u589e\u52a0\u4e86\u672a\u6cbb\u7597\u766b\u75eb\u6d3b\u52a8\u548c\u540e\u7eed\u8111\u635f\u4f24\u7684\u98ce\u9669\u3002\u867d\u7136\u8fde\u7eed\u89c6\u9891\u8111\u7535\u56fe\u76d1\u6d4b\u662f\u766b\u75eb\u68c0\u6d4b\u7684\u91d1\u6807\u51c6\uff0c\u4f46\u8fd9\u79cd\u65b9\u6cd5\u6602\u8d35\u4e14\u9700\u8981\u4e13\u4e1a\u77e5\u8bc6\u548c\u65f6\u95f4\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u7684\u6a21\u578b\uff0c\u4f7f\u7528\u4ece\u591a\u901a\u9053EEG\u548cECG\u4fe1\u53f7\u63d0\u53d6\u7684\u6885\u5c14\u9891\u7387\u5012\u8c31\u7cfb\u6570\u77e9\u9635\u4f5c\u4e3a\u8f93\u5165\u7279\u5f81\u3002\u6a21\u578b\u91c7\u7528\u60a3\u8005\u72ec\u7acb\u8bbe\u8ba1\uff0c\u80fd\u591f\u5728\u591a\u4e2a\u53d7\u8bd5\u8005\u95f4\u6cdb\u5316\u3002\u8bad\u7ec3\u548c\u9a8c\u8bc1\u4f7f\u7528\u8d6b\u5c14\u8f9b\u57fa\u65b0\u751f\u513fEEG\u6570\u636e\u96c6\uff0c\u91c7\u752810\u6298\u4ea4\u53c9\u9a8c\u8bc1\uff0c\u5e76\u6574\u5408\u6ce8\u610f\u529b\u673a\u5236\u548cSHAP\u53ef\u89e3\u91caAI\u65b9\u6cd5\u3002", "result": "\u6a21\u578b\u5e73\u5747\u51c6\u786e\u738797.52%\uff0c\u654f\u611f\u602798.31%\uff0c\u7279\u5f02\u602796.39%\uff0cF1\u5206\u657097.95%\u3002\u7ed3\u5408ECG\u4f7fF1\u5206\u6570\u63d0\u9ad81.42%\uff0c\u6ce8\u610f\u529b\u673a\u5236\u989d\u5916\u63d0\u9ad80.5%\u3002\u80fd\u591f\u51c6\u786e\u9884\u6d4b\u766b\u75eb\u53d1\u4f5c\u524d30\u5206\u949f\u3002", "conclusion": "\u8be5\u6a21\u578b\u5728\u65b0\u751f\u513f\u91cd\u75c7\u76d1\u62a4\u5ba4\u5177\u6709\u6700\u5c0f\u76d1\u7763\u90e8\u7f72\u7684\u6f5c\u529b\uff0c\u80fd\u591f\u53ca\u65f6\u53ef\u9760\u5730\u9884\u6d4b\u65b0\u751f\u513f\u766b\u75eb\u53d1\u4f5c\uff0c\u5e76\u901a\u8fc7\u8fc1\u79fb\u5b66\u4e60\u5728\u672a\u89c1\u8fc7\u7684\u53d7\u8bd5\u8005\u4e0a\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2511.14307", "categories": ["cs.SD", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.14307", "abs": "https://arxiv.org/abs/2511.14307", "authors": ["Marcel Gibier", "Nolwenn Celton", "Rapha\u00ebl Duroselle", "Pierre Serrano", "Olivier Boeffard", "Jean-Fran\u00e7ois Bonastre"], "title": "Audio Question Answering with GRPO-Based Fine-Tuning and Calibrated Segment-Level Predictions", "comment": "Submission to Track 5 of the DCASE 2025 Challenge", "summary": "In this report, we describe our submission to Track 5 of the DCASE 2025 Challenge for the task of Audio Question Answering(AQA). Our system leverages the SSL backbone BEATs to extract frame-level audio features, which are then processed by a classification head to generate segment-level predictions of acoustic events, following the Audioset ontology. These segment-level predictions are subsequently calibrated before producing event-level predictions. Finally, these predictions are incorporated into a structured prompt, along with the question and candidate answers. This prompt is then fed to a fine-tuned version of Qwen2.5-7B-Instruct, trained using the GRPO algorithm with a simple reward function. Our method achieves an accuracy of 62.6 % on the development set, demonstrating the effectiveness of combining acoustic event reasoning with instruction-tuned large language models for AQA.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u97f3\u9891\u95ee\u7b54\u7cfb\u7edf\uff0c\u7ed3\u5408BEATs\u97f3\u9891\u7279\u5f81\u63d0\u53d6\u3001\u58f0\u5b66\u4e8b\u4ef6\u5206\u7c7b\u548c\u5fae\u8c03\u7684Qwen2.5-7B\u8bed\u8a00\u6a21\u578b\uff0c\u5728DCASE 2025\u6311\u6218\u8d5b\u4e2d\u53d6\u5f9762.6%\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u63a2\u7d22\u5982\u4f55\u5c06\u58f0\u5b66\u4e8b\u4ef6\u63a8\u7406\u4e0e\u6307\u4ee4\u8c03\u4f18\u7684\u5927\u8bed\u8a00\u6a21\u578b\u76f8\u7ed3\u5408\uff0c\u4ee5\u89e3\u51b3\u97f3\u9891\u95ee\u7b54\u4efb\u52a1\uff0c\u63d0\u9ad8\u7cfb\u7edf\u6027\u80fd\u3002", "method": "\u4f7f\u7528BEATs\u63d0\u53d6\u5e27\u7ea7\u97f3\u9891\u7279\u5f81\uff0c\u901a\u8fc7\u5206\u7c7b\u5934\u751f\u6210\u6bb5\u7ea7\u58f0\u5b66\u4e8b\u4ef6\u9884\u6d4b\uff0c\u7ecf\u6821\u51c6\u540e\u7ed3\u5408\u95ee\u9898\u548c\u5019\u9009\u7b54\u6848\u6784\u5efa\u7ed3\u6784\u5316\u63d0\u793a\uff0c\u8f93\u5165\u5230GRPO\u7b97\u6cd5\u5fae\u8c03\u7684Qwen2.5-7B\u6a21\u578b\u4e2d\u8fdb\u884c\u63a8\u7406\u3002", "result": "\u5728\u5f00\u53d1\u96c6\u4e0a\u8fbe\u523062.6%\u7684\u51c6\u786e\u7387\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u5728\u97f3\u9891\u95ee\u7b54\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u58f0\u5b66\u4e8b\u4ef6\u63a8\u7406\u4e0e\u6307\u4ee4\u8c03\u4f18\u5927\u8bed\u8a00\u6a21\u578b\u7684\u7ed3\u5408\u662f\u89e3\u51b3\u97f3\u9891\u95ee\u7b54\u4efb\u52a1\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u5728DCASE 2025\u6311\u6218\u8d5b\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2511.14421", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.14421", "abs": "https://arxiv.org/abs/2511.14421", "authors": ["Ali Hanif", "Yuchen Zhang", "Pinjun Zheng", "Tareq Y. Al-Naffouri"], "title": "Integrated Positioning and Communication for Cooperative Multi-LEO Uplink Communications: A Dual-Timescale Kalman Filter-Aided Approach", "comment": null, "summary": "Low Earth orbit (LEO) satellites are a crucial component of the future non-terrestrial networks (NTN) due to lower latency, robust signal strengths, shorter revisit times, and dense constellations. However, acquiring reliable channel state information (CSI) in LEO satellite communication remains challenging owing to severe signal attenuation over long propagation distances and short coherence times. Despite these challenges, LEO channels benefit from pronounced line-of-sight dominance and geometric properties inherently tied to positioning information. In this work, we propose an integrated positioning and communication (IPAC) framework for multi-LEO satellite networks to address the unique challenges posed by LEO channels. Specifically, we leverage in-the-loop LEO positioning to exploit users' position information for improving uplink CSI acquisition. To overcome the link-budget limitations of single-satellite systems, cooperative multi-LEO uplink data detection is adopted. By exploiting the different coherent timescales of position-related parameters and random channel gains, we develop a dual-timescale Kalman filter-based IPAC framework: an unscented Kalman filter (UKF) for tracking users' position and velocity in the large-timescale, and a Kalman filter that leverages the position information obtained in the large-timescale for improved data-aided uplink channel estimation in the small-timescale. Finally, the two tasks of channel estimation and cooperative data detection are jointly addressed through the expectation maximization (EM) algorithm. Numerical results demonstrate that the proposed IPAC approach outperforms the conventional baseline in terms of channel estimation accuracy and communication performance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u96c6\u6210\u5b9a\u4f4d\u4e0e\u901a\u4fe1(IPAC)\u6846\u67b6\uff0c\u5229\u7528LEO\u536b\u661f\u5b9a\u4f4d\u4fe1\u606f\u6539\u8fdb\u4e0a\u884c\u94fe\u8defCSI\u83b7\u53d6\uff0c\u901a\u8fc7\u53cc\u65f6\u95f4\u5c3a\u5ea6\u5361\u5c14\u66fc\u6ee4\u6ce2\u548c\u671f\u671b\u6700\u5927\u5316\u7b97\u6cd5\u63d0\u5347\u4fe1\u9053\u4f30\u8ba1\u7cbe\u5ea6\u548c\u901a\u4fe1\u6027\u80fd\u3002", "motivation": "LEO\u536b\u661f\u901a\u4fe1\u9762\u4e34\u4fe1\u9053\u72b6\u6001\u4fe1\u606f\u83b7\u53d6\u56f0\u96be\u7684\u95ee\u9898\uff0c\u4f46LEO\u4fe1\u9053\u5177\u6709\u89c6\u8ddd\u4e3b\u5bfc\u7279\u6027\u548c\u4e0e\u5b9a\u4f4d\u4fe1\u606f\u76f8\u5173\u7684\u51e0\u4f55\u7279\u6027\uff0c\u8fd9\u4e3a\u5229\u7528\u5b9a\u4f4d\u4fe1\u606f\u6539\u8fdb\u901a\u4fe1\u63d0\u4f9b\u4e86\u673a\u4f1a\u3002", "method": "\u91c7\u7528\u53cc\u65f6\u95f4\u5c3a\u5ea6\u5361\u5c14\u66fc\u6ee4\u6ce2\u6846\u67b6\uff1a\u5927\u65f6\u95f4\u5c3a\u5ea6\u4f7f\u7528\u65e0\u8ff9\u5361\u5c14\u66fc\u6ee4\u6ce2\u8ddf\u8e2a\u7528\u6237\u4f4d\u7f6e\u548c\u901f\u5ea6\uff0c\u5c0f\u65f6\u95f4\u5c3a\u5ea6\u5229\u7528\u4f4d\u7f6e\u4fe1\u606f\u8fdb\u884c\u6570\u636e\u8f85\u52a9\u4e0a\u884c\u4fe1\u9053\u4f30\u8ba1\uff0c\u5e76\u901a\u8fc7\u671f\u671b\u6700\u5927\u5316\u7b97\u6cd5\u8054\u5408\u5904\u7406\u4fe1\u9053\u4f30\u8ba1\u548c\u534f\u4f5c\u6570\u636e\u68c0\u6d4b\u3002", "result": "\u6570\u503c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684IPAC\u65b9\u6cd5\u5728\u4fe1\u9053\u4f30\u8ba1\u7cbe\u5ea6\u548c\u901a\u4fe1\u6027\u80fd\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u96c6\u6210\u5b9a\u4f4d\u4e0e\u901a\u4fe1\u6846\u67b6\u80fd\u591f\u6709\u6548\u5229\u7528LEO\u536b\u661f\u7f51\u7edc\u7684\u51e0\u4f55\u7279\u6027\uff0c\u901a\u8fc7\u4f4d\u7f6e\u4fe1\u606f\u663e\u8457\u63d0\u5347\u4e0a\u884c\u94fe\u8def\u4fe1\u9053\u4f30\u8ba1\u548c\u901a\u4fe1\u6027\u80fd\u3002"}}
{"id": "2511.14515", "categories": ["cs.SD", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.14515", "abs": "https://arxiv.org/abs/2511.14515", "authors": ["Xinxin Tang", "Bin Qin", "Yufang Li"], "title": "IMSE: Efficient U-Net-based Speech Enhancement using Inception Depthwise Convolution and Amplitude-Aware Linear Attention", "comment": null, "summary": "Achieving a balance between lightweight design and high performance remains a significant challenge for speech enhancement (SE) tasks on resource-constrained devices. Existing state-of-the-art methods, such as MUSE, have established a strong baseline with only 0.51M parameters by introducing a Multi-path Enhanced Taylor (MET) transformer and Deformable Embedding (DE). However, an in-depth analysis reveals that MUSE still suffers from efficiency bottlenecks: the MET module relies on a complex \"approximate-compensate\" mechanism to mitigate the limitations of Taylor-expansion-based attention, while the offset calculation for deformable embedding introduces additional computational burden. This paper proposes IMSE, a systematically optimized and ultra-lightweight network. We introduce two core innovations: 1) Replacing the MET module with Amplitude-Aware Linear Attention (MALA). MALA fundamentally rectifies the \"amplitude-ignoring\" problem in linear attention by explicitly preserving the norm information of query vectors in the attention calculation, achieving efficient global modeling without an auxiliary compensation branch. 2) Replacing the DE module with Inception Depthwise Convolution (IDConv). IDConv borrows the Inception concept, decomposing large-kernel operations into efficient parallel branches (square, horizontal, and vertical strips), thereby capturing spectrogram features with extremely low parameter redundancy. Extensive experiments on the VoiceBank+DEMAND dataset demonstrate that, compared to the MUSE baseline, IMSE significantly reduces the parameter count by 16.8\\% (from 0.513M to 0.427M) while achieving competitive performance comparable to the state-of-the-art on the PESQ metric (3.373). This study sets a new benchmark for the trade-off between model size and speech quality in ultra-lightweight speech enhancement.", "AI": {"tldr": "IMSE\u63d0\u51fa\u4e86\u4e00\u79cd\u8d85\u8f7b\u91cf\u7ea7\u8bed\u97f3\u589e\u5f3a\u7f51\u7edc\uff0c\u901a\u8fc7\u632f\u5e45\u611f\u77e5\u7ebf\u6027\u6ce8\u610f\u529b(MALA)\u548cInception\u6df1\u5ea6\u5377\u79ef(IDConv)\u66ff\u4ee3MUSE\u4e2d\u7684\u590d\u6742\u6a21\u5757\uff0c\u5728\u663e\u8457\u51cf\u5c1116.8%\u53c2\u6570\u7684\u540c\u65f6\u4fdd\u6301\u7ade\u4e89\u6027\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6700\u8f7b\u91cf\u7ea7\u65b9\u6cd5MUSE\u4ecd\u5b58\u5728\u6548\u7387\u74f6\u9888\uff1aMET\u6a21\u5757\u4f9d\u8d56\u590d\u6742\u7684\"\u8fd1\u4f3c-\u8865\u507f\"\u673a\u5236\uff0c\u53ef\u53d8\u5f62\u5d4c\u5165\u7684\u504f\u79fb\u8ba1\u7b97\u5e26\u6765\u989d\u5916\u8ba1\u7b97\u8d1f\u62c5\u3002\u9700\u8981\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u5b9e\u73b0\u8f7b\u91cf\u8bbe\u8ba1\u548c\u9ad8\u6027\u80fd\u7684\u66f4\u597d\u5e73\u8861\u3002", "method": "1) \u7528MALA\u66ff\u4ee3MET\u6a21\u5757\uff1a\u901a\u8fc7\u663e\u5f0f\u4fdd\u7559\u67e5\u8be2\u5411\u91cf\u8303\u6570\u4fe1\u606f\u89e3\u51b3\u7ebf\u6027\u6ce8\u610f\u529b\u4e2d\u7684\"\u632f\u5e45\u5ffd\u7565\"\u95ee\u9898\uff0c\u65e0\u9700\u8f85\u52a9\u8865\u507f\u5206\u652f\u5373\u53ef\u5b9e\u73b0\u9ad8\u6548\u5168\u5c40\u5efa\u6a21\uff1b2) \u7528IDConv\u66ff\u4ee3DE\u6a21\u5757\uff1a\u501f\u9274Inception\u6982\u5ff5\uff0c\u5c06\u5927\u6838\u64cd\u4f5c\u5206\u89e3\u4e3a\u9ad8\u6548\u5e76\u884c\u5206\u652f\uff08\u65b9\u5f62\u3001\u6c34\u5e73\u3001\u5782\u76f4\u6761\u5e26\uff09\uff0c\u4ee5\u6781\u4f4e\u53c2\u6570\u5197\u4f59\u6355\u83b7\u9891\u8c31\u56fe\u7279\u5f81\u3002", "result": "\u5728VoiceBank+DEMAND\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u76f8\u6bd4MUSE\u57fa\u7ebf\uff0cIMSE\u663e\u8457\u51cf\u5c1116.8%\u53c2\u6570\uff08\u4ece0.513M\u964d\u81f30.427M\uff09\uff0c\u540c\u65f6\u5728PESQ\u6307\u6807\u4e0a\u8fbe\u5230\u4e0e\u6700\u5148\u8fdb\u65b9\u6cd5\u76f8\u5f53\u7684\u7ade\u4e89\u6027\u6027\u80fd\uff083.373\uff09\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u8d85\u8f7b\u91cf\u7ea7\u8bed\u97f3\u589e\u5f3a\u4e2d\u6a21\u578b\u5927\u5c0f\u4e0e\u8bed\u97f3\u8d28\u91cf\u4e4b\u95f4\u7684\u6743\u8861\u8bbe\u5b9a\u4e86\u65b0\u57fa\u51c6\uff0c\u8bc1\u660e\u4e86\u901a\u8fc7\u7cfb\u7edf\u4f18\u5316\u53ef\u4ee5\u5728\u663e\u8457\u51cf\u5c11\u53c2\u6570\u7684\u540c\u65f6\u4fdd\u6301\u9ad8\u6027\u80fd\u3002"}}
{"id": "2511.14490", "categories": ["eess.SP", "cs.IT"], "pdf": "https://arxiv.org/pdf/2511.14490", "abs": "https://arxiv.org/abs/2511.14490", "authors": ["Junyuan Gao", "Weifeng Zhu", "Yanmo Hu", "Shuowen Zhang", "Jiannong Cao", "Yongpeng Wu", "Giuseppe Caire", "Liang Liu"], "title": "Covariance-based Imaging and Multi-View Fusion for Networked Sensing", "comment": null, "summary": "This paper considers multi-view imaging in a sixth-generation (6G) integrated sensing and communication network, which consists of a transmit base-station (BS), multiple receive BSs connected to a central processing unit (CPU), and multiple extended targets. Our goal is to devise an effective multi-view imaging technique that can jointly leverage the targets' echo signals at all the receive BSs to precisely construct the image of these targets. To achieve this goal, we propose a two-phase approach. In Phase I, each receive BS recovers an individual image based on the sample covariance matrix of its received signals. Specifically, we propose a novel covariance-based imaging framework to jointly estimate effective scattering intensity and grid positions, which reduces the number of estimated parameters leveraging channel statistical properties and allows grid adjustment to conform to target geometry. In Phase II, the CPU fuses the individual images of all the receivers to construct a high-quality image of all the targets. Specifically, we design edge-preserving natural neighbor interpolation (EP-NNI) to map individual heterogeneous images onto common and finer grids, and then propose a joint optimization framework to estimate fused scattering intensity and BS fields of view. Extensive numerical results show that the proposed scheme significantly enhances imaging performance, facilitating high-quality environment reconstruction for future 6G networks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e6G\u7f51\u7edc\u7684\u521b\u65b0\u591a\u89c6\u56fe\u6210\u50cf\u65b9\u6848\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u65b9\u6cd5\u8054\u5408\u5904\u7406\u591a\u4e2a\u63a5\u6536\u57fa\u7ad9\u7684\u56de\u6ce2\u4fe1\u53f7\uff0c\u5b9e\u73b0\u7cbe\u786e\u7684\u76ee\u6807\u6210\u50cf\u3002", "motivation": "\u57286G\u96c6\u6210\u611f\u77e5\u4e0e\u901a\u4fe1\u7f51\u7edc\u4e2d\uff0c\u9700\u8981\u5f00\u53d1\u6709\u6548\u7684\u591a\u89c6\u56fe\u6210\u50cf\u6280\u672f\uff0c\u80fd\u591f\u8054\u5408\u5229\u7528\u6240\u6709\u63a5\u6536\u57fa\u7ad9\u7684\u76ee\u6807\u56de\u6ce2\u4fe1\u53f7\u6765\u7cbe\u786e\u6784\u5efa\u76ee\u6807\u56fe\u50cf\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a\u7b2c\u4e00\u9636\u6bb5\u5404\u63a5\u6536\u57fa\u7ad9\u57fa\u4e8e\u63a5\u6536\u4fe1\u53f7\u7684\u6837\u672c\u534f\u65b9\u5dee\u77e9\u9635\u6062\u590d\u4e2a\u4f53\u56fe\u50cf\uff0c\u63d0\u51fa\u57fa\u4e8e\u534f\u65b9\u5dee\u7684\u6210\u50cf\u6846\u67b6\u8054\u5408\u4f30\u8ba1\u6709\u6548\u6563\u5c04\u5f3a\u5ea6\u548c\u7f51\u683c\u4f4d\u7f6e\uff1b\u7b2c\u4e8c\u9636\u6bb5\u4e2d\u592e\u5904\u7406\u5355\u5143\u878d\u5408\u6240\u6709\u63a5\u6536\u5668\u7684\u4e2a\u4f53\u56fe\u50cf\uff0c\u8bbe\u8ba1\u8fb9\u7f18\u4fdd\u6301\u81ea\u7136\u90bb\u57df\u63d2\u503c\u6620\u5c04\u5230\u516c\u5171\u7f51\u683c\uff0c\u5e76\u901a\u8fc7\u8054\u5408\u4f18\u5316\u6846\u67b6\u4f30\u8ba1\u878d\u5408\u6563\u5c04\u5f3a\u5ea6\u548c\u57fa\u7ad9\u89c6\u573a\u3002", "result": "\u5e7f\u6cdb\u7684\u6570\u503c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6848\u663e\u8457\u63d0\u5347\u4e86\u6210\u50cf\u6027\u80fd\uff0c\u4e3a\u672a\u67656G\u7f51\u7edc\u7684\u9ad8\u8d28\u91cf\u73af\u5883\u91cd\u5efa\u63d0\u4f9b\u4e86\u652f\u6301\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a6G\u96c6\u6210\u611f\u77e5\u4e0e\u901a\u4fe1\u7f51\u7edc\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u591a\u89c6\u56fe\u6210\u50cf\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u4e24\u9636\u6bb5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u76ee\u6807\u6210\u50cf\u548c\u73af\u5883\u91cd\u5efa\u3002"}}
{"id": "2511.14600", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2511.14600", "abs": "https://arxiv.org/abs/2511.14600", "authors": ["Dengyun Huang", "Yonghua Zhu"], "title": "A Controllable Perceptual Feature Generative Model for Melody Harmonization via Conditional Variational Autoencoder", "comment": "13 pages, 8 figures, 2 url links", "summary": "While Large Language Models (LLMs) make symbolic music generation increasingly accessible, producing music with distinctive composition and rich expressiveness remains a significant challenge. Many studies have introduced emotion models to guide the generative process. However, these approaches still fall short of delivering novelty and creativity. In the field of Music Information Retrieval (MIR), auditory perception is recognized as a key dimension of musical experience, offering insights into both compositional intent and emotional patterns. To this end, we propose a neural network named CPFG-Net, along with a transformation algorithm that maps perceptual feature values to chord representations, enabling melody harmonization. The system can controllably predict sequences of perceptual features and tonal structures from given melodies, and subsequently generate harmonically coherent chord progressions. Our network is trained on our newly constructed perceptual feature dataset BCPT-220K, derived from classical music. Experimental results show state-of-the-art perceptual feature prediction capability of our model as well as demonstrate our musical expressiveness and creativity in chord inference. This work offers a novel perspective on melody harmonization and contributes to broader music generation tasks. Our symbolic-based model can be easily extended to audio-based models.", "AI": {"tldr": "\u63d0\u51fa\u4e86CPFG-Net\u795e\u7ecf\u7f51\u7edc\u548c\u611f\u77e5\u7279\u5f81\u5230\u548c\u5f26\u7684\u8f6c\u6362\u7b97\u6cd5\uff0c\u7528\u4e8e\u65cb\u5f8b\u548c\u58f0\u5316\uff0c\u901a\u8fc7\u9884\u6d4b\u611f\u77e5\u7279\u5f81\u548c\u8c03\u6027\u7ed3\u6784\u6765\u751f\u6210\u548c\u8c10\u7684\u548c\u5f26\u8fdb\u884c\u3002", "motivation": "\u867d\u7136\u5927\u8bed\u8a00\u6a21\u578b\u4f7f\u7b26\u53f7\u97f3\u4e50\u751f\u6210\u66f4\u6613\u7528\uff0c\u4f46\u521b\u4f5c\u5177\u6709\u72ec\u7279\u6027\u548c\u4e30\u5bcc\u8868\u73b0\u529b\u7684\u97f3\u4e50\u4ecd\u5177\u6311\u6218\u3002\u73b0\u6709\u60c5\u611f\u6a21\u578b\u65b9\u6cd5\u5728\u521b\u65b0\u6027\u548c\u521b\u9020\u529b\u65b9\u9762\u4e0d\u8db3\uff0c\u800c\u542c\u89c9\u611f\u77e5\u4f5c\u4e3a\u97f3\u4e50\u4f53\u9a8c\u7684\u5173\u952e\u7ef4\u5ea6\u80fd\u63d0\u4f9b\u4f5c\u66f2\u610f\u56fe\u548c\u60c5\u611f\u6a21\u5f0f\u7684\u6d1e\u5bdf\u3002", "method": "\u5f00\u53d1\u4e86CPFG-Net\u795e\u7ecf\u7f51\u7edc\u548c\u611f\u77e5\u7279\u5f81\u5230\u548c\u5f26\u8868\u793a\u7684\u8f6c\u6362\u7b97\u6cd5\uff0c\u4f7f\u7528\u65b0\u6784\u5efa\u7684\u53e4\u5178\u97f3\u4e50\u611f\u77e5\u7279\u5f81\u6570\u636e\u96c6BCPT-220K\u8fdb\u884c\u8bad\u7ec3\uff0c\u80fd\u591f\u53ef\u63a7\u5730\u9884\u6d4b\u611f\u77e5\u7279\u5f81\u5e8f\u5217\u548c\u8c03\u6027\u7ed3\u6784\uff0c\u5e76\u751f\u6210\u548c\u8c10\u7684\u548c\u5f26\u8fdb\u884c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u6a21\u578b\u5728\u611f\u77e5\u7279\u5f81\u9884\u6d4b\u65b9\u9762\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u5e76\u5728\u548c\u5f26\u63a8\u65ad\u4e2d\u5c55\u793a\u4e86\u97f3\u4e50\u8868\u73b0\u529b\u548c\u521b\u9020\u529b\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u65cb\u5f8b\u548c\u58f0\u5316\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u6709\u52a9\u4e8e\u66f4\u5e7f\u6cdb\u7684\u97f3\u4e50\u751f\u6210\u4efb\u52a1\uff0c\u4e14\u8be5\u7b26\u53f7\u57fa\u7840\u6a21\u578b\u53ef\u8f7b\u677e\u6269\u5c55\u5230\u97f3\u9891\u57fa\u7840\u6a21\u578b\u3002"}}
{"id": "2511.14495", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.14495", "abs": "https://arxiv.org/abs/2511.14495", "authors": ["Jiaming Zhang", "Jiajun He", "Tianyu Lu", "Jie Zhang", "Okan Yurduseven"], "title": "Adversarial Learning-Based Radio Map Reconstruction for Fingerprinting Localization", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "This letter presents a feature-guided adversarial framework, namely ComGAN, which is designed to reconstruct an incomplete fingerprint database by inferring missing received signal strength (RSS) values at unmeasured reference points (RPs). An auxiliary subnetwork is integrated into a conditional generative adversarial network (cGAN) to enable spatial feature learning. An optimization method is then developed to refine the RSS predictions by aggregating multiple prediction sets, achieving an improved localization performance. Experimental results demonstrate that the proposed scheme achieves a root mean squared error (RMSE) comparable to the ground-truth measurements while outperforming state-of-the-art reconstruction methods. When the reconstructed fingerprint is combined with measured data for training, the fingerprinting localization achieves accuracy comparable to models trained on fully measured datasets.", "AI": {"tldr": "ComGAN\u662f\u4e00\u4e2a\u7279\u5f81\u5f15\u5bfc\u7684\u5bf9\u6297\u6846\u67b6\uff0c\u901a\u8fc7\u63a8\u65ad\u672a\u6d4b\u91cf\u53c2\u8003\u70b9\u7684\u7f3a\u5931RSS\u503c\u6765\u91cd\u5efa\u4e0d\u5b8c\u6574\u7684\u6307\u7eb9\u6570\u636e\u5e93\uff0c\u7ed3\u5408\u6761\u4ef6\u751f\u6210\u5bf9\u6297\u7f51\u7edc\u548c\u7a7a\u95f4\u7279\u5f81\u5b66\u4e60\uff0c\u4f18\u5316\u65b9\u6cd5\u805a\u5408\u591a\u4e2a\u9884\u6d4b\u96c6\u4ee5\u63d0\u5347\u5b9a\u4f4d\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u6307\u7eb9\u6570\u636e\u5e93\u4e0d\u5b8c\u6574\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u63a8\u65ad\u7f3a\u5931\u7684RSS\u503c\u6765\u91cd\u5efa\u6570\u636e\u5e93\uff0c\u4ece\u800c\u63d0\u5347\u6307\u7eb9\u5b9a\u4f4d\u7cfb\u7edf\u7684\u6027\u80fd\u3002", "method": "\u63d0\u51faComGAN\u6846\u67b6\uff0c\u96c6\u6210\u8f85\u52a9\u5b50\u7f51\u7edc\u5230\u6761\u4ef6\u751f\u6210\u5bf9\u6297\u7f51\u7edc\u4e2d\u5b9e\u73b0\u7a7a\u95f4\u7279\u5f81\u5b66\u4e60\uff0c\u5f00\u53d1\u4f18\u5316\u65b9\u6cd5\u901a\u8fc7\u805a\u5408\u591a\u4e2a\u9884\u6d4b\u96c6\u6765\u7cbe\u5316RSS\u9884\u6d4b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6848\u8fbe\u5230\u4e0e\u771f\u5b9e\u6d4b\u91cf\u76f8\u5f53\u7684RMSE\uff0c\u4f18\u4e8e\u73b0\u6709\u91cd\u5efa\u65b9\u6cd5\uff0c\u91cd\u5efa\u6307\u7eb9\u4e0e\u6d4b\u91cf\u6570\u636e\u7ed3\u5408\u8bad\u7ec3\u65f6\uff0c\u5b9a\u4f4d\u7cbe\u5ea6\u4e0e\u5b8c\u5168\u6d4b\u91cf\u6570\u636e\u96c6\u8bad\u7ec3\u7684\u6a21\u578b\u76f8\u5f53\u3002", "conclusion": "ComGAN\u80fd\u6709\u6548\u91cd\u5efa\u4e0d\u5b8c\u6574\u6307\u7eb9\u6570\u636e\u5e93\uff0c\u63d0\u5347\u6307\u7eb9\u5b9a\u4f4d\u6027\u80fd\uff0c\u8fbe\u5230\u4e0e\u5b8c\u6574\u6570\u636e\u96c6\u76f8\u5f53\u7684\u5b9a\u4f4d\u7cbe\u5ea6\u3002"}}
{"id": "2511.14517", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.14517", "abs": "https://arxiv.org/abs/2511.14517", "authors": ["Cheng-Jie Zhao", "Zhaolin Wang", "Hyundong Shin", "Yuanwei Liu"], "title": "Tri-Hybrid Beamforming Design for Fully-Connected Pinching Antenna Systems", "comment": null, "summary": "A novel fully-connected (FC) tri-hybrid beamforming (THB) architecture is proposed for pinching antenna systems (PASS). In contrast to conventional sub-connected (SC) PASS, the proposed FC architecture employs a tunable phase-shifter network to interconnect all radio frequency (RF) chains with all waveguides. This facilitates a THB framework that integrates conventional hybrid analog-digital beamforming with pinching beamforming. A weighted sum-rate (WSR) optimization problem is then formulated to jointly optimize the transmit beamformers and pinching antenna (PA) positions. Two algorithms are developed to address this challenging non-convex problem. 1) Fractional programming (FP)-based algorithm: This algorithm directly maximizes the WSR using an FP-based alternating optimization framework. Particularly, a success-history based adaptive differential evolution (SHADE) method is proposed to optimize PA positions, effectively addressing the intractable multimodal objective function. 2) Zero-forcing (ZF)-based algorithm: To reduce design complexity, zero-forcing is employed for transmit beamforming. The PA positions are subsequently optimized to maximize the WSR via a modified SHADE method. Simulation results validate the effectiveness of the proposed algorithms, revealing that the FC-THB PASS achieves WSR comparable to the SC architecture while delivering superior energy efficiency with fewer RF chains.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u5168\u8fde\u63a5\u4e09\u6df7\u5408\u6ce2\u675f\u6210\u5f62\u67b6\u6784\uff0c\u7528\u4e8e\u5939\u6301\u5929\u7ebf\u7cfb\u7edf\uff0c\u901a\u8fc7\u53ef\u8c03\u79fb\u76f8\u5668\u7f51\u7edc\u8fde\u63a5\u6240\u6709\u5c04\u9891\u94fe\u548c\u6ce2\u5bfc\uff0c\u5b9e\u73b0\u4e86\u6df7\u5408\u6a21\u62df-\u6570\u5b57\u6ce2\u675f\u6210\u5f62\u4e0e\u5939\u6301\u6ce2\u675f\u6210\u5f62\u7684\u96c6\u6210\u3002", "motivation": "\u4f20\u7edf\u5b50\u8fde\u63a5\u5939\u6301\u5929\u7ebf\u7cfb\u7edf\u5b58\u5728\u6027\u80fd\u9650\u5236\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u9ad8\u6548\u7684\u67b6\u6784\u6765\u63d0\u5347\u7cfb\u7edf\u6027\u80fd\u5e76\u964d\u4f4e\u590d\u6742\u5ea6\u3002", "method": "\u5f00\u53d1\u4e86\u4e24\u79cd\u7b97\u6cd5\uff1a1\uff09\u57fa\u4e8e\u5206\u6570\u89c4\u5212\u7684\u4ea4\u66ff\u4f18\u5316\u6846\u67b6\uff0c\u4f7f\u7528\u81ea\u9002\u5e94\u5dee\u5206\u8fdb\u5316\u65b9\u6cd5\u4f18\u5316\u5929\u7ebf\u4f4d\u7f6e\uff1b2\uff09\u57fa\u4e8e\u8feb\u96f6\u7684\u7b80\u5316\u7b97\u6cd5\uff0c\u4f7f\u7528\u6539\u8fdb\u7684\u81ea\u9002\u5e94\u5dee\u5206\u8fdb\u5316\u65b9\u6cd5\u4f18\u5316\u5929\u7ebf\u4f4d\u7f6e\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u5168\u8fde\u63a5\u4e09\u6df7\u5408\u6ce2\u675f\u6210\u5f62\u5939\u6301\u5929\u7ebf\u7cfb\u7edf\u5728\u5b9e\u73b0\u4e0e\u5b50\u8fde\u63a5\u67b6\u6784\u76f8\u5f53\u7684\u52a0\u6743\u548c\u901f\u7387\u7684\u540c\u65f6\uff0c\u80fd\u4ee5\u66f4\u5c11\u7684\u5c04\u9891\u94fe\u63d0\u4f9b\u66f4\u4f18\u7684\u80fd\u6548\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u5168\u8fde\u63a5\u4e09\u6df7\u5408\u6ce2\u675f\u6210\u5f62\u67b6\u6784\u5728\u6027\u80fd\u548c\u80fd\u6548\u65b9\u9762\u5747\u4f18\u4e8e\u4f20\u7edf\u5b50\u8fde\u63a5\u67b6\u6784\uff0c\u4e3a\u5939\u6301\u5929\u7ebf\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.14529", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.14529", "abs": "https://arxiv.org/abs/2511.14529", "authors": ["Haisu Wu", "Hong Ren", "Cunhua Pan", "Boshi Wang", "Jun Tang", "Haoyang Weng", "Feng Shu", "Jiangzhou Wang"], "title": "A Two-Stage ISAC Framework for Low-Altitude Economy Based on 5G NR Signals", "comment": null, "summary": "The evolution of next-generation wireless networks has spurred the vigorous development of the low-altitude economy (LAE). To support this emerging field while remaining compatible with existing network architectures, integrated sensing and communication (ISAC) based on 5G New Radio (NR) signals is regarded as a promising solution. However, merely leveraging standard 5G NR signals, such as the Synchronization Signal Block (SSB), presents fundamental limitations in sensing resolution. To address the issue, this paper proposes a two-stage coarse-to-fine sensing framework that utilizes standard 5G NR initial access signals augmented by a custom-designed sparse pilot structure (SPS) for high-precision unmanned aerial vehicles (UAV) sensing. In Stage I, we first fuse information from the SSB, Type\\#0-PDCCH, and system information block 1 (SIB1) to ensure the initial target detection. In Stage II, a refined estimation algorithm is introduced to overcome the resolution limitations of these signals. Inspired by the sparse array theory, this stage employs a novel SPS, which is inserted into resource blocks (RBs) within the CORSET\\#0 bandwidth. To accurately extract the off-grid range and velocity parameters from these sparse pilots, we develop a corresponding high-resolution algorithm based on the weighted unwrapped phase (WUP) technique and the RELAX-based iterative method. Finally, the density-based spatial clustering of applications with noise (DBSCAN) algorithm is adopted to prune the redundant detections arising from beam overlap. Comprehensive simulation results demonstrate the superior estimation accuracy and computational efficiency of the proposed framework in comparison to other techniques.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e5G NR\u4fe1\u53f7\u7684\u4e24\u9636\u6bb5\u65e0\u4eba\u673a\u611f\u77e5\u6846\u67b6\uff0c\u901a\u8fc7\u6807\u51c6\u521d\u59cb\u63a5\u5165\u4fe1\u53f7\u548c\u81ea\u5b9a\u4e49\u7a00\u758f\u5bfc\u9891\u7ed3\u6784\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u611f\u77e5", "motivation": "5G NR\u6807\u51c6\u4fe1\u53f7\uff08\u5982SSB\uff09\u5728\u611f\u77e5\u5206\u8fa8\u7387\u65b9\u9762\u5b58\u5728\u6839\u672c\u9650\u5236\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u7cbe\u786e\u7684\u65e0\u4eba\u673a\u611f\u77e5\u89e3\u51b3\u65b9\u6848", "method": "\u4e24\u9636\u6bb5\u7c97\u5230\u7cbe\u611f\u77e5\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u878d\u5408SSB\u3001Type#0-PDCCH\u548cSIB1\u4fe1\u606f\u8fdb\u884c\u521d\u59cb\u76ee\u6807\u68c0\u6d4b\uff1b\u7b2c\u4e8c\u9636\u6bb5\u91c7\u7528\u7a00\u758f\u5bfc\u9891\u7ed3\u6784\u548c\u57fa\u4e8e\u52a0\u6743\u89e3\u7f20\u76f8\u4f4d\u6280\u672f\u7684\u9ad8\u5206\u8fa8\u7387\u7b97\u6cd5", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\u8be5\u6846\u67b6\u76f8\u6bd4\u5176\u4ed6\u6280\u672f\u5177\u6709\u4f18\u8d8a\u7684\u4f30\u8ba1\u7cbe\u5ea6\u548c\u8ba1\u7b97\u6548\u7387", "conclusion": "\u8be5\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e865G NR\u6807\u51c6\u4fe1\u53f7\u5728\u611f\u77e5\u5206\u8fa8\u7387\u65b9\u9762\u7684\u9650\u5236\uff0c\u4e3a\u4f4e\u7a7a\u7ecf\u6d4e\u4e2d\u7684\u65e0\u4eba\u673a\u611f\u77e5\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2511.14640", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.14640", "abs": "https://arxiv.org/abs/2511.14640", "authors": ["Avi Bagchi", "Dwight Hutchenson"], "title": "Doppler Invariant CNN for Signal Classification", "comment": null, "summary": "Radio spectrum monitoring in contested environments motivates the need for reliable automatic signal classification technology. Prior work highlights deep learning as a promising approach, but existing models depend on brute-force Doppler augmentation to achieve real-world generalization, which undermines both training efficiency and interpretability. In this paper, we propose a convolutional neural network (CNN) architecture with complex-valued layers that exploits convolutional shift equivariance in the frequency domain. To establish provable frequency bin shift invariance, we use adaptive polyphase sampling (APS) as pooling layers followed by a global average pooling layer at the end of the network. Using a synthetic dataset of common interference signals, experimental results demonstrate that unlike a vanilla CNN, our model maintains consistent classification accuracy with and without random Doppler shifts despite being trained on no Doppler-shifted examples. Overall, our method establishes an invariance-driven framework for signal classification that offers provable robustness against real-world effects.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5177\u6709\u590d\u6570\u5c42\u548c\u81ea\u9002\u5e94\u591a\u76f8\u91c7\u6837\u7684CNN\u67b6\u6784\uff0c\u5728\u9891\u57df\u5b9e\u73b0\u53ef\u8bc1\u660e\u7684\u9891\u7387\u504f\u79fb\u4e0d\u53d8\u6027\uff0c\u65e0\u9700\u591a\u666e\u52d2\u589e\u5f3a\u8bad\u7ec3\u5373\u53ef\u5bf9\u6297\u5b9e\u9645\u73af\u5883\u4e2d\u7684\u591a\u666e\u52d2\u6548\u5e94\u3002", "motivation": "\u5728\u5bf9\u6297\u73af\u5883\u4e2d\u8fdb\u884c\u65e0\u7ebf\u7535\u9891\u8c31\u76d1\u6d4b\u9700\u8981\u53ef\u9760\u7684\u81ea\u52a8\u4fe1\u53f7\u5206\u7c7b\u6280\u672f\uff0c\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u4f9d\u8d56\u66b4\u529b\u591a\u666e\u52d2\u589e\u5f3a\u6765\u83b7\u5f97\u6cdb\u5316\u80fd\u529b\uff0c\u8fd9\u964d\u4f4e\u4e86\u8bad\u7ec3\u6548\u7387\u548c\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u4f7f\u7528\u590d\u6570\u5c42\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff0c\u5728\u9891\u57df\u5229\u7528\u5377\u79ef\u79fb\u4f4d\u7b49\u53d8\u6027\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u591a\u76f8\u91c7\u6837\u4f5c\u4e3a\u6c60\u5316\u5c42\uff0c\u6700\u540e\u4f7f\u7528\u5168\u5c40\u5e73\u5747\u6c60\u5316\u5c42\u5b9e\u73b0\u53ef\u8bc1\u660e\u7684\u9891\u7387\u504f\u79fb\u4e0d\u53d8\u6027\u3002", "result": "\u5728\u5e38\u89c1\u5e72\u6270\u4fe1\u53f7\u7684\u5408\u6210\u6570\u636e\u96c6\u4e0a\uff0c\u5b9e\u9a8c\u8868\u660e\u4e0e\u666e\u901aCNN\u4e0d\u540c\uff0c\u8be5\u6a21\u578b\u5728\u6709\u65e0\u968f\u673a\u591a\u666e\u52d2\u504f\u79fb\u7684\u60c5\u51b5\u4e0b\u90fd\u80fd\u4fdd\u6301\u4e00\u81f4\u7684\u5206\u7c7b\u51c6\u786e\u7387\uff0c\u5c3d\u7ba1\u8bad\u7ec3\u65f6\u672a\u4f7f\u7528\u591a\u666e\u52d2\u504f\u79fb\u6837\u672c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5efa\u7acb\u4e86\u4e00\u4e2a\u57fa\u4e8e\u4e0d\u53d8\u6027\u7684\u4fe1\u53f7\u5206\u7c7b\u6846\u67b6\uff0c\u63d0\u4f9b\u4e86\u5bf9\u5b9e\u9645\u73af\u5883\u6548\u5e94\u7684\u53ef\u8bc1\u660e\u9c81\u68d2\u6027\u3002"}}

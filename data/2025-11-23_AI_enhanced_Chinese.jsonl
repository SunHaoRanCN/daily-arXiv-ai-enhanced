{"id": "2511.15812", "categories": ["eess.SP", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.15812", "abs": "https://arxiv.org/abs/2511.15812", "authors": ["Luke Dosiek", "Akaash Karn", "Frank Liu"], "title": "Rapid and Accurate Changepoint Detection of Power System Forced Oscillations", "comment": "Currently under review for the proceedings of the 2026 IEEE Power and Energy Society General Meeting (PESGM26)", "summary": "This paper describes a new approach for using changepoint detection (CPD) to estimate the starting and stopping times of a forced oscillation (FO) in measured power system data. As with a previous application of CPD to this problem, the pruned exact linear time (PELT) algorithm is used. However, instead of allowing PELT to automatically tune its penalty parameter, a method of manually providing it is presented that dramatically reduces computation time without sacrificing accuracy. Additionally, the new algorithm requires fewer input parameters and provides a formal, data-driven approach to setting the minimum FO segment length to consider as troublesome for an electromechanical mode meter. A low-order ARMAX representation of the minniWECC model is used to test the approach, where a 98\\% reduction in computation time is enjoyed with high estimation accuracy.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u53d8\u70b9\u68c0\u6d4b(CPD)\u4f30\u8ba1\u7535\u529b\u7cfb\u7edf\u6570\u636e\u4e2d\u5f3a\u8feb\u632f\u8361\u8d77\u6b62\u65f6\u95f4\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u624b\u52a8\u8bbe\u7f6ePELT\u7b97\u6cd5\u7684\u60e9\u7f5a\u53c2\u6570\uff0c\u5927\u5e45\u51cf\u5c11\u8ba1\u7b97\u65f6\u95f4\u800c\u4e0d\u635f\u5931\u7cbe\u5ea6\u3002", "motivation": "\u6539\u8fdb\u73b0\u6709\u53d8\u70b9\u68c0\u6d4b\u65b9\u6cd5\u5728\u7535\u529b\u7cfb\u7edf\u5f3a\u8feb\u632f\u8361\u68c0\u6d4b\u4e2d\u7684\u5e94\u7528\uff0c\u89e3\u51b3\u81ea\u52a8\u8c03\u53c2\u5e26\u6765\u7684\u9ad8\u8ba1\u7b97\u6210\u672c\u95ee\u9898\uff0c\u63d0\u4f9b\u66f4\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u68c0\u6d4b\u65b9\u6848\u3002", "method": "\u4f7f\u7528PELT\u7b97\u6cd5\uff0c\u4f46\u624b\u52a8\u63d0\u4f9b\u60e9\u7f5a\u53c2\u6570\u800c\u975e\u81ea\u52a8\u8c03\u53c2\uff1b\u91c7\u7528\u4f4e\u9636ARMAX\u6a21\u578b\u8868\u793aminniWECC\u7cfb\u7edf\u8fdb\u884c\u6d4b\u8bd5\uff1b\u63d0\u4f9b\u6570\u636e\u9a71\u52a8\u7684\u6700\u5c0fFO\u6bb5\u957f\u5ea6\u8bbe\u7f6e\u65b9\u6cd5\u3002", "result": "\u8ba1\u7b97\u65f6\u95f4\u51cf\u5c1198%\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u4f30\u8ba1\u7cbe\u5ea6\uff1b\u7b97\u6cd5\u9700\u8981\u66f4\u5c11\u7684\u8f93\u5165\u53c2\u6570\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u5f3a\u8feb\u632f\u8361\u68c0\u6d4b\u7684\u8ba1\u7b97\u6548\u7387\uff0c\u4e3a\u673a\u7535\u6a21\u5f0f\u8868\u63d0\u4f9b\u4e86\u66f4\u5b9e\u7528\u7684\u6570\u636e\u9a71\u52a8\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.15902", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.15902", "abs": "https://arxiv.org/abs/2511.15902", "authors": ["Roman Dolgopolyi", "Antonis Chatzipanagiotou"], "title": "EEG Emotion Recognition Through Deep Learning", "comment": "This version corresponds to the original manuscript submitted to the 22nd EMCIS conference prior to peer review. The peer-reviewed and accepted version will appear in the Springer conference proceedings", "summary": "An advanced emotion classification model was developed using a CNN-Transformer architecture for emotion recognition from EEG brain wave signals, effectively distinguishing among three emotional states, positive, neutral and negative. The model achieved a testing accuracy of 91%, outperforming traditional models such as SVM, DNN, and Logistic Regression. Training was conducted on a custom dataset created by merging data from SEED, SEED-FRA, and SEED-GER repositories, comprising 1,455 samples with EEG recordings labeled according to emotional states. The combined dataset represents one of the largest and most culturally diverse collections available. Additionally, the model allows for the reduction of the requirements of the EEG apparatus, by leveraging only 5 electrodes of the 62. This reduction demonstrates the feasibility of deploying a more affordable consumer-grade EEG headset, thereby enabling accessible, at-home use, while also requiring less computational power. This advancement sets the groundwork for future exploration into mood changes induced by media content consumption, an area that remains underresearched. Integration into medical, wellness, and home-health platforms could enable continuous, passive emotional monitoring, particularly beneficial in clinical or caregiving settings where traditional behavioral cues, such as facial expressions or vocal tone, are diminished, restricted, or difficult to interpret, thus potentially transforming mental health diagnostics and interventions...", "AI": {"tldr": "\u5f00\u53d1\u4e86\u57fa\u4e8eCNN-Transformer\u67b6\u6784\u7684EEG\u60c5\u7eea\u5206\u7c7b\u6a21\u578b\uff0c\u4f7f\u75285\u4e2a\u7535\u6781\u5b9e\u73b091%\u51c6\u786e\u7387\uff0c\u652f\u6301\u4f4e\u6210\u672c\u5bb6\u7528EEG\u8bbe\u5907\u90e8\u7f72", "motivation": "\u89e3\u51b3\u4f20\u7edf\u60c5\u7eea\u8bc6\u522b\u65b9\u6cd5\u5728\u9762\u90e8\u8868\u60c5\u6216\u58f0\u97f3\u53d7\u9650\u573a\u666f\u4e0b\u7684\u4e0d\u8db3\uff0c\u63a8\u52a8\u53ef\u8d1f\u62c5\u7684\u5bb6\u5ead\u60c5\u7eea\u76d1\u6d4b\u6280\u672f\u53d1\u5c55", "method": "\u4f7f\u7528CNN-Transformer\u6df7\u5408\u67b6\u6784\u5904\u7406EEG\u4fe1\u53f7\uff0c\u5728\u5408\u5e76SEED\u3001SEED-FRA\u3001SEED-GER\u6570\u636e\u96c6\uff081455\u4e2a\u6837\u672c\uff09\u4e0a\u8bad\u7ec3\uff0c\u4ec5\u97005\u4e2a\u7535\u6781", "result": "\u6d4b\u8bd5\u51c6\u786e\u7387\u8fbe\u523091%\uff0c\u4f18\u4e8eSVM\u3001DNN\u548c\u903b\u8f91\u56de\u5f52\u7b49\u4f20\u7edf\u6a21\u578b\uff0c\u7535\u6781\u9700\u6c42\u4ece62\u4e2a\u51cf\u5c11\u52305\u4e2a", "conclusion": "\u8be5\u6a21\u578b\u4e3a\u5a92\u4f53\u5185\u5bb9\u5f15\u53d1\u7684\u60c5\u7eea\u53d8\u5316\u7814\u7a76\u5960\u5b9a\u57fa\u7840\uff0c\u53ef\u5728\u533b\u7597\u3001\u5065\u5eb7\u76d1\u6d4b\u7b49\u573a\u666f\u5b9e\u73b0\u6301\u7eed\u88ab\u52a8\u60c5\u7eea\u76d1\u63a7"}}
{"id": "2511.15947", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.15947", "abs": "https://arxiv.org/abs/2511.15947", "authors": ["Jeongju Jee", "Jeffrey G. Andrews"], "title": "Integrated Coexistence for Satellite and Terrestrial Networks with Multistatic ISAC", "comment": null, "summary": "Tightly integrated low earth orbit (LEO) satellite communications and terrestrial integrated sensing and communication (ISAC) are expected to be key novel aspects of the 6G era. Spectrum sharing between satellite and terrestrial cellular networks may, however, cause severe interference. This paper introduces a cooperation framework for integrated coexistence between satellite and terrestrial networks where the terrestrial network also deploys multistatic ISAC. Unlike prior works that assume ideal channel state information (CSI) acquisition, the proposed approach develops a practical structure consisting of pre-optimization and refinement stages that leverages the predictability of satellite CSI. In addition, a co-design of terrestrial beamforming and satellite power allocation utilizing a weighted minimum mean-squared error algorithm is proposed, and a target-radar association method designed for multistatic ISAC is presented. Simulation results show that the proposed approach significantly enhances the performance of these integrated networks. Furthermore, it is confirmed that the overall performance approaches the interference-free benchmark as the number of spot beams and radar receivers increases, demonstrating the feasibility of spectral coexistence between the two networks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u536b\u661f\u4e0e\u5730\u9762\u7f51\u7edc\u5171\u5b58\u5408\u4f5c\u6846\u67b6\uff0c\u901a\u8fc7\u9884\u4f18\u5316\u548c\u7cbe\u7ec6\u5316\u4e24\u9636\u6bb5\u5904\u7406\u536b\u661fCSI\u7684\u53ef\u9884\u6d4b\u6027\uff0c\u7ed3\u5408\u6ce2\u675f\u6210\u5f62\u548c\u529f\u7387\u5206\u914d\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u96c6\u6210\u7f51\u7edc\u6027\u80fd\u3002", "motivation": "6G\u65f6\u4ee3LEO\u536b\u661f\u901a\u4fe1\u4e0e\u5730\u9762ISAC\u7684\u7d27\u5bc6\u96c6\u6210\u9700\u8981\u9891\u8c31\u5171\u4eab\uff0c\u4f46\u53ef\u80fd\u9020\u6210\u4e25\u91cd\u5e72\u6270\uff0c\u9700\u8981\u89e3\u51b3\u5b9e\u9645CSI\u83b7\u53d6\u95ee\u9898\u3002", "method": "\u91c7\u7528\u9884\u4f18\u5316\u548c\u7cbe\u7ec6\u5316\u4e24\u9636\u6bb5\u7ed3\u6784\uff0c\u5229\u7528\u536b\u661fCSI\u53ef\u9884\u6d4b\u6027\uff0c\u63d0\u51fa\u6ce2\u675f\u6210\u5f62\u4e0e\u529f\u7387\u5206\u914d\u534f\u540c\u8bbe\u8ba1\uff0c\u4ee5\u53ca\u591a\u7ad9ISAC\u7684\u76ee\u6807-\u96f7\u8fbe\u5173\u8054\u65b9\u6cd5\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u663e\u793a\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u96c6\u6210\u7f51\u7edc\u6027\u80fd\uff0c\u968f\u7740\u6ce2\u675f\u548c\u96f7\u8fbe\u63a5\u6536\u5668\u6570\u91cf\u589e\u52a0\uff0c\u6027\u80fd\u63a5\u8fd1\u65e0\u5e72\u6270\u57fa\u51c6\u3002", "conclusion": "\u8bc1\u660e\u4e86\u536b\u661f\u4e0e\u5730\u9762\u7f51\u7edc\u9891\u8c31\u5171\u5b58\u7684\u53ef\u884c\u6027\uff0c\u4e3a6G\u96c6\u6210\u7f51\u7edc\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.16000", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.16000", "abs": "https://arxiv.org/abs/2511.16000", "authors": ["Weijie Xiong", "Jingran Lin", "Zhiling Xiao", "Qiang Li", "Yuhan Zhang"], "title": "Joint Admission Control and Power Minimization in IRS-assisted Networks", "comment": null, "summary": "Joint admission control and power minimization are critical challenges in intelligent reflecting surface (IRS)-assisted networks. Traditional methods often rely on \\( l_1 \\)-norm approximations and alternating optimization (AO) techniques, which suffer from high computational complexity and lack robust convergence guarantees. To address these limitations, we propose a sigmoid-based approximation of the \\( l_0 \\)-norm AC indicator, enabling a more efficient and tractable reformulation of the problem. Additionally, we introduce a penalty dual decomposition (PDD) algorithm to jointly optimize beamforming and admission control, ensuring convergence to a stationary solution. This approach reduces computational complexity and supports distributed implementation. Moreover, it outperforms existing methods by achieving lower power consumption, accommodating more users, and reducing computational time.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8esigmoid\u51fd\u6570\u8fd1\u4f3cl0\u8303\u6570\u7684\u667a\u80fd\u53cd\u5c04\u9762\u7f51\u7edc\u8054\u5408\u51c6\u5165\u63a7\u5236\u548c\u529f\u7387\u6700\u5c0f\u5316\u65b9\u6cd5\uff0c\u4f7f\u7528\u60e9\u7f5a\u5bf9\u5076\u5206\u89e3\u7b97\u6cd5\u8fdb\u884c\u8054\u5408\u4f18\u5316\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u5177\u6709\u66f4\u4f4e\u529f\u8017\u3001\u66f4\u9ad8\u7528\u6237\u5bb9\u91cf\u548c\u66f4\u77ed\u8ba1\u7b97\u65f6\u95f4\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56l1\u8303\u6570\u8fd1\u4f3c\u548c\u4ea4\u66ff\u4f18\u5316\u6280\u672f\uff0c\u5b58\u5728\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u3001\u6536\u655b\u4fdd\u8bc1\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528sigmoid\u51fd\u6570\u8fd1\u4f3cl0\u8303\u6570\u51c6\u5165\u63a7\u5236\u6307\u6807\uff0c\u63d0\u51fa\u60e9\u7f5a\u5bf9\u5076\u5206\u89e3\u7b97\u6cd5\u8054\u5408\u4f18\u5316\u6ce2\u675f\u6210\u5f62\u548c\u51c6\u5165\u63a7\u5236\uff0c\u652f\u6301\u5206\u5e03\u5f0f\u5b9e\u73b0\u3002", "result": "\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u66f4\u4f4e\u7684\u529f\u8017\u3001\u66f4\u9ad8\u7684\u7528\u6237\u5bb9\u7eb3\u80fd\u529b\u548c\u66f4\u77ed\u7684\u8ba1\u7b97\u65f6\u95f4\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u8ba1\u7b97\u590d\u6742\u5ea6\u548c\u6027\u80fd\u65b9\u9762\u5747\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u4e3aIRS\u8f85\u52a9\u7f51\u7edc\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u8054\u5408\u4f18\u5316\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.16114", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2511.16114", "abs": "https://arxiv.org/abs/2511.16114", "authors": ["Rui Sang", "Yuxuan Liu"], "title": "SceneGuard: Training-Time Voice Protection with Scene-Consistent Audible Background Noise", "comment": null, "summary": "Voice cloning technology poses significant privacy threats by enabling unauthorized speech synthesis from limited audio samples. Existing defenses based on imperceptible adversarial perturbations are vulnerable to common audio preprocessing such as denoising and compression. We propose SceneGuard, a training-time voice protection method that applies scene-consistent audible background noise to speech recordings. Unlike imperceptible perturbations, SceneGuard leverages naturally occurring acoustic scenes (e.g., airport, street, park) to create protective noise that is contextually appropriate and robust to countermeasures. We evaluate SceneGuard on text-to-speech training attacks, demonstrating 5.5% speaker similarity degradation with extremely high statistical significance (p < 10^{-15}, Cohen's d = 2.18) while preserving 98.6% speech intelligibility (STOI = 0.986). Robustness evaluation shows that SceneGuard maintains or enhances protection under five common countermeasures including MP3 compression, spectral subtraction, lowpass filtering, and downsampling. Our results suggest that audible, scene-consistent noise provides a more robust alternative to imperceptible perturbations for training-time voice protection. The source code are available at: https://github.com/richael-sang/SceneGuard.", "AI": {"tldr": "SceneGuard\u662f\u4e00\u79cd\u8bad\u7ec3\u65f6\u8bed\u97f3\u4fdd\u62a4\u65b9\u6cd5\uff0c\u901a\u8fc7\u6dfb\u52a0\u573a\u666f\u4e00\u81f4\u7684\u53ef\u542c\u80cc\u666f\u566a\u58f0\u6765\u9632\u6b62\u8bed\u97f3\u514b\u9686\u653b\u51fb\uff0c\u76f8\u6bd4\u4e0d\u53ef\u611f\u77e5\u7684\u5bf9\u6297\u6270\u52a8\u66f4\u9c81\u68d2\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u4e0d\u53ef\u611f\u77e5\u5bf9\u6297\u6270\u52a8\u7684\u8bed\u97f3\u4fdd\u62a4\u65b9\u6cd5\u5bb9\u6613\u53d7\u5230\u97f3\u9891\u9884\u5904\u7406\uff08\u5982\u53bb\u566a\u548c\u538b\u7f29\uff09\u7684\u653b\u51fb\uff0c\u9700\u8981\u66f4\u9c81\u68d2\u7684\u9632\u5fa1\u65b9\u6848\u3002", "method": "\u5728\u8bed\u97f3\u5f55\u5236\u65f6\u5e94\u7528\u573a\u666f\u4e00\u81f4\u7684\u53ef\u542c\u80cc\u666f\u566a\u58f0\uff0c\u5229\u7528\u81ea\u7136\u58f0\u5b66\u573a\u666f\uff08\u5982\u673a\u573a\u3001\u8857\u9053\u3001\u516c\u56ed\uff09\u521b\u5efa\u4e0a\u4e0b\u6587\u9002\u5f53\u7684\u4fdd\u62a4\u6027\u566a\u58f0\u3002", "result": "\u5728\u6587\u672c\u8f6c\u8bed\u97f3\u8bad\u7ec3\u653b\u51fb\u4e2d\uff0cSceneGuard\u4f7f\u8bf4\u8bdd\u4eba\u76f8\u4f3c\u5ea6\u964d\u4f4e5.5%\uff08p < 10^{-15}\uff0cCohen's d = 2.18\uff09\uff0c\u540c\u65f6\u4fdd\u630198.6%\u7684\u8bed\u97f3\u53ef\u61c2\u5ea6\uff08STOI = 0.986\uff09\uff0c\u5728\u4e94\u79cd\u5e38\u89c1\u5bf9\u6297\u63aa\u65bd\u4e0b\u4fdd\u6301\u6216\u589e\u5f3a\u4fdd\u62a4\u6548\u679c\u3002", "conclusion": "\u53ef\u542c\u4e14\u573a\u666f\u4e00\u81f4\u7684\u566a\u58f0\u4e3a\u8bad\u7ec3\u65f6\u8bed\u97f3\u4fdd\u62a4\u63d0\u4f9b\u4e86\u6bd4\u4e0d\u53ef\u611f\u77e5\u6270\u52a8\u66f4\u9c81\u68d2\u7684\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2511.15766", "categories": ["eess.AS", "eess.SP"], "pdf": "https://arxiv.org/pdf/2511.15766", "abs": "https://arxiv.org/abs/2511.15766", "authors": ["Mohit Sharma", "Robbe Van Rompaey", "Wouter Lanneer", "Marc Moonen"], "title": "A Generalized Weighted Overlap-Add (WOLA) Filter Bank for Improved Subband System Identification", "comment": "For associated MatLab script: https://github.com/mohit-nith/GeneralizedWOLA-SystemIdentification.git", "summary": "This paper addresses the challenges in short-time Fourier transform (STFT) domain subband adaptive filtering, in particular, subband system identification. Previous studies in this area have primarily focused on setups with subband filtering at a downsampled rate, implemented using the weighted overlap-add (WOLA) filter bank, popular in audio and speech-processing for its reduced complexity. However, this traditional approach imposes constraints on the subband filters when transformed to their full-rate representation. This paper makes three key contributions. First, it introduces a generalized WOLA filter bank that repositions subband filters before the downsampling operation, eliminating the constraints on subband filters inherent in the conventional WOLA filter bank. Second, it investigates the mean square error (MSE) performance of the generalized WOLA filter bank for full-band system identification, establishing analytical ties between the order of subband filters, the full-band system impulse response length, the decimation factor, and the prototype filters. Third, to address the increased computational complexity of the generalized WOLA, the paper proposes a low-complexity implementation termed per-tone weighted overlap-add (PT-WOLA), which maintains computational complexity on par with conventional WOLA. Analytical and empirical evidence demonstrates that the proposed generalized WOLA filter bank significantly enhances the performance of subband system identification.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5e7f\u4e49WOLA\u6ee4\u6ce2\u5668\u7ec4\uff0c\u5c06\u5b50\u5e26\u6ee4\u6ce2\u5668\u91cd\u65b0\u5b9a\u4f4d\u5230\u964d\u91c7\u6837\u64cd\u4f5c\u4e4b\u524d\uff0c\u6d88\u9664\u4e86\u4f20\u7edfWOLA\u6ee4\u6ce2\u5668\u7ec4\u4e2d\u5b50\u5e26\u6ee4\u6ce2\u5668\u7684\u7ea6\u675f\uff0c\u5e76\u901a\u8fc7PT-WOLA\u5b9e\u73b0\u4fdd\u6301\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "motivation": "\u4f20\u7edfSTFT\u57df\u5b50\u5e26\u81ea\u9002\u5e94\u6ee4\u6ce2\u4e3b\u8981\u5173\u6ce8\u964d\u91c7\u6837\u7387\u4e0b\u7684\u5b50\u5e26\u6ee4\u6ce2\uff0c\u4f7f\u7528WOLA\u6ee4\u6ce2\u5668\u7ec4\uff0c\u4f46\u8fd9\u79cd\u65b9\u6cd5\u5728\u8f6c\u6362\u4e3a\u5168\u901f\u7387\u8868\u793a\u65f6\u5bf9\u5b50\u5e26\u6ee4\u6ce2\u5668\u65bd\u52a0\u4e86\u7ea6\u675f\u3002", "method": "1. \u5f15\u5165\u5e7f\u4e49WOLA\u6ee4\u6ce2\u5668\u7ec4\u91cd\u65b0\u5b9a\u4f4d\u5b50\u5e26\u6ee4\u6ce2\u5668\u4f4d\u7f6e\uff1b2. \u5206\u6790MSE\u6027\u80fd\u4e0e\u5404\u53c2\u6570\u5173\u7cfb\uff1b3. \u63d0\u51faPT-WOLA\u4f4e\u590d\u6742\u5ea6\u5b9e\u73b0\u3002", "result": "\u5206\u6790\u548c\u5b9e\u9a8c\u8bc1\u636e\u8868\u660e\uff0c\u5e7f\u4e49WOLA\u6ee4\u6ce2\u5668\u7ec4\u663e\u8457\u63d0\u5347\u4e86\u5b50\u5e26\u7cfb\u7edf\u8bc6\u522b\u7684\u6027\u80fd\u3002", "conclusion": "\u5e7f\u4e49WOLA\u6ee4\u6ce2\u5668\u7ec4\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u7ea6\u675f\u95ee\u9898\uff0c\u540c\u65f6\u901a\u8fc7PT-WOLA\u4fdd\u6301\u4e86\u8ba1\u7b97\u6548\u7387\uff0c\u4e3a\u5b50\u5e26\u7cfb\u7edf\u8bc6\u522b\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.16169", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.16169", "abs": "https://arxiv.org/abs/2511.16169", "authors": ["Zijian Wang", "Xiaoyu Bao", "Chenhao Zhao", "Jihui Zhang", "Sizhi Ai", "Yuanqing Li"], "title": "UT-OSANet: A Multimodal Deep Learning model for Evaluating and Classifying Obstructive Sleep Apnea", "comment": "12 pages,8 figures", "summary": "Obstructive sleep apnea (OSA) is a highly prevalent sleep disorder that is associated with increased risks of cardiovascular morbidity and all-cause mortality. While existing diagnostic approaches can roughly classify OSA severity or detect isolated respiratory events, they lack the precision and comprehensiveness required for high resolution, event level diagnosis. Here, we present UT OSANet, a deep learning based model designed as a event level, multi scenario diagnostic tool for OSA. This model facilitates detailed identification of events associated with OSA, including apnea, hypopnea, oxygen desaturation, and arousal. Moreover, the model employs flexibly adjustable input modalities such as electroencephalography (EEG), airflow, and SpO 2. It utilizes a random masked modality combination training strategy, allowing it to comprehend cross-modal relationships while sustaining consistent performance across varying modality conditions. This model was trained and evaluated utilizing 9,021 polysomnography (PSG) recordings from five independent datasets. achieving sensitivities up to 0.93 and macro F1 scores of 0.84, 0.85 across home, clinical, and research scenarios. This model serves as an event-level, multi-scenario diagnostic instrument for real-world applications of OSA, while also establishing itself as a means to deepen the mechanistic comprehension of respiratory processes in sleep disorders and their extensive health implications.", "AI": {"tldr": "UT OSANet\u662f\u4e00\u4e2a\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u963b\u585e\u6027\u7761\u7720\u547c\u5438\u6682\u505c(OSA)\u4e8b\u4ef6\u7ea7\u8bca\u65ad\u5de5\u5177\uff0c\u80fd\u591f\u8bc6\u522b\u547c\u5438\u6682\u505c\u3001\u4f4e\u901a\u6c14\u3001\u8840\u6c27\u4e0b\u964d\u548c\u89c9\u9192\u7b49\u4e8b\u4ef6\uff0c\u652f\u6301\u591a\u79cd\u8f93\u5165\u6a21\u6001\uff0c\u5728\u5bb6\u5ead\u3001\u4e34\u5e8a\u548c\u7814\u7a76\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709OSA\u8bca\u65ad\u65b9\u6cd5\u53ea\u80fd\u7c97\u7565\u5206\u7c7b\u4e25\u91cd\u7a0b\u5ea6\u6216\u68c0\u6d4b\u5b64\u7acb\u547c\u5438\u4e8b\u4ef6\uff0c\u7f3a\u4e4f\u9ad8\u5206\u8fa8\u7387\u7684\u4e8b\u4ef6\u7ea7\u8bca\u65ad\u7cbe\u5ea6\u548c\u5168\u9762\u6027\u3002", "method": "\u5f00\u53d1UT OSANet\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u91c7\u7528\u968f\u673a\u63a9\u7801\u6a21\u6001\u7ec4\u5408\u8bad\u7ec3\u7b56\u7565\uff0c\u652f\u6301EEG\u3001\u6c14\u6d41\u548cSpO2\u7b49\u7075\u6d3b\u8f93\u5165\u6a21\u6001\uff0c\u7406\u89e3\u8de8\u6a21\u6001\u5173\u7cfb\u5e76\u5728\u4e0d\u540c\u6a21\u6001\u6761\u4ef6\u4e0b\u4fdd\u6301\u7a33\u5b9a\u6027\u80fd\u3002", "result": "\u4f7f\u75285\u4e2a\u72ec\u7acb\u6570\u636e\u96c6\u76849,021\u4e2a\u591a\u5bfc\u7761\u7720\u56fe\u8bb0\u5f55\u8fdb\u884c\u8bad\u7ec3\u548c\u8bc4\u4f30\uff0c\u5728\u5bb6\u5ead\u3001\u4e34\u5e8a\u548c\u7814\u7a76\u573a\u666f\u4e2d\u7075\u654f\u5ea6\u9ad8\u8fbe0.93\uff0c\u5b8f\u89c2F1\u5206\u6570\u5206\u522b\u4e3a0.84\u548c0.85\u3002", "conclusion": "\u8be5\u6a21\u578b\u53ef\u4f5c\u4e3aOSA\u5b9e\u9645\u5e94\u7528\u7684\u4e8b\u4ef6\u7ea7\u3001\u591a\u573a\u666f\u8bca\u65ad\u5de5\u5177\uff0c\u540c\u65f6\u4e3a\u6df1\u5165\u7406\u89e3\u7761\u7720\u969c\u788d\u4e2d\u547c\u5438\u8fc7\u7a0b\u7684\u673a\u5236\u53ca\u5176\u5e7f\u6cdb\u5065\u5eb7\u5f71\u54cd\u63d0\u4f9b\u4e86\u624b\u6bb5\u3002"}}
{"id": "2511.16228", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2511.16228", "abs": "https://arxiv.org/abs/2511.16228", "authors": ["Pedro Ramoneda", "Emilia Parada-Cabaleiro", "Dasaem Jeong", "Xavier Serra"], "title": "Difficulty-Controlled Simplification of Piano Scores with Synthetic Data for Inclusive Music Education", "comment": null, "summary": "Despite its potential, AI advances in music education are hindered by proprietary systems that limit the democratization of technology in this domain. In particular, AI-driven music difficulty adjustment is especially promising, as simplifying complex pieces can make music education more inclusive and accessible to learners of all ages and contexts. Nevertheless, recent efforts have relied on proprietary datasets, which prevents the research community from reproducing, comparing, or extending the current state of the art. In addition, while these generative methods offer great potential, most of them use the MIDI format, which, unlike others, such as MusicXML, lacks readability and layout information, thereby limiting their practical use for human performers. This work introduces a transformer-based method for adjusting the difficulty of MusicXML piano scores. Unlike previous methods, which rely on annotated datasets, we propose a synthetic dataset composed of pairs of piano scores ordered by estimated difficulty, with each pair comprising a more challenging and easier arrangement of the same piece. We generate these pairs by creating variations conditioned on the same melody and harmony and leverage pretrained models to assess difficulty and style, ensuring appropriate pairing. The experimental results illustrate the validity of the proposed approach, showing accurate control of playability and target difficulty, as highlighted through qualitative and quantitative evaluations. In contrast to previous work, we openly release all resources (code, dataset, and models), ensuring reproducibility while fostering open-source innovation to help bridge the digital divide.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eTransformer\u7684MusicXML\u94a2\u7434\u8c31\u96be\u5ea6\u8c03\u6574\u65b9\u6cd5\uff0c\u901a\u8fc7\u5408\u6210\u6570\u636e\u96c6\u548c\u9884\u8bad\u7ec3\u6a21\u578b\u6765\u8bc4\u4f30\u96be\u5ea6\u548c\u98ce\u683c\uff0c\u786e\u4fdd\u51c6\u786e\u63a7\u5236\u53ef\u6f14\u594f\u6027\u548c\u76ee\u6807\u96be\u5ea6\u3002", "motivation": "AI\u5728\u97f3\u4e50\u6559\u80b2\u4e2d\u7684\u6f5c\u529b\u53d7\u5230\u4e13\u6709\u7cfb\u7edf\u7684\u9650\u5236\uff0c\u7279\u522b\u662f\u96be\u5ea6\u8c03\u6574\u6280\u672f\u53ef\u4ee5\u4fc3\u8fdb\u97f3\u4e50\u6559\u80b2\u7684\u5305\u5bb9\u6027\u548c\u53ef\u53ca\u6027\u3002\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u4e13\u6709\u6570\u636e\u96c6\u548cMIDI\u683c\u5f0f\uff0c\u7f3a\u4e4f\u53ef\u8bfb\u6027\u548c\u5e03\u5c40\u4fe1\u606f\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u4f7f\u7528\u57fa\u4e8eTransformer\u7684\u65b9\u6cd5\u8c03\u6574MusicXML\u94a2\u7434\u8c31\u96be\u5ea6\uff0c\u521b\u5efa\u5408\u6210\u6570\u636e\u96c6\u5305\u542b\u6309\u4f30\u8ba1\u96be\u5ea6\u6392\u5e8f\u7684\u94a2\u7434\u8c31\u5bf9\uff0c\u6bcf\u4e2a\u5bf9\u5305\u542b\u540c\u4e00\u66f2\u76ee\u7684\u66f4\u96be\u548c\u66f4\u7b80\u5355\u7248\u672c\u3002\u5229\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u8bc4\u4f30\u96be\u5ea6\u548c\u98ce\u683c\u4ee5\u786e\u4fdd\u914d\u5bf9\u9002\u5f53\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u6709\u6548\uff0c\u901a\u8fc7\u5b9a\u6027\u548c\u5b9a\u91cf\u8bc4\u4f30\u663e\u793a\u80fd\u591f\u51c6\u786e\u63a7\u5236\u53ef\u6f14\u594f\u6027\u548c\u76ee\u6807\u96be\u5ea6\u3002", "conclusion": "\u4e0e\u5148\u524d\u5de5\u4f5c\u4e0d\u540c\uff0c\u672c\u6587\u516c\u5f00\u6240\u6709\u8d44\u6e90\uff08\u4ee3\u7801\u3001\u6570\u636e\u96c6\u548c\u6a21\u578b\uff09\uff0c\u786e\u4fdd\u53ef\u91cd\u73b0\u6027\uff0c\u540c\u65f6\u4fc3\u8fdb\u5f00\u6e90\u521b\u65b0\u4ee5\u5e2e\u52a9\u5f25\u5408\u6570\u5b57\u9e3f\u6c9f\u3002"}}
{"id": "2511.16046", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2511.16046", "abs": "https://arxiv.org/abs/2511.16046", "authors": ["Mohan Shi", "Xiong Xiao", "Ruchao Fan", "Shaoshi Ling", "Jinyu Li"], "title": "Train Short, Infer Long: Speech-LLM Enables Zero-Shot Streamable Joint ASR and Diarization on Long Audio", "comment": "Submitted to ICASSP2026", "summary": "Joint automatic speech recognition (ASR) and speaker diarization aim to answer the question \"who spoke what\" in multi-speaker scenarios. In this paper, we present an end-to-end speech large language model (Speech-LLM) for Joint strEamable DIarization and aSr (JEDIS-LLM). The model is trained only on short audio under 20s but is capable of streamable inference on long-form audio without additional training. This is achieved by introducing a Speaker Prompt Cache (SPC) with an on-the-fly update mechanism during chunk-wise streaming inference, inspired by the autoregressive nature of LLMs. The SPC also allows the seamless use of pre-enrolled speaker profiles which is common in many scenarios like meeting transcription. To further enhance diarization capability, we incorporate word-level speaker supervision into the speech encoder during training. Experimental results demonstrate that our system outperforms strong baselines, including Sortformer and Meta-Cat in the local setting on audio up to 20s, and DiarizationLM on long-form audio, despite being fully end-to-end and streamable while DiarizationLM follows a cascaded offline pipeline. To the best of our knowledge, this is the first work enabling zero-shot streamable joint ASR and diarization on long audio using a Speech-LLM trained only on short audio, achieving state-of-the-art performance.", "AI": {"tldr": "\u63d0\u51fa\u4e86JEDIS-LLM\uff0c\u4e00\u79cd\u7aef\u5230\u7aef\u7684\u8bed\u97f3\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u7528\u4e8e\u8054\u5408\u6d41\u5f0f\u8bf4\u8bdd\u4eba\u65e5\u5fd7\u548c\u8bed\u97f3\u8bc6\u522b\uff0c\u4ec5\u9700\u5728\u77ed\u97f3\u9891\u4e0a\u8bad\u7ec3\u5373\u53ef\u5b9e\u73b0\u957f\u97f3\u9891\u7684\u96f6\u6837\u672c\u6d41\u5f0f\u63a8\u7406\u3002", "motivation": "\u89e3\u51b3\u591a\u8bf4\u8bdd\u4eba\u573a\u666f\u4e0b\"\u8c01\u8bf4\u4e86\u4ec0\u4e48\"\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u8054\u5408ASR\u548c\u8bf4\u8bdd\u4eba\u65e5\u5fd7\u7684\u7aef\u5230\u7aef\u6d41\u5f0f\u5904\u7406\uff0c\u907f\u514d\u4f20\u7edf\u7ea7\u8054\u65b9\u6cd5\u7684\u590d\u6742\u6027\u3002", "method": "\u5f15\u5165\u8bf4\u8bdd\u4eba\u63d0\u793a\u7f13\u5b58(SPC)\u673a\u5236\uff0c\u5728\u5206\u5757\u6d41\u5f0f\u63a8\u7406\u65f6\u8fdb\u884c\u52a8\u6001\u66f4\u65b0\uff1b\u5728\u8bed\u97f3\u7f16\u7801\u5668\u4e2d\u52a0\u5165\u8bcd\u7ea7\u8bf4\u8bdd\u4eba\u76d1\u7763\uff1b\u652f\u6301\u9884\u6ce8\u518c\u8bf4\u8bdd\u4eba\u914d\u7f6e\u3002", "result": "\u572820\u79d2\u97f3\u9891\u4e0a\u8d85\u8d8aSortformer\u548cMeta-Cat\uff0c\u5728\u957f\u97f3\u9891\u4e0a\u4f18\u4e8eDiarizationLM\uff0c\u4e14\u5b8c\u5168\u7aef\u5230\u7aef\u548c\u6d41\u5f0f\u5904\u7406\u3002", "conclusion": "\u8fd9\u662f\u9996\u4e2a\u4ec5\u7528\u77ed\u97f3\u9891\u8bad\u7ec3\u5c31\u80fd\u5728\u957f\u97f3\u9891\u4e0a\u5b9e\u73b0\u96f6\u6837\u672c\u6d41\u5f0f\u8054\u5408ASR\u548c\u8bf4\u8bdd\u4eba\u65e5\u5fd7\u7684Speech-LLM\uff0c\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002"}}
{"id": "2511.16260", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.16260", "abs": "https://arxiv.org/abs/2511.16260", "authors": ["Hao Wu", "Shanchi Wu", "Xinyuan Yao", "Rui Ni", "Chen Gong"], "title": "Low-Complexity Rydberg Array Reuse: Modeling and Receiver Design for Sparse Channels", "comment": null, "summary": "Rydberg atomic quantum receivers have been seen as novel radio frequency measurements and the high sensitivity to a large range of frequencies makes it attractive for communications reception. However, current implementations of Rydberg array antennas predominantly rely on simple stacking of multiple single-antenna units. While conceptually straightforward, this approach leads to substantial system bulkiness due to the unique requirements of atomic sensors, particularly the need for multiple spatially separated laser setups, rendering such designs both impractical for real-world applications and challenging to fabricate. This limitation underscores the critical need for developing multiplexed Rydberg sensor array architectures. In the domain of conventional RF array antennas, hybrid analog-digital beamforming has emerged as a pivotal architecture for large-scale millimeter-wave (mmWave) multiple-input multiple-output (MIMO) systems, as it substantially reduces the hardware complexity associated with fully-digital beamforming while closely approaching its performance. Drawing inspiration from this methodology, we conduct a systematic study in this work on the design principles, equivalent modeling, and precoding strategies for low-complexity multiplexed Rydberg array, an endeavor crucial to enabling practical and scalable quantum-enhanced communication systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f4e\u590d\u6742\u5ea6\u7684\u591a\u8def\u590d\u7528\u91cc\u5fb7\u5821\u9635\u5217\u8bbe\u8ba1\uff0c\u901a\u8fc7\u6df7\u5408\u6a21\u62df-\u6570\u5b57\u6ce2\u675f\u6210\u5f62\u6280\u672f\u89e3\u51b3\u5f53\u524d\u91cc\u5fb7\u5821\u539f\u5b50\u91cf\u5b50\u63a5\u6536\u5668\u9635\u5217\u56e0\u9700\u8981\u591a\u4e2a\u6fc0\u5149\u8bbe\u7f6e\u800c\u5bfc\u81f4\u7684\u7cfb\u7edf\u5e9e\u5927\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u91cc\u5fb7\u5821\u9635\u5217\u5929\u7ebf\u4e3b\u8981\u4f9d\u8d56\u7b80\u5355\u5806\u53e0\u591a\u4e2a\u5355\u5929\u7ebf\u5355\u5143\uff0c\u7531\u4e8e\u539f\u5b50\u4f20\u611f\u5668\u7684\u7279\u6b8a\u9700\u6c42\uff08\u7279\u522b\u662f\u9700\u8981\u591a\u4e2a\u7a7a\u95f4\u5206\u79bb\u7684\u6fc0\u5149\u8bbe\u7f6e\uff09\uff0c\u5bfc\u81f4\u7cfb\u7edf\u5e9e\u5927\u3001\u4e0d\u5b9e\u7528\u4e14\u5236\u9020\u56f0\u96be\uff0c\u8feb\u5207\u9700\u8981\u5f00\u53d1\u591a\u8def\u590d\u7528\u91cc\u5fb7\u5821\u4f20\u611f\u5668\u9635\u5217\u67b6\u6784\u3002", "method": "\u501f\u9274\u4f20\u7edf\u5c04\u9891\u9635\u5217\u5929\u7ebf\u4e2d\u7684\u6df7\u5408\u6a21\u62df-\u6570\u5b57\u6ce2\u675f\u6210\u5f62\u65b9\u6cd5\uff0c\u7cfb\u7edf\u7814\u7a76\u4e86\u4f4e\u590d\u6742\u5ea6\u591a\u8def\u590d\u7528\u91cc\u5fb7\u5821\u9635\u5217\u7684\u8bbe\u8ba1\u539f\u7406\u3001\u7b49\u6548\u5efa\u6a21\u548c\u9884\u7f16\u7801\u7b56\u7565\u3002", "result": "\u8be5\u65b9\u6cd5\u663e\u8457\u964d\u4f4e\u4e86\u4e0e\u5168\u6570\u5b57\u6ce2\u675f\u6210\u5f62\u76f8\u5173\u7684\u786c\u4ef6\u590d\u6742\u6027\uff0c\u540c\u65f6\u63a5\u8fd1\u5176\u6027\u80fd\u8868\u73b0\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u5bf9\u4e8e\u5b9e\u73b0\u5b9e\u7528\u4e14\u53ef\u6269\u5c55\u7684\u91cf\u5b50\u589e\u5f3a\u901a\u4fe1\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\uff0c\u4e3a\u91cc\u5fb7\u5821\u9635\u5217\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u4f4e\u590d\u6742\u5ea6\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.16126", "categories": ["eess.AS", "eess.SP"], "pdf": "https://arxiv.org/pdf/2511.16126", "abs": "https://arxiv.org/abs/2511.16126", "authors": ["Ryo Aihara", "Yoshiki Masuyama", "Francesco Paissan", "Fran\u00e7ois G. Germain", "Gordon Wichern", "Jonathan Le Roux"], "title": "SUNAC: Source-aware Unified Neural Audio Codec", "comment": "Submitted to ICASSP 2026", "summary": "Neural audio codecs (NACs) provide compact representations that can be leveraged in many downstream applications, in particular large language models. Yet most NACs encode mixtures of multiple sources in an entangled manner, which may impede efficient downstream processing in applications that need access to only a subset of the sources (e.g., analysis of a particular type of sound, transcription of a given speaker, etc). To address this, we propose a source-aware codec that encodes individual sources directly from mixtures, conditioned on source type prompts. This enables user-driven selection of which source(s) to encode, including separately encoding multiple sources of the same type (e.g., multiple speech signals). Experiments show that our model achieves competitive resynthesis and separation quality relative to a cascade of source separation followed by a conventional NAC, with lower computational cost.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6e90\u611f\u77e5\u97f3\u9891\u7f16\u89e3\u7801\u5668\uff0c\u80fd\u591f\u76f4\u63a5\u4ece\u6df7\u5408\u97f3\u9891\u4e2d\u7f16\u7801\u5355\u4e2a\u58f0\u6e90\uff0c\u901a\u8fc7\u6e90\u7c7b\u578b\u63d0\u793a\u5b9e\u73b0\u7528\u6237\u9a71\u52a8\u7684\u58f0\u6e90\u9009\u62e9\u3002", "motivation": "\u4f20\u7edf\u795e\u7ecf\u97f3\u9891\u7f16\u89e3\u7801\u5668\u5bf9\u591a\u4e2a\u58f0\u6e90\u7684\u6df7\u5408\u8fdb\u884c\u7ea0\u7f20\u7f16\u7801\uff0c\u8fd9\u5728\u9700\u8981\u8bbf\u95ee\u7279\u5b9a\u58f0\u6e90\u5b50\u96c6\u7684\u4e0b\u6e38\u5e94\u7528\u4e2d\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u5f00\u53d1\u6e90\u611f\u77e5\u7f16\u89e3\u7801\u5668\uff0c\u901a\u8fc7\u6e90\u7c7b\u578b\u63d0\u793a\u6761\u4ef6\u5316\uff0c\u76f4\u63a5\u4ece\u6df7\u5408\u97f3\u9891\u4e2d\u7f16\u7801\u5355\u4e2a\u58f0\u6e90\uff0c\u652f\u6301\u7528\u6237\u9009\u62e9\u8981\u7f16\u7801\u7684\u58f0\u6e90\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u6a21\u578b\u5728\u91cd\u5408\u6210\u548c\u5206\u79bb\u8d28\u91cf\u4e0a\u4e0e\u6e90\u5206\u79bb+\u4f20\u7edf\u7f16\u89e3\u7801\u5668\u7ea7\u8054\u65b9\u6cd5\u76f8\u5f53\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u66f4\u4f4e\u3002", "conclusion": "\u6e90\u611f\u77e5\u7f16\u89e3\u7801\u5668\u4e3a\u4e0b\u6e38\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u9ad8\u6548\u7684\u58f0\u6e90\u7f16\u7801\u65b9\u6848\uff0c\u7279\u522b\u9002\u7528\u4e8e\u9700\u8981\u8bbf\u95ee\u7279\u5b9a\u58f0\u6e90\u7684\u573a\u666f\u3002"}}
{"id": "2511.16277", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.16277", "abs": "https://arxiv.org/abs/2511.16277", "authors": ["Manjun Cui", "Ziqi Yan", "Yangfan He", "Zhichao Zhang"], "title": "Dynamic Multiple-Parameter Joint Time-Vertex Fractional Fourier Transform and its Intelligent Filtering Methods", "comment": null, "summary": "Dynamic graph signal processing provides a principled framework for analyzing time-varying data defined on irregular graph domains. However, existing joint time-vertex transforms such as the joint time-vertex fractional Fourier transform assign only one fractional order to the spatial domain and another one to the temporal domain, thereby restricting their capacity to model the complex and continuously varying dynamics of graph signals. To address this limitation, we propose a novel dynamic multiple-parameter joint time-vertex fractional Fourier transform (DMPJFRFT) framework, which introduces time-varying fractional parameters to achieve adaptive spectral modeling of dynamic graph structures. By assigning distinct fractional orders to each time step, the proposed transform enables dynamic and flexible representation of spatio-temporal signal evolution in the joint time-vertex spectral domain. Theoretical properties of the DMPJFRFT are systematically analyzed, and two filtering approaches: a gradient descent-based method and a neural network-based method, are developed for dynamic signal restoration. Experimental results on dynamic graph and video datasets demonstrate that the proposed framework effectively captures temporal topology variations and achieves superior performance in denoising and deblurring tasks compared with some state-of-the-art graph-based transforms and neural networks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u591a\u53c2\u6570\u8054\u5408\u65f6-\u9876\u70b9\u5206\u6570\u5085\u91cc\u53f6\u53d8\u6362(DMPJFRFT)\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u65f6\u53d8\u5206\u6570\u53c2\u6570\u6765\u5b9e\u73b0\u52a8\u6001\u56fe\u7ed3\u6784\u7684\u81ea\u9002\u5e94\u8c31\u5efa\u6a21\uff0c\u5728\u52a8\u6001\u56fe\u4fe1\u53f7\u53bb\u566a\u548c\u53bb\u6a21\u7cca\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u73b0\u6709\u8054\u5408\u65f6-\u9876\u70b9\u53d8\u6362\u4ec5\u80fd\u4e3a\u7a7a\u95f4\u57df\u548c\u65f6\u95f4\u57df\u5206\u522b\u5206\u914d\u4e00\u4e2a\u5206\u6570\u9636\uff0c\u9650\u5236\u4e86\u5176\u5efa\u6a21\u56fe\u4fe1\u53f7\u590d\u6742\u52a8\u6001\u53d8\u5316\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86DMPJFRFT\u6846\u67b6\uff0c\u4e3a\u6bcf\u4e2a\u65f6\u95f4\u6b65\u5206\u914d\u4e0d\u540c\u7684\u5206\u6570\u9636\uff0c\u5b9e\u73b0\u52a8\u6001\u7075\u6d3b\u7684\u65f6\u7a7a\u4fe1\u53f7\u8868\u793a\uff1b\u5f00\u53d1\u4e86\u57fa\u4e8e\u68af\u5ea6\u4e0b\u964d\u548c\u795e\u7ecf\u7f51\u7edc\u4e24\u79cd\u6ee4\u6ce2\u65b9\u6cd5\u7528\u4e8e\u52a8\u6001\u4fe1\u53f7\u6062\u590d\u3002", "result": "\u5728\u52a8\u6001\u56fe\u548c\u89c6\u9891\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u80fd\u6709\u6548\u6355\u6349\u65f6\u95f4\u62d3\u6251\u53d8\u5316\uff0c\u5728\u53bb\u566a\u548c\u53bb\u6a21\u7cca\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u56fe\u57fa\u53d8\u6362\u548c\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\u3002", "conclusion": "DMPJFRFT\u901a\u8fc7\u65f6\u53d8\u5206\u6570\u53c2\u6570\u5b9e\u73b0\u4e86\u5bf9\u52a8\u6001\u56fe\u7ed3\u6784\u7684\u81ea\u9002\u5e94\u8c31\u5efa\u6a21\uff0c\u4e3a\u52a8\u6001\u56fe\u4fe1\u53f7\u5904\u7406\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u6709\u6548\u7684\u5de5\u5177\u3002"}}
{"id": "2511.16639", "categories": ["eess.AS", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.16639", "abs": "https://arxiv.org/abs/2511.16639", "authors": ["Wei-Cheng Tseng", "David Harwath"], "title": "Codec2Vec: Self-Supervised Speech Representation Learning Using Neural Speech Codecs", "comment": "To be presented at ASRU 2025", "summary": "Recent advancements in neural audio codecs have not only enabled superior audio compression but also enhanced speech synthesis techniques. Researchers are now exploring their potential as universal acoustic feature extractors for a broader range of speech processing tasks. Building on this trend, we introduce Codec2Vec, the first speech representation learning framework that relies exclusively on discrete audio codec units. This approach offers several advantages, including improved data storage and transmission efficiency, faster training, and enhanced data privacy. We explore masked prediction with various training target derivation strategies to thoroughly understand the effectiveness of this framework. Evaluated on the SUPERB benchmark, Codec2Vec achieves competitive performance compared to continuous-input models while reducing storage requirements by up to 16.5x and training time by 2.3x, showcasing its scalability and efficiency.", "AI": {"tldr": "Codec2Vec\u662f\u9996\u4e2a\u57fa\u4e8e\u79bb\u6563\u97f3\u9891\u7f16\u89e3\u7801\u5355\u5143\u7684\u8bed\u97f3\u8868\u793a\u5b66\u4e60\u6846\u67b6\uff0c\u5728SUPERB\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u5b58\u50a8\u9700\u6c42\u548c\u8bad\u7ec3\u65f6\u95f4\u3002", "motivation": "\u63a2\u7d22\u795e\u7ecf\u97f3\u9891\u7f16\u89e3\u7801\u5668\u4f5c\u4e3a\u901a\u7528\u58f0\u5b66\u7279\u5f81\u63d0\u53d6\u5668\u7684\u6f5c\u529b\uff0c\u5229\u7528\u79bb\u6563\u97f3\u9891\u7f16\u89e3\u7801\u5355\u5143\u7684\u4f18\u52bf\u6765\u6539\u8fdb\u8bed\u97f3\u5904\u7406\u4efb\u52a1\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u79bb\u6563\u97f3\u9891\u7f16\u89e3\u7801\u5355\u5143\u7684\u8868\u793a\u5b66\u4e60\u6846\u67b6\uff0c\u4f7f\u7528\u63a9\u7801\u9884\u6d4b\u548c\u591a\u79cd\u8bad\u7ec3\u76ee\u6807\u63a8\u5bfc\u7b56\u7565\u3002", "result": "\u5728SUPERB\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u4e0e\u8fde\u7eed\u8f93\u5165\u6a21\u578b\u7ade\u4e89\u7684\u6027\u80fd\uff0c\u540c\u65f6\u5b58\u50a8\u9700\u6c42\u51cf\u5c1116.5\u500d\uff0c\u8bad\u7ec3\u65f6\u95f4\u51cf\u5c112.3\u500d\u3002", "conclusion": "Codec2Vec\u5c55\u793a\u4e86\u79bb\u6563\u97f3\u9891\u7f16\u89e3\u7801\u5355\u5143\u5728\u8bed\u97f3\u8868\u793a\u5b66\u4e60\u4e2d\u7684\u6709\u6548\u6027\uff0c\u5177\u6709\u66f4\u597d\u7684\u53ef\u6269\u5c55\u6027\u548c\u6548\u7387\u3002"}}
{"id": "2511.16327", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.16327", "abs": "https://arxiv.org/abs/2511.16327", "authors": ["Deqiao Gan", "Xiaoxia Xu", "Xiaohu Ge", "Yuanwei Liu"], "title": "Revealing computation-communication trade-off in Segmented Pinching Antenna System (PASS)", "comment": null, "summary": "A joint communication and computation (JCC) framework using segmented pinching antenna system (PASS) is proposed, where both the communication bit streams and computation data are simultaneously transmitted via uplink communications. The segmented PASS design is used to yield the tractable uplink transmission, and to mitigate large-scale path loss and in-waveguide loss. Based on three operating protocols, namely segment selection (SS), segment aggregation (SA), and segment multiplexing (SM), the joint transmit and receive beamforming problem is formulated: 1) The mean square error (MSE) minimization problem is formulated for computation-oriented cases. To address this problem, a low-complexity alternating optimization-minimum mean square error (AO-MMSE) algorithm is developed. This problem is decomposed into receiver-side and transmitter-side MSE subproblems that are iteratively optimized by MMSE receivers to obtain the closed-form solutions. It is mathematically proved that the segmented JCC-PASS framework significantly outperforms the conventional PASS for the average in-waveguide propagation gain. 2) The weighted sum rate (WSR) maximization problem is formulated for communication-oriented cases. To solve the decomposed receiver-side and transmitter-side MSE subproblems, the AO-weighted minimum mean square error (AO-WMMSE) algorithm is further developed. An auxiliary weight variable is introduced to linearize the WSR function and is alternatively optimized based on WMMSE to derive the closed-form solutions. Simulation results demonstrate that: i) The proposed JCC-PASS framework achieves up to 70.65% and 45.32% reductions in MSE compared with conventional MIMO and conventional PASS, and ii) it reaches 87.70% and 51.35% improvements in WSR compared with conventional MIMO and conventional PASS, respectively.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u57fa\u4e8e\u5206\u6bb5\u5939\u634f\u5929\u7ebf\u7cfb\u7edf\u7684\u8054\u5408\u901a\u4fe1\u4e0e\u8ba1\u7b97\u6846\u67b6\uff0c\u901a\u8fc7\u4e09\u79cd\u64cd\u4f5c\u534f\u8bae\u5b9e\u73b0\u901a\u4fe1\u6bd4\u7279\u6d41\u548c\u8ba1\u7b97\u6570\u636e\u7684\u540c\u6b65\u4e0a\u884c\u4f20\u8f93\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u5747\u65b9\u8bef\u5dee\u5e76\u63d0\u9ad8\u4e86\u52a0\u6743\u548c\u901f\u7387\u3002", "motivation": "\u4f20\u7edf\u901a\u4fe1\u7cfb\u7edf\u5728\u5904\u7406\u8054\u5408\u901a\u4fe1\u4e0e\u8ba1\u7b97\u4efb\u52a1\u65f6\u5b58\u5728\u6027\u80fd\u9650\u5236\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u540c\u65f6\u4f20\u8f93\u901a\u4fe1\u6570\u636e\u548c\u8ba1\u7b97\u6570\u636e\u7684\u9ad8\u6548\u6846\u67b6\uff0c\u4ee5\u5e94\u5bf9\u5927\u89c4\u6a21\u8def\u5f84\u635f\u8017\u548c\u6ce2\u5bfc\u5185\u635f\u8017\u3002", "method": "\u91c7\u7528\u5206\u6bb5\u5939\u634f\u5929\u7ebf\u7cfb\u7edf\u8bbe\u8ba1\uff0c\u63d0\u51fa\u4e09\u79cd\u64cd\u4f5c\u534f\u8bae\uff1a\u6bb5\u9009\u62e9\u3001\u6bb5\u805a\u5408\u548c\u6bb5\u590d\u7528\u3002\u9488\u5bf9\u8ba1\u7b97\u5bfc\u5411\u573a\u666f\u5f00\u53d1AO-MMSE\u7b97\u6cd5\uff0c\u9488\u5bf9\u901a\u4fe1\u5bfc\u5411\u573a\u666f\u5f00\u53d1AO-WMMSE\u7b97\u6cd5\uff0c\u901a\u8fc7\u4ea4\u66ff\u4f18\u5316\u6c42\u89e3\u6536\u53d1\u6ce2\u675f\u6210\u5f62\u95ee\u9898\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u663e\u793a\uff1a\u4e0e\u4f20\u7edf\u7684MIMO\u548cPASS\u76f8\u6bd4\uff0c\u63d0\u51fa\u7684JCC-PASS\u6846\u67b6\u5728\u5747\u65b9\u8bef\u5dee\u4e0a\u5206\u522b\u964d\u4f4e\u4e8670.65%\u548c45.32%\uff0c\u5728\u52a0\u6743\u548c\u901f\u7387\u4e0a\u5206\u522b\u63d0\u9ad8\u4e8687.70%\u548c51.35%\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u5206\u6bb5JCC-PASS\u6846\u67b6\u5728\u8054\u5408\u901a\u4fe1\u4e0e\u8ba1\u7b97\u6027\u80fd\u4e0a\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u4e3a\u672a\u6765\u65e0\u7ebf\u901a\u4fe1\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.16346", "categories": ["eess.SP", "cs.LG", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.16346", "abs": "https://arxiv.org/abs/2511.16346", "authors": ["Deniz Kasap", "Taraneh Aminosharieh Najafi", "J\u00e9r\u00f4me Paul R\u00e9my Thevenot", "Jonathan Dan", "Stefano Albini", "David Atienza"], "title": "VersaPants: A Loose-Fitting Textile Capacitive Sensing System for Lower-Body Motion Capture", "comment": "14 pages, 8 figures", "summary": "We present VersaPants, the first loose-fitting, textile-based capacitive sensing system for lower-body motion capture, built on the open-hardware VersaSens platform. By integrating conductive textile patches and a compact acquisition unit into a pair of pants, the system reconstructs lower-body pose without compromising comfort. Unlike IMU-based systems that require user-specific fitting or camera-based methods that compromise privacy, our approach operates without fitting adjustments and preserves user privacy. VersaPants is a custom-designed smart garment featuring 6 capacitive channels per leg. We employ a lightweight Transformer-based deep learning model that maps capacitance signals to joint angles, enabling embedded implementation on edge platforms. To test our system, we collected approximately 3.7 hours of motion data from 11 participants performing 16 daily and exercise-based movements. The model achieves a mean per-joint position error (MPJPE) of 11.96 cm and a mean per-joint angle error (MPJAE) of 12.3 degrees across the hip, knee, and ankle joints, indicating the model's ability to generalize to unseen users and movements. A comparative analysis of existing textile-based deep learning architectures reveals that our model achieves competitive reconstruction performance with up to 22 times fewer parameters and 18 times fewer FLOPs, enabling real-time inference at 42 FPS on a commercial smartwatch without quantization. These results position VersaPants as a promising step toward scalable, comfortable, and embedded motion-capture solutions for fitness, healthcare, and wellbeing applications.", "AI": {"tldr": "VersaPants\u662f\u9996\u4e2a\u57fa\u4e8e\u7eba\u7ec7\u54c1\u7684\u7535\u5bb9\u4f20\u611f\u7cfb\u7edf\uff0c\u7528\u4e8e\u6355\u6349\u4e0b\u534a\u8eab\u8fd0\u52a8\uff0c\u65e0\u9700\u7528\u6237\u7279\u5b9a\u6821\u51c6\uff0c\u4fdd\u62a4\u9690\u79c1\uff0c\u5e76\u80fd\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u5b9e\u65f6\u8fd0\u884c\u3002", "motivation": "\u73b0\u6709IMU\u7cfb\u7edf\u9700\u8981\u7528\u6237\u7279\u5b9a\u6821\u51c6\uff0c\u6444\u50cf\u5934\u65b9\u6cd5\u4fb5\u72af\u9690\u79c1\uff0c\u9700\u8981\u4e00\u79cd\u65e2\u8212\u9002\u53c8\u4fdd\u62a4\u9690\u79c1\u7684\u8fd0\u52a8\u6355\u6349\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5728\u88e4\u5b50\u4e2d\u96c6\u6210\u5bfc\u7535\u7eba\u7ec7\u8d34\u7247\u548c\u7d27\u51d1\u91c7\u96c6\u5355\u5143\uff0c\u4f7f\u7528\u8f7b\u91cf\u7ea7Transformer\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5c06\u7535\u5bb9\u4fe1\u53f7\u6620\u5c04\u5230\u5173\u8282\u89d2\u5ea6\u3002", "result": "\u572811\u540d\u53c2\u4e0e\u8005\u7684\u6d4b\u8bd5\u4e2d\uff0c\u5e73\u5747\u5173\u8282\u4f4d\u7f6e\u8bef\u5dee11.96\u5398\u7c73\uff0c\u5e73\u5747\u5173\u8282\u89d2\u5ea6\u8bef\u5dee12.3\u5ea6\uff0c\u6a21\u578b\u80fd\u5728\u667a\u80fd\u624b\u8868\u4e0a\u4ee542FPS\u5b9e\u65f6\u8fd0\u884c\u3002", "conclusion": "VersaPants\u4e3a\u5065\u8eab\u3001\u533b\u7597\u548c\u5065\u5eb7\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u3001\u8212\u9002\u4e14\u53ef\u5d4c\u5165\u7684\u8fd0\u52a8\u6355\u6349\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.16352", "categories": ["eess.SP", "cs.IT"], "pdf": "https://arxiv.org/pdf/2511.16352", "abs": "https://arxiv.org/abs/2511.16352", "authors": ["Till-Yannic M\u00fcller", "Frederik Zumegen", "Reinhard Wiesmayr", "Emre G\u00f6n\u00fclta\u015f", "Christoph Studer"], "title": "Neural Positioning Without External Reference", "comment": "Submitted to a journal", "summary": "Channel state information (CSI)-based user equipment (UE) positioning with neural networks -- referred to as neural positioning -- is a promising approach for accurate off-device UE localization. Most existing methods train their neural networks with ground-truth position labels obtained from external reference positioning systems, which requires costly hardware and renders label acquisition difficult in large areas. In this work, we propose a novel neural positioning pipeline that avoids the need for any external reference positioning system. Our approach trains the positioning network only using CSI acquired off-device and relative displacement commands executed on commercial off-the-shelf (COTS) robot platforms, such as robotic vacuum cleaners -- such an approach enables inexpensive training of accurate neural positioning functions over large areas. We evaluate our method in three real-world scenarios, ranging from small line-of-sight (LoS) areas to larger non-line-of-sight (NLoS) environments, using CSI measurements acquired in IEEE 802.11 Wi-Fi and 5G New Radio (NR) systems. Our experiments demonstrate that the proposed neural positioning pipeline achieves UE localization accuracies close to state-of-the-art methods that require externally acquired high-precision ground-truth position labels for training.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u5916\u90e8\u5b9a\u4f4d\u7cfb\u7edf\u7684\u795e\u7ecf\u7f51\u7edc\u5b9a\u4f4d\u65b9\u6cd5\uff0c\u4ec5\u4f7f\u7528\u4fe1\u9053\u72b6\u6001\u4fe1\u606f\u548c\u673a\u5668\u4eba\u76f8\u5bf9\u4f4d\u79fb\u547d\u4ee4\u8fdb\u884c\u8bad\u7ec3\uff0c\u5728Wi-Fi\u548c5G\u7cfb\u7edf\u4e2d\u5b9e\u73b0\u4e86\u63a5\u8fd1\u6700\u5148\u8fdb\u65b9\u6cd5\u7684\u5b9a\u4f4d\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eCSI\u7684\u795e\u7ecf\u7f51\u7edc\u5b9a\u4f4d\u65b9\u6cd5\u9700\u8981\u5916\u90e8\u53c2\u8003\u5b9a\u4f4d\u7cfb\u7edf\u63d0\u4f9b\u5730\u9762\u771f\u5b9e\u4f4d\u7f6e\u6807\u7b7e\uff0c\u786c\u4ef6\u6210\u672c\u9ad8\u4e14\u5728\u5927\u9762\u79ef\u533a\u57df\u96be\u4ee5\u83b7\u53d6\u6807\u7b7e\u3002", "method": "\u4f7f\u7528\u5546\u7528\u673a\u5668\u4eba\u5e73\u53f0\uff08\u5982\u626b\u5730\u673a\u5668\u4eba\uff09\u7684\u76f8\u5bf9\u4f4d\u79fb\u547d\u4ee4\u548c\u83b7\u53d6\u7684CSI\u6570\u636e\u8fdb\u884c\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\uff0c\u65e0\u9700\u5916\u90e8\u53c2\u8003\u5b9a\u4f4d\u7cfb\u7edf\u3002", "result": "\u5728\u4e09\u4e2a\u771f\u5b9e\u573a\u666f\uff08\u4ece\u5c0f\u7684\u89c6\u8ddd\u533a\u57df\u5230\u8f83\u5927\u7684\u975e\u89c6\u8ddd\u73af\u5883\uff09\u4e2d\u8bc4\u4f30\uff0c\u4f7f\u7528IEEE 802.11 Wi-Fi\u548c5G NR\u7cfb\u7edf\u7684CSI\u6d4b\u91cf\uff0c\u5b9e\u73b0\u4e86\u63a5\u8fd1\u9700\u8981\u5916\u90e8\u9ad8\u7cbe\u5ea6\u6807\u7b7e\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\u7684\u5b9a\u4f4d\u7cbe\u5ea6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u5728\u5927\u9762\u79ef\u533a\u57df\u4ee5\u4f4e\u6210\u672c\u8bad\u7ec3\u51c6\u786e\u7684\u795e\u7ecf\u7f51\u7edc\u5b9a\u4f4d\u529f\u80fd\uff0c\u907f\u514d\u4e86\u5bf9\u5916\u90e8\u53c2\u8003\u5b9a\u4f4d\u7cfb\u7edf\u7684\u4f9d\u8d56\u3002"}}
{"id": "2511.16369", "categories": ["eess.SP", "cs.NI"], "pdf": "https://arxiv.org/pdf/2511.16369", "abs": "https://arxiv.org/abs/2511.16369", "authors": ["Jaron Fontaine", "Mohammad Cheraghinia", "John Strassner", "Adnan Shahid", "Eli De Poorter"], "title": "Reasoning Meets Representation: Envisioning Neuro-Symbolic Wireless Foundation Models", "comment": "Accepted at the 39th Conference on Neural Information Processing Systems (NeurIPS 2025) Workshop: AI and ML for Next-Generation Wireless Communications and Networking (AI4NextG)", "summary": "Recent advances in Wireless Physical Layer Foundation Models (WPFMs) promise a new paradigm of universal Radio Frequency (RF) representations. However, these models inherit critical limitations found in deep learning such as the lack of explainability, robustness, adaptability, and verifiable compliance with physical and regulatory constraints. In addition, the vision for an AI-native 6G network demands a level of intelligence that is deeply embedded into the systems and is trustworthy. In this vision paper, we argue that the neuro-symbolic paradigm, which integrates data-driven neural networks with rule- and logic-based symbolic reasoning, is essential for bridging this gap. We envision a novel Neuro-Symbolic framework that integrates universal RF embeddings with symbolic knowledge graphs and differentiable logic layers. This hybrid approach enables models to learn from large datasets while reasoning over explicit domain knowledge, enabling trustworthy, generalizable, and efficient wireless AI that can meet the demands of future networks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u795e\u7ecf\u7b26\u53f7\u8303\u5f0f\u6765\u89e3\u51b3\u65e0\u7ebf\u7269\u7406\u5c42\u57fa\u7840\u6a21\u578b\u5728\u53ef\u89e3\u91ca\u6027\u3001\u9c81\u68d2\u6027\u3001\u9002\u5e94\u6027\u7b49\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u901a\u8fc7\u6574\u5408\u795e\u7ecf\u7f51\u7edc\u7684\u8868\u793a\u5b66\u4e60\u4e0e\u7b26\u53f7\u63a8\u7406\u6765\u6784\u5efa\u53ef\u4fe1\u8d56\u7684\u65e0\u7ebfAI\u7cfb\u7edf\u3002", "motivation": "\u73b0\u6709\u65e0\u7ebf\u7269\u7406\u5c42\u57fa\u7840\u6a21\u578b\u7ee7\u627f\u4e86\u6df1\u5ea6\u5b66\u4e60\u7684\u5c40\u9650\u6027\uff0c\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u3001\u9c81\u68d2\u6027\u548c\u5bf9\u7269\u7406\u7ea6\u675f\u7684\u5408\u89c4\u6027\u9a8c\u8bc1\u30026G\u7f51\u7edc\u9700\u8981\u6df1\u5ea6\u5d4c\u5165\u4e14\u53ef\u4fe1\u8d56\u7684\u667a\u80fd\u7cfb\u7edf\u3002", "method": "\u63d0\u51fa\u795e\u7ecf\u7b26\u53f7\u6846\u67b6\uff0c\u6574\u5408\u901a\u7528RF\u5d4c\u5165\u3001\u7b26\u53f7\u77e5\u8bc6\u56fe\u8c31\u548c\u53ef\u5fae\u5206\u903b\u8f91\u5c42\uff0c\u5b9e\u73b0\u6570\u636e\u9a71\u52a8\u5b66\u4e60\u4e0e\u9886\u57df\u77e5\u8bc6\u63a8\u7406\u7684\u878d\u5408\u3002", "result": "\u8be5\u6df7\u5408\u65b9\u6cd5\u80fd\u591f\u4ece\u5927\u6570\u636e\u4e2d\u5b66\u4e60\uff0c\u540c\u65f6\u57fa\u4e8e\u663e\u5f0f\u9886\u57df\u77e5\u8bc6\u8fdb\u884c\u63a8\u7406\uff0c\u4e3a\u672a\u6765\u7f51\u7edc\u9700\u6c42\u63d0\u4f9b\u53ef\u4fe1\u8d56\u3001\u53ef\u6cdb\u5316\u4e14\u9ad8\u6548\u7684\u65e0\u7ebfAI\u3002", "conclusion": "\u795e\u7ecf\u7b26\u53f7\u8303\u5f0f\u5bf9\u4e8e\u5f25\u5408\u5f53\u524d\u65e0\u7ebfAI\u5c40\u9650\u6027\u4e0e6G\u7f51\u7edc\u53ef\u4fe1\u667a\u80fd\u9700\u6c42\u4e4b\u95f4\u7684\u5dee\u8ddd\u81f3\u5173\u91cd\u8981\uff0c\u662f\u5b9e\u73b0\u4e0b\u4e00\u4ee3\u65e0\u7ebfAI\u7cfb\u7edf\u7684\u5173\u952e\u8def\u5f84\u3002"}}
{"id": "2511.16472", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.16472", "abs": "https://arxiv.org/abs/2511.16472", "authors": ["Niko Lindvall", "Mikko Heino", "Mikko Valkama"], "title": "3-20 GHz Wideband Tightly-Coupled Dual-Polarized Vivaldi Antenna Array", "comment": null, "summary": "Very wideband apertures are needed in positioning, sensing, spectrum monitoring, and modern spread spectrum, e.g., frequency hopping systems. Vivaldi antennas are one of the prominent choices for the aforementioned systems due to their natural wideband characteristics. Furthermore, tightly-coupled antenna arrays have been researched in the recent years to extend the lower band edge of compact arrays by taking advantage of the strong mutual coupling between the elements especially with dipole elements, but not with dual-polarized Vivaldi antennas. This paper presents a novel tightly-coupled dual-polarized antipodal Vivaldi antenna (TC-AVA) with -6 dB impedance bandwidth of 3 to 20 GHz. The tight coupling by overlapping the Vivaldi leaves is shown to extend the lower band edge from 3.75 to 3 GHz and 2.75 GHz, an improvement of 20% to 25% for both polarizations, compared with an isolated antipodal Vivaldi element.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7d27\u5bc6\u8026\u5408\u53cc\u6781\u5316\u5bf9\u8dd6\u7ef4\u74e6\u5c14\u7b2c\u5929\u7ebf\uff0c\u901a\u8fc7\u91cd\u53e0\u5929\u7ebf\u53f6\u7247\u5b9e\u73b0\u7d27\u5bc6\u8026\u5408\uff0c\u5c06\u5de5\u4f5c\u9891\u5e26\u4ece3.75GHz\u6269\u5c55\u52303GHz\u548c2.75GHz\uff0c\u5e26\u5bbd\u63d0\u534720-25%\u3002", "motivation": "\u5b9a\u4f4d\u3001\u4f20\u611f\u3001\u9891\u8c31\u76d1\u6d4b\u548c\u73b0\u4ee3\u6269\u9891\u7cfb\u7edf\u9700\u8981\u8d85\u5bbd\u5e26\u5929\u7ebf\u5b54\u5f84\u3002\u7ef4\u74e6\u5c14\u7b2c\u5929\u7ebf\u56e0\u5176\u5929\u7136\u5bbd\u5e26\u7279\u6027\u6210\u4e3a\u7406\u60f3\u9009\u62e9\uff0c\u4f46\u4f20\u7edf\u8bbe\u8ba1\u5728\u7d27\u51d1\u9635\u5217\u4e2d\u7684\u4f4e\u9891\u6027\u80fd\u6709\u9650\u3002", "method": "\u91c7\u7528\u7d27\u5bc6\u8026\u5408\u53cc\u6781\u5316\u5bf9\u8dd6\u7ef4\u74e6\u5c14\u7b2c\u5929\u7ebf\u8bbe\u8ba1\uff0c\u901a\u8fc7\u91cd\u53e0\u7ef4\u74e6\u5c14\u7b2c\u5929\u7ebf\u53f6\u7247\u5b9e\u73b0\u5143\u4ef6\u95f4\u7684\u5f3a\u4e92\u8026\uff0c\u4ece\u800c\u6269\u5c55\u4f4e\u9891\u5de5\u4f5c\u8303\u56f4\u3002", "result": "\u5b9e\u73b0\u4e863-20GHz\u7684-6dB\u963b\u6297\u5e26\u5bbd\uff0c\u76f8\u6bd4\u5b64\u7acb\u5929\u7ebf\u5143\u4ef6\uff0c\u4f4e\u9891\u8fb9\u7f18\u4ece3.75GHz\u6269\u5c55\u52303GHz\u548c2.75GHz\uff0c\u6027\u80fd\u63d0\u534720-25%\u3002", "conclusion": "\u7d27\u5bc6\u8026\u5408\u6280\u672f\u6210\u529f\u6269\u5c55\u4e86\u53cc\u6781\u5316\u7ef4\u74e6\u5c14\u7b2c\u5929\u7ebf\u7684\u4f4e\u9891\u5de5\u4f5c\u8303\u56f4\uff0c\u4e3a\u8d85\u5bbd\u5e26\u7cfb\u7edf\u63d0\u4f9b\u4e86\u66f4\u7d27\u51d1\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.16627", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.16627", "abs": "https://arxiv.org/abs/2511.16627", "authors": ["Pengxin Li", "Yimin Zhou", "Jie Min", "Yirong Wang", "Wei Liang", "Wang Li"], "title": "TFCDiff: Robust ECG Denoising via Time-Frequency Complementary Diffusion", "comment": null, "summary": "Ambulatory electrocardiogram (ECG) readings are prone to mixed noise from physical activities, including baseline wander (BW), muscle artifact (MA), and electrode motion artifact (EM). Developing a method to remove such complex noise and reconstruct high-fidelity signals is clinically valuable for diagnostic accuracy. However, denoising of multi-beat ECG segments remains understudied and poses technical challenges. To address this, we propose Time-Frequency Complementary Diffusion (TFCDiff), a novel approach that operates in the Discrete Cosine Transform (DCT) domain and uses the DCT coefficients of noisy signals as conditioning input. To refine waveform details, we incorporate Temporal Feature Enhancement Mechanism (TFEM) to reinforce temporal representations and preserve key physiological information. Comparative experiments on a synthesized dataset demonstrate that TFCDiff achieves state-of-the-art performance across five evaluation metrics. Furthermore, TFCDiff shows superior generalization on the unseen SimEMG Database, outperforming all benchmark models. Notably, TFCDiff processes raw 10-second sequences and maintains robustness under flexible random mixed noise (fRMN), enabling plug-and-play deployment in wearable ECG monitors for high-motion scenarios. Source code is available at https://github.com/Miroircivil/TFCDiff.", "AI": {"tldr": "TFCDiff\u662f\u4e00\u79cd\u57fa\u4e8e\u79bb\u6563\u4f59\u5f26\u53d8\u6362\u7684\u6269\u6563\u6a21\u578b\uff0c\u7528\u4e8e\u53bb\u9664\u52a8\u6001\u5fc3\u7535\u56fe\u4e2d\u7684\u6df7\u5408\u566a\u58f0\uff0c\u5728\u591a\u4e2a\u8bc4\u4f30\u6307\u6807\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5e76\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u52a8\u6001\u5fc3\u7535\u56fe\u5bb9\u6613\u53d7\u5230\u57fa\u7ebf\u6f02\u79fb\u3001\u808c\u8089\u4f2a\u5f71\u548c\u7535\u6781\u8fd0\u52a8\u4f2a\u5f71\u7b49\u6df7\u5408\u566a\u58f0\u5e72\u6270\uff0c\u5f71\u54cd\u8bca\u65ad\u51c6\u786e\u6027\uff0c\u800c\u591a\u62cd\u5fc3\u7535\u4fe1\u53f7\u6bb5\u7684\u53bb\u566a\u7814\u7a76\u4ecd\u4e0d\u8db3\u4e14\u6280\u672f\u6311\u6218\u5927\u3002", "method": "\u63d0\u51faTFCDiff\u65b9\u6cd5\uff0c\u5728DCT\u57df\u64cd\u4f5c\uff0c\u4f7f\u7528\u566a\u58f0\u4fe1\u53f7\u7684DCT\u7cfb\u6570\u4f5c\u4e3a\u6761\u4ef6\u8f93\u5165\uff0c\u5e76\u5f15\u5165\u65f6\u95f4\u7279\u5f81\u589e\u5f3a\u673a\u5236\u6765\u5f3a\u5316\u65f6\u95f4\u8868\u5f81\u5e76\u4fdd\u7559\u5173\u952e\u751f\u7406\u4fe1\u606f\u3002", "result": "\u5728\u5408\u6210\u6570\u636e\u96c6\u4e0a\u7684\u6bd4\u8f83\u5b9e\u9a8c\u663e\u793aTFCDiff\u5728\u4e94\u4e2a\u8bc4\u4f30\u6307\u6807\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5728\u672a\u89c1\u8fc7\u7684SimEMG\u6570\u636e\u5e93\u4e0a\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4f18\u4e8e\u6240\u6709\u57fa\u51c6\u6a21\u578b\u3002", "conclusion": "TFCDiff\u80fd\u591f\u5904\u7406\u539f\u59cb10\u79d2\u5e8f\u5217\u5e76\u5728\u7075\u6d3b\u968f\u673a\u6df7\u5408\u566a\u58f0\u4e0b\u4fdd\u6301\u9c81\u68d2\u6027\uff0c\u53ef\u5728\u53ef\u7a7f\u6234\u5fc3\u7535\u56fe\u76d1\u6d4b\u5668\u4e2d\u5373\u63d2\u5373\u7528\u90e8\u7f72\uff0c\u9002\u7528\u4e8e\u9ad8\u8fd0\u52a8\u573a\u666f\u3002"}}

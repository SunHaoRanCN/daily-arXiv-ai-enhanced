{"id": "2511.16827", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.16827", "abs": "https://arxiv.org/abs/2511.16827", "authors": ["Bassel Abou Ali Modad", "Xin Yu", "Yao-Yi Chiang", "Andreas F. Molisch"], "title": "Line-of-Sight Probability in Macrocells: Framework, Statistical Models, and Parametrization from Massive Real World Datasets in the USA", "comment": null, "summary": "Accurate modeling of line-of-sight (LOS) probability is crucial for wireless channel description and coverage planning. The presence of a LOS impacts other channel characteristics such as pathloss, fading depth, delay- and angular spread, etc.. Existing models, although useful, are based on very limited datasets. In this paper, we establish a framework to produce high accuracy LOS models from geospatial data in different environments, and apply it to create a LOS model for macrocells, using datasets of the United States (US) on a nationalscale, using more than 13, 000 locations of real-world macrocells. Based on this we create a new, fully parameterized model that better describes macrocell deployments in the US than the 3GPP model. We furthermore demonstrate that for improved accuracy the LOS probability should be modeled on a per cell basis, and the model parameters treated as random variables; we provide a full description and parameterization of this novel approach and by simulations show that it better predicts the inter-cell interference at the cell-edge than an average-based model."}
{"id": "2511.16888", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.16888", "abs": "https://arxiv.org/abs/2511.16888", "authors": ["Haiquan Zhao", "Xiong Yin", "Jinhui Hu"], "title": "State-of-charge estimation of lithium-ion batteries using a tree seed and genetic algorithm-optimized generalized mixture minimum error entropy-based square root cubature Kalman filter", "comment": null, "summary": "The cubature Kalman filter based on minimum error entropy (MEE-CKF) offers accurate and robust performance in state of charge (SOC) estimation. However, due to the inflexibility of the minimum error entropy (MEE), this algorithm demonstrates limited robustness when confronted with more complex noise environments. To address these limitations, this paper proposes a generalized mixture minimum error entropy-based (GMMEE) square-root cubature Kalman filter (GMMEE-SRCKF). The square-root algorithm ensures improved numerical stability and avoids covariance degeneration, while the GMMEE criterion with two flexible kernels adapts effectively to non-Gaussian noise. Moreover, a hybrid tree seed and genetic algorithm (TSGA) is introduced to optimize the kernel parameters automatically. Experimental results confirm that the TSGA-optimized GMMEE-SRCKF outperforms existing robust filters, achieving the root mean square error (RMSE) of less than 0.5%."}
{"id": "2511.17007", "categories": ["eess.SP", "cs.LG", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.17007", "abs": "https://arxiv.org/abs/2511.17007", "authors": ["Wangqian Chen", "Junting Chen", "Shuguang Cui"], "title": "Generative MIMO Beam Map Construction for Location Recovery and Beam Tracking", "comment": null, "summary": "Machine learning (ML) has greatly advanced data-driven channel modeling and resource optimization in wireless communication systems. However, most existing ML-based methods rely on large, accurately labeled datasets with location information, which are often difficult and costly to obtain. This paper proposes a generative framework to recover location labels directly from sequences of sparse channel state information (CSI) measurements, without explicit location labels for radio map construction. Instead of directly storing raw CSI, we learn a compact low-dimensional radio map embedding and leverage a generative model to reconstruct the high-dimensional CSI. Specifically, to address the uncertainty of sparse CSI, a dual-scale feature extraction scheme is designed to enhance feature representation by jointly exploiting correlations from angular space and across neighboring samples. We develop a hybrid recurrent-convolutional encoder to learn mobility patterns, which combines a truncation strategy and multi-scale convolutions in the recurrent neural network (RNN) to ensure feature robustness against short-term fluctuations. Unlike conventional Gaussian priors in latent space, we embed a learnable radio map to capture the location information by encoding high-level positional features from CSI measurements. Finally, a diffusion-based generative decoder reconstructs the full CSI with high fidelity by conditioning on the positional features in the radio map. Numerical experiments demonstrate that the proposed model can improve localization accuracy by over 30% and achieve a 20% capacity gain in non-line-of-sight (NLOS) scenarios compared with model-based Kalman filter approaches."}
{"id": "2511.17058", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.17058", "abs": "https://arxiv.org/abs/2511.17058", "authors": ["Ziyuan Zheng", "Qingqing Wu", "Wen Chen", "Weiren Zhu", "Ying Gao"], "title": "Movable Intelligent Surface-Enabled Wireless Communications: Static Phase Shifts with Mechanical Reconfigurability", "comment": "16 pages, 18 figures, submitted for possible publication in IEEE journals", "summary": "Intelligent surfaces that reshape electromagnetic waves are regarded as disruptive technologies for wireless networks. However, existing designs sit at two costly extremes: dynamic reconfigurable intelligent surfaces (RISs) offer fine beam control but require dense cabling, continuous power consumption, and substantial signaling overhead, whereas low-cost static surfaces require no control lines or electronics but are limited to a single beam pattern. This disparity leaves a practical gap for quasi-static environments, such as industrial Internet-of-things and smart agriculture scenarios, where channels are stable with user demands changing only occasionally or periodically, and neither extreme is sufficiently economical or flexible. To bridge this gap, we propose a novel movable intelligent surface (MIS) architecture, whose beam patterns are switched not by electronic phase tuning but by mechanically sliding a small, pre-phased secondary metasurface layer across a larger, likewise static primary layer. We develop an MIS signal model that characterizes the interaction between static phase elements with dynamic geometry via binary selection matrices. Based on this model, we formulate a new type of optimization problems that jointly design static phase shifts and the overlapping position selection of MS2 (equal to beam pattern scheduling). Efficient algorithms based on the penalty method, block coordinate descent, and Riemannian manifold optimization are proposed to tackle these mixed-integer non-convex problems. Simulation results demonstrate that the proposed MIS architecture substantially narrows the performance gap between single-layer static surfaces and dynamic RISs, providing a practical and flexible solution for quasi-static wireless applications."}
{"id": "2511.16757", "categories": ["eess.AS", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.16757", "abs": "https://arxiv.org/abs/2511.16757", "authors": ["Wei-Cheng Tseng", "Xuanru Zhou", "Mingyue Huo", "Yiwen Shao", "Hao Zhang", "Dong Yu"], "title": "Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation", "comment": "Work in progress", "summary": "Audio-language pretraining holds promise for general-purpose audio understanding, yet remains underexplored compared to its vision counterpart. While vision-language models like CLIP serve as widely adopted foundations, existing audio-language models primarily excel at retrieval tasks with limited adoption as general-purpose encoders. We identify three key barriers: limited large-scale audio-text corpora, insufficient caption diversity, and lack of systematic exploration and evaluation. To this end, we introduce CaptionStew, a 10.7M caption dataset aggregating diverse open-source audio-text corpora across multiple domains and captioning styles. Using this resource, we conduct the first comprehensive evaluation comparing contrastive and captioning objectives for audio representation learning across speech, music, and environmental sound tasks. Our results demonstrate that audio-language pretraining yields competitive, transferable representations. Through systematic data-scaling experiments, we reveal complementary objective strengths: contrastive learning achieves superior data efficiency at smaller scales, while captioning demonstrates better scalability on language-involved audio understanding tasks. We also find that common supervised initialization practices provide diminishing returns at scale, challenging current approaches. These findings establish audio-language pretraining as a viable pathway toward general-purpose audio representations, guiding future research. To accelerate progress, we release data preparation recipes, training protocols, and pretrained models, paving the way toward universal audio understanding."}
{"id": "2511.17136", "categories": ["cs.SD", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.17136", "abs": "https://arxiv.org/abs/2511.17136", "authors": ["Manh Pham Hung", "Changshuo Hu", "Ting Dang", "Dong Ma"], "title": "Device-Guided Music Transfer", "comment": null, "summary": "Device-guided music transfer adapts playback across unseen devices for users who lack them. Existing methods mainly focus on modifying the timbre, rhythm, harmony, or instrumentation to mimic genres or artists, overlooking the diverse hardware properties of the playback device (i.e., speaker). Therefore, we propose DeMT, which processes a speaker's frequency response curve as a line graph using a vision-language model to extract device embeddings. These embeddings then condition a hybrid transformer via feature-wise linear modulation. Fine-tuned on a self-collected dataset, DeMT enables effective speaker-style transfer and robust few-shot adaptation for unseen devices, supporting applications like device-style augmentation and quality enhancement."}
{"id": "2511.17062", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.17062", "abs": "https://arxiv.org/abs/2511.17062", "authors": ["Keying Zhu", "Xingyu Zhou", "Jie Yang", "Le Liang", "Shi Jin"], "title": "Super-Resolution ISAC Receivers: An MCMC-Based Gridless Sparse Bayesian Learning Approach", "comment": null, "summary": "Integrated sensing and communication (ISAC) is crucial for low-altitude wireless networks (LAWNs), where the safety-critical demand for high-accuracy sensing creates a trade-off between precision and complexity for conventional methods. To address this, we propose a novel gridless sparse Bayesian learning (SBL) framework for joint super-resolution multi-target detection and high-accuracy parameter estimation with manageable computational cost. Our model treats target parameters as continuous variables to bypass the grid limitations of conventional approaches. This SBL formulation, however, transforms the estimation task into a challenging high-dimensional inference problem, which we address by developing an enhanced gradient-based Markov chain Monte Carlo algorithm. Our method integrates mini-batch sampling and the Adam optimizer to ensure computational efficiency and rapid convergence. Finally, we validate the framework's robustness in strong clutter and provide a theoretical benchmark by deriving the corresponding Bayesian Cramer-Rao bound. Simulation results demonstrate remarkable super-resolution capabilities, successfully resolving multiple targets separated by merely 50% of the Rayleigh limit in range, 17% in velocity, and 52% in angle. At a signal-to-noise ratio of 20 dB, the algorithm achieves a multi-target detection probability exceeding 90% while concurrently delivering ultra-high accuracy, with root mean square error of 0.07 m, 0.024 m/s, and 0.015 degree for range, velocity, and angle, respectively. This robust performance, demonstrated against strong clutter, showcases its suitability for practical ISAC-LAWNs applications."}
{"id": "2511.17404", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2511.17404", "abs": "https://arxiv.org/abs/2511.17404", "authors": ["Guilherme Coelho"], "title": "The Artist is Present: Traces of Artists Resigind and Spawning in Text-to-Audio AI", "comment": null, "summary": "Text-to-audio (TTA) systems are rapidly transforming music creation and distribution, with platforms like Udio and Suno generating thousands of tracks daily and integrating into mainstream music platforms and ecosystems. These systems, trained on vast and largely undisclosed datasets, are fundamentally reshaping how music is produced, reproduced and consumed. This paper presents empirical evidence that artist-conditioned regions can be systematically microlocated through metatag-based prompt design, effectively enabling the spawning of artist-like content through strategic prompt engineering. Through systematic exploration of metatag-based prompt engineering techniques this research reveals how users can access the distinctive sonic signatures of specific artists, evidencing their inclusion in training datasets. Using descriptor constellations drawn from public music taxonomies, the paper demonstrates reproducible proximity to artists such as Bon Iver, Philip Glass, Panda Bear and William Basinski. The results indicate stable text-audio correspondences consistent with artist-specific training signals, enabling precise traversal of stylistic microlocations without explicitly naming artists. This capacity to summon artist-specific outputs shows that artists' creative works fuction as foundational material from which these systems generate new content, often without explicit consent or attribuition. Conceptually, the work clarifies how textual descriptors act as navigational cues in high-dimensional representation spaces; methodologically, it provides a replicable protocol for auditing stylistic inducibility. The findings raise immediate queestions for governance-attribution, consent and disclosure standards-and for creative practice, where induced stylistic proximity complicates boundaries between ownership, reproduction, imitation, creative agency and the ethics of algorithmic creation."}
{"id": "2511.17323", "categories": ["cs.SD", "cs.AI", "cs.CL", "cs.MM"], "pdf": "https://arxiv.org/pdf/2511.17323", "abs": "https://arxiv.org/abs/2511.17323", "authors": ["Callie C. Liao", "Duoduo Liao", "Ellie L. Zhang"], "title": "MusicAIR: A Multimodal AI Music Generation Framework Powered by an Algorithm-Driven Core", "comment": "Accepted by IEEE Big Data 2025", "summary": "Recent advances in generative AI have made music generation a prominent research focus. However, many neural-based models rely on large datasets, raising concerns about copyright infringement and high-performance costs. In contrast, we propose MusicAIR, an innovative multimodal AI music generation framework powered by a novel algorithm-driven symbolic music core, effectively mitigating copyright infringement risks. The music core algorithms connect critical lyrical and rhythmic information to automatically derive musical features, creating a complete, coherent melodic score solely from the lyrics. The MusicAIR framework facilitates music generation from lyrics, text, and images. The generated score adheres to established principles of music theory, lyrical structure, and rhythmic conventions. We developed Generate AI Music (GenAIM), a web tool using MusicAIR for lyric-to-song, text-to-music, and image-to-music generation. In our experiments, we evaluated AI-generated music scores produced by the system using both standard music metrics and innovative analysis that compares these compositions with original works. The system achieves an average key confidence of 85%, outperforming human composers at 79%, and aligns closely with established music theory standards, demonstrating its ability to generate diverse, human-like compositions. As a co-pilot tool, GenAIM can serve as a reliable music composition assistant and a possible educational composition tutor while simultaneously lowering the entry barrier for all aspiring musicians, which is innovative and significantly contributes to AI for music generation."}
{"id": "2511.17066", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.17066", "abs": "https://arxiv.org/abs/2511.17066", "authors": ["Duc Viet Nguyen", "Haiquan Zhao", "Jinhui Hu"], "title": "Distributed Cubature Kalman Filter based on MEEF with Adaptive Cauchy Kernel for State Estimation", "comment": "13 pages, 11 figures", "summary": "Nowadays, with the development of multi-sensor networks, the distributed cubature Kalman filter is one of the well-known existing schemes for state estimation, for which the influence of the non-Gaussian noise, abnormal data, and communication burden are urgent challenges. In this paper, a distributed cubature Kalman filter based on adaptive minimum error entropy with fiducial points (AMEEF) criterion (AMEEF-DCKF) is proposed to overcome the above limitations. Specifically, firstly, in order to solve the influence of various types of non-Gaussian noise and abnormal data, the AMEEF optimization criterion is designed, in which the kernels used are Cauchy kernels with adaptive bandwidth. At the same time, the designed optimization criterion has enhanced the numerical stability and optimized the kernel bandwidth value. Next, in order to address the communication burden problem in multi-sensor networks, where a leader and a follower are distinguished, a distributed algorithm is constructed to achieve an average consensus among these sensors, called leader-follower average consensus (LFAC). Additionally, the convergence proof of the average consensus algorithm and the computational complexity analysis of the AMEEF-DCKF algorithm are also presented. Finally, through a 10-node sensor network, the effectiveness of the proposed algorithm is demonstrated in estimating the state of the power system and navigating land vehicles in complex environments."}
{"id": "2511.17425", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2511.17425", "abs": "https://arxiv.org/abs/2511.17425", "authors": ["Guilherme Coelho"], "title": "AI in Music and Sound: Pedagogical Reflections, Post-Structuralist Approaches and Creative Outcomes in Seminar Practice", "comment": null, "summary": "This paper presents a pedagogical and conceptual account of the course AI in Music and Sound: Modalities, Tools and Creative Applications, offered within the Music Informatics and Media Art module of an M.Sc. in Audio Communication. The course engaged students with a range of AI modalities such as symbolic composition, voice synthesis, timbre transfer, neural audio synthesis, and text-to-audio systems, combining theoretical reflection with practice-based experimentation. Its central pedagogical move is a paired-études design: each modality is approached first through its intended affordances and then through a deliberately reframed or \"misused\" exercise that surfaces representational limits and alternative behaviours. Framed by medium theory and post-structuralist inquiry, we treated AI as a transmodal conduit-a system that translates and perturbs musical signs across textual, symbolic, timbral and audio domains. Evidence from student work and reflection indicates growth in technical fluency, medium awareness, and critical literacy, alongside the cultivation of experimental method and process-oriented listening. The paper outlines the course architecture, assessment design, and representative projects, and distils a set of design patterns for AI-music pedagogy (eg., prompt-conditioned interplays and semantic destabilisation in text-to-audio; latent space materialism in timbre transfer). It concludes with pedagogical recommendations that integrate creative practice with medium awareness and with cultural-epistemic analysis of AI technologies, preparing students to participate in how AI is understood, developed, and deployed with creative communities."}
{"id": "2511.17346", "categories": ["cs.SD", "cs.AI", "eess.SP", "physics.class-ph", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.17346", "abs": "https://arxiv.org/abs/2511.17346", "authors": ["Marius Rodrigues", "Louis Bahrman", "Roland Badeau", "Gaël Richard"], "title": "Is Phase Really Needed for Weakly-Supervised Dereverberation ?", "comment": null, "summary": "In unsupervised or weakly-supervised approaches for speech dereverberation, the target clean (dry) signals are considered to be unknown during training. In that context, evaluating to what extent information can be retrieved from the sole knowledge of reverberant (wet) speech becomes critical. This work investigates the role of the reverberant (wet) phase in the time-frequency domain. Based on Statistical Wave Field Theory, we show that late reverberation perturbs phase components with white, uniformly distributed noise, except at low frequencies. Consequently, the wet phase carries limited useful information and is not essential for weakly supervised dereverberation. To validate this finding, we train dereverberation models under a recent weak supervision framework and demonstrate that performance can be significantly improved by excluding the reverberant phase from the loss function."}
{"id": "2511.17122", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.17122", "abs": "https://arxiv.org/abs/2511.17122", "authors": ["Aron Schott", "Berk Acikgöz", "Omar Massoud", "Marina Petrova", "Ljiljana Simić"], "title": "Unleashing Sensor-Aided Environment Awareness for Beam Management in Beyond-5G Networks: An OpenAirInterface Experimental Platform", "comment": null, "summary": "Large antenna arrays and beamforming techniques are key components for exploiting the spectrum-rich FR2 bands in next-generation mobile communication networks. Given the site-specific spatio-temporal variations of the mm-wave channel, non-RF sensor inputs and environment awareness can be leveraged to greatly enhance beam management decisions, e.g. via machine learning (ML) techniques. However, the current literature lacks open platforms to gather datasets for the training of such ML techniques and to evaluate novel beam management approaches in real-time, real-world scenarios and full-stack endto-end networks. In this work, we present our SDR-based experimental platform based on OpenAirInterface and are the first to integrate popular low-cost antenna array transceivers, beam sweeping capabilities, and a highly-modular sensor framework and associated interfaces into such a full-stack experimental platform. This enables beam management experimentation in real-world, real-time scenarios and facilitates gathering datasets necessary for developing ML-based beam management protocols that incorporate environment awareness via sensor modalities."}
{"id": "2511.17429", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2511.17429", "abs": "https://arxiv.org/abs/2511.17429", "authors": ["Guilherme Coelho"], "title": "Semantic and Semiotic Interplays in Text-to-Audio AI: Exploring Cognitive Dynamics and Musical Interactions", "comment": null, "summary": "This paper investigates the emerging text-to-audio paradigm in artificial intelligence (AI), examining its transformative implications for musical creation, interpretation, and cognition. I explore the complex semantic and semiotic interplays that occur when descriptive natural language prompts are translated into nuanced sound objects across the text-to-audio modality. Drawing from structuralist and post-structuralist perspectives, as well as cognitive theories of schema dynamics and metacognition, the paper explores how these AI systems reconfigure musical signification processes and navigate established cognitive frameworks. The research analyzes some of the cognitive dynamics at play in AI-mediated musicking, including processes of schema assimilation and accommodation, metacognitive reflection, and constructive perception. The paper argues that text-to-audio AI models function as quasi-objects of musical signification, simultaneously stabilizing and destabilizing conventional forms while fostering new modes of listening and aesthetic reflexivity.Using Udio as a primary case study, this study explores how these models navigate the liminal spaces between linguistic prompts and sonic outputs. This process not only generates novel musical expressions but also prompts listeners to engage in forms of critical and \"structurally-aware listening.\", encouraging a deeper understanding of music's structures, semiotic nuances, and the socio-cultural contexts that shape our musical cognition. The paper concludes by reflecting on the potential of text-to-audio AI models to serve as epistemic tools and quasi-objects, facilitating a significant shift in musical interactions and inviting users to develop a more nuanced comprehension of the cognitive and cultural foundations of music."}
{"id": "2511.17404", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2511.17404", "abs": "https://arxiv.org/abs/2511.17404", "authors": ["Guilherme Coelho"], "title": "The Artist is Present: Traces of Artists Resigind and Spawning in Text-to-Audio AI", "comment": null, "summary": "Text-to-audio (TTA) systems are rapidly transforming music creation and distribution, with platforms like Udio and Suno generating thousands of tracks daily and integrating into mainstream music platforms and ecosystems. These systems, trained on vast and largely undisclosed datasets, are fundamentally reshaping how music is produced, reproduced and consumed. This paper presents empirical evidence that artist-conditioned regions can be systematically microlocated through metatag-based prompt design, effectively enabling the spawning of artist-like content through strategic prompt engineering. Through systematic exploration of metatag-based prompt engineering techniques this research reveals how users can access the distinctive sonic signatures of specific artists, evidencing their inclusion in training datasets. Using descriptor constellations drawn from public music taxonomies, the paper demonstrates reproducible proximity to artists such as Bon Iver, Philip Glass, Panda Bear and William Basinski. The results indicate stable text-audio correspondences consistent with artist-specific training signals, enabling precise traversal of stylistic microlocations without explicitly naming artists. This capacity to summon artist-specific outputs shows that artists' creative works fuction as foundational material from which these systems generate new content, often without explicit consent or attribuition. Conceptually, the work clarifies how textual descriptors act as navigational cues in high-dimensional representation spaces; methodologically, it provides a replicable protocol for auditing stylistic inducibility. The findings raise immediate queestions for governance-attribution, consent and disclosure standards-and for creative practice, where induced stylistic proximity complicates boundaries between ownership, reproduction, imitation, creative agency and the ethics of algorithmic creation."}
{"id": "2511.17164", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.17164", "abs": "https://arxiv.org/abs/2511.17164", "authors": ["Ioanna Chourdaki", "Kleanthis Avramidis", "Christos Garoufis", "Athanasia Zlatintsi", "Petros Maragos"], "title": "Teager-Kaiser Energy Methods For EEG Feature Extraction In Biomedical Applications", "comment": "Submitted to ICASSP 2026", "summary": "Electroencephalography (EEG) signals are inherently non-linear, non-stationary, and vulnerable to noise sources, making the extraction of discriminative features a long-standing challenge. In this work, we investigate the non-linear Teager-Kaiser Energy Operator (TKEO) for modeling the underlying energy dynamics of EEG in three representative tasks: motor imagery, emotion recognition, and epilepsy detection. To accommodate the narrowband nature of the operator, we employ Gabor filterbanks to isolate canonical frequency bands, followed by the Energy Separation Algorithm to decompose the TKEO output into amplitude envelope and instantaneous frequency components. We then derive a set of energy descriptors based on this demodulation and compare their classification performance against established signal energy and power spectrum features. TKEO features outperform the respective baselines in motor imagery and epilepsy detection, whereas they perform on par in emotion recognition. Our findings suggest that the proposed TKEO-based pipeline provides an intuitive framework for extracting EEG signal dynamics."}
{"id": "2511.17425", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2511.17425", "abs": "https://arxiv.org/abs/2511.17425", "authors": ["Guilherme Coelho"], "title": "AI in Music and Sound: Pedagogical Reflections, Post-Structuralist Approaches and Creative Outcomes in Seminar Practice", "comment": null, "summary": "This paper presents a pedagogical and conceptual account of the course AI in Music and Sound: Modalities, Tools and Creative Applications, offered within the Music Informatics and Media Art module of an M.Sc. in Audio Communication. The course engaged students with a range of AI modalities such as symbolic composition, voice synthesis, timbre transfer, neural audio synthesis, and text-to-audio systems, combining theoretical reflection with practice-based experimentation. Its central pedagogical move is a paired-études design: each modality is approached first through its intended affordances and then through a deliberately reframed or \"misused\" exercise that surfaces representational limits and alternative behaviours. Framed by medium theory and post-structuralist inquiry, we treated AI as a transmodal conduit-a system that translates and perturbs musical signs across textual, symbolic, timbral and audio domains. Evidence from student work and reflection indicates growth in technical fluency, medium awareness, and critical literacy, alongside the cultivation of experimental method and process-oriented listening. The paper outlines the course architecture, assessment design, and representative projects, and distils a set of design patterns for AI-music pedagogy (eg., prompt-conditioned interplays and semantic destabilisation in text-to-audio; latent space materialism in timbre transfer). It concludes with pedagogical recommendations that integrate creative practice with medium awareness and with cultural-epistemic analysis of AI technologies, preparing students to participate in how AI is understood, developed, and deployed with creative communities."}
{"id": "2511.17440", "categories": ["eess.SP", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.17440", "abs": "https://arxiv.org/abs/2511.17440", "authors": ["Omar A. Alotaibi", "Brian L. Mark", "Mohammad Reza Fasihi"], "title": "Incorporating Bayesian Transfer Learning into Particle Filter for Dual-Tracking System with Asymmetric Noise Intensities", "comment": "20 pages, 8 figures", "summary": "Using Bayesian transfer learning, we develop a particle filter approach for tracking a nonlinear dynamical model in a dual-tracking system where intensities of measurement noise for both sensors are asymmetric. The densities for Bayesian transfer learning are approximated with the sum of weighted particles to improve the tracking performance of the primary sensor, which experiences a higher noise intensity compared to the source sensor. We present simulation results that validate the effectiveness of the proposed approach compared to an isolated particle filter and transfer learning applied to the unscented Kalman filter and the cubature Kalman filter. Furthermore, increasing the number of particles shows an improvement in the performance of transfer learning applied to the particle filter with a higher rate compared to the isolated particle filter. However, increasing the number of particles raises computational time per step. Moreover, the performance gain from incorporating Bayesian transfer learning is approximately linearly proportional to the absolute difference value between the noise intensities of the sensors in the dual-tracking system."}
{"id": "2511.17429", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2511.17429", "abs": "https://arxiv.org/abs/2511.17429", "authors": ["Guilherme Coelho"], "title": "Semantic and Semiotic Interplays in Text-to-Audio AI: Exploring Cognitive Dynamics and Musical Interactions", "comment": null, "summary": "This paper investigates the emerging text-to-audio paradigm in artificial intelligence (AI), examining its transformative implications for musical creation, interpretation, and cognition. I explore the complex semantic and semiotic interplays that occur when descriptive natural language prompts are translated into nuanced sound objects across the text-to-audio modality. Drawing from structuralist and post-structuralist perspectives, as well as cognitive theories of schema dynamics and metacognition, the paper explores how these AI systems reconfigure musical signification processes and navigate established cognitive frameworks. The research analyzes some of the cognitive dynamics at play in AI-mediated musicking, including processes of schema assimilation and accommodation, metacognitive reflection, and constructive perception. The paper argues that text-to-audio AI models function as quasi-objects of musical signification, simultaneously stabilizing and destabilizing conventional forms while fostering new modes of listening and aesthetic reflexivity.Using Udio as a primary case study, this study explores how these models navigate the liminal spaces between linguistic prompts and sonic outputs. This process not only generates novel musical expressions but also prompts listeners to engage in forms of critical and \"structurally-aware listening.\", encouraging a deeper understanding of music's structures, semiotic nuances, and the socio-cultural contexts that shape our musical cognition. The paper concludes by reflecting on the potential of text-to-audio AI models to serve as epistemic tools and quasi-objects, facilitating a significant shift in musical interactions and inviting users to develop a more nuanced comprehension of the cognitive and cultural foundations of music."}
{"id": "2511.17346", "categories": ["cs.SD", "cs.AI", "eess.SP", "physics.class-ph", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.17346", "abs": "https://arxiv.org/abs/2511.17346", "authors": ["Marius Rodrigues", "Louis Bahrman", "Roland Badeau", "Gaël Richard"], "title": "Is Phase Really Needed for Weakly-Supervised Dereverberation ?", "comment": null, "summary": "In unsupervised or weakly-supervised approaches for speech dereverberation, the target clean (dry) signals are considered to be unknown during training. In that context, evaluating to what extent information can be retrieved from the sole knowledge of reverberant (wet) speech becomes critical. This work investigates the role of the reverberant (wet) phase in the time-frequency domain. Based on Statistical Wave Field Theory, we show that late reverberation perturbs phase components with white, uniformly distributed noise, except at low frequencies. Consequently, the wet phase carries limited useful information and is not essential for weakly supervised dereverberation. To validate this finding, we train dereverberation models under a recent weak supervision framework and demonstrate that performance can be significantly improved by excluding the reverberant phase from the loss function."}
{"id": "2511.17477", "categories": ["cs.SD", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.17477", "abs": "https://arxiv.org/abs/2511.17477", "authors": ["Ayhan Kucukmanisa", "Derya Gelmez", "Sukru Selim Calik", "Zeynep Hilal Kilimci"], "title": "Enhancing Quranic Learning: A Multimodal Deep Learning Approach for Arabic Phoneme Recognition", "comment": "11 pages, 2 figures, 3 tables", "summary": "Recent advances in multimodal deep learning have greatly enhanced the capability of systems for speech analysis and pronunciation assessment. Accurate pronunciation detection remains a key challenge in Arabic, particularly in the context of Quranic recitation, where subtle phonetic differences can alter meaning. Addressing this challenge, the present study proposes a transformer-based multimodal framework for Arabic phoneme mispronunciation detection that combines acoustic and textual representations to achieve higher precision and robustness. The framework integrates UniSpeech-derived acoustic embeddings with BERT-based textual embeddings extracted from Whisper transcriptions, creating a unified representation that captures both phonetic detail and linguistic context. To determine the most effective integration strategy, early, intermediate, and late fusion methods were implemented and evaluated on two datasets containing 29 Arabic phonemes, including eight hafiz sounds, articulated by 11 native speakers. Additional speech samples collected from publicly available YouTube recordings were incorporated to enhance data diversity and generalization. Model performance was assessed using standard evaluation metrics: accuracy, precision, recall, and F1-score, allowing a detailed comparison of the fusion strategies. Experimental findings show that the UniSpeech-BERT multimodal configuration provides strong results and that fusion-based transformer architectures are effective for phoneme-level mispronunciation detection. The study contributes to the development of intelligent, speaker-independent, and multimodal Computer-Aided Language Learning (CALL) systems, offering a practical step toward technology-supported Quranic pronunciation training and broader speech-based educational applications."}

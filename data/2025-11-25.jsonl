{"id": "2511.17926", "categories": ["cs.SD", "cs.HC"], "pdf": "https://arxiv.org/pdf/2511.17926", "abs": "https://arxiv.org/abs/2511.17926", "authors": ["Xiangrui Xiong", "Zhou Zhou", "Guocai Nong", "Junlin Deng", "Ning Wu"], "title": "Three-Class Emotion Classification for Audiovisual Scenes Based on Ensemble Learning Scheme", "comment": null, "summary": "Emotion recognition plays a pivotal role in enhancing human-computer interaction, particularly in movie recommendation systems where understanding emotional content is essential. While multimodal approaches combining audio and video have demonstrated effectiveness, their reliance on high-performance graphical computing limits deployment on resource-constrained devices such as personal computers or home audiovisual systems. To address this limitation, this study proposes a novel audio-only ensemble learning framework capable of classifying movie scenes into three emotional categories: Good, Neutral, and Bad. The model integrates ten support vector machines and six neural networks within a stacking ensemble architecture to enhance classification performance. A tailored data preprocessing pipeline, including feature extraction, outlier handling, and feature engineering, is designed to optimize emotional information from audio inputs. Experiments on a simulated dataset achieve 67% accuracy, while a real-world dataset collected from 15 diverse films yields an impressive 86% accuracy. These results underscore the potential of audio-based, lightweight emotion recognition methods for broader consumer-level applications, offering both computational efficiency and robust classification capabilities."}
{"id": "2511.18078", "categories": ["cs.SD", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18078", "abs": "https://arxiv.org/abs/2511.18078", "authors": ["Kexin Li", "Mandar Chitre"], "title": "Diffusion-based Surrogate Model for Time-varying Underwater Acoustic Channels", "comment": null, "summary": "Accurate modeling of time-varying underwater acoustic channels is essential for the design, evaluation, and deployment of reliable underwater communication systems. Conventional physics models require detailed environmental knowledge, while stochastic replay methods are constrained by the limited diversity of measured channels and often fail to generalize to unseen scenarios, reducing their practical applicability. To address these challenges, we propose StableUASim, a pre-trained conditional latent diffusion surrogate model that captures the stochastic dynamics of underwater acoustic communication channels. Leveraging generative modeling, StableUASim produces diverse and statistically realistic channel realizations, while supporting conditional generation from specific measurement samples. Pre-training enables rapid adaptation to new environments using minimal additional data, and the autoencoder latent representation facilitates efficient channel analysis and compression. Experimental results demonstrate that StableUASim accurately reproduces key channel characteristics and communication performance, providing a scalable, data-efficient, and physically consistent surrogate model for both system design and machine learning-driven underwater applications."}
{"id": "2511.18384", "categories": ["cs.SD", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18384", "abs": "https://arxiv.org/abs/2511.18384", "authors": ["Plein Versace"], "title": "NSTR: Neural Spectral Transport Representation for Space-Varying Frequency Fields", "comment": null, "summary": "Implicit Neural Representations (INRs) have emerged as a powerful paradigm for representing signals such as images, audio, and 3D scenes. However, existing INR frameworks -- including MLPs with Fourier features, SIREN, and multiresolution hash grids -- implicitly assume a \\textit{global and stationary} spectral basis. This assumption is fundamentally misaligned with real-world signals whose frequency characteristics vary significantly across space, exhibiting local high-frequency textures, smooth regions, and frequency drift phenomena. We propose \\textbf{Neural Spectral Transport Representation (NSTR)}, the first INR framework that \\textbf{explicitly models a spatially varying local frequency field}. NSTR introduces a learnable \\emph{frequency transport equation}, a PDE that governs how local spectral compositions evolve across space. Given a learnable local spectrum field $S(x)$ and a frequency transport network $F_θ$ enforcing $\\nabla S(x) \\approx F_θ(x, S(x))$, NSTR reconstructs signals by spatially modulating a compact set of global sinusoidal bases. This formulation enables strong local adaptivity and offers a new level of interpretability via visualizing frequency flows. Experiments on 2D image regression, audio reconstruction, and implicit 3D geometry show that NSTR achieves significantly better accuracy-parameter trade-offs than SIREN, Fourier-feature MLPs, and Instant-NGP. NSTR requires fewer global frequencies, converges faster, and naturally explains signal structure through spectral transport fields. We believe NSTR opens a new direction in INR research by introducing explicit modeling of space-varying spectrum."}
{"id": "2511.18421", "categories": ["cs.SD", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.18421", "abs": "https://arxiv.org/abs/2511.18421", "authors": ["Weichuang Shao", "Iman Yi Liao", "Tomas Henrique Bode Maul", "Tissa Chandesa"], "title": "DHAuDS: A Dynamic and Heterogeneous Audio Benchmark for Test-Time Adaptation", "comment": null, "summary": "Audio classifiers frequently face domain shift, when models trained on one dataset lose accuracy on data recorded in acoustically different conditions. Previous Test-Time Adaptation (TTA) research in speech and sound analysis often evaluates models under fixed or mismatched noise settings, that fail to mimic real-world variability. To overcome these limitations, this paper presents DHAuDS (Dynamic and Heterogeneous Audio Domain Shift), a benchmark designed to assess TTA approaches under more realistic and diverse acoustic shifts. DHAuDS comprises four standardized benchmarks: UrbanSound8K-C, SpeechCommandsV2-C, VocalSound-C, and ReefSet-C, each constructed with dynamic corruption severity levels and heterogeneous noise types to simulate authentic audio degradation scenarios. The framework defines 14 evaluation criteria for each benchmark (8 for UrbanSound8K-C), resulting in 50 unrepeated criteria (124 experiments) that collectively enable fair, reproducible, and cross-domain comparison of TTA algorithms. Through the inclusion of dynamic and mixed-domain noise settings, DHAuDS offers a consistent and publicly reproducible testbed to support ongoing studies in robust and adaptive audio modeling."}
{"id": "2511.17555", "categories": ["eess.AS", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.17555", "abs": "https://arxiv.org/abs/2511.17555", "authors": ["Guansu Wang", "Peijie Sun"], "title": "Speech Recognition Model Improves Text-to-Speech Synthesis using Fine-Grained Reward", "comment": "The paper makes an important contribution to the very challenging problem of training TTS models, with a novel application of reinforcement learning and demonstrating convincing improvements", "summary": "Recent advances in text-to-speech (TTS) have enabled models to clone arbitrary unseen speakers and synthesize high-quality, natural-sounding speech. However, evaluation methods lag behind: typical mean opinion score (MOS) estimators perform regression over entire utterances, while failures usually occur in a few problematic words. We observe that encoder-decoder ASR models (e.g., Whisper) surface word-level mismatches between speech and text via cross-attention, providing a fine-grained reward signal. Building on this, we introduce Word-level TTS Alignment by ASR-driven Attentive Reward (W3AR). Without explicit reward annotations, W3AR uses attention from a pre-trained ASR model to drive finer-grained alignment and optimization of sequences predicted by a TTS model. Experiments show that W3AR improves the quality of existing TTS systems and strengthens zero-shot robustness on unseen speakers. More broadly, our results suggest a simple recipe for generative modeling: understanding models can act as evaluators, delivering informative, fine-grained feedback for optimization."}
{"id": "2511.17547", "categories": ["eess.SP", "cs.AI", "cs.CV", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.17547", "abs": "https://arxiv.org/abs/2511.17547", "authors": ["Jeyoung Lee", "Hochul Kang"], "title": "SYNAPSE: Synergizing an Adapter and Finetuning for High-Fidelity EEG Synthesis from a CLIP-Aligned Encoder", "comment": null, "summary": "Recent progress in diffusion-based generative models has enabled high-quality image synthesis conditioned on diverse modalities. Extending such models to brain signals could deepen our understanding of human perception and mental representations. However,electroencephalography (EEG) presents major challenges for image generation due to high noise, low spatial resolution, and strong inter-subject variability. Existing approaches,such as DreamDiffusion, BrainVis, and GWIT, primarily adapt EEG features to pre-trained Stable Diffusion models using complex alignment or classification pipelines, often resulting in large parameter counts and limited interpretability. We introduce SYNAPSE, a two-stage framework that bridges EEG signal representation learning and high-fidelity image synthesis. In Stage1, a CLIP-aligned EEG autoencoder learns a semantically structured latent representation by combining signal reconstruction and cross-modal alignment objectives. In Stage2, the pretrained encoder is frozen and integrated with a lightweight adaptation of Stable Diffusion, enabling efficient conditioning on EEG features with minimal trainable parameters. Our method achieves a semantically coherent latent space and state-of-the-art perceptual fidelity on the CVPR40 dataset, outperforming prior EEG-to-image models in both reconstruction efficiency and image quality. Quantitative and qualitative analyses demonstrate that SYNAPSE generalizes effectively across subjects, preserving visual semantics even when class-level agreement is reduced. These results suggest that reconstructing what the brain perceives, rather than what it classifies, is key to faithful EEG-based image generation."}
{"id": "2511.18698", "categories": ["cs.SD", "cs.AI", "cs.CV", "cs.LG", "cs.MM"], "pdf": "https://arxiv.org/pdf/2511.18698", "abs": "https://arxiv.org/abs/2511.18698", "authors": ["Aman Verma", "Keshav Samdani", "Mohd. Samiuddin Shafi"], "title": "Multimodal Real-Time Anomaly Detection and Industrial Applications", "comment": null, "summary": "This paper presents the design, implementation, and evolution of a comprehensive multimodal room-monitoring system that integrates synchronized video and audio processing for real-time activity recognition and anomaly detection. We describe two iterations of the system: an initial lightweight implementation using YOLOv8, ByteTrack, and the Audio Spectrogram Transformer (AST), and an advanced version that incorporates multi-model audio ensembles, hybrid object detection, bidirectional cross-modal attention, and multi-method anomaly detection. The evolution demonstrates significant improvements in accuracy, robustness, and industrial applicability. The advanced system combines three audio models (AST, Wav2Vec2, and HuBERT) for comprehensive audio understanding, dual object detectors (YOLO and DETR) for improved accuracy, and sophisticated fusion mechanisms for enhanced cross-modal learning. Experimental evaluation shows the system's effectiveness in general monitoring scenarios as well as specialized industrial safety applications, achieving real-time performance on standard hardware while maintaining high accuracy."}
{"id": "2511.18487", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2511.18487", "abs": "https://arxiv.org/abs/2511.18487", "authors": ["Chunyu Qiang", "Kang Yin", "Xiaopeng Wang", "Yuzhe Liang", "Jiahui Zhao", "Ruibo Fu", "Tianrui Wang", "Cheng Gong", "Chen Zhang", "Longbiao Wang", "Jianwu Dang"], "title": "InstructAudio: Unified speech and music generation with natural language instruction", "comment": null, "summary": "Text-to-speech (TTS) and text-to-music (TTM) models face significant limitations in instruction-based control. TTS systems usually depend on reference audio for timbre, offer only limited text-level attribute control, and rarely support dialogue generation. TTM systems are constrained by input conditioning requirements that depend on expert knowledge annotations. The high heterogeneity of these input control conditions makes them difficult to joint modeling with speech synthesis. Despite sharing common acoustic modeling characteristics, these two tasks have long been developed independently, leaving open the challenge of achieving unified modeling through natural language instructions. We introduce InstructAudio, a unified framework that enables instruction-based (natural language descriptions) control of acoustic attributes including timbre (gender, age), paralinguistic (emotion, style, accent), and musical (genre, instrument, rhythm, atmosphere). It supports expressive speech, music, and dialogue generation in English and Chinese. The model employs joint and single diffusion transformer layers with a standardized instruction-phoneme input format, trained on 50K hours of speech and 20K hours of music data, enabling multi-task learning and cross-modal alignment. Fig. 1 visualizes performance comparisons with mainstream TTS and TTM models, demonstrating that InstructAudio achieves optimal results on most metrics. To our best knowledge, InstructAudio represents the first instruction-controlled framework unifying speech and music generation. Audio samples are available at: https://qiangchunyu.github.io/InstructAudio/"}
{"id": "2511.17552", "categories": ["eess.SP", "eess.IV"], "pdf": "https://arxiv.org/pdf/2511.17552", "abs": "https://arxiv.org/abs/2511.17552", "authors": ["Jialin Wang", "Jianhua Zhang", "Yu Li", "Yutong Sun", "Yuxiang Zhang"], "title": "Semantic-driven Wireless Environment Knowledge Representation for Efficiency-Accuracy Balanced Beam Prediction in Vehicular Networks", "comment": null, "summary": "The rapid evolution of the internet of vehicles demands ultra-reliable low-latency communication in high-mobility environments, where conventional beam prediction methods suffer from high-dimensional inputs, prolonged training times, and limited interpretability. To address these challenges, the propagation environment semantics-aware wireless environment knowledge beam prediction (PES-WEKBP) framework is proposed. PES-WEKBP pioneers a novel electromagnetic (EM)-grounded knowledge distillation method, transforming raw visual data into an ultra-lean, interpretable material and location-related wireless environment knowledge matrix. This matrix explicitly encodes critical propagation environment semantics, which is material EM properties and spatial relationships through a physics-informed parameterization process, distilling the environment and channel interplay into a minimal yet information-dense representation. A lightweight decision network then leverages this highly compressed knowledge for low-complexity beam prediction. To holistically evaluate the performance of PES-WEKBP, we first design the prediction consistency-efficiency index (PCEI), which combines prediction accuracy with a stability-penalized logarithmic training time to ensure a balanced optimization of reliability and computational efficiency. Experiments validate that PES-WEKBP achieves a 99.75% to 99.96% dimension reduction and improves accuracy by 5.52% to 8.19%, which outperforms state-of-the-art methods in PCEI scores across diverse vehicular scenarios."}
{"id": "2511.18833", "categories": ["cs.SD", "cs.CV", "eess.AS", "eess.IV"], "pdf": "https://arxiv.org/pdf/2511.18833", "abs": "https://arxiv.org/abs/2511.18833", "authors": ["Huadai Liu", "Kaicheng Luo", "Wen Wang", "Qian Chen", "Peiwen Sun", "Rongjie Huang", "Xiangang Li", "Jieping Ye", "Wei Xue"], "title": "PrismAudio: Decomposed Chain-of-Thoughts and Multi-dimensional Rewards for Video-to-Audio Generation", "comment": "Preprint", "summary": "Video-to-Audio (V2A) generation requires balancing four critical perceptual dimensions: semantic consistency, audio-visual temporal synchrony, aesthetic quality, and spatial accuracy; yet existing methods suffer from objective entanglement that conflates competing goals in single loss functions and lack human preference alignment. We introduce PrismAudio, the first framework to integrate Reinforcement Learning into V2A generation with specialized Chain-of-Thought (CoT) planning. Our approach decomposes monolithic reasoning into four specialized CoT modules (Semantic, Temporal, Aesthetic, and Spatial CoT), each paired with targeted reward functions. This CoT-reward correspondence enables multidimensional RL optimization that guides the model to jointly generate better reasoning across all perspectives, solving the objective entanglement problem while preserving interpretability. To make this optimization computationally practical, we propose Fast-GRPO, which employs hybrid ODE-SDE sampling that dramatically reduces the training overhead compared to existing GRPO implementations. We also introduce AudioCanvas, a rigorous benchmark that is more distributionally balanced and covers more realistically diverse and challenging scenarios than existing datasets, with 300 single-event classes and 501 multi-event samples. Experimental results demonstrate that PrismAudio achieves state-of-the-art performance across all four perceptual dimensions on both the in-domain VGGSound test set and out-of-domain AudioCanvas benchmark. The project page is available at https://PrismAudio-Project.github.io."}
{"id": "2511.18725", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2511.18725", "abs": "https://arxiv.org/abs/2511.18725", "authors": ["Dongqi Zhu", "Zhuwen Xu", "Youyuan Chen", "Minghao Jin", "Wan Zheng", "Yi Zhou", "Huiwu Li", "Yongyun Chang", "Feng Hong", "Zanjing Zhai"], "title": "First Deep Learning Approach to Hammering Acoustics for Stem Stability Assessment in Total Hip Arthroplasty", "comment": null, "summary": "Audio event classification has recently emerged as a promising approach in medical applications. In total hip arthroplasty (THA), intra-operative hammering acoustics provide critical cues for assessing the initial stability of the femoral stem, yet variability due to femoral morphology, implant size, and surgical technique constrains conventional assessment methods. We propose the first deep learning framework for this task, employing a TimeMIL model trained on Log-Mel Spectrogram features and enhanced with pseudo-labeling. On intra-operative recordings, the method achieved 91.17 % +/- 2.79 % accuracy, demonstrating reliable estimation of stem stability. Comparative experiments further show that reducing the diversity of femoral stem brands improves model performance, although limited dataset size remains a bottleneck. These results establish deep learning-based audio event classification as a feasible approach for intra-operative stability assessment in THA."}
{"id": "2511.17558", "categories": ["eess.SP", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.17558", "abs": "https://arxiv.org/abs/2511.17558", "authors": ["Chunlei Shi", "Han Xu", "Yinghao Li", "Yi-Lin Wei", "Yongchao Feng", "Yecheng Zhang", "Dan Niu"], "title": "WaveC2R: Wavelet-Driven Coarse-to-Refined Hierarchical Learning for Radar Retrieval", "comment": "AAAI2026 Project's webpage at this URL:https://spring-lovely.github.io/WaveC2R/", "summary": "Satellite-based radar retrieval methods are widely employed to fill coverage gaps in ground-based radar systems, especially in remote areas affected by terrain blockage and limited detection range. Existing methods predominantly rely on overly simplistic spatial-domain architectures constructed from a single data source, limiting their ability to accurately capture complex precipitation patterns and sharply defined meteorological boundaries. To address these limitations, we propose WaveC2R, a novel wavelet-driven coarse-to-refined framework for radar retrieval. WaveC2R integrates complementary multi-source data and leverages frequency-domain decomposition to separately model low-frequency components for capturing precipitation patterns and high-frequency components for delineating sharply defined meteorological boundaries. Specifically, WaveC2R consists of two stages (i)Intensity-Boundary Decoupled Learning, which leverages wavelet decomposition and frequency-specific loss functions to separately optimize low-frequency intensity and high-frequency boundaries; and (ii)Detail-Enhanced Diffusion Refinement, which employs frequency-aware conditional priors and multi-source data to progressively enhance fine-scale precipitation structures while preserving coarse-scale meteorological consistency. Experimental results on the publicly available SEVIR dataset demonstrate that WaveC2R achieves state-of-the-art performance in satellite-based radar retrieval, particularly excelling at preserving high-intensity precipitation features and sharply defined meteorological boundaries."}
{"id": "2511.18869", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2511.18869", "abs": "https://arxiv.org/abs/2511.18869", "authors": ["Shuyang Liu", "Yuan Jin", "Rui Lin", "Shizhe Chen", "Junyu Dai", "Tao Jiang"], "title": "Multidimensional Music Aesthetic Evaluation via Semantically Consistent C-Mixup Augmentation", "comment": null, "summary": "Evaluating the aesthetic quality of generated songs is challenging due to the multi-dimensional nature of musical perception. We propose a robust music aesthetic evaluation framework that combines (1) multi-source multi-scale feature extraction to obtain complementary segment- and track-level representations, (2) a hierarchical audio augmentation strategy to enrich training data, and (3) a hybrid training objective that integrates regression and ranking losses for accurate scoring and reliable top-song identification. Experiments on the ICASSP 2026 SongEval benchmark demonstrate that our approach consistently outperforms baseline methods across correlation and top-tier metrics."}
{"id": "2511.18833", "categories": ["cs.SD", "cs.CV", "eess.AS", "eess.IV"], "pdf": "https://arxiv.org/pdf/2511.18833", "abs": "https://arxiv.org/abs/2511.18833", "authors": ["Huadai Liu", "Kaicheng Luo", "Wen Wang", "Qian Chen", "Peiwen Sun", "Rongjie Huang", "Xiangang Li", "Jieping Ye", "Wei Xue"], "title": "PrismAudio: Decomposed Chain-of-Thoughts and Multi-dimensional Rewards for Video-to-Audio Generation", "comment": "Preprint", "summary": "Video-to-Audio (V2A) generation requires balancing four critical perceptual dimensions: semantic consistency, audio-visual temporal synchrony, aesthetic quality, and spatial accuracy; yet existing methods suffer from objective entanglement that conflates competing goals in single loss functions and lack human preference alignment. We introduce PrismAudio, the first framework to integrate Reinforcement Learning into V2A generation with specialized Chain-of-Thought (CoT) planning. Our approach decomposes monolithic reasoning into four specialized CoT modules (Semantic, Temporal, Aesthetic, and Spatial CoT), each paired with targeted reward functions. This CoT-reward correspondence enables multidimensional RL optimization that guides the model to jointly generate better reasoning across all perspectives, solving the objective entanglement problem while preserving interpretability. To make this optimization computationally practical, we propose Fast-GRPO, which employs hybrid ODE-SDE sampling that dramatically reduces the training overhead compared to existing GRPO implementations. We also introduce AudioCanvas, a rigorous benchmark that is more distributionally balanced and covers more realistically diverse and challenging scenarios than existing datasets, with 300 single-event classes and 501 multi-event samples. Experimental results demonstrate that PrismAudio achieves state-of-the-art performance across all four perceptual dimensions on both the in-domain VGGSound test set and out-of-domain AudioCanvas benchmark. The project page is available at https://PrismAudio-Project.github.io."}
{"id": "2511.17878", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.17878", "abs": "https://arxiv.org/abs/2511.17878", "authors": ["Peishi Li", "Ming Li", "Rang Liu", "Qian Liu", "A. Lee Swindlehurst"], "title": "OFDM-ISAC Beyond CP Limit: Performance Analysis and Mitigation Algorithms", "comment": "submitted to IEEE Trans. Signal Process", "summary": "Orthogonal frequency division multiplexing (OFDM) is well-suited for integrated sensing and communications (ISAC), yet its cyclic prefix (CP) is dimensioned for communications-grade multipath and is generally insufficient for sensing. When echoes exceed the CP duration, inter-symbol and inter-carrier interference (ISI/ICI) break subcarrier orthogonality and degrade sensing. This paper presents a unified analytical and algorithmic framework for OFDM-ISAC beyond the CP limit. We first develop a general echo model that explicitly captures the structured coupling of ISI and ICI caused by CP insufficiency. Building on this model, we derive closed-form expressions for the sensing signal-to-interference-plus-noise ratio (SINR) and the range-Doppler peak sidelobe level ratio (PSLR), both of which are shown to deteriorate approximately linearly with the normalized excess delay beyond the CP. To mitigate these effects, we propose two standard-compatible successive interference cancellation (SIC) methods: SIC-DFT, a low-complexity DFT-based scheme, and SIC-ESPRIT, a super-resolution subspace approach. Simulations corroborate the analysis and demonstrate consistent gains over representative benchmarks. Both algorithms provide more than $4$dB SINR improvement under CP-insufficient conditions, while SIC-ESPRIT reduces range/velocity root-mean-square-errors (RMSE) by about one order of magnitude, approaching the performance achievable with a sufficiently long CP. These results offer both theoretical insight and practical solutions for reliable long-range OFDM-ISAC sensing beyond the CP limit."}
{"id": "2511.19275", "categories": ["cs.SD", "cs.AI", "eess.AS", "eess.SP"], "pdf": "https://arxiv.org/pdf/2511.19275", "abs": "https://arxiv.org/abs/2511.19275", "authors": ["Ellie L. Zhang", "Duoduo Liao", "Callie C. Liao"], "title": "Dynamic Multi-Species Bird Soundscape Generation with Acoustic Patterning and 3D Spatialization", "comment": "Accepted by IEEE Big Data 2025", "summary": "Generation of dynamic, scalable multi-species bird soundscapes remains a significant challenge in computer music and algorithmic sound design. Birdsongs involve rapid frequency-modulated chirps, complex amplitude envelopes, distinctive acoustic patterns, overlapping calls, and dynamic inter-bird interactions, all of which require precise temporal and spatial control in 3D environments. Existing approaches, whether Digital Signal Processing (DSP)-based or data-driven, typically focus only on single species modeling, static call structures, or synthesis directly from recordings, and often suffer from noise, limited flexibility, or large data needs. To address these challenges, we present a novel, fully algorithm-driven framework that generates dynamic multi-species bird soundscapes using DSP-based chirp generation and 3D spatialization, without relying on recordings or training data. Our approach simulates multiple independently-moving birds per species along different moving 3D trajectories, supporting controllable chirp sequences, overlapping choruses, and realistic 3D motion in scalable soundscapes while preserving species-specific acoustic patterns. A visualization interface provides bird trajectories, spectrograms, activity timelines, and sound waves for analytical and creative purposes. Both visual and audio evaluations demonstrate the ability of the system to generate dense, immersive, and ecologically inspired soundscapes, highlighting its potential for computer music, interactive virtual environments, and computational bioacoustics research."}
{"id": "2511.18869", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2511.18869", "abs": "https://arxiv.org/abs/2511.18869", "authors": ["Shuyang Liu", "Yuan Jin", "Rui Lin", "Shizhe Chen", "Junyu Dai", "Tao Jiang"], "title": "Multidimensional Music Aesthetic Evaluation via Semantically Consistent C-Mixup Augmentation", "comment": null, "summary": "Evaluating the aesthetic quality of generated songs is challenging due to the multi-dimensional nature of musical perception. We propose a robust music aesthetic evaluation framework that combines (1) multi-source multi-scale feature extraction to obtain complementary segment- and track-level representations, (2) a hierarchical audio augmentation strategy to enrich training data, and (3) a hybrid training objective that integrates regression and ranking losses for accurate scoring and reliable top-song identification. Experiments on the ICASSP 2026 SongEval benchmark demonstrate that our approach consistently outperforms baseline methods across correlation and top-tier metrics."}
{"id": "2511.17980", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.17980", "abs": "https://arxiv.org/abs/2511.17980", "authors": ["Anubhab Chowdhury", "Erik G. Larsson"], "title": "On the Performance of Dual-Antenna Repeater Assisted Bi-Static MIMO ISAC", "comment": "5 pages, 4 Figures", "summary": "This paper presents a framework for target detection and downlink data transmission in a repeater-assisted bi-static integrated sensing and communication system. A repeater is an active scatterer that retransmits incoming signals with a complex gain almost instantaneously, thereby enhancing sensing performance by amplifying the echoes reflected by the targets. The same mechanism can also improve downlink communication by mitigating coverage holes. However, the repeater introduces noise and increases interference at the sensing receiver, while also amplifying the interference from target detection signals at the downlink users. The proposed framework accounts for these sensing-communication trade-offs and demonstrates the potential benefits achievable through a carefully designed precoder at the transmitting base station. In particular, our finding is that a higher value of probability of detection can be attained with considerably lower target radar-cross-section variance by deploying repeaters in the target hot-spot areas."}
{"id": "2511.19342", "categories": ["cs.SD", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.19342", "abs": "https://arxiv.org/abs/2511.19342", "authors": ["Maral Ebrahimzadeh", "Gilberto Bernardes", "Sebastian Stober"], "title": "Explicit Tonal Tension Conditioning via Dual-Level Beam Search for Symbolic Music Generation", "comment": "12 pages, 2 Figures, Accepted at the 17th International Symposium on Computer Music Multidisciplinary Research (CMMR) 2025", "summary": "State-of-the-art symbolic music generation models have recently achieved remarkable output quality, yet explicit control over compositional features, such as tonal tension, remains challenging. We propose a novel approach that integrates a computational tonal tension model, based on tonal interval vector analysis, into a Transformer framework. Our method employs a two-level beam search strategy during inference. At the token level, generated candidates are re-ranked using model probability and diversity metrics to maintain overall quality. At the bar level, a tension-based re-ranking is applied to ensure that the generated music aligns with a desired tension curve. Objective evaluations indicate that our approach effectively modulates tonal tension, and subjective listening tests confirm that the system produces outputs that align with the target tension. These results demonstrate that explicit tension conditioning through a dual-level beam search provides a powerful and intuitive tool to guide AI-generated music. Furthermore, our experiments demonstrate that our method can generate multiple distinct musical interpretations under the same tension condition."}
{"id": "2511.19275", "categories": ["cs.SD", "cs.AI", "eess.AS", "eess.SP"], "pdf": "https://arxiv.org/pdf/2511.19275", "abs": "https://arxiv.org/abs/2511.19275", "authors": ["Ellie L. Zhang", "Duoduo Liao", "Callie C. Liao"], "title": "Dynamic Multi-Species Bird Soundscape Generation with Acoustic Patterning and 3D Spatialization", "comment": "Accepted by IEEE Big Data 2025", "summary": "Generation of dynamic, scalable multi-species bird soundscapes remains a significant challenge in computer music and algorithmic sound design. Birdsongs involve rapid frequency-modulated chirps, complex amplitude envelopes, distinctive acoustic patterns, overlapping calls, and dynamic inter-bird interactions, all of which require precise temporal and spatial control in 3D environments. Existing approaches, whether Digital Signal Processing (DSP)-based or data-driven, typically focus only on single species modeling, static call structures, or synthesis directly from recordings, and often suffer from noise, limited flexibility, or large data needs. To address these challenges, we present a novel, fully algorithm-driven framework that generates dynamic multi-species bird soundscapes using DSP-based chirp generation and 3D spatialization, without relying on recordings or training data. Our approach simulates multiple independently-moving birds per species along different moving 3D trajectories, supporting controllable chirp sequences, overlapping choruses, and realistic 3D motion in scalable soundscapes while preserving species-specific acoustic patterns. A visualization interface provides bird trajectories, spectrograms, activity timelines, and sound waves for analytical and creative purposes. Both visual and audio evaluations demonstrate the ability of the system to generate dense, immersive, and ecologically inspired soundscapes, highlighting its potential for computer music, interactive virtual environments, and computational bioacoustics research."}
{"id": "2511.17991", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.17991", "abs": "https://arxiv.org/abs/2511.17991", "authors": ["Chaoyuan Bai", "Pingzhi Fan", "Zhengchun Zhou", "Zilong Liu"], "title": "Orthogonal Chirp Delay-Doppler Division Multiplexing (CDDM) Modulation for High Mobility Communications", "comment": null, "summary": "This paper proposes a novel multi-carrier modulation framework for high-mobility communication scenarios. Our key idea lies in spreading data symbols across the delay-Doppler (DD) domain through orthogonal chirp-Zak transform (CZT). To enable efficient signal multiplexing, the proposed modulation scheme employs a transmitter signal that maintains orthogonality with the inherent resolution characteristics of the DD plane. Termed as Orthogonal Chirp Delay-Doppler Division Multiplexing (CDDM), we demonstrate a synergistic integration of chirp waveform properties with the channel structure of the DD domain, thereby achieving advantages with both lower computational efficiency and improved detection performance. We introduce a novel CZT-based superimposed sparse pilot structure to enable simultaneous estimation of delay-Doppler shifts and channel coefficients. For enhanced performance, we further develop an embedded pilot scheme that demonstrates channel estimation performance comparable to that of Orthogonal Delay-Doppler Division Multiplexing (ODDM) systems. Simulation results demonstrate that CDDM achieves significant bit error rate (BER) improvements over existing modulation schemes , under perfect channel state information (CSI), as well as superior out-of-band emissions (OOBE). Further, for the imperfect CSI case, the proposed CZT-based superimposed pilot scheme leads to significantly reduced normalized mean square error (NMSE), whilst attaining equivalent estimation accuracy to that of ODDM with lower computational complexity."}
{"id": "2511.19396", "categories": ["cs.SD", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.19396", "abs": "https://arxiv.org/abs/2511.19396", "authors": ["Jorge Ortigoso-Narro", "Jose A. Belloch", "Adrian Amor-Martin", "Sandra Roger", "Maximo Cobos"], "title": "Real-Time Object Tracking with On-Device Deep Learning for Adaptive Beamforming in Dynamic Acoustic Environments", "comment": null, "summary": "Advances in object tracking and acoustic beamforming are driving new capabilities in surveillance, human-computer interaction, and robotics. This work presents an embedded system that integrates deep learning-based tracking with beamforming to achieve precise sound source localization and directional audio capture in dynamic environments. The approach combines single-camera depth estimation and stereo vision to enable accurate 3D localization of moving objects. A planar concentric circular microphone array constructed with MEMS microphones provides a compact, energy-efficient platform supporting 2D beam steering across azimuth and elevation. Real-time tracking outputs continuously adapt the array's focus, synchronizing the acoustic response with the target's position. By uniting learned spatial awareness with dynamic steering, the system maintains robust performance in the presence of multiple or moving sources. Experimental evaluation demonstrates significant gains in signal-to-interference ratio, making the design well-suited for teleconferencing, smart home devices, and assistive technologies."}
{"id": "2511.18009", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.18009", "abs": "https://arxiv.org/abs/2511.18009", "authors": ["Taihao Zhang", "Zhendong Peng", "Cunhua Pan", "Hong Ren", "Jiangzhou Wang"], "title": "Channel Estimation for RIS-Aided MU-MIMO mmWave Systems with Direct Channel Links", "comment": "13 pages,11 figures, journal", "summary": "In this paper, we propose a three-stage unified channel estimation strategy for reconfigurable intelligent surface (RIS)-aided multi-user (MU) multiple-input multiple-output (MIMO) millimeter wave (mmWave) systems with the existence of the direct channels, where the base station (BS), the users and the RIS are equipped with uniform planar array (UPA). The effectiveness of the developed three-stage strategy stems from the careful design of both the pilot signal sequence of the users and the vectors of RIS. Specifically, in Stage I, the cascaded channel components are eliminated by configuring the RIS phase shift vectors with a π difference to estimate the direct channels for all users. The orthogonal subspace projection is employed in Stage II to obtain equivalent signal matrices, enabling the estimation of angles of departure (AoDs) of the user-RIS channel for all users. In Stage III, we combine the signals of the time slots with the same pilots and project obtained measurement matrix to the orthogonal complement space of the component consisting of the portion of the direct channel, which removes the direct components and thus prevents error propagation from the direct channels to the cascaded channels. Then, we estimate the angles of arrival (AoAs) of the RIS-BS channel and remaining parameters of the cascaded channel for all users by exploiting the sparsity and correlation in the obtained equivalent matrices. Simulation results demonstrate that the proposed method yields better estimation performance than the existing methods."}
{"id": "2511.19403", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2511.19403", "abs": "https://arxiv.org/abs/2511.19403", "authors": ["Jorge Ortigoso-Narro", "Jose A. Belloch", "Maximo Morales-Cespedes", "Maximo Cobos"], "title": "Frequency-Invariant Beamforming in Elevation and Azimuth via Autograd and Concentric Circular Microphone Arrays", "comment": null, "summary": "The use of planar and concentric circular microphone arrays in beamforming has gained attention due to their ability to optimize both azimuth and elevation angles, making them ideal for spatial audio tasks like sound source localization and noise suppression. Unlike linear arrays, which restrict steering to a single axis, 2D arrays offer dual-axis optimization, although elevation control remains challenging. This study explores the integration of autograd, an automatic differentiation tool, with concentric circular arrays to impose beamwidth and frequency invariance constraints. This enables continuous optimization over both angles while maintaining performance across a wide frequency range. We evaluate our method through simulations of beamwidth, white noise gain, and directivity across multiple frequencies. A comparative analysis is presented against standard and advanced beamformers, including delay-and-sum, modified delay-and-sum, a Jacobi-Anger expansion-based method, and a Gaussian window-based gradient descent approach. Our method achieves superior spatial selectivity and narrower mainlobes, particularly in the elevation axis at lower frequencies. These results underscore the effectiveness of our approach in enhancing beamforming performance for acoustic sensing and spatial audio applications requiring precise dual-axis control."}
{"id": "2511.18130", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.18130", "abs": "https://arxiv.org/abs/2511.18130", "authors": ["Konstantinos Alexoudis", "Luke Silvestre", "Tom Huiskamp", "Jasper Müller", "Vincent Sleiffer", "Florian Azendorf", "Sander Jansen", "Chigo Okonkwo", "Tom Bradley"], "title": "Precise Localization of High-Voltage Breakdown Events using $φ$-Optical Time-Domain Reflectometry on an Optical Ground Wire", "comment": "This work has been partially funded by the German Federal Ministry of Education and Research in the project HYPERCORE (#16KIS2098). We also acknowledge the Bilateral Project \"DistraSignalSense\" between the Eindhoven University of Technology, The Netherlands, and Adtran Networks SE", "summary": "We present $φ$-OTDR for detecting and localising full spark-gap breakdowns by analysing backscattered light phase and frequency-domain signatures during high-voltage discharges synchronised with oscilloscope-recorded events. Measuring sub-kHz confirms clear discharge signatures and acoustic reconstruction over long links with $\\approx$ 10 m spatial resolution."}
{"id": "2511.18487", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2511.18487", "abs": "https://arxiv.org/abs/2511.18487", "authors": ["Chunyu Qiang", "Kang Yin", "Xiaopeng Wang", "Yuzhe Liang", "Jiahui Zhao", "Ruibo Fu", "Tianrui Wang", "Cheng Gong", "Chen Zhang", "Longbiao Wang", "Jianwu Dang"], "title": "InstructAudio: Unified speech and music generation with natural language instruction", "comment": null, "summary": "Text-to-speech (TTS) and text-to-music (TTM) models face significant limitations in instruction-based control. TTS systems usually depend on reference audio for timbre, offer only limited text-level attribute control, and rarely support dialogue generation. TTM systems are constrained by input conditioning requirements that depend on expert knowledge annotations. The high heterogeneity of these input control conditions makes them difficult to joint modeling with speech synthesis. Despite sharing common acoustic modeling characteristics, these two tasks have long been developed independently, leaving open the challenge of achieving unified modeling through natural language instructions. We introduce InstructAudio, a unified framework that enables instruction-based (natural language descriptions) control of acoustic attributes including timbre (gender, age), paralinguistic (emotion, style, accent), and musical (genre, instrument, rhythm, atmosphere). It supports expressive speech, music, and dialogue generation in English and Chinese. The model employs joint and single diffusion transformer layers with a standardized instruction-phoneme input format, trained on 50K hours of speech and 20K hours of music data, enabling multi-task learning and cross-modal alignment. Fig. 1 visualizes performance comparisons with mainstream TTS and TTM models, demonstrating that InstructAudio achieves optimal results on most metrics. To our best knowledge, InstructAudio represents the first instruction-controlled framework unifying speech and music generation. Audio samples are available at: https://qiangchunyu.github.io/InstructAudio/"}
{"id": "2511.18358", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.18358", "abs": "https://arxiv.org/abs/2511.18358", "authors": ["Jiachen Zhu", "Fangjiong Chen", "Jie Wu", "Ming Xia"], "title": "CT-CFAR A Robust CFAR Detector Based on CLEAN and Truncated Statistics in Sidelobe-Contaminated Environments", "comment": null, "summary": "This paper proposes a constant false alarm rate (CFAR) target detection algorithm based on the CLEAN concept and truncated statistics to mitigate the non-homogeneity of reference samples caused by sidelobe contamination and other abnormal interferences within the reference window. The proposed algorithm employs truncated statistics to separate target and noise components in the radar echo power spectrum, thereby restoring the homogeneity assumption of the reference window. In addition, learnable historical sidelobe information is introduced to enhance the robustness and environmental adaptability of the detection process. Furthermore, based on multichannel echo data, a target reconstruction model that combines the Candan algorithm with least-squares estimation is established, incorporating the CLEAN concept to suppress sidelobe interference. Monte Carlo simulations and real-world measurement experiments demonstrate that the proposed CT-CFAR algorithm achieves high-precision target detection without requiring prior knowledge of abnormal samples. Compared with various CFAR algorithms, the proposed approach overcomes the limitations of the reference window, accurately estimates the noise spectrum, and exhibits superior detection performance and computational efficiency in complex scenarios affected by sidelobe contamination."}
{"id": "2511.18376", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.18376", "abs": "https://arxiv.org/abs/2511.18376", "authors": ["Haohan Wang", "Xu Shi", "Hengyu Zhang", "Yashuai Cao", "Sufang Yang", "Jintao Wang", "Kaibin Huang"], "title": "BeamCKM: A Framework of Channel Knowledge Map Construction for Multi-Antenna Systems", "comment": null, "summary": "The channel knowledge map (CKM) enables efficient construction of high-fidelity mapping between spatial environments and channel parameters via electromagnetic information analysis. Nevertheless, existing studies are largely confined to single-antenna systems, failing to offer dedicated guidance for multi-antenna communication scenarios. To address the inherent conflict between traditional real-value pathloss map and multi-degree-of-freedom (DoF) coherent beamforming in B5G/6G systems, this paper proposes a novel concept of BeamCKM and CKMTransUNet architecture. The CKMTransUNet approach combines a UNet backbone for multi-scale feature extraction with a vision transformer (ViT) module to capture global dependencies among encoded linear vectors, utilizing a composite loss function to characterize the beam propagation characteristics. Furthermore, based on the CKMTransUNet backbone, this paper presents a methodology named M3ChanNet. It leverages the multi-modal learning technique and cross-attention mechanisms to extract intrinsic side information from environmental profiles and real-time multi-beam observations, thereby further improving the map construction accuracy. Simulation results demonstrate that the proposed method consistently outperforms state-of-the-art (SOTA) interpolation methods and deep learning (DL) approaches, delivering superior performance even when environmental contours are inaccurate. For reproducibility, the code is publicly accessible at https://github.com/github-whh/BeamCKM."}
{"id": "2511.18414", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.18414", "abs": "https://arxiv.org/abs/2511.18414", "authors": ["Dingli Yuan", "Jingchen Peng", "Jie Fan", "Boxiang Ren", "Lu Yang", "Peng Liu"], "title": "AutoMAS: A Generic Multi-Agent System for Algorithm Self-Adaptation in Wireless Networks", "comment": null, "summary": "The wireless communication environment has the characteristic of strong dynamics. Conventional wireless networks operate based on the static rules with predefined algorithms, lacking the self-adaptation ability. The rapid development of artificial intelligence (AI) provides a possibility for wireless networks to become more intelligent and fully automated. As such, we plan to integrate the cognitive capability and high intelligence of the emerging AI agents into wireless networks. In this work, we propose AutoMAS, a generic multi-agent system which can autonomously select the most suitable wireless optimization algorithm according to the dynamic wireless environment. Our AutoMAS combines theoretically guaranteed wireless algorithms with agents' perception ability, thereby providing sounder solutions to complex tasks no matter how the environment changes. As an example, we conduct a case study on the classical channel estimation problem, where the mobile user moves in diverse environments with different channel propagation characteristics. Simulation results demonstrate that our AutoMAS can guarantee the highest accuracy in changing scenarios. Similarly, our AutoMAS can be generalized to autonomously handle various tasks in 6G wireless networks with high accuracy."}
{"id": "2511.18419", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.18419", "abs": "https://arxiv.org/abs/2511.18419", "authors": ["Mahmoud Ghazal", "Nadhir Ben Rached", "Tareq Al-Naffouri"], "title": "A Comparative Study of Rare-Event Simulation Methods for Outage Probability in GSC/MRC Systems under Rician Fading", "comment": null, "summary": "This paper explores the use of enhanced Monte-Carlo (MC) techniques to evaluate the outage probability of single-input-multiple-output (SIMO) systems under Rician fading, in which the input is combined using generalized selection combining with maximum ratio combining (GSC/MRC). The studied set of methods includes previously established methods: universal importance sampling (UIS) and multilevel splitting (MLS), alongside readapted methods: exponential twisting (ET) and cross-entropy (CE), and a novel method introduced here: partition importance sampling (PIS). Performance is assessed across standard efficiency metrics, revealing that ET, CE, and PIS exhibit the best performance. CE is found to be the most robust method among them."}
{"id": "2511.18455", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.18455", "abs": "https://arxiv.org/abs/2511.18455", "authors": ["Diego Tuzi", "Thomas Delamotte", "Andreas Knopp"], "title": "6G Satellite Direct-to-Cell Connectivity: \"To distribute, or not to distribute, that is the question\"", "comment": "4 pages, 2 figures, ESA SatNEx School 2023 workshop \"Satellite 6G: Challenges and Solutions\", University of Siena, Italy (April 18-20, 2023), Best Idea Award, not peer-reviewed", "summary": "Direct-to-cell connectivity between satellites and common terrestrial handheld devices represents an essential feature of 6G. The industry is considering different type of constellations but using classical single satellite solutions based on phased array antennas. This article proposes to decompose a classical single satellite into a swarm of multiple small platforms (e.g. CubeSats) each equipped with one or a small number of radiating elements. The platforms are spaced far apart to create a large virtual aperture. The use of small satellites promises cost reduction for production and launch, while the distributed nature of the system introduces interesting features, such as scalability and fault tolerance. This perspective article provides insights into the opportunities and a discussion of the research challenges for the feasibility of the proposed approach."}
{"id": "2511.18594", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.18594", "abs": "https://arxiv.org/abs/2511.18594", "authors": ["Ahmad A. Aziz El-Banna", "Octavia A. Dobre"], "title": "Autoencoder for Position-Assisted Beam Prediction in mmWave ISAC Systems", "comment": null, "summary": "Integrated sensing and communication and millimeter wave (mmWave) have emerged as pivotal technologies for 6G networks. However, the narrow nature of mmWave beams requires precise alignments that typically necessitate large training overhead. This overhead can be reduced by incorporating the position information with beam adjustments. This letter proposes a lightweight autorencoder (LAE) model that addresses the position-assisted beam prediction problem while significantly reducing computational complexity compared to the conventional baseline method, i.e., deep fully connected neural network. The proposed LAE is designed as a three-layer undercomplete network to exploit its dimensionality reduction capabilities and thereby mitigate the computational requirements of the trained model. Simulation results show that the proposed model achieves a similar beam prediction accuracy to the baseline with an 83% complexity reduction."}
{"id": "2511.18599", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.18599", "abs": "https://arxiv.org/abs/2511.18599", "authors": ["Eric A. Bai", "Minling Zhou", "Ricardo Henao", "Kyle M. Schwing", "Lawrence Carin"], "title": "Leveraging Language Models for Interpretable Analysis of Narratives in a Large Corpus", "comment": null, "summary": "Narratives drive human behavior and lay at the core of geopolitics, but have eluded quantification that would permit measurement of their overlap and evolution. We present an interpretable model that integrates an established bag-of-words (BoW) topical representation and a novel LLM-based question answering (Q&A) narrative model, which share a latent Reproducing Kernel Hilbert Space representation, to quantify written documents. Our approach mitigates the cost, interpretability, and generalization challenges of using a LLM to analyze large corpora without full inference. We derive efficient functional gradient descent updates that are interpretable and structurally analogous to the self-attention mechanism in Transformers. We further introduce an in-context Q&A extrapolation method inspired by Transformer architectures, enabling accurate prediction of Q&A outcomes for unqueried documents."}
{"id": "2511.18690", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.18690", "abs": "https://arxiv.org/abs/2511.18690", "authors": ["Xinyu Pan", "Boxun Liu", "Xiang Cheng", "Chen Chen"], "title": "LLM4AMC: Adapting Large Language Models for Adaptive Modulation and Coding", "comment": null, "summary": "Adaptive modulation and coding (AMC) is a key technology in 5G new radio (NR), enabling dynamic link adaptation by balancing transmission efficiency and reliability based on channel conditions. However, traditional methods often suffer from performance degradation due to the aging issues of channel quality indicator (CQI). Recently, the emerging capabilities of large language models (LLMs) in contextual understanding and temporal modeling naturally align with the dynamic channel adaptation requirements of AMC technology. Leveraging pretrained LLMs, we propose a channel quality prediction method empowered by LLMs to optimize AMC, termed LLM4AMC. We freeze most parameters of the LLM and fine-tune it to fully utilize the knowledge acquired during pretraining while better adapting it to the AMC task. We design a network architecture composed of four modules, a preprocessing layer, an embedding layer, a backbone network, and an output layer, effectively capturing the time-varying characteristics of channel quality to achieve accurate predictions of future channel conditions. Simulation experiments demonstrate that our proposed method significantly improves link performance and exhibits potential for practical deployment."}
{"id": "2511.18752", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.18752", "abs": "https://arxiv.org/abs/2511.18752", "authors": ["Xiaokun Tuo", "Zijian Chen", "Ming-Min Zhao", "Changsheng You", "Min-Jian Zhao"], "title": "Near-Field Sparse Bayesian Channel Estimation and Tracking for XL-IRS-Aided Wideband mmWave Systems", "comment": "15 pages, 9 figures", "summary": "The rapid development of 6G systems demands advanced technologies to boost network capacity and spectral efficiency, particularly in the context of intelligent reflecting surfaces (IRS)-aided millimeter-wave (mmWave) communications. A key challenge here is obtaining accurate channel state information (CSI), especially with extremely large IRS (XL-IRS), due to near-field propagation, high-dimensional wideband cascaded channels, and the passive nature of the XL-IRS. In addition, most existing CSI acquisition methods fail to leverage the spatio-temporal sparsity inherent in the channel, resulting in suboptimal estimation performance. To address these challenges, we consider an XL-IRS-aided wideband multiple-input multiple-output orthogonal frequency division multiplexing (MIMO-OFDM) system and propose an efficient channel estimation and tracking (CET) algorithm. Specifically, a unified near-field cascaded channel representation model is presented first, and a hierarchical spatio-temporal sparse prior is then constructed to capture two-dimensional (2D) block sparsity in the polar domain, one-dimensional (1D) clustered sparsity in the angle-delay domain, and temporal correlations across different channel estimation frames. Based on these priors, a tensor-based sparse CET (TS-CET) algorithm is proposed that integrates tensor-based orthogonal matching pursuit (OMP) with particle-based variational Bayesian inference (VBI) and message passing. Simulation results demonstrate that the TS-CET framework significantly improves the estimation accuracy and reduces the pilot overhead as compared to existing benchmark methods."}
{"id": "2511.18884", "categories": ["eess.SP", "cs.IT"], "pdf": "https://arxiv.org/pdf/2511.18884", "abs": "https://arxiv.org/abs/2511.18884", "authors": ["Jihun Park", "Junyong Shin", "Jinsung Park", "Yo-Seb Jeon"], "title": "Robust Nonlinear Transform Coding: A Framework for Generalizable Joint Source-Channel Coding", "comment": null, "summary": "This paper proposes robust nonlinear transform coding (Robust-NTC), a generalizable digital joint source-channel coding (JSCC) framework that couples variational latent modeling with channel adaptive transmission. Unlike learning-based JSCC methods that implicitly absorb channel variations, Robust-NTC explicitly models element-wise latent distributions via a variational objective with a Gaussian proxy for quantization and channel noise, allowing encoder-decoder to capture latent uncertainty without channel-specific training. Using the learned statistics, Robust-NTC also facilitates rate-distortion optimization to adaptively select element-wise quantizers and bit depths according to online channel condition. To support practical deployment, Robust-NTC is integrated into an orthogonal frequency-division multiplexing (OFDM) system, where a unified resource allocation framework jointly optimizes latent quantization, bit allocation, modulation order, and power allocation to minimize transmission latency while guaranteeing learned distortion targets. Simulation results demonstrate that for practical OFDM systems, Robust-NTC achieves superior rate-distortion efficiency and stable reconstruction fidelity compared to digital JSCC baselines across wide-ranging SNR conditions."}
{"id": "2511.18892", "categories": ["eess.SP", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.18892", "abs": "https://arxiv.org/abs/2511.18892", "authors": ["Qiaoyan Peng", "Qingqing Wu", "Wen Chen", "Guangji Chen", "Ying Gao", "Lexi Xu", "Shaodan Ma"], "title": "Semi-Passive IRS Enabled Sensing with Group Movable Sensors", "comment": null, "summary": "The performance of the sensing system is limited by the signal attenuation and the number of receiving components. In this letter, we investigate the sensor position selection in a semi-passive intelligent reflecting surface (IRS) enabled non-line-of-sight (NLoS) sensing system. The IRS consists of passive elements and active sensors, where the sensors can receive and process the echo signal for direction-of-arrival (DoA) estimation. Motivated by the movable antenna array and fluid antenna system, we consider the case where the sensors are integrated into a group for movement and derive the corresponding Cramer-Rao bound (CRB). Then, the optimal solution for the positions of the movable sensors (MSs) to the CRB minimization problem is derived in closed form. Moreover, we characterize the relationship between the CRB and system parameters. Theoretical analysis and numerical results are provided to demonstrate the superiority of the proposed MS scheme over the fixed-position (FP) scheme."}
{"id": "2511.18907", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.18907", "abs": "https://arxiv.org/abs/2511.18907", "authors": ["Haobin Mao", "Lipeng Zhu", "Wenyan Ma", "Zhenyu Xiao", "Xiang-Gen Xia", "Rui Zhang"], "title": "Movable-Antenna Array Enhanced Multi-Target Sensing: CRB Characterization and Optimization", "comment": "13 pages, 12 figures, submitted to an IEEE journal for possible publication", "summary": "Movable antennas (MAs) have emerged as a promising technology to improve wireless communication and sensing performance towards sixth-generation (6G) networks through flexible antenna movement. In this paper, we propose a novel wireless sensing system based on MA arrays to enhance multi-target spatial angle estimation performance. We begin by characterizing the Cramér-Rao bound (CRB) matrix for multi-target angle of arrival (AoA) estimation as a function of the antenna's positions in MA arrays, thereby establishing a theoretical foundation for antenna position optimization. Then, aiming at improving the sensing coverage performance, we formulate an optimization problem to minimize the expectation of the trace of the CRB matrix over random target angles subject to a given distribution by optimizing the antennas' positions. To tackle the formulated challenging optimization problem, the Monte Carlo method is employed to approximate the intractable objective function, and a swarm-based gradient descent algorithm is subsequently proposed to address the approximated problem. In addition, a lower-bound on the sum of CRBs for multi-target AoA estimation is derived. Numerical results demonstrate that the proposed MA-based design achieves superior sensing performance compared to conventional systems using fixed-position antenna (FPA) arrays and single-target-oriented MA arrays, in terms of decreasing both CRB and the actual AoA estimation mean square error (MSE). Fundamentally, the designed MA array geometry exhibits low correlation and high effective power of sensitivity vectors for multi-target sensing in the angular domain, leading to significant CRB performance improvement. The resultant low correlation of steering vectors over multiple targets' directions further helps mitigate angle estimation ambiguity and thus enhances MSE performance."}
{"id": "2511.18911", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.18911", "abs": "https://arxiv.org/abs/2511.18911", "authors": ["Jingtian Liu", "Xiongwei Yang", "Yi Wei", "Jianjun Yu", "Feng Zhao"], "title": "Adaptive Probabilistic Constellation Shaping based on Enumerative Sphere Shaping for FSO Channel with Turbulence and Pointing Errors", "comment": "Under review for publication in IEEE Transactions on Communications", "summary": "Free-space optical (FSO) transmission enables fast, secure, and efficient next-generation communications with abundant spectrum resources. However, atmospheric turbulence, pointing errors, path loss, and atmospheric loss induce random attenuation, challenging link reliability. Adaptive rate control technology enhances spectrum utilization and reliability. We propose an adaptive probabilistic constellation shaping (A-PCS) coherent system utilizing enumerated spherical shaping (ESS) for distribution matching. With PCS-64QAM, the system achieves continuous rate control from conventional QPSK-equivalent to 64QAM spectral efficiency, providing quasi-continuous control with granularities of approximately $0.05$~bits/4D for spectral efficiency and $0.1$~dB for the post-FEC SNR threshold, and a maximum control depth of $12.5$~dB. Leveraging ESS for efficient sequence utilization, it offers higher spectral utilization and finer control granularity than constant composition DM (CCDM)-based A-PCS systems. We further model and analyze the FSO channel, presenting calculations and comparisons of outage probability and ergodic capacity under varying turbulence intensities and pointing errors. Results demonstrate 99.999~\\% reliability at maximum $σ_\\mathrm{R}^2 = 1.39$ and $σ_\\mathrm{s} = 0.5~\\mathrm{m}$, meeting requirements under severe turbulence and large pointing errors."}
{"id": "2511.19310", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.19310", "abs": "https://arxiv.org/abs/2511.19310", "authors": ["Mohammadhadi Mesmarian", "Mohammad Mahdi Kharidar", "Hossein Nejat Pishkenari"], "title": "Development of a Transit-Time Ultrasonic Flow Measurement System for Partially Filled Pipes: Incorporating Flow Profile Correction Factor and Real-Time Clogging Detection", "comment": "8 pages, 10 figures, preprint submitted to IEEE Sensors Journal (under second review)", "summary": "Flow measurement in partially filled pipes presents greater complexity compared to fully filled systems, primarily due to the complex velocity distribution within the cross-section, which is a key source of measurement inaccuracy. To address this challenge, an ultrasonic flow meter was designed and developed, capable of simultaneously measuring both flow velocity and fluid level. To improve measurement accuracy, a flow profile correction factor (FPCF) was derived based on the velocity distribution characteristics and applied to the raw flow meter output. A dedicated open-channel flow loop incorporating a 250 mm diameter pipe was constructed to test and calibrate the system under controlled conditions. Flow rates in the loop varied from 2 to 6 liters per second. The accuracy of the flow meter was evaluated using the Flow-Weighted Mean Error (FWME) metric. Experimental results showed that applying the FPCF significantly improved accuracy, reducing the maximum flow measurement error from 8.51% to 2.44%. Furthermore, calibration led to a substantial decrease in FWME from 1.78% to 0.08%, confirming the effectiveness of the proposed methodology. The flow meter was also subjected to clogging scenarios by artificially obstructing the flow. Under these conditions, the device was able to reliably measure the flow and successfully detected the clogging, triggering an alarm to the operator to take necessary action."}
{"id": "2511.19321", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.19321", "abs": "https://arxiv.org/abs/2511.19321", "authors": ["Weijie Xiong", "Zhenglan Zhao", "Jingran Lin", "Zhiling Xiao", "Qiang Li"], "title": "Secure Beamforming Design for IRS-ISAC Systems with a Hardware-Efficient Hybrid Beamforming Architecture", "comment": null, "summary": "In this paper, we employ a hardware-efficient hybrid beamforming (HB) architecture to achieve balanced performance in an intelligent reflecting surface (IRS)-assisted integrated sensing and communication (ISAC) system. We consider a scenario where a multi-antenna, dual-function base station (BS) performs secure beamforming for a multi-antenna legitimate receiver while simultaneously detecting potential targets. Our objective is to maximize the communication secrecy gap by jointly optimizing the analog and digital beamformers, IRS reflection coefficients, and radar scaling factor, subject to constraints on beampattern similarity, total transmit power budget, and the constant modulus of both the analog beamformer and IRS reflection coefficients. This secrecy gap maximization problem is generally non-convex. To address this, we incorporate the exterior penalty method by adding the radar constraint as a penalty term in the objective function. We then propose an efficient approach based on the penalty dual decomposition (PDD) framework to solve the reformulated problem, featuring closed-form solutions at each step and guaranteeing convergence to a stationary point. Simulation results validate the effectiveness of the proposed algorithm and demonstrate the superiority of the IRS-ISAC system with HB architecture in balancing performance and hardware costs."}
{"id": "2511.19360", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.19360", "abs": "https://arxiv.org/abs/2511.19360", "authors": ["Weijie Xiong", "Jingran Lin", "Kai Zhong", "Liu Yang", "Hongli Liu", "Qiang Li", "Cunhua Pan"], "title": "Secure Analog Beamforming for Multi-user MISO Systems with Movable Antennas", "comment": null, "summary": "Movable antennas (MAs) represent a novel approach that enables flexible adjustments to antenna positions, effectively altering the channel environment and thereby enhancing the performance of wireless communication systems. However, conventional MA implementations often adopt fully digital beamforming (FDB), which requires a dedicated RF chain for each antenna. This requirement significantly increase hardware costs, making such systems impractical for multi-antenna deployments. To address this, hardware-efficient analog beamforming (AB) offers a cost-effective alternative. This paper investigates the physical layer security (PLS) in an MA-enabled multiple-input single-output (MISO) communication system with an emphasis on AB. In this scenario, an MA-enabled transmitter with AB broadcasts common confidential information to a group of legitimate receivers, while a number of eavesdroppers overhear the transmission and attempt to intercept the information. Our objective is to maximize the multicast secrecy rate (MSR) by jointly optimizing the phase shifts of the AB and the positions of the MAs, subject to constraints on the movement area of the MAs and the constant modulus (CM) property of the analog phase shifters. This MSR maximization problem is highly challenging, as we have formally proven it to be NP-hard. To solve it efficiently, we propose a penalty constrained product manifold (PCPM) framework. Specifically, we first reformulate the position constraints as a penalty function, enabling unconstrained optimization on a product manifold space (PMS), and then propose a parallel conjugate gradient descent algorithm to efficiently update the variables. Simulation results demonstrate that MA-enabled systems with AB can achieve a well-balanced performance in terms of MSR and hardware costs."}
{"id": "2511.19369", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.19369", "abs": "https://arxiv.org/abs/2511.19369", "authors": ["Mohammed Almekhlafi", "Antoine Lesage-Landry", "Gunes Karabulut Kurt"], "title": "Connectivity-Aware Task Offloading for Remote Northern Regions: a Hybrid LEO-MEO Architecture", "comment": null, "summary": "Arctic regions, such as northern Canada, face significant challenges in achieving consistent connectivity and low-latency computing services due to the sparse coverage of Low Earth Orbit (LEO) satellites. To enhance service reliability in remote areas, this paper proposes a hybrid satellite architecture for task offloading that combines Medium Earth Orbit (MEO) and LEO satellites. We develop an optimization framework to maximize task offloading admission rate while balancing the energy consumption and delay requirements. Accounting for satellite visibility and limited computing resources, our approach integrates dynamic path selection with frequency and computational resource allocation. Because the formulated problem is NP-hard, we reformulate it into a mixed-integer convex form using disjunctive constraints and convex relaxation techniques, enabling efficient use of off-the-shelf optimization solvers. Simulation results show that, compared to a standalone LEO network, the proposed hybrid LEO-MEO architecture improves the task admission rate by 15\\% and reduces the average delay by 12\\%. These findings highlight the architecture's potential to enhance connectivity and user experience in remote Arctic areas."}
{"id": "2511.19275", "categories": ["cs.SD", "cs.AI", "eess.AS", "eess.SP"], "pdf": "https://arxiv.org/pdf/2511.19275", "abs": "https://arxiv.org/abs/2511.19275", "authors": ["Ellie L. Zhang", "Duoduo Liao", "Callie C. Liao"], "title": "Dynamic Multi-Species Bird Soundscape Generation with Acoustic Patterning and 3D Spatialization", "comment": "Accepted by IEEE Big Data 2025", "summary": "Generation of dynamic, scalable multi-species bird soundscapes remains a significant challenge in computer music and algorithmic sound design. Birdsongs involve rapid frequency-modulated chirps, complex amplitude envelopes, distinctive acoustic patterns, overlapping calls, and dynamic inter-bird interactions, all of which require precise temporal and spatial control in 3D environments. Existing approaches, whether Digital Signal Processing (DSP)-based or data-driven, typically focus only on single species modeling, static call structures, or synthesis directly from recordings, and often suffer from noise, limited flexibility, or large data needs. To address these challenges, we present a novel, fully algorithm-driven framework that generates dynamic multi-species bird soundscapes using DSP-based chirp generation and 3D spatialization, without relying on recordings or training data. Our approach simulates multiple independently-moving birds per species along different moving 3D trajectories, supporting controllable chirp sequences, overlapping choruses, and realistic 3D motion in scalable soundscapes while preserving species-specific acoustic patterns. A visualization interface provides bird trajectories, spectrograms, activity timelines, and sound waves for analytical and creative purposes. Both visual and audio evaluations demonstrate the ability of the system to generate dense, immersive, and ecologically inspired soundscapes, highlighting its potential for computer music, interactive virtual environments, and computational bioacoustics research."}

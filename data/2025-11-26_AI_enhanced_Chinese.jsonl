{"id": "2511.19805", "categories": ["eess.SP", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.19805", "abs": "https://arxiv.org/abs/2511.19805", "authors": ["Y. A. Rouzoumka", "E. Terreaux", "C. Morisseau", "J. -P. Ovarlez", "C. Ren"], "title": "Latent-space metrics for Complex-Valued VAE out-of-distribution detection under radar clutter", "comment": "Under review at ICASSP 2026", "summary": "We investigate complex-valued Variational AutoEncoders (CVAE) for radar Out-Of-Distribution (OOD) detection in complex radar environments. We proposed several detection metrics: the reconstruction error of CVAE (CVAE-MSE), the latent-based scores (Mahalanobis, Kullback-Leibler divergence (KLD)), and compared their performance against the classical ANMF-Tyler detector (ANMF-FP). The performance of all these detectors is analyzed on synthetic and experimental radar data, showing the advantages and the weaknesses of each detector.", "AI": {"tldr": "\u7814\u7a76\u590d\u6742\u503c\u53d8\u5206\u81ea\u7f16\u7801\u5668(CVAE)\u5728\u590d\u6742\u96f7\u8fbe\u73af\u5883\u4e2d\u7684\u79bb\u7fa4\u5206\u5e03\u68c0\u6d4b\uff0c\u63d0\u51fa\u4e86\u591a\u79cd\u68c0\u6d4b\u6307\u6807\u5e76\u4e0e\u7ecf\u5178ANMF-Tyler\u68c0\u6d4b\u5668\u8fdb\u884c\u6bd4\u8f83", "motivation": "\u89e3\u51b3\u590d\u6742\u96f7\u8fbe\u73af\u5883\u4e2d\u7684\u79bb\u7fa4\u5206\u5e03\u68c0\u6d4b\u95ee\u9898\uff0c\u63a2\u7d22\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u68c0\u6d4b\u65b9\u6cd5\u5728\u96f7\u8fbe\u4fe1\u53f7\u5904\u7406\u4e2d\u7684\u5e94\u7528", "method": "\u4f7f\u7528\u590d\u6742\u503c\u53d8\u5206\u81ea\u7f16\u7801\u5668(CVAE)\uff0c\u63d0\u51fa\u591a\u79cd\u68c0\u6d4b\u6307\u6807\uff1aCVAE\u91cd\u5efa\u8bef\u5dee\u3001\u57fa\u4e8e\u6f5c\u5728\u7a7a\u95f4\u7684\u9a6c\u6c0f\u8ddd\u79bb\u548cKL\u6563\u5ea6\uff0c\u5e76\u4e0eANMF-Tyler\u68c0\u6d4b\u5668\u8fdb\u884c\u5bf9\u6bd4", "result": "\u5728\u5408\u6210\u548c\u5b9e\u9a8c\u96f7\u8fbe\u6570\u636e\u4e0a\u5206\u6790\u4e86\u6240\u6709\u68c0\u6d4b\u5668\u7684\u6027\u80fd\uff0c\u5c55\u793a\u4e86\u6bcf\u79cd\u68c0\u6d4b\u5668\u7684\u4f18\u52bf\u548c\u5f31\u70b9", "conclusion": "\u590d\u6742\u503c\u53d8\u5206\u81ea\u7f16\u7801\u5668\u5728\u96f7\u8fbe\u79bb\u7fa4\u5206\u5e03\u68c0\u6d4b\u4e2d\u5177\u6709\u5e94\u7528\u6f5c\u529b\uff0c\u4e0d\u540c\u68c0\u6d4b\u6307\u6807\u5404\u6709\u4f18\u7f3a\u70b9\uff0c\u9700\u8981\u6839\u636e\u5177\u4f53\u573a\u666f\u9009\u62e9\u5408\u9002\u7684\u65b9\u6cd5"}}
{"id": "2511.19809", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.19809", "abs": "https://arxiv.org/abs/2511.19809", "authors": ["Rahul Gulia", "Feyisayo Favour Popoola", "Ashish Sheikh"], "title": "White-Box Modeling of V2X Link Performance Using Stabilized Symbolic Regression", "comment": null, "summary": "Reliable modeling of block error rate in vehicle-to-everything wireless networks is critical for designing robust communication systems under dynamic mobility and diverse channel conditions. Traditional machine learning approaches, such as deep neural networks, achieve high predictive accuracy but lack interpretability and impose significant computational costs, limiting their applicability in real-time, resource-constrained environments. In this work, we propose a stabilized symbolic regression framework to derive compact, analytically interpretable expressions for block error rate prediction. Trained on realistic vehicle-to-everything simulation data, the symbolic regression framework for vehicle-to-everything model accurately captures nonlinear dependencies on key system parameters, including signal-to-noise ratio, relative velocity, modulation and coding schemes, number of demodulation reference signal symbols, and environmental factors (line of sight/non-line of sight). Our final symbolic expression comprises only 158 nodes, enabling ultra-fast inference suitable for embedded deployment. On the test set, the symbolic regression framework for vehicle-to-everything model achieves a coefficient of determination $R^2 = 0.8684$ and mean squared error $= 2.08 \\times 10^{-2}$ in the original block error rate domain, outperforming conventional fixed-form regressions and offering comparable accuracy to neural networks while remaining fully interpretable. Overall, the proposed Stabilized Symbolic Regression Framework for V2X combines predictive performance, physical fidelity, and computational efficiency thus providing a powerful tool for real-time V2X communication system design, adaptive resource allocation, and rapid scenario evaluation.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7a33\u5b9a\u7684\u7b26\u53f7\u56de\u5f52\u6846\u67b6\uff0c\u7528\u4e8e\u63a8\u5bfc\u7d27\u51d1\u3001\u53ef\u89e3\u6790\u89e3\u91ca\u7684\u5757\u9519\u8bef\u7387\u9884\u6d4b\u8868\u8fbe\u5f0f\uff0c\u5728\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u7684\u540c\u65f6\u63d0\u4f9b\u53ef\u89e3\u91ca\u6027\u548c\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff08\u5982\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff09\u5728\u8f66\u8054\u7f51\u65e0\u7ebf\u7f51\u7edc\u4e2d\u9884\u6d4b\u5757\u9519\u8bef\u7387\u65f6\uff0c\u867d\u7136\u7cbe\u5ea6\u9ad8\u4f46\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u4e14\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u9650\u5236\u4e86\u5728\u5b9e\u65f6\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u4f7f\u7528\u7a33\u5b9a\u7684\u7b26\u53f7\u56de\u5f52\u6846\u67b6\uff0c\u57fa\u4e8e\u771f\u5b9e\u8f66\u8054\u7f51\u4eff\u771f\u6570\u636e\u8bad\u7ec3\uff0c\u6355\u6349\u5173\u952e\u7cfb\u7edf\u53c2\u6570\u7684\u975e\u7ebf\u6027\u4f9d\u8d56\u5173\u7cfb\uff0c\u5305\u62ec\u4fe1\u566a\u6bd4\u3001\u76f8\u5bf9\u901f\u5ea6\u3001\u8c03\u5236\u7f16\u7801\u65b9\u6848\u3001\u89e3\u8c03\u53c2\u8003\u4fe1\u53f7\u7b26\u53f7\u6570\u4ee5\u53ca\u73af\u5883\u56e0\u7d20\u3002", "result": "\u6700\u7ec8\u7b26\u53f7\u8868\u8fbe\u5f0f\u4ec5\u5305\u542b158\u4e2a\u8282\u70b9\uff0c\u5728\u6d4b\u8bd5\u96c6\u4e0a\u8fbe\u5230\u51b3\u5b9a\u7cfb\u6570R\u00b2=0.8684\u548c\u5747\u65b9\u8bef\u5dee2.08\u00d710\u207b\u00b2\uff0c\u4f18\u4e8e\u4f20\u7edf\u56fa\u5b9a\u5f62\u5f0f\u56de\u5f52\uff0c\u4e0e\u795e\u7ecf\u7f51\u7edc\u7cbe\u5ea6\u76f8\u5f53\u4f46\u5b8c\u5168\u53ef\u89e3\u91ca\u3002", "conclusion": "\u8be5\u7a33\u5b9a\u7b26\u53f7\u56de\u5f52\u6846\u67b6\u7ed3\u5408\u4e86\u9884\u6d4b\u6027\u80fd\u3001\u7269\u7406\u4fdd\u771f\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\uff0c\u4e3a\u5b9e\u65f6\u8f66\u8054\u7f51\u901a\u4fe1\u7cfb\u7edf\u8bbe\u8ba1\u3001\u81ea\u9002\u5e94\u8d44\u6e90\u5206\u914d\u548c\u5feb\u901f\u573a\u666f\u8bc4\u4f30\u63d0\u4f9b\u4e86\u5f3a\u5927\u5de5\u5177\u3002"}}
{"id": "2511.19866", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.19866", "abs": "https://arxiv.org/abs/2511.19866", "authors": ["Yutaka Jitsumatsu", "Liangchen Sun"], "title": "Parallel Delay-Doppler Estimation via Order-Reversed Two-Stage Prony Method", "comment": "5pages and 3 figures", "summary": "This paper proposes a Prony-based parallel two-stage method for delay-Doppler estimation in OTFS systems. By performing delay-first and Doppler-first estimations in parallel and fusing the results, the method resolves ambiguities caused by similar path characteristics. The simulation results demonstrate the superior accuracy and robustness of the proposed method under various conditions. This method provides a promising solution for future applications such as Vehicle-to-Vehicle (V2V) and Integrated Sensing and Communication (ISAC).", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eProny\u7684\u5e76\u884c\u4e24\u9636\u6bb5\u65b9\u6cd5\uff0c\u7528\u4e8eOTFS\u7cfb\u7edf\u4e2d\u7684\u5ef6\u8fdf-\u591a\u666e\u52d2\u4f30\u8ba1\uff0c\u901a\u8fc7\u5e76\u884c\u6267\u884c\u5ef6\u8fdf\u4f18\u5148\u548c\u591a\u666e\u52d2\u4f18\u5148\u4f30\u8ba1\u5e76\u878d\u5408\u7ed3\u679c\uff0c\u89e3\u51b3\u4e86\u8def\u5f84\u7279\u6027\u76f8\u4f3c\u5f15\u8d77\u7684\u6a21\u7cca\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3OTFS\u7cfb\u7edf\u4e2d\u7531\u4e8e\u8def\u5f84\u7279\u6027\u76f8\u4f3c\u5bfc\u81f4\u7684\u5ef6\u8fdf-\u591a\u666e\u52d2\u4f30\u8ba1\u6a21\u7cca\u95ee\u9898\uff0c\u63d0\u9ad8\u4f30\u8ba1\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u3002", "method": "\u91c7\u7528\u57fa\u4e8eProny\u7684\u5e76\u884c\u4e24\u9636\u6bb5\u65b9\u6cd5\uff0c\u5206\u522b\u8fdb\u884c\u5ef6\u8fdf\u4f18\u5148\u548c\u591a\u666e\u52d2\u4f18\u5148\u4f30\u8ba1\uff0c\u7136\u540e\u5c06\u7ed3\u679c\u878d\u5408\u4ee5\u6d88\u9664\u6a21\u7cca\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5404\u79cd\u6761\u4ef6\u4e0b\u90fd\u5177\u6709\u4f18\u8d8a\u7684\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3aV2V\u548cISAC\u7b49\u672a\u6765\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.19891", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.19891", "abs": "https://arxiv.org/abs/2511.19891", "authors": ["Pantelis Stefanakis", "Ming Shen"], "title": "Joint Classification and Regression Deep Learning Model for Universal Phase-based Ranging in Multiple Environments", "comment": null, "summary": "Phase-Based Ranging (PBR) offers several advantages for estimating distances between wirelessly connected devices, including high accuracy over large distances and the removal of the need for antenna arrays at each transceiver. This study investigates the use of Neural Network (NN)-based models for accurate PBR in three distinct environments: Openfield, Office, and Near Buildings, comparing their performance with established non-NN methods. A novel 2NN Model is proposed, integrating two neural networks: one to classify the environment and another to predict distances. Performance was evaluated over 20 trials for each method and dataset using root mean square error (RMSE) and maximum prediction error.\n  Results show that the 2NN Model consistently outperformed other methods, frequently ranking among the top methods in minimizing both RMSE and maximum error. In addition, the 2NN Model achieved the best average RMSE and the lowest maximum error. To assess the effect of environment misclassification, filtered versions of the NN models were evaluated by omitting misclassified measurements prior to RMSE calculation. Although unsuitable for production use, the filtered models revealed that misclassifications in the 2NN Model had a significant impact. Its filtered variant achieved the lowest RMSE and maximum error across all datasets, and ranked first in the frequency of attaining the lowest maximum error over 20 trials.\n  Overall, the findings show that NN models deliver robust, high-accuracy ranging across diverse environments, outperforming non-NN methods and reinforcing their potential as universal PBR solutions when trained on comprehensive distance datasets.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53cc\u795e\u7ecf\u7f51\u7edc(2NN)\u7684\u76f8\u4f4d\u6d4b\u8ddd\u65b9\u6cd5\uff0c\u5728\u5f00\u653e\u573a\u5730\u3001\u529e\u516c\u5ba4\u548c\u8fd1\u5efa\u7b51\u7269\u4e09\u79cd\u73af\u5883\u4e2d\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u975e\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u6d4b\u8ddd\u7cbe\u5ea6\u548c\u66f4\u4f4e\u7684\u8bef\u5dee\u3002", "motivation": "\u76f8\u4f4d\u6d4b\u8ddd(PBR)\u5177\u6709\u9ad8\u7cbe\u5ea6\u548c\u65e0\u9700\u5929\u7ebf\u9635\u5217\u7684\u4f18\u52bf\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u5728\u4e0d\u540c\u73af\u5883\u4e0b\u7684\u6027\u80fd\u6709\u9650\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u5728\u591a\u79cd\u73af\u5883\u4e2d\u5b9e\u73b0\u51c6\u786e\u76f8\u4f4d\u6d4b\u8ddd\u7684\u6f5c\u529b\u3002", "method": "\u63d0\u51fa\u65b0\u9896\u76842NN\u6a21\u578b\uff0c\u5305\u542b\u4e24\u4e2a\u795e\u7ecf\u7f51\u7edc\uff1a\u4e00\u4e2a\u7528\u4e8e\u73af\u5883\u5206\u7c7b\uff0c\u53e6\u4e00\u4e2a\u7528\u4e8e\u8ddd\u79bb\u9884\u6d4b\u3002\u5728\u4e09\u79cd\u4e0d\u540c\u73af\u5883\u4e2d\u8fdb\u884c\u6d4b\u8bd5\uff0c\u5e76\u4e0e\u4f20\u7edf\u975e\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\u8fdb\u884c\u5bf9\u6bd4\u3002", "result": "2NN\u6a21\u578b\u5728\u6240\u6709\u73af\u5883\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u5e73\u5747RMSE\u548c\u6700\u5927\u8bef\u5dee\u5747\u6700\u4f4e\u3002\u5373\u4f7f\u8003\u8651\u73af\u5883\u8bef\u5206\u7c7b\u7684\u5f71\u54cd\uff0c\u8fc7\u6ee4\u540e\u76842NN\u6a21\u578b\u4ecd\u80fd\u5b9e\u73b0\u6700\u4f4e\u7684RMSE\u548c\u6700\u5927\u8bef\u5dee\u3002", "conclusion": "\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u5728\u4e0d\u540c\u73af\u5883\u4e2d\u90fd\u80fd\u63d0\u4f9b\u7a33\u5065\u7684\u9ad8\u7cbe\u5ea6\u6d4b\u8ddd\uff0c\u4f18\u4e8e\u975e\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u5176\u4f5c\u4e3a\u901a\u7528PBR\u89e3\u51b3\u65b9\u6848\u7684\u6f5c\u529b\uff0c\u7279\u522b\u662f\u5728\u57fa\u4e8e\u5168\u9762\u8ddd\u79bb\u6570\u636e\u96c6\u8bad\u7ec3\u65f6\u3002"}}
{"id": "2511.20006", "categories": ["eess.AS", "cs.AI", "cs.SD"], "pdf": "https://arxiv.org/pdf/2511.20006", "abs": "https://arxiv.org/abs/2511.20006", "authors": ["Sungjae Kim", "Kihyun Na", "Jinyoung Choi", "Injung Kim"], "title": "BERT-APC: A Reference-free Framework for Automatic Pitch Correction via Musical Context Inference", "comment": "12 pages, 6 figures, 5 tables", "summary": "Automatic Pitch Correction (APC) enhances vocal recordings by aligning pitch deviations with the intended musical notes. However, existing APC systems either rely on reference pitches, which limits their practical applicability, or employ simple pitch estimation algorithms that often fail to preserve expressiveness and naturalness. We propose BERT-APC, a novel reference-free APC framework that corrects pitch errors while maintaining the natural expressiveness of vocal performances. In BERT-APC, a novel stationary pitch predictor first estimates the perceived pitch of each note from the detuned singing voice. A context-aware note pitch predictor estimates the intended pitch sequence by leveraging a music language model repurposed to incorporate musical context. Finally, a note-level correction algorithm fixes pitch errors while preserving intentional pitch deviations for emotional expression. In addition, we introduce a learnable data augmentation strategy that improves the robustness of the music language model by simulating realistic detuning patterns. Compared to two recent singing voice transcription models, BERT-APC demonstrated superior performance in note pitch prediction, outperforming the second-best model, ROSVOT, by 10.49%p on highly detuned samples in terms of the raw pitch accuracy. In the MOS test, BERT-APC achieved the highest score of $4.32 \\pm 0.15$, which is significantly higher than those of the widely-used commercial APC tools, AutoTune ($3.22 \\pm 0.18$) and Melodyne ($3.08 \\pm 0.18$), while maintaining a comparable ability to preserve expressive nuances. To the best of our knowledge, this is the first APC model that leverages a music language model to achieve reference-free pitch correction with symbolic musical context. The corrected audio samples of BERT-APC are available online.", "AI": {"tldr": "BERT-APC\u662f\u4e00\u4e2a\u65e0\u53c2\u8003\u7684\u81ea\u52a8\u97f3\u9ad8\u6821\u6b63\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u97f3\u4e50\u8bed\u8a00\u6a21\u578b\u6765\u6821\u6b63\u97f3\u9ad8\u8bef\u5dee\uff0c\u540c\u65f6\u4fdd\u6301\u6f14\u5531\u7684\u81ea\u7136\u8868\u73b0\u529b\u3002", "motivation": "\u73b0\u6709APC\u7cfb\u7edf\u8981\u4e48\u4f9d\u8d56\u53c2\u8003\u97f3\u9ad8\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u6027\uff0c\u8981\u4e48\u4f7f\u7528\u7b80\u5355\u7684\u97f3\u9ad8\u4f30\u8ba1\u7b97\u6cd5\uff0c\u5f80\u5f80\u65e0\u6cd5\u4fdd\u6301\u8868\u73b0\u529b\u548c\u81ea\u7136\u5ea6\u3002", "method": "\u4f7f\u7528\u56fa\u5b9a\u97f3\u9ad8\u9884\u6d4b\u5668\u4f30\u8ba1\u611f\u77e5\u97f3\u9ad8\uff0c\u4e0a\u4e0b\u6587\u611f\u77e5\u97f3\u7b26\u97f3\u9ad8\u9884\u6d4b\u5668\u901a\u8fc7\u97f3\u4e50\u8bed\u8a00\u6a21\u578b\u4f30\u8ba1\u76ee\u6807\u97f3\u9ad8\u5e8f\u5217\uff0c\u4ee5\u53ca\u97f3\u7b26\u7ea7\u6821\u6b63\u7b97\u6cd5\u5728\u4fdd\u7559\u60c5\u611f\u8868\u8fbe\u7684\u540c\u65f6\u4fee\u6b63\u97f3\u9ad8\u8bef\u5dee\u3002", "result": "\u5728\u9ad8\u5ea6\u5931\u8c10\u6837\u672c\u4e0a\uff0cBERT-APC\u5728\u539f\u59cb\u97f3\u9ad8\u51c6\u786e\u7387\u4e0a\u6bd4\u7b2c\u4e8c\u597d\u7684\u6a21\u578bROSVOT\u9ad8\u51fa10.49%\u3002MOS\u6d4b\u8bd5\u4e2d\u5f97\u5206\u4e3a4.32\u00b10.15\uff0c\u663e\u8457\u9ad8\u4e8eAutoTune(3.22\u00b10.18)\u548cMelodyne(3.08\u00b10.18)\u3002", "conclusion": "\u8fd9\u662f\u7b2c\u4e00\u4e2a\u5229\u7528\u97f3\u4e50\u8bed\u8a00\u6a21\u578b\u5b9e\u73b0\u65e0\u53c2\u8003\u97f3\u9ad8\u6821\u6b63\u5e76\u5305\u542b\u7b26\u53f7\u97f3\u4e50\u4e0a\u4e0b\u6587\u7684APC\u6a21\u578b\uff0c\u5728\u4fdd\u6301\u8868\u73b0\u529b\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u97f3\u9ad8\u6821\u6b63\u6027\u80fd\u3002"}}
{"id": "2511.19734", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2511.19734", "abs": "https://arxiv.org/abs/2511.19734", "authors": ["Luca A. Lanzend\u00f6rfer", "Florian Gr\u00f6tschla"], "title": "Evaluating Objective Speech Quality Metrics for Neural Audio Codecs", "comment": null, "summary": "Neural audio codecs have gained recent popularity for their use in generative modeling as they offer high-fidelity audio reconstruction at low bitrates. While human listening studies remain the gold standard for assessing perceptual quality, they are time-consuming and impractical. In this work, we examine the reliability of existing objective quality metrics in assessing the performance of recent neural audio codecs. To this end, we conduct a MUSHRA listening test on high-fidelity speech signals and analyze the correlation between subjective scores and widely used objective metrics. Our results show that, while some metrics align well with human perception, others struggle to capture relevant distortions. Our findings provide practical guidance for selecting appropriate evaluation metrics when using neural audio codecs for speech.", "AI": {"tldr": "\u8bc4\u4f30\u795e\u7ecf\u97f3\u9891\u7f16\u89e3\u7801\u5668\u65f6\uff0c\u73b0\u6709\u5ba2\u89c2\u8d28\u91cf\u6307\u6807\u4e0e\u4eba\u7c7b\u611f\u77e5\u7684\u76f8\u5173\u6027\u5b58\u5728\u5dee\u5f02\uff0c\u90e8\u5206\u6307\u6807\u8868\u73b0\u826f\u597d\uff0c\u90e8\u5206\u5219\u96be\u4ee5\u6355\u6349\u76f8\u5173\u5931\u771f\u3002", "motivation": "\u795e\u7ecf\u97f3\u9891\u7f16\u89e3\u7801\u5668\u5728\u751f\u6210\u5efa\u6a21\u4e2d\u5e94\u7528\u5e7f\u6cdb\uff0c\u4f46\u4eba\u7c7b\u542c\u529b\u7814\u7a76\u8017\u65f6\u4e14\u4e0d\u5b9e\u7528\uff0c\u9700\u8981\u53ef\u9760\u7684\u5ba2\u89c2\u8d28\u91cf\u8bc4\u4f30\u6307\u6807\u3002", "method": "\u5bf9\u9ad8\u4fdd\u771f\u8bed\u97f3\u4fe1\u53f7\u8fdb\u884cMUSHRA\u542c\u529b\u6d4b\u8bd5\uff0c\u5206\u6790\u4e3b\u89c2\u8bc4\u5206\u4e0e\u5e38\u7528\u5ba2\u89c2\u6307\u6807\u4e4b\u95f4\u7684\u76f8\u5173\u6027\u3002", "result": "\u90e8\u5206\u6307\u6807\u4e0e\u4eba\u7c7b\u611f\u77e5\u4e00\u81f4\uff0c\u4f46\u5176\u4ed6\u6307\u6807\u96be\u4ee5\u6355\u6349\u76f8\u5173\u5931\u771f\u3002", "conclusion": "\u4e3a\u4f7f\u7528\u795e\u7ecf\u97f3\u9891\u7f16\u89e3\u7801\u5668\u8fdb\u884c\u8bed\u97f3\u5904\u7406\u65f6\u9009\u62e9\u5408\u9002\u7684\u8bc4\u4f30\u6307\u6807\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\u3002"}}
{"id": "2511.19943", "categories": ["eess.SP", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.19943", "abs": "https://arxiv.org/abs/2511.19943", "authors": ["Akash Doshi", "Pinar Sen", "Kirill Ivanov", "Wei Yang", "June Namgoong", "Runxin Wang", "Rachel Wang", "Taesang Yoo", "Jing Jiang", "Tingfang Ji"], "title": "AI/ML based Joint Source and Channel Coding for HARQ-ACK Payload", "comment": "39 pages, 15 figures. Under consideration for publication in Journal of Sel. Areas in Information Theory. This paper was presented in part at the International Symposium on Topics in Coding, August 2025 in the Session for Coding and AI", "summary": "Channel coding from 2G to 5G has assumed the inputs bits at the physical layer to be uniformly distributed. However, hybrid automatic repeat request acknowledgement (HARQ-ACK) bits transmitted in the uplink are inherently non-uniformly distributed. For such sources, significant performance gains could be obtained by employing joint source channel coding, aided by deep learning-based techniques. In this paper, we learn a transformer-based encoder using a novel \"free-lunch\" training algorithm and propose per-codeword power shaping to exploit the source prior at the encoder whilst being robust to small changes in the HARQ-ACK distribution. Furthermore, any HARQ-ACK decoder has to achieve a low negative acknowledgement (NACK) error rate to avoid radio link failures resulting from multiple NACK errors. We develop an extension of the Neyman-Pearson test to a coded bit system with multiple information bits to achieve Unequal Error Protection of NACK over ACK bits at the decoder. Finally, we apply the proposed encoder and decoder designs to a 5G New Radio (NR) compliant uplink setup under a fading channel, describing the optimal receiver design and a low complexity coherent approximation to it. Our results demonstrate 3-6 dB reduction in the average transmit power required to achieve the target error rates compared to the NR baseline, while also achieving a 2-3 dB reduction in the maximum transmit power, thus providing for significant coverage gains and power savings.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u8054\u5408\u6e90\u4fe1\u9053\u7f16\u7801\u65b9\u6cd5\uff0c\u4e13\u95e8\u9488\u5bf95G\u4e0a\u884c\u94fe\u8def\u4e2d\u975e\u5747\u5300\u5206\u5e03\u7684HARQ-ACK\u6bd4\u7279\uff0c\u901a\u8fc7\u65b0\u578b\u8bad\u7ec3\u7b97\u6cd5\u548c\u529f\u7387\u6574\u5f62\u6280\u672f\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u4f20\u8f93\u529f\u7387\u9700\u6c42\u3002", "motivation": "\u4f20\u7edf\u4fe1\u9053\u7f16\u7801\u5047\u8bbe\u7269\u7406\u5c42\u8f93\u5165\u6bd4\u7279\u5747\u5300\u5206\u5e03\uff0c\u4f46HARQ-ACK\u6bd4\u7279\u672c\u8d28\u4e0a\u662f\u975e\u5747\u5300\u5206\u5e03\u7684\u3002\u5bf9\u4e8e\u6b64\u7c7b\u4fe1\u6e90\uff0c\u91c7\u7528\u8054\u5408\u6e90\u4fe1\u9053\u7f16\u7801\u53ef\u4ee5\u83b7\u5f97\u663e\u8457\u7684\u6027\u80fd\u589e\u76ca\u3002", "method": "\u4f7f\u7528\u57fa\u4e8eTransformer\u7684\u7f16\u7801\u5668\uff0c\u91c7\u7528\u65b0\u578b\"\u514d\u8d39\u5348\u9910\"\u8bad\u7ec3\u7b97\u6cd5\u548c\u6bcf\u7801\u5b57\u529f\u7387\u6574\u5f62\u6280\u672f\uff1b\u5f00\u53d1\u4e86Neyman-Pearson\u6d4b\u8bd5\u7684\u6269\u5c55\u7248\u672c\uff0c\u5728\u89e3\u7801\u5668\u4e2d\u5b9e\u73b0NACK\u6bd4\u7279\u76f8\u5bf9\u4e8eACK\u6bd4\u7279\u7684\u4e0d\u7b49\u9519\u8bef\u4fdd\u62a4\u3002", "result": "\u4e0e5G NR\u57fa\u7ebf\u76f8\u6bd4\uff0c\u5728\u8fbe\u5230\u76ee\u6807\u9519\u8bef\u7387\u65f6\uff0c\u5e73\u5747\u4f20\u8f93\u529f\u7387\u964d\u4f4e\u4e863-6 dB\uff0c\u6700\u5927\u4f20\u8f93\u529f\u7387\u964d\u4f4e\u4e862-3 dB\uff0c\u63d0\u4f9b\u4e86\u663e\u8457\u7684\u8986\u76d6\u589e\u76ca\u548c\u529f\u7387\u8282\u7701\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\u8bbe\u8ba1\u57285G NR\u517c\u5bb9\u7684\u4e0a\u884c\u94fe\u8def\u8bbe\u7f6e\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3aHARQ-ACK\u4f20\u8f93\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u91cd\u8981\u7684\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2511.19974", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2511.19974", "abs": "https://arxiv.org/abs/2511.19974", "authors": ["Wangjie Li", "Lin Li", "Qingyang Hong"], "title": "Continual Audio Deepfake Detection via Universal Adversarial Perturbation", "comment": null, "summary": "The rapid advancement of speech synthesis and voice conversion technologies has raised significant security concerns in multimedia forensics. Although current detection models demonstrate impressive performance, they struggle to maintain effectiveness against constantly evolving deepfake attacks. Additionally, continually fine-tuning these models using historical training data incurs substantial computational and storage costs. To address these limitations, we propose a novel framework that incorporates Universal Adversarial Perturbation (UAP) into audio deepfake detection, enabling models to retain knowledge of historical spoofing distribution without direct access to past data. Our method integrates UAP seamlessly with pre-trained self-supervised audio models during fine-tuning. Extensive experiments validate the effectiveness of our approach, showcasing its potential as an efficient solution for continual learning in audio deepfake detection.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u901a\u7528\u5bf9\u6297\u6270\u52a8\u96c6\u6210\u5230\u97f3\u9891\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u4e2d\u7684\u65b0\u6846\u67b6\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u4fdd\u7559\u5386\u53f2\u6b3a\u9a97\u5206\u5e03\u77e5\u8bc6\u800c\u65e0\u9700\u76f4\u63a5\u8bbf\u95ee\u8fc7\u53bb\u6570\u636e", "motivation": "\u8bed\u97f3\u5408\u6210\u548c\u8bed\u97f3\u8f6c\u6362\u6280\u672f\u7684\u5feb\u901f\u53d1\u5c55\u5f15\u53d1\u4e86\u591a\u5a92\u4f53\u53d6\u8bc1\u4e2d\u7684\u91cd\u5927\u5b89\u5168\u95ee\u9898\u3002\u73b0\u6709\u68c0\u6d4b\u6a21\u578b\u96be\u4ee5\u5e94\u5bf9\u4e0d\u65ad\u6f14\u53d8\u7684\u6df1\u5ea6\u4f2a\u9020\u653b\u51fb\uff0c\u4e14\u6301\u7eed\u4f7f\u7528\u5386\u53f2\u8bad\u7ec3\u6570\u636e\u8fdb\u884c\u5fae\u8c03\u4f1a\u4ea7\u751f\u9ad8\u6602\u7684\u8ba1\u7b97\u548c\u5b58\u50a8\u6210\u672c", "method": "\u5728\u9884\u8bad\u7ec3\u7684\u81ea\u76d1\u7763\u97f3\u9891\u6a21\u578b\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u65e0\u7f1d\u96c6\u6210\u901a\u7528\u5bf9\u6297\u6270\u52a8\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u4fdd\u7559\u5386\u53f2\u6b3a\u9a97\u5206\u5e03\u77e5\u8bc6", "result": "\u5927\u91cf\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5c55\u793a\u4e86\u5176\u4f5c\u4e3a\u97f3\u9891\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u4e2d\u6301\u7eed\u5b66\u4e60\u7684\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u7684\u6f5c\u529b", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u89e3\u51b3\u97f3\u9891\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u4e2d\u7684\u6301\u7eed\u5b66\u4e60\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u5728\u4e0d\u76f4\u63a5\u8bbf\u95ee\u5386\u53f2\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u4fdd\u6301\u5bf9\u5386\u53f2\u6b3a\u9a97\u5206\u5e03\u7684\u8bc6\u522b\u80fd\u529b"}}
{"id": "2511.20000", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.20000", "abs": "https://arxiv.org/abs/2511.20000", "authors": ["Mingyi Lu", "Guowei Liu", "Le Liang", "Chongtao Guo", "Hao Ye", "Shi Jin"], "title": "Cross-Modal Semantic Communication for Heterogeneous Collaborative Perception", "comment": null, "summary": "Collaborative perception, an emerging paradigm in autonomous driving, has been introduced to mitigate the limitations of single-vehicle systems, such as limited sensor range and occlusion. To improve the robustness of inter-vehicle data sharing, semantic communication has recently further been integrated into collaborative perception systems to enhance overall performance. However, practical deployment of such systems is challenged by the heterogeneity of sensors across different connected autonomous vehicles (CAVs). This diversity in perceptual data complicates the design of a unified communication framework and impedes the effective fusion of shared information. To address this challenge, we propose a novel cross-modal semantic communication (CMSC) framework to facilitate effective collaboration among CAVs with disparate sensor configurations. Specifically, the framework first transforms heterogeneous perceptual features from different sensor modalities into a unified and standardized semantic space. Subsequently, encoding, transmission, and decoding are performed within this semantic space, enabling seamless and effective information fusion. Extensive experiments demonstrate that CMSC achieves significantly stronger perception performance than existing methods, particularly in low signal-to-noise ratio (SNR) regimes.", "AI": {"tldr": "\u63d0\u51fa\u8de8\u6a21\u6001\u8bed\u4e49\u901a\u4fe1(CMSC)\u6846\u67b6\uff0c\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76\u4e2d\u4e0d\u540c\u4f20\u611f\u5668\u914d\u7f6e\u8f66\u8f86\u95f4\u7684\u534f\u4f5c\u611f\u77e5\u95ee\u9898\uff0c\u901a\u8fc7\u7edf\u4e00\u8bed\u4e49\u7a7a\u95f4\u5b9e\u73b0\u5f02\u6784\u611f\u77e5\u7279\u5f81\u7684\u6709\u6548\u878d\u5408\u3002", "motivation": "\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76\u534f\u4f5c\u611f\u77e5\u4e2d\u56e0\u8f66\u8f86\u4f20\u611f\u5668\u5f02\u6784\u6027\u5bfc\u81f4\u7684\u901a\u4fe1\u6846\u67b6\u8bbe\u8ba1\u56f0\u96be\u548c\u4fe1\u606f\u878d\u5408\u6548\u7387\u4f4e\u4e0b\u7684\u95ee\u9898\u3002", "method": "\u5c06\u4e0d\u540c\u4f20\u611f\u5668\u6a21\u6001\u7684\u5f02\u6784\u611f\u77e5\u7279\u5f81\u8f6c\u6362\u5230\u7edf\u4e00\u7684\u8bed\u4e49\u7a7a\u95f4\uff0c\u5728\u8be5\u7a7a\u95f4\u5185\u8fdb\u884c\u7f16\u7801\u3001\u4f20\u8f93\u548c\u89e3\u7801\uff0c\u5b9e\u73b0\u65e0\u7f1d\u4fe1\u606f\u878d\u5408\u3002", "result": "\u5728\u4f4e\u4fe1\u566a\u6bd4\u6761\u4ef6\u4e0b\uff0cCMSC\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u611f\u77e5\u6027\u80fd\u3002", "conclusion": "CMSC\u6846\u67b6\u80fd\u591f\u6709\u6548\u4fc3\u8fdb\u5f02\u6784\u4f20\u611f\u5668\u914d\u7f6e\u8f66\u8f86\u95f4\u7684\u534f\u4f5c\u611f\u77e5\uff0c\u5728\u6076\u52a3\u901a\u4fe1\u73af\u5883\u4e0b\u4ecd\u80fd\u4fdd\u6301\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2511.20224", "categories": ["cs.SD", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.20224", "abs": "https://arxiv.org/abs/2511.20224", "authors": ["Rui Lin", "Zhiyue Wu", "Jiahe Le", "Kangdi Wang", "Weixiong Chen", "Junyu Dai", "Tao Jiang"], "title": "DUO-TOK: Dual-Track Semantic Music Tokenizer for Vocal-Accompaniment Generation", "comment": "17 pages, 5 figures, 8 tables. Project page: https://eps-acoustic-revolution-lab.github.io/DUO_TOK/", "summary": "Duo-Tok is a source-aware dual-codebook tokenizer for vocal-accompaniment music that targets the growing tension between reconstruction quality and language-model (LM) learnability in modern lyrics-to-song systems. Existing codecs either prioritize high-fidelity reconstruction with difficult-to-model acoustic tokens or compress aggressively into semantic tokens that are LM-friendly but lossy, and they rarely make the tokenizer itself aware of dual-track structure. Duo-Tok follows a four-stage, SSL-centered pipeline: we first pretrain a BEST-RQ-style encoder on large-scale audio, then stabilize and factorize the representation with Gaussian replacement noise and multi-task supervision, before freezing the encoder to learn SimVQ-based dual codebooks with hard routing for vocals and accompaniment, and finally training latent diffusion decoders on top of the discrete tokens. Duo-Tok at 0.75 kbps shifts the empirical reconstruction-generation Pareto frontier, achieving the best music-tagging AP and the lowest vocabulary-normalized LM perplexity among compared codecs while maintaining reconstruction quality comparable to state-of-the-art music tokenizers.", "AI": {"tldr": "Duo-Tok\u662f\u4e00\u4e2a\u9762\u5411\u4eba\u58f0-\u4f34\u594f\u97f3\u4e50\u7684\u53cc\u7801\u672c\u5206\u8bcd\u5668\uff0c\u901a\u8fc7\u56db\u9636\u6bb5SSL\u6d41\u7a0b\u5e73\u8861\u91cd\u5efa\u8d28\u91cf\u4e0e\u8bed\u8a00\u6a21\u578b\u5b66\u4e60\u80fd\u529b\uff0c\u57280.75kbps\u7801\u7387\u4e0b\u5b9e\u73b0\u4e86\u91cd\u5efa-\u751f\u6210\u7684\u5e15\u7d2f\u6258\u524d\u6cbf\u4f18\u5316\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u6b4c\u8bcd\u8f6c\u6b4c\u66f2\u7cfb\u7edf\u4e2d\u91cd\u5efa\u8d28\u91cf\u4e0e\u8bed\u8a00\u6a21\u578b\u5b66\u4e60\u80fd\u529b\u4e4b\u95f4\u7684\u5f20\u529b\uff0c\u73b0\u6709\u7f16\u89e3\u7801\u5668\u8981\u4e48\u4f18\u5148\u9ad8\u4fdd\u771f\u91cd\u5efa\u4f46\u96be\u4ee5\u5efa\u6a21\uff0c\u8981\u4e48\u538b\u7f29\u8fc7\u5ea6\u5bfc\u81f4\u8bed\u4e49\u635f\u5931\uff0c\u4e14\u7f3a\u4e4f\u5bf9\u53cc\u8f68\u7ed3\u6784\u7684\u611f\u77e5\u3002", "method": "\u56db\u9636\u6bb5\u81ea\u76d1\u7763\u5b66\u4e60\u6d41\u7a0b\uff1a1) \u5728\u5927\u89c4\u6a21\u97f3\u9891\u4e0a\u9884\u8bad\u7ec3BEST-RQ\u98ce\u683c\u7f16\u7801\u5668\uff1b2) \u4f7f\u7528\u9ad8\u65af\u66ff\u6362\u566a\u58f0\u548c\u591a\u4efb\u52a1\u76d1\u7763\u7a33\u5b9a\u548c\u5206\u89e3\u8868\u793a\uff1b3) \u51bb\u7ed3\u7f16\u7801\u5668\u5b66\u4e60\u57fa\u4e8eSimVQ\u7684\u53cc\u7801\u672c\uff0c\u5bf9\u4eba\u58f0\u548c\u4f34\u594f\u8fdb\u884c\u786c\u8def\u7531\uff1b4) \u5728\u79bb\u6563\u6807\u8bb0\u4e0a\u8bad\u7ec3\u6f5c\u5728\u6269\u6563\u89e3\u7801\u5668\u3002", "result": "\u57280.75kbps\u7801\u7387\u4e0b\uff0cDuo-Tok\u5728\u91cd\u5efa-\u751f\u6210\u5e15\u7d2f\u6258\u524d\u6cbf\u4e0a\u53d6\u5f97\u6700\u4f73\u8868\u73b0\uff0c\u83b7\u5f97\u6700\u9ad8\u7684\u97f3\u4e50\u6807\u7b7e\u5e73\u5747\u7cbe\u5ea6\u548c\u6700\u4f4e\u7684\u8bcd\u6c47\u5f52\u4e00\u5316\u8bed\u8a00\u6a21\u578b\u56f0\u60d1\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u6700\u5148\u8fdb\u97f3\u4e50\u5206\u8bcd\u5668\u76f8\u5f53\u7684\u91cd\u5efa\u8d28\u91cf\u3002", "conclusion": "Duo-Tok\u901a\u8fc7\u6e90\u611f\u77e5\u7684\u53cc\u7801\u672c\u8bbe\u8ba1\u548cSSL\u6d41\u7a0b\uff0c\u6210\u529f\u5e73\u8861\u4e86\u97f3\u4e50\u91cd\u5efa\u8d28\u91cf\u4e0e\u8bed\u8a00\u6a21\u578b\u5b66\u4e60\u80fd\u529b\uff0c\u4e3a\u6b4c\u8bcd\u8f6c\u6b4c\u66f2\u7cfb\u7edf\u63d0\u4f9b\u4e86\u66f4\u4f18\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.20003", "categories": ["eess.SP", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.20003", "abs": "https://arxiv.org/abs/2511.20003", "authors": ["Simin Zhu", "Satish Ravindran", "Alexander Yarovoy", "Francesco Fioranelli"], "title": "Redefining Radar Segmentation: Simultaneous Static-Moving Segmentation and Ego-Motion Estimation using Radar Point Clouds", "comment": "16 pages, 9 figures, under review at IEEE Transactions on Radar Systems", "summary": "Conventional radar segmentation research has typically focused on learning category labels for different moving objects. Although fundamental differences between radar and optical sensors lead to differences in the reliability of predicting accurate and consistent category labels, a review of common radar perception tasks in automotive reveals that determining whether an object is moving or static is a prerequisite for most tasks. To fill this gap, this study proposes a neural network based solution that can simultaneously segment static and moving objects from radar point clouds. Furthermore, since the measured radial velocity of static objects is correlated with the motion of the radar, this approach can also estimate the instantaneous 2D velocity of the moving platform or vehicle (ego motion). However, despite performing dual tasks, the proposed method employs very simple yet effective building blocks for feature extraction: multi layer perceptrons (MLPs) and recurrent neural networks (RNNs). In addition to being the first of its kind in the literature, the proposed method also demonstrates the feasibility of extracting the information required for the dual task directly from unprocessed point clouds, without the need for cloud aggregation, Doppler compensation, motion compensation, or any other intermediate signal processing steps. To measure its performance, this study introduces a set of novel evaluation metrics and tests the proposed method using a challenging real world radar dataset, RadarScenes. The results show that the proposed method not only performs well on the dual tasks, but also has broad application potential in other radar perception tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u540c\u65f6\u4ece\u96f7\u8fbe\u70b9\u4e91\u4e2d\u5206\u5272\u9759\u6001\u548c\u52a8\u6001\u7269\u4f53\uff0c\u5e76\u4f30\u8ba1\u79fb\u52a8\u5e73\u53f0\u7684\u77ac\u65f62D\u901f\u5ea6\uff0c\u65e0\u9700\u590d\u6742\u7684\u4e2d\u95f4\u4fe1\u53f7\u5904\u7406\u6b65\u9aa4\u3002", "motivation": "\u4f20\u7edf\u96f7\u8fbe\u5206\u5272\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u79fb\u52a8\u7269\u4f53\u7684\u7c7b\u522b\u6807\u7b7e\uff0c\u4f46\u96f7\u8fbe\u4e0e\u5149\u5b66\u4f20\u611f\u5668\u7684\u672c\u8d28\u5dee\u5f02\u4f7f\u5f97\u7c7b\u522b\u6807\u7b7e\u9884\u6d4b\u7684\u53ef\u9760\u6027\u4e0d\u540c\u3002\u5728\u6c7d\u8f66\u96f7\u8fbe\u611f\u77e5\u4efb\u52a1\u4e2d\uff0c\u5224\u65ad\u7269\u4f53\u662f\u9759\u6001\u8fd8\u662f\u52a8\u6001\u662f\u5927\u591a\u6570\u4efb\u52a1\u7684\u5148\u51b3\u6761\u4ef6\u3002", "method": "\u4f7f\u7528\u591a\u5c42\u611f\u77e5\u673a(MLPs)\u548c\u5faa\u73af\u795e\u7ecf\u7f51\u7edc(RNNs)\u7b49\u7b80\u5355\u6709\u6548\u7684\u6784\u5efa\u5757\u8fdb\u884c\u7279\u5f81\u63d0\u53d6\uff0c\u76f4\u63a5\u4ece\u539f\u59cb\u70b9\u4e91\u4e2d\u63d0\u53d6\u6240\u9700\u4fe1\u606f\uff0c\u65e0\u9700\u70b9\u4e91\u805a\u5408\u3001\u591a\u666e\u52d2\u8865\u507f\u3001\u8fd0\u52a8\u8865\u507f\u7b49\u4e2d\u95f4\u5904\u7406\u6b65\u9aa4\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u96f7\u8fbe\u6570\u636e\u96c6RadarScenes\u4e0a\u8fdb\u884c\u6d4b\u8bd5\uff0c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u53cc\u91cd\u4efb\u52a1\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u5e76\u5728\u5176\u4ed6\u96f7\u8fbe\u611f\u77e5\u4efb\u52a1\u4e2d\u5177\u6709\u5e7f\u6cdb\u5e94\u7528\u6f5c\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9996\u6b21\u5728\u6587\u732e\u4e2d\u5b9e\u73b0\u540c\u65f6\u5206\u5272\u9759\u6001\u548c\u52a8\u6001\u7269\u4f53\u5e76\u4f30\u8ba1\u5e73\u53f0\u901f\u5ea6\u7684\u53cc\u91cd\u4efb\u52a1\uff0c\u8bc1\u660e\u4e86\u76f4\u63a5\u4ece\u539f\u59cb\u96f7\u8fbe\u70b9\u4e91\u63d0\u53d6\u6240\u9700\u4fe1\u606f\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2511.20380", "categories": ["cs.SD", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.20380", "abs": "https://arxiv.org/abs/2511.20380", "authors": ["Ilias Ibnyahya", "Joshua D. Reiss"], "title": "Differentiable Attenuation Filters for Feedback Delay Networks", "comment": null, "summary": "We introduce a novel method for designing attenuation filters in digital audio reverberation systems based on Feedback Delay Networks (FDNs). Our approach uses Second Order Sections (SOS) of Infinite Impulse Response (IIR) filters arranged as parametric equalizers (PEQ), enabling fine control over frequency-dependent reverberation decay. Unlike traditional graphic equalizer designs, which require numerous filters per delay line, we propose a scalable solution where the number of filters can be adjusted. The frequency, gain, and quality factor (Q) parameters are shared parameters across delay lines and only the gain is adjusted based on delay length. This design not only reduces the number of optimization parameters, but also remains fully differentiable and compatible with gradient-based learning frameworks. Leveraging principles of analog filter design, our method allows for efficient and accurate filter fitting using supervised learning. Our method delivers a flexible and differentiable design, achieving state-of-the-art performance while significantly reducing computational cost.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u53cd\u9988\u5ef6\u8fdf\u7f51\u7edc(FDN)\u7684\u6570\u5b57\u97f3\u9891\u6df7\u54cd\u7cfb\u7edf\u8870\u51cf\u6ee4\u6ce2\u5668\u8bbe\u8ba1\u65b0\u65b9\u6cd5\uff0c\u4f7f\u7528\u4e8c\u9636IIR\u6ee4\u6ce2\u5668\u4f5c\u4e3a\u53c2\u6570\u5747\u8861\u5668\uff0c\u5b9e\u73b0\u9891\u7387\u76f8\u5173\u6df7\u54cd\u8870\u51cf\u7684\u7cbe\u7ec6\u63a7\u5236\u3002", "motivation": "\u4f20\u7edf\u56fe\u5f62\u5747\u8861\u5668\u8bbe\u8ba1\u9700\u8981\u5728\u6bcf\u4e2a\u5ef6\u8fdf\u7ebf\u4e0a\u4f7f\u7528\u5927\u91cf\u6ee4\u6ce2\u5668\uff0c\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u53c2\u6570\u4f18\u5316\u56f0\u96be\uff0c\u9700\u8981\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u53ef\u5fae\u5206\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u4e8c\u9636IIR\u6ee4\u6ce2\u5668\u4f5c\u4e3a\u53c2\u6570\u5747\u8861\u5668\uff0c\u9891\u7387\u3001\u589e\u76ca\u548cQ\u56e0\u5b50\u53c2\u6570\u5728\u5ef6\u8fdf\u7ebf\u95f4\u5171\u4eab\uff0c\u4ec5\u6839\u636e\u5ef6\u8fdf\u957f\u5ea6\u8c03\u6574\u589e\u76ca\uff0c\u51cf\u5c11\u4f18\u5316\u53c2\u6570\u6570\u91cf\u5e76\u4fdd\u6301\u5b8c\u5168\u53ef\u5fae\u5206\u3002", "result": "\u5b9e\u73b0\u4e86\u7075\u6d3b\u4e14\u53ef\u5fae\u5206\u7684\u6ee4\u6ce2\u5668\u8bbe\u8ba1\uff0c\u5728\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u7684\u540c\u65f6\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u6027\u80fd\u8868\u73b0\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u57fa\u4e8e\u6a21\u62df\u6ee4\u6ce2\u5668\u8bbe\u8ba1\u539f\u7406\uff0c\u5229\u7528\u76d1\u7763\u5b66\u4e60\u5b9e\u73b0\u9ad8\u6548\u51c6\u786e\u7684\u6ee4\u6ce2\u5668\u62df\u5408\uff0c\u4e3a\u6570\u5b57\u97f3\u9891\u6df7\u54cd\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u8ba1\u7b97\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.20082", "categories": ["eess.SP", "cs.IT"], "pdf": "https://arxiv.org/pdf/2511.20082", "abs": "https://arxiv.org/abs/2511.20082", "authors": ["James Delfeld", "Gian Marti", "Chris Dick"], "title": "Sparse MIMO-OFDM Channel Estimation via RKHS Regularization", "comment": null, "summary": "We propose a method for channel estimation in multiple-input multiple-output (MIMO) orthogonal frequency-division multiplexing (OFDM) wireless communication systems. The method exploits the band-sparsity of wireless channels in the delay-beamspace domain by solving a regularized optimization problem in a reproducing kernel Hilbert space (RKHS). A suitable representer theorem allows us to transform the infinite-dimensional optimization problem into a finite-dimensional one, which we then approximate with a low-dimensional surrogate. We solve the resulting optimization problem using a forward-backward splitting (FBS)-based algorithm. By exploiting the problem's modulation structure, we achieve a computational complexity per iteration that is quasi-linear in the number of unknown variables. We also propose a data-driven deep-unfolding based extension to improve the performance at a reduced number of iterations. We evaluate our channel estimators on ray-traced channels generated with SionnaRT. The results show that our methods significantly outperform linear methods such as linear minimum mean squared error (LMMSE) channel estimation based on aggregate channel statistics, both in terms of raw estimation accuracy as well as in downstream performance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eRKHS\u6b63\u5219\u5316\u4f18\u5316\u7684MIMO-OFDM\u4fe1\u9053\u4f30\u8ba1\u65b9\u6cd5\uff0c\u5229\u7528\u5ef6\u8fdf-\u6ce2\u675f\u7a7a\u95f4\u7684\u5e26\u7a00\u758f\u6027\uff0c\u901a\u8fc7FBS\u7b97\u6cd5\u5b9e\u73b0\u51c6\u7ebf\u6027\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u5e76\u901a\u8fc7\u6df1\u5ea6\u5c55\u5f00\u6280\u672f\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u7ebf\u6027\u4fe1\u9053\u4f30\u8ba1\u65b9\u6cd5\uff08\u5982LMMSE\uff09\u5728MIMO-OFDM\u7cfb\u7edf\u4e2d\u6027\u80fd\u6709\u9650\uff0c\u9700\u8981\u5229\u7528\u4fe1\u9053\u5728\u5ef6\u8fdf-\u6ce2\u675f\u7a7a\u95f4\u7684\u7a00\u758f\u7279\u6027\u6765\u63d0\u9ad8\u4f30\u8ba1\u7cbe\u5ea6\u548c\u4e0b\u6e38\u6027\u80fd\u3002", "method": "\u5728RKHS\u4e2d\u6784\u5efa\u6b63\u5219\u5316\u4f18\u5316\u95ee\u9898\uff0c\u5229\u7528\u8868\u793a\u5b9a\u7406\u8f6c\u5316\u4e3a\u6709\u9650\u7ef4\u95ee\u9898\uff0c\u91c7\u7528FBS\u7b97\u6cd5\u6c42\u89e3\uff0c\u5e76\u5f15\u5165\u6df1\u5ea6\u5c55\u5f00\u6280\u672f\u52a0\u901f\u6536\u655b\u3002", "result": "\u5728SionnaRT\u751f\u6210\u7684\u5c04\u7ebf\u8ffd\u8e2a\u4fe1\u9053\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u539f\u59cb\u4f30\u8ba1\u7cbe\u5ea6\u548c\u4e0b\u6e38\u6027\u80fd\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u57fa\u4e8e\u805a\u5408\u4fe1\u9053\u7edf\u8ba1\u7684LMMSE\u7b49\u7ebf\u6027\u65b9\u6cd5\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684RKHS\u6b63\u5219\u5316\u4fe1\u9053\u4f30\u8ba1\u65b9\u6cd5\u80fd\u6709\u6548\u5229\u7528\u4fe1\u9053\u7a00\u758f\u6027\uff0c\u5728\u4fdd\u6301\u51c6\u7ebf\u6027\u8ba1\u7b97\u590d\u6742\u5ea6\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347MIMO-OFDM\u7cfb\u7edf\u7684\u4fe1\u9053\u4f30\u8ba1\u6027\u80fd\u3002"}}
{"id": "2511.20470", "categories": ["cs.SD", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.20470", "abs": "https://arxiv.org/abs/2511.20470", "authors": ["Gen\u00eds Plaja-Roglans", "Yun-Ning Hung", "Xavier Serra", "Igor Pereira"], "title": "Efficient and Fast Generative-Based Singing Voice Separation using a Latent Diffusion Model", "comment": "Accepted for oral presentation at IJCNN 2025", "summary": "Extracting individual elements from music mixtures is a valuable tool for music production and practice. While neural networks optimized to mask or transform mixture spectrograms into the individual source(s) have been the leading approach, the source overlap and correlation in music signals poses an inherent challenge. Also, accessing all sources in the mixture is crucial to train these systems, while complicated. Attempts to address these challenges in a generative fashion exist, however, the separation performance and inference efficiency remain limited. In this work, we study the potential of diffusion models to advance toward bridging this gap, focusing on generative singing voice separation relying only on corresponding pairs of isolated vocals and mixtures for training. To align with creative workflows, we leverage latent diffusion: the system generates samples encoded in a compact latent space, and subsequently decodes these into audio. This enables efficient optimization and faster inference. Our system is trained using only open data. We outperform existing generative separation systems, and level the compared non-generative systems on a list of signal quality measures and on interference removal. We provide a noise robustness study on the latent encoder, providing insights on its potential for the task. We release a modular toolkit for further research on the topic.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6f5c\u5728\u6269\u6563\u6a21\u578b\u7684\u6b4c\u5531\u4eba\u58f0\u5206\u79bb\u65b9\u6cd5\uff0c\u4ec5\u9700\u5b64\u7acb\u7684\u6b4c\u58f0\u548c\u6df7\u5408\u97f3\u9891\u5bf9\u8fdb\u884c\u8bad\u7ec3\uff0c\u5728\u5206\u79bb\u6027\u80fd\u548c\u63a8\u7406\u6548\u7387\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u751f\u6210\u5f0f\u5206\u79bb\u7cfb\u7edf\uff0c\u5e76\u4e0e\u975e\u751f\u6210\u5f0f\u7cfb\u7edf\u6027\u80fd\u76f8\u5f53\u3002", "motivation": "\u97f3\u4e50\u6df7\u5408\u4e2d\u63d0\u53d6\u5355\u4e2a\u5143\u7d20\u5bf9\u97f3\u4e50\u5236\u4f5c\u548c\u7ec3\u4e60\u5f88\u6709\u4ef7\u503c\u3002\u73b0\u6709\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684\u5206\u79bb\u65b9\u6cd5\u9762\u4e34\u6e90\u91cd\u53e0\u548c\u76f8\u5173\u6027\u7684\u56fa\u6709\u6311\u6218\uff0c\u4e14\u9700\u8981\u8bbf\u95ee\u6df7\u5408\u4e2d\u7684\u6240\u6709\u6e90\u8fdb\u884c\u8bad\u7ec3\u3002\u751f\u6210\u5f0f\u65b9\u6cd5\u5c1d\u8bd5\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u4f46\u5206\u79bb\u6027\u80fd\u548c\u63a8\u7406\u6548\u7387\u4ecd\u6709\u5c40\u9650\u3002", "method": "\u91c7\u7528\u6f5c\u5728\u6269\u6563\u6a21\u578b\u8fdb\u884c\u751f\u6210\u5f0f\u6b4c\u5531\u4eba\u58f0\u5206\u79bb\uff0c\u4ec5\u5728\u7d27\u51d1\u7684\u6f5c\u5728\u7a7a\u95f4\u4e2d\u751f\u6210\u6837\u672c\uff0c\u7136\u540e\u89e3\u7801\u4e3a\u97f3\u9891\u3002\u5229\u7528\u6f5c\u5728\u6269\u6563\u5b9e\u73b0\u9ad8\u6548\u4f18\u5316\u548c\u66f4\u5feb\u63a8\u7406\uff0c\u4ec5\u4f7f\u7528\u5f00\u653e\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u5728\u4fe1\u53f7\u8d28\u91cf\u6307\u6807\u548c\u5e72\u6270\u53bb\u9664\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u751f\u6210\u5f0f\u5206\u79bb\u7cfb\u7edf\uff0c\u5e76\u4e0e\u6bd4\u8f83\u7684\u975e\u751f\u6210\u5f0f\u7cfb\u7edf\u6027\u80fd\u76f8\u5f53\u3002\u63d0\u4f9b\u4e86\u6f5c\u5728\u7f16\u7801\u5668\u7684\u566a\u58f0\u9c81\u68d2\u6027\u7814\u7a76\uff0c\u53d1\u5e03\u4e86\u6a21\u5757\u5316\u5de5\u5177\u5305\u4f9b\u8fdb\u4e00\u6b65\u7814\u7a76\u3002", "conclusion": "\u6269\u6563\u6a21\u578b\u5728\u97f3\u4e50\u6e90\u5206\u79bb\u9886\u57df\u5177\u6709\u6f5c\u529b\uff0c\u6f5c\u5728\u6269\u6563\u65b9\u6cd5\u5728\u4fdd\u6301\u9ad8\u8d28\u91cf\u5206\u79bb\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u63a8\u7406\uff0c\u4e3a\u521b\u9020\u6027\u5de5\u4f5c\u6d41\u7a0b\u63d0\u4f9b\u4e86\u652f\u6301\u3002"}}
{"id": "2511.20113", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.20113", "abs": "https://arxiv.org/abs/2511.20113", "authors": ["Xiaojing Yan", "Carlo Fischione"], "title": "Joint Bit-Partitioning and Modulation Design for Digital AirComp", "comment": null, "summary": "For digital over-the-air computation, the ChannelComp framework has recently been proposed to design digital modulations to compute any arbitrary function over a multiple access channel. To reduce modulation design complexity while increasing computation reliability, this paper integrates a bit-partitioning procedure into ChannelComp. The key process is to partition the input bit sequence into several groups, map each group to a single modulation symbol and transmit the encoded symbol sequence across multiple time slots. With the objective to maximize a worst-case constellation distance, we develop two bit-partitioning methods. In uniform bit-partitioning, bits are evenly distributed across groups and modulation is designed via a max-min optimization, which is handled by a CCCP that solves a sequence of second-order cone programming subproblems. In importance-adaptive bit-partitioning (IABP), the bit allocation is adapted to the significance of individual bit positions, and the modulation and partitioning are jointly optimized. To keep the overall complexity manageable, simulated annealing is employed in the outer loop to update the partitioning, while a CCCP-based solver is used in the inner loop for modulation design. Numerical results show that both methods provide robust computation in noisy channels, and IABP achieves up to a 5 dB reduction in computation error compared to Sequential Modulation for AirComp, especially for product computation.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5c06\u6bd4\u7279\u5206\u5272\u8fc7\u7a0b\u96c6\u6210\u5230ChannelComp\u6846\u67b6\u4e2d\uff0c\u901a\u8fc7\u5c06\u8f93\u5165\u6bd4\u7279\u5e8f\u5217\u5206\u5272\u6210\u591a\u4e2a\u7ec4\uff0c\u6bcf\u7ec4\u6620\u5c04\u4e3a\u5355\u4e2a\u8c03\u5236\u7b26\u53f7\uff0c\u5728\u591a\u65f6\u9699\u4e2d\u4f20\u8f93\u7f16\u7801\u7b26\u53f7\u5e8f\u5217\uff0c\u4ee5\u964d\u4f4e\u8c03\u5236\u8bbe\u8ba1\u590d\u6742\u5ea6\u5e76\u63d0\u9ad8\u8ba1\u7b97\u53ef\u9760\u6027\u3002", "motivation": "\u4e3a\u4e86\u964d\u4f4e\u6570\u5b57\u7a7a\u4e2d\u8ba1\u7b97\u7684\u8c03\u5236\u8bbe\u8ba1\u590d\u6742\u5ea6\uff0c\u540c\u65f6\u63d0\u9ad8\u8ba1\u7b97\u53ef\u9760\u6027\uff0c\u9700\u8981\u6539\u8fdb\u73b0\u6709\u7684ChannelComp\u6846\u67b6\u3002", "method": "\u5f00\u53d1\u4e86\u4e24\u79cd\u6bd4\u7279\u5206\u5272\u65b9\u6cd5\uff1a\u5747\u5300\u6bd4\u7279\u5206\u5272\uff08\u901a\u8fc7max-min\u4f18\u5316\u8bbe\u8ba1\u8c03\u5236\uff0c\u4f7f\u7528CCCP\u5904\u7406\u4e8c\u9636\u9525\u89c4\u5212\u5b50\u95ee\u9898\uff09\u548c\u91cd\u8981\u6027\u81ea\u9002\u5e94\u6bd4\u7279\u5206\u5272\uff08IABP\uff0c\u6839\u636e\u6bd4\u7279\u4f4d\u7f6e\u7684\u91cd\u8981\u6027\u8c03\u6574\u6bd4\u7279\u5206\u914d\uff0c\u8054\u5408\u4f18\u5316\u8c03\u5236\u548c\u5206\u5272\uff09\u3002", "result": "\u6570\u503c\u7ed3\u679c\u8868\u660e\uff0c\u4e24\u79cd\u65b9\u6cd5\u5728\u566a\u58f0\u4fe1\u9053\u4e2d\u90fd\u80fd\u63d0\u4f9b\u7a33\u5065\u7684\u8ba1\u7b97\uff0cIABP\u76f8\u6bd4Sequential Modulation for AirComp\u5728\u4e58\u79ef\u8ba1\u7b97\u4e2d\u53ef\u5c06\u8ba1\u7b97\u8bef\u5dee\u964d\u4f4e\u8fbe5 dB\u3002", "conclusion": "\u96c6\u6210\u6bd4\u7279\u5206\u5272\u7684ChannelComp\u6846\u67b6\u80fd\u6709\u6548\u964d\u4f4e\u8bbe\u8ba1\u590d\u6742\u5ea6\u5e76\u63d0\u9ad8\u8ba1\u7b97\u53ef\u9760\u6027\uff0c\u7279\u522b\u662fIABP\u65b9\u6cd5\u5728\u4e58\u79ef\u8ba1\u7b97\u4e2d\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2511.20203", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.20203", "abs": "https://arxiv.org/abs/2511.20203", "authors": ["Junjie Ye", "Zhaolin Wang", "Yuanwei Liu", "Peichang Zhang", "Lei Huang", "Arumugam Nallanathan"], "title": "Optimal Waveform Design for Continuous Aperture Array (CAPA)-aided ISAC Systems", "comment": "Submitted to IEEE journal for future publication", "summary": "A novel continuous-aperture-array (CAPA)-aided integrated sensing and communication (ISAC) framework is proposed. Specifically, an optimal continuous ISAC waveform is designed to form a directive beampattern for multi-target sensing while suppressing the multi-user interference (MUI). To achieve the goal of optimal waveform design, the directional beampattern of CAPA is first derived based on Green's function, whereafter a reference sensing waveform is obtained through wavenumber-domain optimization. Based on the reference sensing waveform, a weighted functional programming on the tradeoff between sensing beampattern mismatch and MUI is formulated. To solve the resulting problem, an optimal CAPA-ISAC waveform structure is analytically derived using a Lagrangian-transformation and calculus-of-variations method, where the Lagrangian multiplier associated with the optimal waveform structure is determined via Bisection search. The obtained optimal waveform reveals that it is concurrently affected by the reference sensing waveform, the channel correlations and the channel-symbol correlations. Finally, numerical results validate the effectiveness of the proposed system and waveform design, demonstrating that CAPA can achieve significant performance gains against the ISAC designs based on conventional spatially discrete array in both sensing accuracy and communication reliability.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8fde\u7eed\u5b54\u5f84\u9635\u5217\u7684\u96c6\u6210\u611f\u77e5\u4e0e\u901a\u4fe1\u6846\u67b6\uff0c\u901a\u8fc7\u4f18\u5316\u6ce2\u5f62\u8bbe\u8ba1\u5b9e\u73b0\u591a\u76ee\u6807\u611f\u77e5\u548c\u6291\u5236\u591a\u7528\u6237\u5e72\u6270", "motivation": "\u4f20\u7edf\u79bb\u6563\u9635\u5217\u5728\u96c6\u6210\u611f\u77e5\u4e0e\u901a\u4fe1\u4e2d\u5b58\u5728\u6027\u80fd\u9650\u5236\uff0c\u9700\u8981\u63a2\u7d22\u8fde\u7eed\u5b54\u5f84\u9635\u5217\u7684\u6f5c\u529b\u6765\u63d0\u5347\u611f\u77e5\u7cbe\u5ea6\u548c\u901a\u4fe1\u53ef\u9760\u6027", "method": "\u57fa\u4e8e\u683c\u6797\u51fd\u6570\u63a8\u5bfcCAPA\u65b9\u5411\u6ce2\u675f\u6a21\u5f0f\uff0c\u901a\u8fc7\u6ce2\u6570\u57df\u4f18\u5316\u83b7\u5f97\u53c2\u8003\u611f\u77e5\u6ce2\u5f62\uff0c\u4f7f\u7528\u62c9\u683c\u6717\u65e5\u53d8\u6362\u548c\u53d8\u5206\u6cd5\u63a8\u5bfc\u6700\u4f18\u6ce2\u5f62\u7ed3\u6784", "result": "\u6570\u503c\u7ed3\u679c\u8868\u660eCAPA\u76f8\u6bd4\u4f20\u7edf\u79bb\u6563\u9635\u5217\u5728\u611f\u77e5\u7cbe\u5ea6\u548c\u901a\u4fe1\u53ef\u9760\u6027\u65b9\u9762\u83b7\u5f97\u663e\u8457\u6027\u80fd\u589e\u76ca", "conclusion": "\u8fde\u7eed\u5b54\u5f84\u9635\u5217\u4e3a\u96c6\u6210\u611f\u77e5\u4e0e\u901a\u4fe1\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u6700\u4f18\u6ce2\u5f62\u7ed3\u6784\u53d7\u53c2\u8003\u611f\u77e5\u6ce2\u5f62\u3001\u4fe1\u9053\u76f8\u5173\u6027\u548c\u4fe1\u9053\u7b26\u53f7\u76f8\u5173\u6027\u5171\u540c\u5f71\u54cd"}}
{"id": "2511.20265", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.20265", "abs": "https://arxiv.org/abs/2511.20265", "authors": ["Can Zheng", "Jiguang He", "Chung G. Kang", "Guofa Cai", "Chongwen Huang", "Henk Wymeersch"], "title": "Rectified Flow for Vision-Aided mmWave V2I Beam Prediction", "comment": "6 pages, 5 figures, submitted to conference", "summary": "This paper proposes a flow matching (FM) framework based on rectified flow for vision-aided beam prediction in vehicle-to-infrastructure (V2I) links. Instead of modeling discrete beam index sequences, the method learns a continuous latent flow governed by an ordinary differential equation (ODE)-based vector field, enabling smooth beam trajectories and fast sampling. A terminal flow constraint enforces global consistency under finite-step integration, stabilizing long-term prediction. The resulting FM-based model significantly improves top-K accuracy over RNN and LSTM baselines, approaches the performance of large language model-based approaches, and achieves inference speedups on the order of 10 x and 10^4 x on identical GPU and CPU deployments, respectively.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6574\u6d41\u6d41\u7684\u6d41\u5339\u914d\u6846\u67b6\u7528\u4e8eV2I\u94fe\u8def\u7684\u89c6\u89c9\u8f85\u52a9\u6ce2\u675f\u9884\u6d4b\uff0c\u901a\u8fc7\u8fde\u7eed\u6f5c\u5728\u6d41\u5b66\u4e60\u5b9e\u73b0\u5e73\u6ed1\u6ce2\u675f\u8f68\u8ff9\u548c\u5feb\u901f\u91c7\u6837\uff0c\u663e\u8457\u63d0\u5347\u9884\u6d4b\u7cbe\u5ea6\u548c\u63a8\u7406\u901f\u5ea6\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5efa\u6a21\u79bb\u6563\u6ce2\u675f\u7d22\u5f15\u5e8f\u5217\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u6ce2\u675f\u9884\u6d4b\u65b9\u6cd5\u6765\u652f\u6301\u8f66\u8f86\u5230\u57fa\u7840\u8bbe\u65bd\u901a\u4fe1\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u5e38\u5fae\u5206\u65b9\u7a0b\u7684\u6d41\u5339\u914d\u6846\u67b6\u5b66\u4e60\u8fde\u7eed\u6f5c\u5728\u6d41\uff0c\u5f15\u5165\u7ec8\u7aef\u6d41\u7ea6\u675f\u786e\u4fdd\u6709\u9650\u6b65\u79ef\u5206\u4e0b\u7684\u5168\u5c40\u4e00\u81f4\u6027\uff0c\u5b9e\u73b0\u7a33\u5b9a\u957f\u671f\u9884\u6d4b\u3002", "result": "\u76f8\u6bd4RNN\u548cLSTM\u57fa\u7ebf\u663e\u8457\u63d0\u5347top-K\u51c6\u786e\u7387\uff0c\u63a5\u8fd1\u5927\u8bed\u8a00\u6a21\u578b\u65b9\u6cd5\u6027\u80fd\uff0c\u5728GPU\u548cCPU\u4e0a\u5206\u522b\u5b9e\u73b010\u500d\u548c10^4\u500d\u7684\u63a8\u7406\u52a0\u901f\u3002", "conclusion": "\u6d41\u5339\u914d\u6846\u67b6\u4e3a\u89c6\u89c9\u8f85\u52a9\u6ce2\u675f\u9884\u6d4b\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u7cbe\u5ea6\u548c\u901f\u5ea6\u65b9\u9762\u5747\u8868\u73b0\u4f18\u5f02\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2511.20298", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.20298", "abs": "https://arxiv.org/abs/2511.20298", "authors": ["Godfred Kumi Tenkorang", "Michel Daoud Yacoub"], "title": "Log-Mu Fading Process: Second-Order Statistics for Diversity-Combining Techniques", "comment": null, "summary": "This paper derives second-order statistics for diversity-combining techniques over Log-mu fading channels. Closed-form expressions for the level crossing rate (LCR) and average fading duration (AFD) are derived for pure selection combining (PSC), while exact multidimensional integral expressions are obtained for equal gain combining (EGC) and maximal ratio combining (MRC). The analysis considers M unbalanced, independent, and non-identically distributed (i.n.i.d.) Log-mu fading channels. Monte Carlo simulations are conducted to validate the theoretical results, demonstrating excellent agreement and confirming the accuracy of the proposed expressions.", "AI": {"tldr": "\u672c\u6587\u63a8\u5bfc\u4e86Log-mu\u8870\u843d\u4fe1\u9053\u4e0a\u5206\u96c6\u5408\u5e76\u6280\u672f\u7684\u4e8c\u9636\u7edf\u8ba1\u7279\u6027\uff0c\u5305\u62ec\u7eaf\u9009\u62e9\u5408\u5e76(PSC)\u7684\u95ed\u5f0f\u8868\u8fbe\u5f0f\uff0c\u4ee5\u53ca\u7b49\u589e\u76ca\u5408\u5e76(EGC)\u548c\u6700\u5927\u6bd4\u5408\u5e76(MRC)\u7684\u591a\u7ef4\u79ef\u5206\u8868\u8fbe\u5f0f\u3002", "motivation": "\u7814\u7a76Log-mu\u8870\u843d\u4fe1\u9053\u4e0b\u4e0d\u540c\u5206\u96c6\u5408\u5e76\u6280\u672f\u7684\u6027\u80fd\uff0c\u4e3a\u65e0\u7ebf\u901a\u4fe1\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u7406\u8bba\u4f9d\u636e\u3002", "method": "\u63a8\u5bfc\u4e86PSC\u7684\u95ed\u5f0f\u8868\u8fbe\u5f0f\uff0c\u4ee5\u53caEGC\u548cMRC\u7684\u591a\u7ef4\u79ef\u5206\u8868\u8fbe\u5f0f\uff0c\u8003\u8651\u4e86M\u4e2a\u4e0d\u5e73\u8861\u3001\u72ec\u7acb\u4e14\u975e\u540c\u5206\u5e03\u7684\u4fe1\u9053\uff0c\u5e76\u901a\u8fc7\u8499\u7279\u5361\u6d1b\u4eff\u771f\u9a8c\u8bc1\u7406\u8bba\u7ed3\u679c\u3002", "result": "\u7406\u8bba\u63a8\u5bfc\u4e0e\u8499\u7279\u5361\u6d1b\u4eff\u771f\u7ed3\u679c\u9ad8\u5ea6\u4e00\u81f4\uff0c\u9a8c\u8bc1\u4e86\u6240\u63d0\u8868\u8fbe\u5f0f\u7684\u51c6\u786e\u6027\u3002", "conclusion": "\u6210\u529f\u5efa\u7acb\u4e86Log-mu\u8870\u843d\u4fe1\u9053\u4e0b\u5206\u96c6\u5408\u5e76\u6280\u672f\u7684\u4e8c\u9636\u7edf\u8ba1\u6a21\u578b\uff0c\u4e3a\u7cfb\u7edf\u6027\u80fd\u5206\u6790\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2511.20309", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.20309", "abs": "https://arxiv.org/abs/2511.20309", "authors": ["Kawon Han", "Christos Masouros", "Taneli Riihonen", "Moeness G. Amin"], "title": "Next-Generation MIMO Transceivers for Integrated Sensing and Communications: Unique Security Vulnerabilities and Solutions", "comment": "29 pages, 24 figures", "summary": "Integrated sensing and communications (ISAC), which is recognized as a key enabler for sixth generation (6G), has brought new opportunities for intelligent, sustainable, and connected wireless networks. Multiple-input multiple-output (MIMO) transceiver technology lies at the core of this paradigm, providing the degrees of freedom required for simultaneous data transmission and accurate radar sensing. The tight integration of sensing and communication introduces unique security vulnerabilities that extend beyond conventional physical-layer security (PLS). In particular, high-power transmissions directed at sensing targets may empower adversarial eavesdroppers, whereas passive interception of ISAC echoes can reveal sensitive information such as target locations and mobility patterns. This article presents an overview of recent advances in MIMO ISAC transceiver design, considering transmitter perspectives, receiver architectures, and full-duplex implementations. We examine MIMO transceiver designs under unique security threats specific to ISAC and highlight emerging countermeasures, including secure signaling design, interference exploitation, and transceiver optimization under adversarial conditions. Finally, we discuss challenges and research opportunities for developing secure ISAC systems in next-generation wireless networks.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86MIMO ISAC\u6536\u53d1\u5668\u8bbe\u8ba1\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u91cd\u70b9\u5173\u6ce8\u611f\u77e5\u4e0e\u901a\u4fe1\u878d\u5408\u5e26\u6765\u7684\u72ec\u7279\u5b89\u5168\u5a01\u80c1\u53ca\u76f8\u5e94\u9632\u62a4\u63aa\u65bd\u3002", "motivation": "ISAC\u4f5c\u4e3a6G\u5173\u952e\u6280\u672f\uff0c\u5728\u5b9e\u73b0\u667a\u80fd\u53ef\u6301\u7eed\u65e0\u7ebf\u7f51\u7edc\u7684\u540c\u65f6\uff0c\u5f15\u5165\u4e86\u8d85\u8d8a\u4f20\u7edf\u7269\u7406\u5c42\u5b89\u5168\u7684\u65b0\u5b89\u5168\u6f0f\u6d1e\uff0c\u5982\u9ad8\u529f\u7387\u611f\u77e5\u4f20\u8f93\u53ef\u80fd\u589e\u5f3a\u7a83\u542c\u8005\u80fd\u529b\uff0c\u88ab\u52a8\u62e6\u622aISAC\u56de\u6ce2\u53ef\u80fd\u6cc4\u9732\u654f\u611f\u4fe1\u606f\u3002", "method": "\u4ece\u53d1\u5c04\u673a\u3001\u63a5\u6536\u673a\u548c\u5168\u53cc\u5de5\u5b9e\u73b0\u4e09\u4e2a\u89d2\u5ea6\u5206\u6790MIMO ISAC\u6536\u53d1\u5668\u8bbe\u8ba1\uff0c\u7814\u7a76\u5728ISAC\u7279\u6709\u5b89\u5168\u5a01\u80c1\u4e0b\u7684\u6536\u53d1\u5668\u8bbe\u8ba1\u7b56\u7565\uff0c\u5305\u62ec\u5b89\u5168\u4fe1\u4ee4\u8bbe\u8ba1\u3001\u5e72\u6270\u5229\u7528\u548c\u5bf9\u6297\u6761\u4ef6\u4e0b\u7684\u6536\u53d1\u5668\u4f18\u5316\u3002", "result": "\u63d0\u51fa\u4e86\u9488\u5bf9ISAC\u7cfb\u7edf\u7279\u6709\u5b89\u5168\u5a01\u80c1\u7684\u9632\u62a4\u5bf9\u7b56\uff0c\u5f3a\u8c03\u4e86\u5b89\u5168\u4fe1\u4ee4\u8bbe\u8ba1\u3001\u5e72\u6270\u5229\u7528\u7b49\u6280\u672f\u5728\u4fdd\u969cISAC\u7cfb\u7edf\u5b89\u5168\u65b9\u9762\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u8ba8\u8bba\u4e86\u5728\u4e0b\u4e00\u4ee3\u65e0\u7ebf\u7f51\u7edc\u4e2d\u5f00\u53d1\u5b89\u5168ISAC\u7cfb\u7edf\u9762\u4e34\u7684\u6311\u6218\u548c\u7814\u7a76\u673a\u9047\uff0c\u4e3a\u672a\u67656G\u7f51\u7edc\u7684\u5b89\u5168\u53d1\u5c55\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2511.20334", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.20334", "abs": "https://arxiv.org/abs/2511.20334", "authors": ["Salah Abdeljabar", "Mohamed-Slim Alouini"], "title": "Bridging the Educational Divide: A Delay-Tolerant Networking Approach for Equitable Digital Learning in Rural Areas", "comment": null, "summary": "Access to quality education remains unequal, particularly in rural areas where Internet connectivity is limited or nonexistent. This paper introduces a framework for a digital learning platform that uses Delay Tolerant Networking (DTN) to extend educational opportunities to underserved communities. Unlike conventional models that rely on continuous Internet access, DTN offers an affordable and sustainable solution by leveraging existing transportation infrastructure. Beyond its technical contributions, the framework addresses ethical imperatives by promoting educational equity and digital inclusion. We present a prototype tested on a university campus, demonstrating the feasibility of DTN for educational delivery. By addressing the digital divide, this framework aligns with global goals of inclusive education and sustainable development.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5ef6\u8fdf\u5bb9\u5fcd\u7f51\u7edc(DTN)\u7684\u6570\u5b57\u5b66\u4e60\u5e73\u53f0\u6846\u67b6\uff0c\u4e3a\u7f51\u7edc\u8fde\u63a5\u6709\u9650\u7684\u519c\u6751\u5730\u533a\u63d0\u4f9b\u6559\u80b2\u670d\u52a1\uff0c\u901a\u8fc7\u5229\u7528\u73b0\u6709\u4ea4\u901a\u57fa\u7840\u8bbe\u65bd\u5b9e\u73b0\u53ef\u6301\u7eed\u7684\u6559\u80b2\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u89e3\u51b3\u519c\u6751\u5730\u533a\u56e0\u7f51\u7edc\u8fde\u63a5\u6709\u9650\u800c\u5bfc\u81f4\u7684\u6559\u80b2\u4e0d\u5e73\u7b49\u95ee\u9898\uff0c\u4fc3\u8fdb\u6559\u80b2\u516c\u5e73\u548c\u6570\u5b57\u5305\u5bb9\uff0c\u7b26\u5408\u5168\u7403\u5305\u5bb9\u6027\u6559\u80b2\u548c\u53ef\u6301\u7eed\u53d1\u5c55\u7684\u76ee\u6807\u3002", "method": "\u4f7f\u7528\u5ef6\u8fdf\u5bb9\u5fcd\u7f51\u7edc(DTN)\u6280\u672f\uff0c\u5229\u7528\u73b0\u6709\u4ea4\u901a\u57fa\u7840\u8bbe\u65bd\u6784\u5efa\u6570\u5b57\u5b66\u4e60\u5e73\u53f0\uff0c\u4e0d\u4f9d\u8d56\u6301\u7eed\u4e92\u8054\u7f51\u8fde\u63a5\uff0c\u5e76\u5728\u5927\u5b66\u6821\u56ed\u6d4b\u8bd5\u539f\u578b\u9a8c\u8bc1\u53ef\u884c\u6027\u3002", "result": "\u539f\u578b\u6d4b\u8bd5\u8bc1\u660e\u4e86DTN\u5728\u6559\u80b2\u4ea4\u4ed8\u65b9\u9762\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u7f51\u7edc\u8fde\u63a5\u6709\u9650\u7684\u5730\u533a\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u6559\u80b2\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "DTN\u6846\u67b6\u4e3a\u7f29\u5c0f\u6570\u5b57\u9e3f\u6c9f\u63d0\u4f9b\u4e86\u7ecf\u6d4e\u53ef\u6301\u7eed\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u6709\u6548\u6269\u5c55\u6559\u80b2\u673a\u4f1a\u5230\u670d\u52a1\u4e0d\u8db3\u7684\u793e\u533a\uff0c\u4fc3\u8fdb\u6559\u80b2\u516c\u5e73\u3002"}}
{"id": "2511.20453", "categories": ["eess.SP", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.20453", "abs": "https://arxiv.org/abs/2511.20453", "authors": ["Ziqin Zhou", "Hui Chen", "Gerhard Steinb\u00f6ck", "Henk Wymeersch"], "title": "Digital Twin-Assisted High-Precision Massive MIMO Localization in Urban Canyons", "comment": "6 pages, 5 figures. accepted to 2026 IEEE JC&S", "summary": "High-precision wireless localization in urban canyons is challenged by noisy measurements and severe non-line-of-sight (NLOS) propagation. This paper proposes a robust three-stage algorithm synergizing a digital twin (DT) model with the random sample consensus (RANSAC) algorithm to overcome these limitations. The method leverages the DT for geometric path association and employs RANSAC to identify reliable line-of-sight (LOS) and single-bounce NLOS paths while rejecting multi-bounce outliers. A final optimization on the resulting inlier set estimates the user's position and clock bias. Simulations validate that by effectively turning NLOS paths into valuable geometric information via the DT, the approach enables accurate localization, reduces reliance on direct LOS, and significantly lowers system deployment costs, making it suitable for practical deployment.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6570\u5b57\u5b6a\u751f\u6a21\u578b\u548cRANSAC\u7b97\u6cd5\u7684\u4e09\u9636\u6bb5\u65e0\u7ebf\u5b9a\u4f4d\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u57ce\u5e02\u5ce1\u8c37\u73af\u5883\u4e2d\u6709\u6548\u5904\u7406NLOS\u4f20\u64ad\u95ee\u9898\uff0c\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u5b9a\u4f4d\u3002", "motivation": "\u57ce\u5e02\u5ce1\u8c37\u73af\u5883\u4e2d\u7684\u65e0\u7ebf\u5b9a\u4f4d\u9762\u4e34\u566a\u58f0\u6d4b\u91cf\u548c\u4e25\u91cd\u975e\u89c6\u8ddd\u4f20\u64ad\u7684\u6311\u6218\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u514b\u670d\u8fd9\u4e9b\u9650\u5236\u7684\u9c81\u68d2\u5b9a\u4f4d\u7b97\u6cd5\u3002", "method": "\u4e09\u9636\u6bb5\u7b97\u6cd5\uff1a1) \u5229\u7528\u6570\u5b57\u5b6a\u751f\u6a21\u578b\u8fdb\u884c\u51e0\u4f55\u8def\u5f84\u5173\u8054\uff1b2) \u4f7f\u7528RANSAC\u7b97\u6cd5\u8bc6\u522b\u53ef\u9760\u7684LOS\u548c\u5355\u8df3NLOS\u8def\u5f84\uff0c\u6392\u9664\u591a\u8df3\u5f02\u5e38\u503c\uff1b3) \u5728\u5f97\u5230\u7684\u5185\u90e8\u70b9\u96c6\u4e0a\u8fdb\u884c\u6700\u7ec8\u4f18\u5316\uff0c\u4f30\u8ba1\u7528\u6237\u4f4d\u7f6e\u548c\u65f6\u949f\u504f\u5dee\u3002", "result": "\u4eff\u771f\u9a8c\u8bc1\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u6570\u5b57\u5b6a\u751f\u5c06NLOS\u8def\u5f84\u8f6c\u5316\u4e3a\u6709\u4ef7\u503c\u7684\u51e0\u4f55\u4fe1\u606f\uff0c\u80fd\u591f\u5b9e\u73b0\u7cbe\u786e\u5b9a\u4f4d\uff0c\u51cf\u5c11\u5bf9\u76f4\u63a5LOS\u7684\u4f9d\u8d56\uff0c\u5e76\u663e\u8457\u964d\u4f4e\u7cfb\u7edf\u90e8\u7f72\u6210\u672c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9002\u7528\u4e8e\u5b9e\u9645\u90e8\u7f72\uff0c\u4e3a\u57ce\u5e02\u5ce1\u8c37\u73af\u5883\u4e0b\u7684\u9ad8\u7cbe\u5ea6\u65e0\u7ebf\u5b9a\u4f4d\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.20551", "categories": ["eess.SP", "cs.AI", "eess.IV"], "pdf": "https://arxiv.org/pdf/2511.20551", "abs": "https://arxiv.org/abs/2511.20551", "authors": ["Tatiana Gelvez-Barrera", "Barbara Nicolas", "Denis Kouam\u00e9", "Bruno Gilles", "Adrian Basarab"], "title": "Time-Domain Linear Model-based Framework for Passive Acoustic Mapping of Cavitation Activity", "comment": null, "summary": "Passive acoustic mapping enables the spatial mapping and temporal monitoring of cavitation activity, playing a crucial role in therapeutic ultrasound applications. Most conventional beamforming methods, whether implemented in the time or frequency domains, suffer from limited axial resolution due to the absence of a reference emission onset time. While frequency-domain methods, the most efficient of which are based on the cross-spectral matrix, require long signals for accurate estimation, time-domain methods typically achieve lower spatial resolution. To address these limitations, we propose a linear model-based beamforming framework fully formulated in the time domain. The linear forward model relates a discretized spatiotemporal distribution of cavitation activity to the temporal signals recorded by a probe, explicitly accounting for time-of-flight delays dictated by the acquisition geometry. This model is then inverted using regularization techniques that exploit prior knowledge of cavitation activity in both spatial and temporal domains. Experimental results show that the proposed framework achieves enhanced or competitive cavitation map quality while using only 20\\% of the data typically required by frequency-domain methods. This highlights the substantial gain in data efficiency and the flexibility of our spatiotemporal regularization to adapt to diverse passive cavitation scenarios, outperforming state-of-the-art techniques.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7ebf\u6027\u6a21\u578b\u7684\u65f6\u57df\u6ce2\u675f\u5f62\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u65f6\u7a7a\u6b63\u5219\u5316\u6280\u672f\u63d0\u9ad8\u88ab\u52a8\u58f0\u5b66\u6620\u5c04\u7684\u6570\u636e\u6548\u7387\u548c\u5206\u8fa8\u7387\u3002", "motivation": "\u4f20\u7edf\u6ce2\u675f\u5f62\u6210\u65b9\u6cd5\u5b58\u5728\u8f74\u5411\u5206\u8fa8\u7387\u9650\u5236\uff0c\u9891\u57df\u65b9\u6cd5\u9700\u8981\u957f\u4fe1\u53f7\uff0c\u65f6\u57df\u65b9\u6cd5\u7a7a\u95f4\u5206\u8fa8\u7387\u8f83\u4f4e\u3002", "method": "\u6784\u5efa\u7ebf\u6027\u524d\u5411\u6a21\u578b\uff0c\u5c06\u7a7a\u5316\u6d3b\u52a8\u7684\u79bb\u6563\u65f6\u7a7a\u5206\u5e03\u4e0e\u63a2\u5934\u8bb0\u5f55\u7684\u65f6\u95f4\u4fe1\u53f7\u5173\u8054\uff0c\u4f7f\u7528\u65f6\u57df\u6b63\u5219\u5316\u6280\u672f\u53cd\u6f14\u6a21\u578b\u3002", "result": "\u4ec5\u9700\u9891\u57df\u65b9\u6cd520%\u7684\u6570\u636e\u91cf\u5c31\u80fd\u83b7\u5f97\u76f8\u5f53\u6216\u66f4\u597d\u7684\u7a7a\u5316\u56fe\u8d28\u91cf\uff0c\u6570\u636e\u6548\u7387\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u6570\u636e\u6548\u7387\u548c\u9002\u5e94\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u9002\u7528\u4e8e\u5404\u79cd\u88ab\u52a8\u7a7a\u5316\u573a\u666f\u3002"}}
{"id": "2511.20572", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.20572", "abs": "https://arxiv.org/abs/2511.20572", "authors": ["Mohamadreza Delbari", "George C. Alexandropoulos", "Robert Schober", "H. Vincent Poor", "Vahid Jamali"], "title": "Near-Field Multipath MIMO Channels: Modeling Reflectors and Exploiting NLOS Paths", "comment": null, "summary": "Near-field (NF) communications is receiving renewed interest in the context of multiple-input multiple-output (MIMO) systems involving large physical apertures with respect to the signal wavelength. While line-of-sight (LOS) links are typically expected to dominate in NF scenarios, the impact of non-LOS (NLOS) components at both in centimeter- and millimeter-wave frequencies may be in general non-negligible. Moreover, although weaker than the LOS path, NLOS links may be essential for achieving multiplexing gains in MIMO systems. The commonly used NF channel models for NLOS links in the literature are based on the point scattering assumption, which is not valid for large reflectors such as walls, ceilings, and the ground. In this paper, we develop a generalized statistical NF MIMO channel model that extends the widely adopted point scattering framework to account for imperfect reflections from large surfaces. This model is then leveraged to investigate how the physical characteristics of these reflectors influence the resulting NF MIMO channel. In addition, using the proposed channel model, we analytically demonstrate for a multi-user scenario that, even when users are located within the NF regime, relying solely on LOS NF links may be insufficient to achieve multiplexing gains, thus exploiting NLOS links becomes essential. Our simulation results validate the accuracy of the proposed model and show that, in many practical settings, the contribution of NLOS components is non-negligible and must be carefully accounted for in the system design.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5e7f\u4e49\u7684\u8fd1\u573aMIMO\u4fe1\u9053\u6a21\u578b\uff0c\u6269\u5c55\u4e86\u70b9\u6563\u5c04\u6846\u67b6\u4ee5\u8003\u8651\u5927\u8868\u9762\u7684\u975e\u5b8c\u7f8e\u53cd\u5c04\uff0c\u5e76\u8bc1\u660e\u5728\u8fd1\u573a\u591a\u7528\u6237\u573a\u666f\u4e2d\u4ec5\u4f9d\u8d56LOS\u94fe\u8def\u53ef\u80fd\u65e0\u6cd5\u5b9e\u73b0\u590d\u7528\u589e\u76ca\uff0c\u5fc5\u987b\u5229\u7528NLOS\u94fe\u8def\u3002", "motivation": "\u73b0\u6709\u8fd1\u573a\u4fe1\u9053\u6a21\u578b\u57fa\u4e8e\u70b9\u6563\u5c04\u5047\u8bbe\uff0c\u4e0d\u9002\u7528\u4e8e\u5899\u58c1\u3001\u5929\u82b1\u677f\u7b49\u5927\u53cd\u5c04\u4f53\u3002\u5728\u8fd1\u573aMIMO\u7cfb\u7edf\u4e2d\uff0c\u867d\u7136LOS\u94fe\u8def\u901a\u5e38\u5360\u4e3b\u5bfc\uff0c\u4f46NLOS\u94fe\u8def\u5bf9\u5b9e\u73b0\u590d\u7528\u589e\u76ca\u81f3\u5173\u91cd\u8981\uff0c\u4e14\u5176\u5f71\u54cd\u4e0d\u53ef\u5ffd\u7565\u3002", "method": "\u5f00\u53d1\u4e86\u5e7f\u4e49\u7edf\u8ba1\u8fd1\u573aMIMO\u4fe1\u9053\u6a21\u578b\uff0c\u6269\u5c55\u70b9\u6563\u5c04\u6846\u67b6\u4ee5\u5305\u542b\u5927\u8868\u9762\u7684\u975e\u5b8c\u7f8e\u53cd\u5c04\uff0c\u5206\u6790\u53cd\u5c04\u4f53\u7269\u7406\u7279\u6027\u5bf9\u4fe1\u9053\u7684\u5f71\u54cd\uff0c\u5e76\u5728\u591a\u7528\u6237\u573a\u666f\u4e2d\u7406\u8bba\u5206\u6790LOS\u548cNLOS\u94fe\u8def\u7684\u590d\u7528\u80fd\u529b\u3002", "result": "\u4eff\u771f\u9a8c\u8bc1\u4e86\u6240\u63d0\u6a21\u578b\u7684\u51c6\u786e\u6027\uff0c\u8868\u660e\u5728\u8bb8\u591a\u5b9e\u9645\u573a\u666f\u4e2dNLOS\u5206\u91cf\u7684\u8d21\u732e\u4e0d\u53ef\u5ffd\u7565\uff0c\u5fc5\u987b\u4ed4\u7ec6\u8003\u8651\u5728\u7cfb\u7edf\u8bbe\u8ba1\u4e2d\u3002\u7406\u8bba\u5206\u6790\u8bc1\u660e\u4ec5\u4f9d\u8d56LOS\u8fd1\u573a\u94fe\u8def\u53ef\u80fd\u4e0d\u8db3\u4ee5\u5b9e\u73b0\u590d\u7528\u589e\u76ca\u3002", "conclusion": "\u63d0\u51fa\u7684\u5e7f\u4e49\u8fd1\u573aMIMO\u4fe1\u9053\u6a21\u578b\u80fd\u66f4\u51c6\u786e\u5730\u63cf\u8ff0\u5927\u53cd\u5c04\u4f53\u7684\u5f71\u54cd\uff0c\u5728\u591a\u7528\u6237\u8fd1\u573a\u901a\u4fe1\u4e2d\uff0c\u5fc5\u987b\u540c\u65f6\u5229\u7528LOS\u548cNLOS\u94fe\u8def\u624d\u80fd\u5b9e\u73b0\u6709\u6548\u7684\u590d\u7528\u589e\u76ca\u3002"}}

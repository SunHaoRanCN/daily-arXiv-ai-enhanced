<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 13]
- [eess.AS](#eess.AS) [Total: 4]
- [cs.SD](#cs.SD) [Total: 10]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [WiRainbow: Single-Antenna Direction-Aware Wi-Fi Sensing via Dispersion Effect](https://arxiv.org/abs/2511.20671)
*Zhaoxin Chang,Shuguang Xiao,Fusang Zhang,Xujun Ma,Badii Jouaber,Qingfeng Zhang,Daqing Zhang*

Main category: eess.SP

TL;DR: WiRainbow是一种利用频率扫描天线实现单天线Wi-Fi方向感知的新方法，通过扩展天线视场和开发信噪比信号处理框架，在复杂环境中实现准确、鲁棒且经济的方向估计。


<details>
  <summary>Details</summary>
Motivation: 现有Wi-Fi方向估计方法依赖昂贵复杂的天线阵列，难以在实际场景中部署。需要开发成本更低、部署更简单的单天线方向感知方案。

Method: 提出基于耦合谐振器的天线架构扩展传统频率扫描天线的窄视场，开发基于感知信噪比的信号处理框架在复杂多径环境中可靠估计目标方向。

Result: 原型系统评估表明WiRainbow能够为各种Wi-Fi传感应用提供准确、鲁棒且经济的方向感知能力。

Conclusion: WiRainbow通过创新的天线设计和信号处理，实现了单天线Wi-Fi方向感知，解决了现有方法成本高、部署复杂的问题。

Abstract: Recently, Wi-Fi signals have emerged as a powerful tool for contactless sensing. During the sensing process, obtaining target direction information can provide valuable contextual insights for various applications. Existing direction estimation methods typically rely on antenna arrays, which are costly and complex to deploy in real-world scenarios. In this paper, we present WiRainbow, a novel approach that enables single-antenna-based direction awareness for Wi-Fi sensing by leveraging the dispersion effect of frequency-scanning antennas (FSAs), which can naturally steer Wi-Fi subcarriers toward distinct angles during signal transmission. To address key challenges in antenna design and signal processing, we propose a coupled-resonator-based antenna architecture that significantly expands the narrow Field-of-View inherent in conventional FSAs, improving sensing coverage. Additionally, we develop a sensing signal-to-noise-ratio-based signal processing framework that reliably estimates target direction in multipath-rich environments. We prototype WiRainbow and evaluate its performance through benchmark experiments and real-world case studies, demonstrating its ability to achieve accurate, robust, and cost-effective direction awareness for diverse Wi-Fi sensing applications.

</details>


### [2] [A Fully Multivariate Multifractal Detrended Fluctuation Analysis Method for Fault Diagnosis](https://arxiv.org/abs/2511.20831)
*Khuram Naveed,Naveed ur Rehman*

Main category: eess.SP

TL;DR: 提出完全多变量MFDFA方法，结合MVMD分解，用于多通道机器振动数据的故障诊断，能有效区分健康与故障状态。


<details>
  <summary>Details</summary>
Motivation: 传统MFDFA方法无法充分捕捉多通道振动数据中的跨通道依赖关系和方差偏差，需要开发能更好表征多变量信号多尺度结构的方法。

Method: 基于马氏距离引入协方差加权的Lpq矩阵范数定义多变量波动函数，结合多变量变分模态分解(MVMD)分离故障相关分量。

Result: 在风力涡轮机齿轮箱数据上的测试表明，该方法优于传统MFDFA方法，即使在噪声条件下也能有效区分健康与故障状态。

Conclusion: 提出的FM-MFDFA框架为多通道机器振动数据提供了更准确的故障诊断能力，能够有效捕捉跨通道依赖关系。

Abstract: We propose a fully multivariate generalization of multifractal detrended fluctuation analysis (MFDFA) and leverage it to develop a fault diagnosis framework for multichannel machine vibration data. We introduce a novel covariance-weighted $L_{pq}$ matrix norm based on Mahalanobis distance to define a fully multivariate fluctuation function that uniquely captures cross-channel dependencies and variance biases in multichannel vibration data. This formulation, termed FM-MFDFA, allows for a more accurate characterization of the multiscale structure of multivariate signals. To enhance feature relevance, the proposed framework integrates multivariate variational mode decomposition (MVMD) to isolate fault-relevant components before applying FM-MFDFA. Results on wind turbine gearbox data demonstrate that the proposed method outperforms conventional MFDFA approaches by effectively distinguishing between healthy and faulty machine states, even under noisy conditions.

</details>


### [3] [Wavelet-Guided Water-Level Estimation for ISAC](https://arxiv.org/abs/2511.20936)
*Ayoob Salari,Kai Wu,Khawaja Fahad Masood,Y. Jay Guo,J. Andrew Zhang*

Main category: eess.SP

TL;DR: 提出了一种基于LTE下行功率指标的被动、低成本水位监测方法，通过连续小波变换提取潮汐特征，结合轻量级神经网络实现高精度水位跟踪。


<details>
  <summary>Details</summary>
Motivation: 传统水位监测方法依赖固定仪器，成本高、维护困难且在极端事件中易受损，需要一种低成本、易部署的替代方案。

Method: 使用商用接收器报告的LTE下行功率指标（RSRP、RSSI、RSRQ），应用连续小波变换提取半日潮分量，形成特征签名，并通过轻量级神经网络学习水位变化。支持多基站协作模式提升稳定性。

Result: 在420米河流路径实验中，视距条件下RMSE为0.8cm，MAE为0.5cm；非视距条件下经过微调后RMSE为1.7cm，MAE为0.8cm。

Conclusion: 该方法无需阵列校准，可在标准硬件上运行，相比基于CSI的方法更实用，多基站融合可进一步提高鲁棒性，适合大规模部署。

Abstract: Real-time water-level monitoring across many locations is vital for flood response, infrastructure management, and environmental forecasting. Yet many sensing methods rely on fixed instruments - acoustic, radar, camera, or pressure probes - that are costly to install and maintain and are vulnerable during extreme events. We propose a passive, low-cost water-level tracking scheme that uses only LTE downlink power metrics reported by commodity receivers. The method extracts per-antenna RSRP, RSSI, and RSRQ, applies a continuous wavelet transform (CWT) to the RSRP to isolate the semidiurnal tide component, and forms a summed-coefficient signature that simultaneously marks high/low tide (tide-turn times) and tracks the tide-rate (flow speed) over time. These wavelet features guide a lightweight neural network that learns water-level changes over time from a short training segment. Beyond a single serving base station, we also show a multi-base-station cooperative mode: independent CWTs are computed per carrier and fused by a robust median to produce one tide-band feature that improves stability and resilience to local disturbances. Experiments over a 420 m river path under line-of-sight conditions achieve root-mean-square and mean-absolute errors of 0.8 cm and 0.5 cm, respectively. Under a non-line-of-sight setting with vegetation and vessel traffic, the same model transfers successfully after brief fine-tuning, reaching 1.7 cm RMSE and 0.8 cm MAE. Unlike CSI-based methods, the approach needs no array calibration and runs on standard hardware, making wide deployment practical. When signals from multiple base stations are available, fusion further improves robustness.

</details>


### [4] [Evaluating the Performance of a Modified Skin Temperature Sensor for Lower Limb Prostheses: An Experimental Comparison](https://arxiv.org/abs/2511.21068)
*Anirshu Devroy,Gregor Fritz,Mathias Brandstoetter*

Main category: eess.SP

TL;DR: 本文研究用于假肢系统的温度监测传感器，比较了普通热敏电阻和改进型热敏电阻在户外环境中的性能表现，旨在开发舒适的可穿戴传感器来实时监测皮肤温度


<details>
  <summary>Details</summary>
Motivation: 当前下肢假肢康复面临皮肤状况、刺激和不适等挑战，需要开发能够实时监测皮肤温度的舒适可穿戴传感器，为假肢用户和骨科技术人员提供反馈和调整依据

Method: 进行了一系列实验来理解和表征系统行为，比较普通热敏电阻和改进型热敏电阻作为假肢户外使用温度测量方法的性能

Result: 初步结果显示，部分改进型热敏电阻相比其他传感器表现出更好的温度记录性能

Conclusion: 改进型热敏电阻可以作为假肢系统中舒适温度测量的潜在替代方案，这种系统能够提供温度分布的有价值见解和皮肤问题的早期预警

Abstract: Current rehabilitation of lower limb prostheses has significant challenges, especially with skin conditions, irritation and discomfort. Understanding the skin temperature and having comfortable wearable sensors that would monitor skin temperature in a real-time outdoor environment would be useful. The system would help the user and orthopedic technician to provide feedback and changes that might be required in the prosthesis. Hence in this paper, a series of experiments are conducted in order to understand and characterize the system behavior and compare a general thermistor and a modified thermistor as a potential method of temperature measurement for outdoor usage of prostheses. The paper goes on to compare the different modified thermistors behavior with their regular counterpart and highlights the challenges and improvement areas needed for such a modified thermistor for outdoor temperature monitoring in a prosthetic system. Initial results show that some of the modified thermistors showed better temperature recording compared to the rest. Finally, such modified thermistors can be a potential alternative for comfortable temperature measurement embedded in the prosthesis system. Such a system can provide valuable insights into temperature distribution and an early warning system for skin problems

</details>


### [5] [Data-Driven Assessment of Concrete Slab Integrity via Impact-Echo Signals and Neural Networks](https://arxiv.org/abs/2511.21080)
*Yeswanth Ravichandran,Duoduo Liao,Charan Teja Kurakula*

Main category: eess.SP

TL;DR: 提出基于机器学习的冲击回波框架，自动定位混凝土缺陷并进行多分类，包括浅层剥离、深层剥离、空隙和蜂窝状缺陷，准确率达73%。


<details>
  <summary>Details</summary>
Motivation: 混凝土桥面板的亚表面缺陷（如剥离、空隙、蜂窝）严重影响耐久性，但视觉检测和手动敲击难以可靠检测。

Method: 将原始冲击回波信号通过FFT转换为峰值频率特征并插值为空间图，使用k-means聚类识别缺陷区域，构建空间有序的峰值频率序列输入堆叠LSTM网络进行分类。

Result: 在实验室数据上训练的分类器在实地桥面板验证中表现良好，总体准确率达73%，证明模型在真实耦合、噪声和环境变化下具有泛化能力。

Conclusion: 该框架提高了无损评估的客观性、可扩展性和可重复性，支持网络规模的智能、数据驱动的桥梁健康监测。

Abstract: Subsurface defects such as delamination, voids, and honeycombing critically affect the durability of concrete bridge decks but are difficult to detect reliably using visual inspection or manual sounding. This paper presents a machine learning based Impact Echo (IE) framework that automates both defect localization and multi-class classification of common concrete defects. Raw IE signals from Federal Highway Administration (FHWA) laboratory slabs and in-service bridge decks are transformed via Fast Fourier Transform (FFT) into dominant peak-frequency features and interpolated into spatial maps for defect zone visualization. Unsupervised k-means clustering highlights low-frequency, defect-prone regions, while Ground Truth Masks (GTMs) derived from seeded lab defects are used to validate spatial accuracy and generate high-confidence training labels. From these validated regions, spatially ordered peak-frequency sequences are constructed and fed into a stacked Long Short-Term Memory (LSTM) network that classifies four defect types shallow delamination, deep delamination, voids, and honeycombing with 73% overall accuracy. Field validation on the bridge deck demonstrates that models trained on laboratory data generalize under realistic coupling, noise, and environmental variability. The proposed framework enhances the objectivity, scalability, and repeatability of Non-Destructive Evaluation (NDE), supporting intelligent, data-driven bridge health monitoring at a network scale.

</details>


### [6] [2D Sparse Array Design via Reweighted L1 Second Order Cone Programming for 3D Ultrasound Imaging](https://arxiv.org/abs/2511.21133)
*Xi Zhang,Miguel Bernal,Wei-Ning Lee*

Main category: eess.SP

TL;DR: 提出一种基于二阶锥规划和重加权L1技术的稀疏阵列设计方法，用于减少3D超声成像中的通道数量，在保持分辨率的同时优化对比度性能。


<details>
  <summary>Details</summary>
Motivation: 传统2D全寻址阵列需要数千个独立通道，成本高昂。现有随机优化方法设计的稀疏阵列结果不稳定，需要更可靠的设计方法。

Method: 将稀疏阵列合成问题建模为二阶锥规划问题，并采用重加权L1技术进行序列优化，设计具有准平坦旁瓣的2D稀疏阵列。

Result: 设计的Q-Flats阵列具有-21.26dB的旁瓣水平和252个激活单元，在256通道限制下，相比Fermat螺旋阵列分辨率提高约3%，对比度略差。

Conclusion: 重加权L1 SOCP方法是一种有前景且灵活的方法，能够在分辨率、对比度和激活单元数量之间寻求平衡。

Abstract: Two-dimensional (2D) fully-addressed arrays can conveniently realize three-dimensional (3D) ultrasound imaging while fully controlled such arrays usually demands thousands of independent channels, which is costly. Sparse array technique using stochastic optimization methods is one of promising techniques to reduce channel counts while due to the stochastic nature of these methods, the optimized results are usually unstable. In this work, we introduce a sparse array design approach that formulates the synthesis problem of sparse arrays as second-order cone programming (SOCP) and a re-weighted L1 technique is implemented to sequentially optimize the SOCP. Based on this method, an on-grid quasi-flatten side-lobe (Q-Flats) 2D sparse array with side-lobe level (SLL) no more than -21.26 dB and 252 activated elements is designed, which aims to achieve as high contrast performance as possible under the limits of resolution and maximum number of independent channels (i.e., 256). The imaging performance of the Q-Flats array was compared with those of a corresponding dense array (Dense), a Fermat spiral array (Spiral) and a spatially 50%-Tukey tapered spiral array (Spiral-Taper) using Field II simulations in a multi-angle steered diverging wave transmission scheme. It was demonstrated that the Dense achieved the best resolution and contrast and the Spiral-Taper the worst. The Q-Flats showed better resolution (about 3%) but slightly worse contrast than the Spiral. All the results indicate the re-weighted L1 SOCP method is a promising and flexible method for seeking trade-offs among resolution, contrast, and number of activated elements.

</details>


### [7] [Multiport Analytical Pixel Electromagnetic Simulator (MAPES) for AI-assisted RFIC and Microwave Circuit Design](https://arxiv.org/abs/2511.21274)
*Junhui Rao,Yi Liu,Jichen Zhang,Zhaoyang Ming,Tianrui Qiao,Yujie Zhang,Chi Yuk Chiu,Hua Wang,Ross Murch*

Main category: eess.SP

TL;DR: 提出了一种名为MAPES的多端口分析像素电磁模拟器，能够高效准确地预测任意基于像素的微波和RFIC结构的电磁性能，仅需约1%的全波仿真数据即可构建多端口阻抗矩阵，实现600-2000倍的速度提升。


<details>
  <summary>Details</summary>
Motivation: 传统AI辅助电磁设计需要大量数据集，存在数据驱动过拟合问题，需要一种更高效准确的方法来预测像素基微波和RFIC结构的电磁性能。

Method: 基于集成内部多端口方法，引入虚拟像素和对角虚拟像素，在关键位置插入虚拟端口，通过少量全波仿真构建多端口阻抗矩阵，使用闭式多端口关系分析评估任意像素配置。

Result: 在单层和双层CMOS工艺及PCB上验证，MAPES实现了高预测精度，相比CST仿真速度提升600-2000倍，消除了数据驱动过拟合问题。

Conclusion: MAPES因其高效性、可扩展性和可靠性，为跨多种制造技术的AI辅助微波电路和RFIC设计提供了实用且多功能的工具。

Abstract: This paper proposes a novel analytical framework, termed the Multiport Analytical Pixel Electromagnetic Simulator (MAPES). MAPES enables efficient and accurate prediction of the electromagnetic (EM) performance of arbitrary pixel-based microwave (MW) and RFIC structures. Inspired by the Integrated Internal Multiport Method (IMPM), MAPES extends the concept to the pixel presence/absence domain used in AI-assisted EM design. By introducing virtual pixels and diagonal virtual pixels and inserting virtual ports at critical positions, MAPES captures all horizontal, vertical, and diagonal electromagnetic couplings within a single multiport impedance matrix. Only a small set of full-wave simulations (typically about 1% of the datasets required by AI-assisted EM simulators) is needed to construct this matrix. Subsequently, any arbitrary pixel configuration can be evaluated analytically using a closed-form multiport relation without additional full-wave calculations. The proposed approach eliminates data-driven overfitting and ensures accurate results across all design variations. Comprehensive examples for single- and double-layer CMOS processes (180 nm and 65 nm) and PCBs confirm that MAPES achieves high prediction accuracy with 600- 2000x speed improvement compared to CST simulations. Owing to its efficiency, scalability and reliability, MAPES provides a practical and versatile tool for AI-assisted MW circuit and RFIC design across diverse fabrication technologies.

</details>


### [8] [Phase-Aware Code-Aided EM Algorithm for Blind Channel Estimation in PSK-Modulated OFDM](https://arxiv.org/abs/2511.21340)
*Chin-Hung Chen,Ivana Nikoloska,Wim van Houtum,Yan Wu,Alex Alvarado*

Main category: eess.SP

TL;DR: 提出一种用于OFDM系统的全盲相位感知EM算法，通过利用解码器的外部信息作为模型证据指标，解决PSK调制中相位模糊导致的EM算法局部最大值问题。


<details>
  <summary>Details</summary>
Motivation: 传统盲EM估计器无法解决信道估计中的未知相位模糊问题，这导致EM算法陷入局部最大值。需要一种方法来可靠地解决相位模糊。

Method: 基于PSK调制的固有对称性生成候选模型集，利用解码器选择最可能的候选模型。在EM初始化阶段后仅调用一次该算法。

Result: 仿真结果表明，结合简单卷积码，相位感知EM算法在初始化阶段可靠地解决了相位模糊，在具有恒定相位模糊的频率选择性信道中，将局部收敛率从80%降低到接近0%。

Conclusion: 所提出的相位感知EM算法有效解决了盲信道估计中的相位模糊问题，且额外复杂度可忽略，适合后续turbo迭代。

Abstract: This paper presents a fully blind phase-aware expectation-maximization (EM) algorithm for OFDM systems with the phase-shift keying (PSK) modulation. We address the well-known local maximum problem of the EM algorithm for blind channel estimation. This is primarily caused by the unknown phase ambiguity in the channel estimates, which conventional blind EM estimators cannot resolve. To overcome this limitation, we propose to exploit the extrinsic information from the decoder as model evidence metrics. A finite set of candidate models is generated based on the inherent symmetries of PSK modulation, and the decoder selects the most likely candidate model. Simulation results demonstrate that, when combined with a simple convolutional code, the phase-aware EM algorithm reliably resolves phase ambiguity during the initialization stage and reduces the local convergence rate from 80% to nearly 0% in frequency-selective channels with a constant phase ambiguity. The algorithm is invoked only once after the EM initialization stage, resulting in negligible additional complexity during subsequent turbo iterations.

</details>


### [9] [Blind Turbo Demodulation for Differentially Encoded OFDM with 2D Trellis Decomposition](https://arxiv.org/abs/2511.21345)
*Chin-Hung Chen,Yan Wu,Wim van Houtum,Alex Alvarado*

Main category: eess.SP

TL;DR: 提出了一种完全盲的turbo-DE-PSK方案，无需导频即可联合估计信道相位、信道增益和噪声方差，在DAB类系统中接近完美信道知识的接收机性能。


<details>
  <summary>Details</summary>
Motivation: DAB类系统使用差分编码PSK传输，turbo-DE-PSK接收机通过迭代解码提供性能增益，但依赖无导频的准确信道估计，这是DAB类场景中的关键挑战。

Method: 利用二维网格分解进行盲相位估计，辅以基于功率的信道增益和噪声方差估计器，开发了完全盲的turbo-DE-PSK方案。

Result: 仿真结果表明，盲二维turbo解调器接近完美信道知识接收机的性能，并在实际传输条件下保持鲁棒性。

Conclusion: 该方案成功解决了DAB类系统中无导频信道估计的挑战，实现了接近理想性能的盲turbo-DE-PSK接收。

Abstract: Digital Audio Broadcasting (DAB)-like systems employ differentially encoded (DE) phase-shift keying (PSK) for transmission. While turbo-DE-PSK receivers offer substantial performance gains through iterative decoding by making the DE-PSK an inner code, they rely on accurate channel estimation without pilots, which is a key challenge in DAB-like scenarios. This paper develops a fully blind turbo-DE-PSK scheme that jointly estimates channel phase, channel gain, and noise variance directly from the received signal. The design leverages a two-dimensional (2D) trellis decomposition for blind phase estimation, complemented by power-based estimators for channel gain and noise variance. We provide a comprehensive system assessment across practical system parameters, including inner code length, phase quantization, and 2D block size. Simulation results show that the blind 2D turbo demodulator approaches the performance of receivers with perfect channel knowledge and remains robust under realistic transmission conditions.

</details>


### [10] [Group-wise Semantic Splitting Multiple Access for Multi-User Semantic Communication](https://arxiv.org/abs/2511.21411)
*Jungyeon Koh,Hyeonho Noh,Hyun Jong Yang*

Main category: eess.SP

TL;DR: 提出了一种基于用户语义特征相似度的分组多址接入框架，通过聚类提取组级公共特征和用户特定私有特征，采用多播和单播混合传输方式，结合重构损失和排斥损失的复合损失函数提升语义分离性和重建质量。


<details>
  <summary>Details</summary>
Motivation: 解决多用户语义通信中如何有效利用共享语义信息和用户特定信息的问题，提高语义传输效率和重建质量。

Method: 使用平衡聚类机制按语义特征相似度对用户分组，提取组级公共特征和用户私有特征；基站通过多播传输公共特征，单播传输私有特征；设计包含重构损失和排斥损失的复合损失函数。

Result: 在各种信道条件下相比传统方案性能提升达3.26%，验证了方法的鲁棒性和语义效率。

Conclusion: 该框架能够有效提升多用户语义通信的性能，为下一代无线网络的语义通信提供了有效的解决方案。

Abstract: In this letter, we propose a group-wise semantic splitting multiple access framework for multi-user semantic communication in downlink scenarios. The framework begins by applying a balanced clustering mechanism that groups users based on the similarity of their semantic characteristics, enabling the extraction of group-level common features and user-specific private features. The base station then transmits the common features via multicast and the private features via unicast, effectively leveraging both shared and user-dependent semantic information. To further enhance semantic separability and reconstruction fidelity, we design a composite loss function that integrates a reconstruction loss with a repulsion loss, improving both the accuracy of semantic recovery and the distinctiveness of common embeddings in the latent space. Simulation results demonstrate that the proposed method achieves up to 3.26% performance improvement over conventional schemes across various channel conditions, validating its robustness and semantic efficiency for next-generation wireless networks.

</details>


### [11] [Design Of A Communication System To Send Text Using Lora At 400 MHz](https://arxiv.org/abs/2511.21434)
*Fabrizio André Farfán Prado,William César Pérez Campos,Steisy Anahi Carreño Tacuri,Favio David Cabrera Alva,Harold Jacobed Carhuas Lizarbe*

Main category: eess.SP

TL;DR: 设计并实现了一个基于ESP32和LoRa DXLR01的低功耗无线文本传输系统，适用于缺乏传统网络覆盖的农村和城市环境。


<details>
  <summary>Details</summary>
Motivation: 解决农村地区和某些城市环境中Wi-Fi或移动网络不可用或受限时的连接性和能效问题。

Method: 集成LoRa技术（使用433MHz频段和CSS调制）与ESP32模块，ESP32负责捕获、处理和发送消息，接收端显示在LCD屏并上传至ThingSpeak平台。

Result: 在受控环境中测试显示文本传输平均延迟为3.2秒，系统可应用于远程监控、基础设施管理和访问控制等场景。

Conclusion: 该系统成功实现了在传统网络不可用环境下的可靠文本通信，具有低功耗和长距离传输的优势。

Abstract: This work describes the design and implementation of a low-power wireless communication system for transmitting text using ESP32 modules and the LoRa DXLR01. The proposal arises as a solution to connectivity and energy-efficiency problems commonly found in rural areas and certain urban environments where Wi-Fi or mobile networks are unavailable or operate with limitations. To address this, LoRa technology known for its long-range capability and low power consumption is integrated with an ESP32 responsible for capturing, processing, and sending messages.
  The LoRa DXLR01 module, which operates in the 433 MHz band, is configured with parameters aimed at maximising both transmission range and efficient energy usage. Messages are sent using Chirp Spread Spectrum (CSS) modulation, improving signal penetration in obstructed areas and reducing the likelihood of errors. On the receiving end, the ESP32 interprets the data and displays it on an LCD screen. Additionally, the received information is sent to the ThingSpeak platform, allowing remote storage and visualisation without relying on conventional network infrastructure.
  Tests conducted in a controlled environment show an average latency of 3.2 seconds for text transmission. It was also verified that the system can be used in applications such as remote monitoring, infrastructure management, and access control.

</details>


### [12] [SIR Analysis for Affine Filter Bank Modulation](https://arxiv.org/abs/2511.21615)
*Henrique L. Senger,Gustavo P. Gonçalves,Bruno S. Chang,Hyeon Seok Rou,Kuranage Roche Rayan Ranasinghe,Giuseppe Thadeu Freitas de Abreu,Didier Le Ruyet*

Main category: eess.SP

TL;DR: 分析了AFBM波形在MMSE均衡下两个域中的SIR性能，发现滤波时域检测方案比仿射域等效方案有显著性能提升


<details>
  <summary>Details</summary>
Motivation: 研究AFBM波形在不同域中的信号干扰比性能差异，解释滤波时域检测方案性能优势的原因

Method: 在仿射域和滤波时域两个域中分析AFBM波形的SIR，采用MMSE均衡，结合DAFT和去扩频/映射操作

Result: 发现滤波时域中存在信道干扰与正交性近似误差的不期望组合的有趣抵消现象，这种现象在仿射域中不会发生

Conclusion: 滤波时域检测方案相比仿射域等效方案具有显著性能优势，BER结果验证了分析的正确性

Abstract: The signal-to-interference ratio (SIR) of the Affine Filter Bank Modulation (AFBM) waveform is analyzed under minimum mean square error (MMSE) equalization in two domains; namely, the affine domain and the filtered time-domain (TD). Due to the incorporation of the discrete affine Fourier transform (DAFT) and despreading/mapping, an interesting and counter-intuitive cancellation of the unwanted combination of the channel induced interference with the orthogonality approximation error is seen in the filtered TD, a process which does not occur in the affine domain. The direct impact on bit error rate (BER) provides a thorough validation of the proposed analysis and explains the substantial gains in performance of the filtered TD detection scheme as opposed to its affine domain equivalent

</details>


### [13] [Optimal Bit Detection in Thermal Noise Communication Systems Under Rician Fading](https://arxiv.org/abs/2511.21649)
*Mohamed El Jbari,Fernando D. A. García,Hugerles S. Silva,Felipe A. P. de Figueiredo,Rausley A. A. de Souza*

Main category: eess.SP

TL;DR: 提出了一个在Rician衰落信道下热噪声通信系统的最优比特检测分析框架，通过卡方统计推导最大似然检测阈值和误码率表达式，相比基于高斯近似的次优检测显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有热噪声通信分析多依赖高斯近似且忽略衰落效应，限制了准确性。需要建立精确的分析框架来支持未来B5G/6G和大规模物联网系统中能效热噪声通信接收机的设计。

Method: 使用卡方统计推导最优最大似然检测阈值，通过Gauss-Laguerre求积法得到误码率表达式，消除近似误差并准确表征有限样本尺寸下的性能。

Result: 蒙特卡洛仿真验证了分析结果，相比次优高斯检测显著改善了误码率性能，并量化了样本尺寸、电阻比和Rician K因子等关键参数的影响。

Conclusion: 该框架为未来B5G/6G和大规模物联网系统中设计能效热噪声通信接收机提供了坚实基础。

Abstract: Thermal noise communication (TNC) enables ultra-low-power wireless links for Internet of Things (IoT) devices by modulating the variance of thermal noise, rather than using active carriers. Existing analyses often rely on Gaussian approximations and overlook fading effects, which limits their accuracy. This paper presents an accurate analytical framework for optimal bit detection in TNC systems under Rician fading. Using chi-squared statistics, we derive the optimal maximum-likelihood detection threshold and an expression for the bit error probability (BEP) via Gauss-Laguerre quadrature. The proposed model eliminates approximation errors and accurately characterizes performance for finite sample sizes. Monte Carlo simulations confirm the analytical results and demonstrate significant improvements in BEP compared with suboptimal Gaussian-based detection. Furthermore, the influence of key parameters, sample size, resistance ratio, and Rician K-factor, is quantified. The proposed framework provides a solid foundation for designing energy-efficient TNC receivers in future B5G/6G and large-scale IoT systems.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [14] [Towards Audio Token Compression in Large Audio Language Models](https://arxiv.org/abs/2511.20973)
*Saurabhchand Bhati,Samuel Thomas,Hilde Kuehne,Rogerio Feris,James Glass*

Main category: eess.AS

TL;DR: 该论文提出通过无监督分割、均匀平均池化等技术压缩LALMs的音频token数量，并使用低秩适配器微调模型以缓解性能下降，在语音识别和语音翻译任务中实现了接近帧级LALMs的性能，同时将输入音频token数量减少最多三倍。


<details>
  <summary>Details</summary>
Motivation: 大型音频语言模型在处理长音频和资源受限平台时面临注意力机制二次复杂度和音频信号高token率的可扩展性限制。

Method: 采用无监督分割、均匀平均池化等技术减少音频编码器生成的token数量，使用低秩适配器微调模型以保持性能。

Result: 在自动语音识别和语音到语音翻译任务中，压缩后的LALMs能够实现接近帧级LALMs的性能，同时将输入音频token数量减少最多三倍。

Conclusion: 提出的压缩方法有效解决了LALMs的可扩展性问题，在保持性能的同时显著减少了计算需求，为在资源受限平台上部署长音频处理应用提供了可行方案。

Abstract: Large Audio Language Models (LALMs) demonstrate impressive performance across diverse tasks, ranging from speech recognition to general audio understanding. However, their scalability is limited by the quadratic complexity of attention and the high token rates of audio signals. These challenges make it difficult to extend LALMs to long-form audio and to deploy them on resource-constrained platforms such as edge devices.
  In this paper, we explore techniques such as unsupervised segmentation, uniform average pooling, etc., to reduce the number of audio tokens generated by the LALM's audio encoder but before they are consumed by the LLM decoder. To mitigate potential performance degradation introduced by the compressed representations, we employ low-rank adapters to finetune the model. We evaluate our proposed models on two tasks, automatic speech recognition and speech-to-speech translation tasks, that are dependent on effectively uncovering the underlying lexical content of the input signal and study the effect of downsampling on these tasks. Experimental results show that compressed LALMs can achieve performance closer to frame-level LALMs while reducing the input audio token count upto three times before the LLM backbone.

</details>


### [15] [RosettaSpeech: Zero-Shot Speech-to-Speech Translation from Monolingual Data](https://arxiv.org/abs/2511.20974)
*Zhisheng Zheng,Xiaohang Sun,Tuan Dinh,Abhishek Yanamandra,Abhinav Jain,Zhu Liu,Sunil Hadap,Vimal Bhat,Manoj Aggarwal,Gerard Medioni,David Harwath*

Main category: eess.AS

TL;DR: RosettaSpeech是一个新颖的零样本语音到语音翻译框架，仅使用单语语音-文本数据和机器翻译监督进行训练，无需平行语音对，在推理时实现端到端的直接语音翻译。


<details>
  <summary>Details</summary>
Motivation: 平行语音语料库的稀缺严重阻碍了语音到语音翻译的发展，通常需要依赖复杂多阶段的流水线方法。

Method: 利用文本作为训练时的中间桥梁，结合基于文本的神经机器翻译模型的语言知识，但推理时作为直接的端到端语音到语音模型运行。

Result: 在CVSS-C测试集上取得最先进结果：德语到英语ASR-BLEU 25.17（相对提升27%），西班牙语到英语29.86（相对提升14%）。单个模型可实现多对一翻译（法/西/德→英）。

Conclusion: 通过优先依赖丰富的平行文本而非难以获取的平行语音，RosettaSpeech为创建高质量、保持说话人特征的语音到语音翻译提供了可扩展的路径。

Abstract: The scarcity of parallel speech corpora critically hampers speech-to-speech translation (S2ST), often forcing reliance on complex, multi-stage pipelines. This paper introduces RosettaSpeech, a novel and simplified framework for zero-shot S2ST that is trained on monolingual speech-text data augmented by machine translation supervision. While our method leverages the linguistic knowledge inherent in text-based NMT models, it strictly eliminates the need for parallel speech-to-speech pairs. Our model uniquely uses text as an intermediate bridge during training but functions as a direct, end-to-end speech-to-speech model at inference. This streamlined approach achieves state-of-the-art results on standard benchmarks. For instance, on the CVSS-C test set, RosettaSpeech outperforms leading systems, achieving an ASR-BLEU score of 25.17 for German-to-English and 29.86 for Spanish-to-English-relative gains of over 27% and 14%, respectively. Furthermore, we demonstrate that a single model can deliver strong many-to-one translation performance (FR/ES/DE -> EN). We also provide a foundational analysis of how training data scaling impacts model performance. By prioritizing reliance on abundant parallel text rather than difficult-to-acquire parallel speech, RosettaSpeech offers a scalable path to creating high-quality, speaker-preserving S2ST for a much broader array of languages.

</details>


### [16] [Evaluation of an ITD-to-ILD Transformation as a Method to Restore the Spatial Benefit in Speech Intelligibility in Hearing Impaired Listeners](https://arxiv.org/abs/2511.21222)
*Timm-Jonas Bäumer,Johannes W. de Vries,Stephan Töpken,Richard C. Hendriks,Peyman Goli,Steven van de Par*

Main category: eess.AS

TL;DR: 本研究探索将低频ITD转换为ILD以恢复听力受损者的双耳听觉优势，实验表明该方法能显著改善言语理解能力。


<details>
  <summary>Details</summary>
Motivation: 听力受损者通常对ITD敏感度有限，导致言语理解能力下降，需要寻找方法来恢复双耳听觉优势。

Method: 通过两个实验：1）测量HI听众的ITD敏感度阈值；2）在不同双耳配置下测量SRT，通过HRTF操作将低频ITD转换为ILD。

Result: 移除ITD使SRT降低约1dB；将低频ITD替换为ILD可改善侧向说话者的理解；在保留ITD的同时添加低频ILD对所有方向说话者都有显著改善。

Conclusion: ITD到ILD的转换方法能有效恢复HI听众的双耳听觉优势，建议在助听器和人工耳蜗中实施此类技术。

Abstract: To improve speech intelligibility in complex everyday situations, the human auditory system partially relies on Interaural Time Differences (ITDs) and Interaural Level Differences (ILDs). However, hearing impaired (HI) listeners often exhibit limited sensitivity to ITDs, resulting in decreased speech intelligibility performance. This study aimed to investigate whether transforming low-frequency ITDs into ILDs could reintroduce a binaural benefit for HI listeners. We conducted two experiments with HI listeners. The first experiment used binaurally phase-shifted sinusoids at different frequencies to evaluate the HI listeners ITD sensitivity threshold. All subjects had an increased ITD threshold at higher frequencies, with different ITD sensitivities between the subjects in the lower frequencies. In the second experiment, Speech Reception Thresholds (SRTs) were measured in different binaural configurations by manipulating Head-Related Transfer Functions (HRTFs). The results showed that, despite the decreased ITD sensitivity, removing ITDs decreased SRTs by approximately 1 dB compared to the unprocessed baseline, where ITDs and ILDs are available. Furthermore, substituting low-frequency ITDs with ILDs yielded an improvement for a lateral target speaker. Adding the low-frequency ILDs while preserving the ITDs caused a significant improvement for speakers in all directions. These findings suggest that the proposed transformation method could be effective in restoring binaural benefits in HI listeners. The results of this study suggest the use of such transformation techniques to be implemented in hearing aids and cochlear implants, directly benefiting HI listeners.

</details>


### [17] [The Spheres Dataset: Multitrack Orchestral Recordings for Music Source Separation and Information Retrieval](https://arxiv.org/abs/2511.21247)
*Jaime Garcia-Martinez,David Diaz-Guerra,John Anderson,Ricardo Falcon-Perez,Pablo Cabañas-Molero,Tuomas Virtanen,Julio J. Carabias-Orti,Pedro Vera-Candeas*

Main category: eess.AS

TL;DR: The Spheres数据集是一个多轨管弦乐录音数据集，包含柴可夫斯基《罗密欧与朱丽叶》和莫扎特《第40交响曲》等作品，用于音乐源分离和MIR任务研究。


<details>
  <summary>Details</summary>
Motivation: 推动古典音乐领域机器学习和音乐信息检索研究，特别是音乐源分离任务，填补管弦乐录音数据集的空白。

Method: 使用23个麦克风录制，包括近距离、主麦克风和环境麦克风，创建具有控制泄漏的立体声混音，并提供分离音轨用于监督训练。

Result: 展示了使用X-UMX模型进行管弦乐家族分离和麦克风去泄漏的基线评估结果，揭示了复杂管弦乐场景中源分离的潜力和挑战。

Conclusion: 该数据集为古典音乐的分离、定位、去混响和沉浸式渲染提供了有价值的基准测试平台。

Abstract: This paper introduces The Spheres dataset, multitrack orchestral recordings designed to advance machine learning research in music source separation and related MIR tasks within the classical music domain. The dataset is composed of over one hour recordings of musical pieces performed by the Colibrì Ensemble at The Spheres recording studio, capturing two canonical works - Tchaikovsky's Romeo and Juliet and Mozart's Symphony No. 40 - along with chromatic scales and solo excerpts for each instrument. The recording setup employed 23 microphones, including close spot, main, and ambient microphones, enabling the creation of realistic stereo mixes with controlled bleeding and providing isolated stems for supervised training of source separation models. In addition, room impulse responses were estimated for each instrument position, offering valuable acoustic characterization of the recording space. We present the dataset structure, acoustic analysis, and baseline evaluations using X-UMX based models for orchestral family separation and microphone debleeding. Results highlight both the potential and the challenges of source separation in complex orchestral scenarios, underscoring the dataset's value for benchmarking and for exploring new approaches to separation, localization, dereverberation, and immersive rendering of classical music.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [18] [Seeing Beyond Sound: Visualization and Abstraction in Audio Data Representation](https://arxiv.org/abs/2511.20658)
*Ashlae Blum'e*

Main category: cs.SD

TL;DR: 在音频信号处理中，通过视觉表示解释复杂信息能增强模式识别，因为它与人类感知系统对齐。具有历史背景隐藏假设的软件工具可能与现代工作流程不匹配。增加维度和交互性的可视化工具能够促进音频信息研究中的复杂工作流程。


<details>
  <summary>Details</summary>
Motivation: 软件工具中继承的历史背景假设可能随着设计起源的模糊而与现代工作流程不匹配，创建与新兴需求对齐的工具可以提高分析和创意输出。

Method: 使用Jellyfish Dynamite软件探索在可视化工具中添加维度和交互性，以促进音频信息研究中的复杂工作流程。

Result: 通过视觉表示与人类感知系统的对齐，增强了复杂音频信息的模式识别能力。

Conclusion: 增加维度和交互性的可视化工具能够更好地支持音频信息研究中的复杂工作流程，提高工具使用亲和力从而改善分析结果。

Abstract: In audio signal processing, the interpretation of complex information using visual representation enhances pattern recognition through its alignment with human perceptual systems. Software tools that carry hidden assumptions inherited from their historical contexts risk misalignment with modern workflows as design origins become obscured. We argue that creating tools that align with emergent needs improves analytical and creative outputs due to an increased affinity for using them. This paper explores the potentials associated with adding dimensionality and interactivity into visualization tools to facilitate complex workflows in audio information research using the Jellyfish Dynamite software.

</details>


### [19] [Musical Score Understanding Benchmark: Evaluating Large Language Models' Comprehension of Complete Musical Scores](https://arxiv.org/abs/2511.20697)
*Congren Dai,Yue Yang,Krinos Li,Huichi Zhou,Shijie Liang,Zhang Bo,Enyang Liu,Ge Jin,Hongran An,Haosen Zhang,Peiyuan Jing,KinHei Lee,Zhenxuan Zhang,Xiaobing Li,Maosong Sun*

Main category: cs.SD

TL;DR: MSU-Bench是首个大规模人工策划的音乐乐谱理解基准，评估文本和视觉模态下的乐谱理解能力，包含1800个生成式问答对，涵盖四个理解层级。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs和VLMs在自然语言和多模态任务上进展迅速，但它们在理解音乐符号方面的能力仍未充分探索。

Method: 构建包含1800个生成式QA对的MSU-Bench基准，涵盖ABC符号和PDF视觉两种模态，分为四个渐进理解层级，并对15+个SOTA模型进行零样本和微调评估。

Result: 评估揭示了显著的模态差距、脆弱的层级成功率以及维持多层级正确性的困难。微调显著提高了两种模态的性能，同时保留了通用知识。

Conclusion: MSU-Bench为AI、音乐学和多模态推理交叉领域的未来研究建立了严格的基础。

Abstract: Understanding complete musical scores requires reasoning over symbolic structures such as pitch, rhythm, harmony, and form. Despite the rapid progress of Large Language Models (LLMs) and Vision-Language Models (VLMs) in natural language and multimodal tasks, their ability to comprehend musical notation remains underexplored. We introduce Musical Score Understanding Benchmark (MSU-Bench), the first large-scale, human-curated benchmark for evaluating score-level musical understanding across both textual (ABC notation) and visual (PDF) modalities. MSU-Bench comprises 1,800 generative question-answer (QA) pairs drawn from works spanning Bach, Beethoven, Chopin, Debussy, and others, organised into four progressive levels of comprehension: Onset Information, Notation & Note, Chord & Harmony, and Texture & Form. Through extensive zero-shot and fine-tuned evaluations of over 15+ state-of-the-art (SOTA) models, we reveal sharp modality gaps, fragile level-wise success rates, and the difficulty of sustaining multilevel correctness. Fine-tuning markedly improves performance in both modalities while preserving general knowledge, establishing MSU-Bench as a rigorous foundation for future research at the intersection of Artificial Intelligence (AI), musicological, and multimodal reasoning.

</details>


### [20] [SingingSDS: A Singing-Capable Spoken Dialogue System for Conversational Roleplay Applications](https://arxiv.org/abs/2511.20972)
*Jionghao Han,Jiatong Shi,Masao Someki,Yuxun Tang,Lan Liu,Yiwen Zhao,Wenhao Feng,Shinji Watanabe*

Main category: cs.SD

TL;DR: SingingSDS是一个基于ASR-LLM-SVS模块化管道的歌唱对话系统，能够通过歌唱而非传统语音进行回应，为角色扮演和互动娱乐场景提供更具情感、记忆性和愉悦性的交互体验。


<details>
  <summary>Details</summary>
Motivation: 现有口语对话系统大多局限于传统的语音回应，缺乏情感表达和娱乐性。SingingSDS旨在通过歌唱回应来创造更具情感、记忆性和愉悦性的交互体验，特别是在角色扮演和互动娱乐场景中。

Method: 采用模块化的ASR-LLM-SVS管道架构，支持多种配置选项，包括角色设定、ASR和LLM后端、SVS模型、旋律来源和声音配置文件，可根据延迟、质量和音乐风格需求进行定制。

Result: 开发了一个即插即用的网页演示系统，提供模块化、开源的代码支持定制和扩展。系统已在Hugging Face和GitHub上公开可用。

Conclusion: SingingSDS成功展示了通过歌唱而非传统语音进行对话交互的可行性，为情感化人机交互和娱乐应用开辟了新方向，其模块化设计支持广泛的定制和扩展。

Abstract: With recent advances in automatic speech recognition (ASR), large language models (LLMs), and text-to-speech (TTS) technologies, spoken dialogue systems (SDS) have become widely accessible. However, most existing SDS are limited to conventional spoken responses. We present SingingSDS, a cascaded SDS that responds through singing rather than speaking, fostering more affective, memorable, and pleasurable interactions in character-based roleplay and interactive entertainment scenarios. SingingSDS employs a modular ASR-LLM-SVS pipeline and supports a wide range of configurations across character personas, ASR and LLM backends, SVS models, melody sources, and voice profiles, tailored to different needs in terms of latency, quality, and musical style. SingingSDS is available as a plug-and-play web demo, featuring modular, open-source code that supports customization and extension. Demo: https://huggingface.co/spaces/espnet/SingingSDS. Code: https://github.com/SingingSDS/SingingSDS.

</details>


### [21] [CartoonSing: Unifying Human and Nonhuman Timbres in Singing Generation](https://arxiv.org/abs/2511.21045)
*Jionghao Han,Jiatong Shi,Zhuoyan Tao,Yuxun Tang,Yiwen Zhao,Gus Xia,Shinji Watanabe*

Main category: cs.SD

TL;DR: 本文提出了非人声歌唱生成（NHSG）任务，包括非人声歌唱合成（NHSVS）和非人声歌唱转换（NHSVC），并开发了CartoonSing统一框架来解决非人声歌唱生成中的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有歌唱合成系统仅限于人类音色，无法生成超出人类范围的音色，而创意应用（如视频游戏、电影、虚拟角色）对此类非人声歌唱的需求日益增长。

Method: 提出CartoonSing统一框架，采用两阶段流程：使用标注人类歌唱数据训练的音符表示编码器，以及能够重建人类和非人声音频波形的音色感知声码器。

Result: 实验证明CartoonSing成功生成了非人声歌唱，能够泛化到新音色，并将传统SVS和SVC扩展到创意性的非人声歌唱生成。

Conclusion: CartoonSing框架有效解决了非人声歌唱生成中的数据稀缺、符号对齐缺失和人声-非人声音色差距等挑战，为创意应用提供了新的可能性。

Abstract: Singing voice synthesis (SVS) and singing voice conversion (SVC) have achieved remarkable progress in generating natural-sounding human singing. However, existing systems are restricted to human timbres and have limited ability to synthesize voices outside the human range, which are increasingly demanded in creative applications such as video games, movies, and virtual characters. We introduce Non-Human Singing Generation (NHSG), covering non-human singing voice synthesis (NHSVS) and non-human singing voice conversion (NHSVC), as a novel machine learning task for generating musically coherent singing with non-human timbral characteristics. NHSG is particularly challenging due to the scarcity of non-human singing data, the lack of symbolic alignment, and the wide timbral gap between human and non-human voices. To address these challenges, we propose CartoonSing, a unified framework that integrates singing voice synthesis and conversion while bridging human and non-human singing generation. CartoonSing employs a two-stage pipeline: a score representation encoder trained with annotated human singing and a timbre-aware vocoder that reconstructs waveforms for both human and non-human audio. Experiments demonstrate that CartoonSing successfully generates non-human singing voices, generalizes to novel timbres, and extends conventional SVS and SVC toward creative, non-human singing generation.

</details>


### [22] [Multi-Reward GRPO for Stable and Prosodic Single-Codebook TTS LLMs at Scale](https://arxiv.org/abs/2511.21270)
*Yicheng Zhong,Peiji Yang,Zhisheng Wang*

Main category: cs.SD

TL;DR: 提出多奖励组相对策略优化(GRPO)框架，直接优化单码本TTS大语言模型的token生成策略，解决韵律不稳定、说话人漂移和自然度下降问题。


<details>
  <summary>Details</summary>
Motivation: 单码本TTS LLMs虽然紧凑且可流式传输，但存在韵律不稳定、说话人漂移和自然度下降的问题，需要直接优化token生成策略来改善这些缺陷。

Method: 使用多奖励GRPO框架，整合三个基于规则的奖励：持续时间一致性的长度惩罚、解码稳定性的熵正则化奖励、以及LLM标注的韵律对齐奖励。在韵律奖励中，外部推理LLM通过上下文学习预测多个可能的停顿结构。

Result: 该方法一致地增强了单码本TTS LLMs的韵律稳定性、说话人相似性和整体语音自然度。在GRPO优化的AR骨干上附加流匹配解码器还能获得额外增益。

Conclusion: 提出的GRPO框架有效解决了单码本TTS LLMs的核心问题，通过强化学习优化内在AR策略，显著提升了语音合成质量。

Abstract: Recent advances in Large Language Models (LLMs) have transformed text-to-speech (TTS) synthesis, inspiring autoregressive frameworks that represent speech as sequences of discrete codec tokens. Among them, single-codebook TTS LLMs have emerged as compact and streamable architectures that jointly model semantic and acoustic integration. However, despite their efficiency, these models often exhibit unstable prosody, speaker drift, and degraded naturalness. To address these issues, we propose a multi-reward Group Relative Policy Optimization (GRPO) framework that directly optimizes the token generation policy of single-codebook TTS LLMs. Beyond standard intelligibility and speaker similarity objectives, our design integrates three rule-based rewards: a length penalty for duration consistency, an entropy regularization reward for decoding stability, and an LLM-annotated prosody alignment reward that explicitly supervises rhythm. In this prosody reward, an external reasoning LLM predicts multiple plausible pause structures via in-context learning, providing a human-preference-aligned supervisory signal for GRPO training. To assess universality, we further attach a flow-matching (FM) decoder on top of the GRPO-optimized AR backbone and observe consistent additional gains, indicating that our reinforcement optimization enhances the intrinsic AR policy. We further conduct a scalability analysis across data sizes and model scales, revealing that the proposed method consistently enhances prosodic stability, speaker similarity, and overall speech naturalness in single-codebook TTS LLMs.

</details>


### [23] [Acoustic neural networks: Identifying design principles and exploring physical feasibility](https://arxiv.org/abs/2511.21313)
*Ivan Kalthoff,Marcel Rey,Raphael Wittkowski*

Main category: cs.SD

TL;DR: 该论文提出了一个设计和模拟声学神经网络（ANN）的框架，通过声波传播进行计算，使用数字孪生方法在物理约束下训练神经网络，并开发了兼容被动声学组件的SincHSRNN混合模型，在AudioMNIST数据集上达到95%的准确率。


<details>
  <summary>Details</summary>
Motivation: 声波导物理系统为超越传统电子学的节能模拟计算提供了有前景的途径，但声学神经网络的系统设计方法尚未充分探索，需要建立连接可学习网络组件与物理可测量声学特性的通用框架。

Method: 采用数字孪生方法，在物理约束（非负信号和权重、无偏置项、兼容强度基非负声学信号的非线性）下训练传统神经网络架构，提出结合可学习声学带通滤波器和分层时间处理的SincHSRNN混合模型。

Result: 受限的循环和分层架构能够准确执行语音分类，SincHSRNN在AudioMNIST数据集上达到95%的准确率，同时保持与被动声学组件的兼容性，学习参数对应可测量的材料和几何特性。

Conclusion: 该研究为物理可实现的声学神经网络建立了通用设计原则，为低功耗、基于波的神经计算开辟了途径，学习参数直接映射到物理可测量特性，实现了从计算到物理实现的直接连接。

Abstract: Wave-guide-based physical systems provide a promising route toward energy-efficient analog computing beyond traditional electronics. Within this landscape, acoustic neural networks represent a promising approach for achieving low-power computation in environments where electronics are inefficient or limited, yet their systematic design has remained largely unexplored. Here we introduce a framework for designing and simulating acoustic neural networks, which perform computation through the propagation of sound waves. Using a digital-twin approach, we train conventional neural network architectures under physically motivated constraints including non-negative signals and weights, the absence of bias terms, and nonlinearities compatible with intensity-based, non-negative acoustic signals. Our work provides a general framework for acoustic neural networks that connects learnable network components directly to physically measurable acoustic properties, enabling the systematic design of realizable acoustic computing systems. We demonstrate that constrained recurrent and hierarchical architectures can perform accurate speech classification, and we propose the SincHSRNN, a hybrid model that combines learnable acoustic bandpass filters with hierarchical temporal processing. The SincHSRNN achieves up to 95% accuracy on the AudioMNIST dataset while remaining compatible with passive acoustic components. Beyond computational performance, the learned parameters correspond to measurable material and geometric properties such as attenuation and transmission. Our results establish general design principles for physically realizable acoustic neural networks and outline a pathway toward low-power, wave-based neural computing.

</details>


### [24] [SONAR: Spectral-Contrastive Audio Residuals for Generalizable Deepfake Detection](https://arxiv.org/abs/2511.21325)
*Ido Nitzan HIdekel,Gal lifshitz,Khen Cohen,Dan Raviv*

Main category: cs.SD

TL;DR: SONAR是一个频率引导的对比学习框架，通过显式解耦音频信号来提升深度伪造音频检测的泛化能力，特别关注高频残差特征。


<details>
  <summary>Details</summary>
Motivation: 解决深度伪造音频检测器在分布外输入上泛化能力差的问题，主要原因是频谱偏差导致神经网络忽略高频细节，而这些高频伪影正是深度伪造生成器留下的关键特征。

Method: 使用XLSR编码器捕获低频内容，同时通过可学习的SRM和值约束高通滤波器提取高频残差，通过频率交叉注意力融合两种表示，并使用频率感知的Jensen-Shannon对比损失进行优化。

Result: 在ASVspoof 2021和真实世界基准测试中达到最先进性能，收敛速度比强基线快四倍。

Conclusion: SONAR通过将微弱高频残差提升为一级学习信号，在潜在空间中分离出自然音频和合成音频的两个不相交流形，为任何依赖细微高频线索的模型提供了架构无关的解决方案。

Abstract: Deepfake (DF) audio detectors still struggle to generalize to out of distribution inputs. A central reason is spectral bias, the tendency of neural networks to learn low-frequency structure before high-frequency (HF) details, which both causes DF generators to leave HF artifacts and leaves those same artifacts under-exploited by common detectors. To address this gap, we propose Spectral-cONtrastive Audio Residuals (SONAR), a frequency-guided framework that explicitly disentangles an audio signal into complementary representations. An XLSR encoder captures the dominant low-frequency content, while the same cloned path, preceded by learnable SRM, value-constrained high-pass filters, distills faint HF residuals. Frequency cross-attention reunites the two views for long- and short-range frequency dependencies, and a frequency-aware Jensen-Shannon contrastive loss pulls real content-noise pairs together while pushing fake embeddings apart, accelerating optimization and sharpening decision boundaries. Evaluated on the ASVspoof 2021 and in-the-wild benchmarks, SONAR attains state-of-the-art performance and converges four times faster than strong baselines. By elevating faint high-frequency residuals to first-class learning signals, SONAR unveils a fully data-driven, frequency-guided contrastive framework that splits the latent space into two disjoint manifolds: natural-HF for genuine audio and distorted-HF for synthetic audio, thereby sharpening decision boundaries. Because the scheme operates purely at the representation level, it is architecture-agnostic and, in future work, can be seamlessly integrated into any model or modality where subtle high-frequency cues are decisive.

</details>


### [25] [Generating Separated Singing Vocals Using a Diffusion Model Conditioned on Music Mixtures](https://arxiv.org/abs/2511.21342)
*Genís Plaja-Roglans,Yun-Ning Hung,Xavier Serra,Igor Pereira*

Main category: cs.SD

TL;DR: 本文探索使用扩散模型从真实音乐录音中分离人声，通过训练模型根据混合音频生成独唱人声，在生成式系统中表现优异，并与非生成式基线方法竞争。


<details>
  <summary>Details</summary>
Motivation: 音乐混合物中分离单个元素对音乐分析和实践至关重要。虽然通常使用神经网络通过掩码或变换时频表示来提取目标源，但生成扩散模型的灵活性和泛化能力为这一复杂任务提供了新的解决方案。

Method: 使用扩散模型，训练模型根据对应的混合音频生成独唱人声。采用迭代扩散采样，允许用户控制质量-效率权衡并在需要时细化输出。

Result: 该方法在生成式系统中表现优异，当使用补充数据训练时，与非生成式基线方法在客观评分上具有竞争力。消融研究展示了采样算法中用户可配置参数的影响。

Conclusion: 扩散模型为音乐源分离提供了有效的生成式解决方案，其迭代采样特性使用户能够灵活控制输出质量和计算效率。

Abstract: Separating the individual elements in a musical mixture is an essential process for music analysis and practice. While this is generally addressed using neural networks optimized to mask or transform the time-frequency representation of a mixture to extract the target sources, the flexibility and generalization capabilities of generative diffusion models are giving rise to a novel class of solutions for this complicated task. In this work, we explore singing voice separation from real music recordings using a diffusion model which is trained to generate the solo vocals conditioned on the corresponding mixture. Our approach improves upon prior generative systems and achieves competitive objective scores against non-generative baselines when trained with supplementary data. The iterative nature of diffusion sampling enables the user to control the quality-efficiency trade-off, and also refine the output when needed. We present an ablation study of the sampling algorithm, highlighting the effects of the user-configurable parameters.

</details>


### [26] [HarmonicAttack: An Adaptive Cross-Domain Audio Watermark Removal](https://arxiv.org/abs/2511.21577)
*Kexin Li,Xiao Hu,Ilya Grishchenko,David Lie*

Main category: cs.SD

TL;DR: HarmonicAttack是一种高效的音频水印移除方法，通过双路径卷积自编码器和GAN训练，仅需目标水印方案的基本生成能力即可移除水印。


<details>
  <summary>Details</summary>
Motivation: AI生成的音频可能被滥用于错误信息传播和语音克隆欺诈，研究有效的水印移除技术对于客观评估音频水印的鲁棒性至关重要。

Method: 采用双路径卷积自编码器在时域和频域操作，结合GAN风格训练，从原始音频中分离水印。

Result: 在AudioSeal、WavMark和Silentcipher等先进水印方案上，HarmonicAttack展现出比先前方法更强的水印移除能力，且具有接近实时的性能。

Conclusion: HarmonicAttack是一种高效且可迁移的水印移除方法，能够客观评估音频水印方案的鲁棒性。

Abstract: The availability of high-quality, AI-generated audio raises security challenges such as misinformation campaigns and voice-cloning fraud. A key defense against the misuse of AI-generated audio is by watermarking it, so that it can be easily distinguished from genuine audio. As those seeking to misuse AI-generated audio may thus seek to remove audio watermarks, studying effective watermark removal techniques is critical to being able to objectively evaluate the robustness of audio watermarks against removal. Previous watermark removal schemes either assume impractical knowledge of the watermarks they are designed to remove or are computationally expensive, potentially generating a false sense of confidence in current watermark schemes.
  We introduce HarmonicAttack, an efficient audio watermark removal method that only requires the basic ability to generate the watermarks from the targeted scheme and nothing else. With this, we are able to train a general watermark removal model that is able to remove the watermarks generated by the targeted scheme from any watermarked audio sample. HarmonicAttack employs a dual-path convolutional autoencoder that operates in both temporal and frequency domains, along with GAN-style training, to separate the watermark from the original audio. When evaluated against state-of-the-art watermark schemes AudioSeal, WavMark, and Silentcipher, HarmonicAttack demonstrates greater watermark removal ability than previous watermark removal methods with near real-time performance. Moreover, while HarmonicAttack requires training, we find that it is able to transfer to out-of-distribution samples with minimal degradation in performance.

</details>


### [27] [Harmonic-Percussive Disentangled Neural Audio Codec for Bandwidth Extension](https://arxiv.org/abs/2511.21580)
*Benoît Giniès,Xiaoyu Bie,Olivier Fercoq,Gaël Richard*

Main category: cs.SD

TL;DR: 本文提出了一种基于音频token预测的带宽扩展方法，使用解耦神经音频编解码器和transformer语言模型，通过谐波-打击乐分解指导解耦过程，实现高质量的音频重建。


<details>
  <summary>Details</summary>
Motivation: 带宽扩展是从低通音频信号重建高频分量的长期问题。传统方法随着信号处理发展而演进，但神经网络架构的进步为音频任务带来了显著性能提升。本文旨在将这些进展扩展到带宽扩展任务中。

Method: 将带宽扩展构建为音频token预测问题，在解耦神经音频编解码器产生的离散表示上训练基于transformer的语言模型。编解码器的解耦由输入信号的谐波-打击乐分解指导，突出对带宽扩展特别相关的频谱结构。

Result: 该方法在客观指标和主观评估中都实现了原始信号的高质量重建，证明了编解码器解耦与生成建模阶段对齐的重要性。

Conclusion: 联合设计实现了编解码器结构与transformer建模的更有效耦合，展示了全局、表示感知设计在推进带宽扩展方面的潜力。

Abstract: Bandwidth extension, the task of reconstructing the high-frequency components of an audio signal from its low-pass counterpart, is a long-standing problem in audio processing. While traditional approaches have evolved alongside the broader trends in signal processing, recent advances in neural architectures have significantly improved performance across a wide range of audio tasks, In this work, we extend these advances by framing bandwidth extension as an audio token prediction problem. Specifically, we train a transformer-based language model on the discrete representations produced by a disentangled neural audio codec, where the disentanglement is guided by a Harmonic-Percussive decomposition of the input signals, highlighting spectral structures particularly relevant for bandwidth extension. Our approach introduces a novel codec design that explicitly accounts for the downstream token prediction task, enabling a more effective coupling between codec structure and transformer modeling. This joint design yields high-quality reconstructions of the original signal, as measured by both objective metrics and subjective evaluations. These results highlight the importance of aligning codec disentanglement and representation learning with the generative modeling stage, and demonstrate the potential of global, representation-aware design for advancing bandwidth extension.

</details>

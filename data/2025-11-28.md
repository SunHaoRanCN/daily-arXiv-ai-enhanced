<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 13]
- [eess.AS](#eess.AS) [Total: 4]
- [cs.SD](#cs.SD) [Total: 10]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [WiRainbow: Single-Antenna Direction-Aware Wi-Fi Sensing via Dispersion Effect](https://arxiv.org/abs/2511.20671)
*Zhaoxin Chang,Shuguang Xiao,Fusang Zhang,Xujun Ma,Badii Jouaber,Qingfeng Zhang,Daqing Zhang*

Main category: eess.SP

TL;DR: WiRainbow是一种利用频率扫描天线(FSA)色散效应的单天线Wi-Fi方向感知方法，通过耦合谐振器天线架构扩展视野范围，在复杂多径环境中实现准确、鲁棒且经济的方向感知。


<details>
  <summary>Details</summary>
Motivation: 现有Wi-Fi方向估计方法依赖昂贵复杂的天线阵列，难以在实际场景中部署。需要开发低成本、单天线解决方案来获取目标方向信息，为各种应用提供有价值的上下文洞察。

Method: 利用频率扫描天线的色散效应，使Wi-Fi子载波在传输时自然指向不同角度；提出耦合谐振器天线架构扩展传统FSA的窄视野；开发基于信噪比的信号处理框架在复杂多径环境中可靠估计目标方向。

Result: 通过基准实验和真实案例研究验证，WiRainbow能够为多种Wi-Fi传感应用实现准确、鲁棒且经济高效的方向感知。

Conclusion: WiRainbow提供了一种创新的单天线Wi-Fi方向感知解决方案，克服了传统天线阵列的成本和部署复杂性限制，具有实际应用价值。

Abstract: Recently, Wi-Fi signals have emerged as a powerful tool for contactless sensing. During the sensing process, obtaining target direction information can provide valuable contextual insights for various applications. Existing direction estimation methods typically rely on antenna arrays, which are costly and complex to deploy in real-world scenarios. In this paper, we present WiRainbow, a novel approach that enables single-antenna-based direction awareness for Wi-Fi sensing by leveraging the dispersion effect of frequency-scanning antennas (FSAs), which can naturally steer Wi-Fi subcarriers toward distinct angles during signal transmission. To address key challenges in antenna design and signal processing, we propose a coupled-resonator-based antenna architecture that significantly expands the narrow Field-of-View inherent in conventional FSAs, improving sensing coverage. Additionally, we develop a sensing signal-to-noise-ratio-based signal processing framework that reliably estimates target direction in multipath-rich environments. We prototype WiRainbow and evaluate its performance through benchmark experiments and real-world case studies, demonstrating its ability to achieve accurate, robust, and cost-effective direction awareness for diverse Wi-Fi sensing applications.

</details>


### [2] [A Fully Multivariate Multifractal Detrended Fluctuation Analysis Method for Fault Diagnosis](https://arxiv.org/abs/2511.20831)
*Khuram Naveed,Naveed ur Rehman*

Main category: eess.SP

TL;DR: 提出了完全多变量MFDFA方法(FM-MFDFA)，基于马氏距离的协方差加权Lpq矩阵范数定义多变量波动函数，结合MVMD进行故障诊断，在风力涡轮机变速箱数据上优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统MFDFA方法无法有效捕捉多通道振动数据中的跨通道依赖关系和方差偏差，需要开发能准确表征多变量信号多尺度结构的完全多变量方法。

Method: 引入基于马氏距离的协方差加权Lpq矩阵范数定义完全多变量波动函数，结合多变量变分模态分解(MVMD)分离故障相关分量，然后应用FM-MFDFA进行特征提取。

Result: 在风力涡轮机变速箱数据上的实验表明，该方法能有效区分健康和故障状态，即使在噪声条件下也优于传统MFDFA方法。

Conclusion: FM-MFDFA框架通过捕获跨通道依赖关系，提供了更准确的多变量信号多尺度结构表征，在多通道机械振动故障诊断中表现出优越性能。

Abstract: We propose a fully multivariate generalization of multifractal detrended fluctuation analysis (MFDFA) and leverage it to develop a fault diagnosis framework for multichannel machine vibration data. We introduce a novel covariance-weighted $L_{pq}$ matrix norm based on Mahalanobis distance to define a fully multivariate fluctuation function that uniquely captures cross-channel dependencies and variance biases in multichannel vibration data. This formulation, termed FM-MFDFA, allows for a more accurate characterization of the multiscale structure of multivariate signals. To enhance feature relevance, the proposed framework integrates multivariate variational mode decomposition (MVMD) to isolate fault-relevant components before applying FM-MFDFA. Results on wind turbine gearbox data demonstrate that the proposed method outperforms conventional MFDFA approaches by effectively distinguishing between healthy and faulty machine states, even under noisy conditions.

</details>


### [3] [Wavelet-Guided Water-Level Estimation for ISAC](https://arxiv.org/abs/2511.20936)
*Ayoob Salari,Kai Wu,Khawaja Fahad Masood,Y. Jay Guo,J. Andrew Zhang*

Main category: eess.SP

TL;DR: 提出一种基于LTE下行功率指标的被动式低成本水位监测方法，利用连续小波变换提取潮汐特征，结合轻量级神经网络实现高精度水位跟踪。


<details>
  <summary>Details</summary>
Motivation: 传统水位监测方法依赖固定设备，成本高且易受极端事件影响，需要一种低成本、易部署的替代方案。

Method: 使用LTE接收器报告的RSRP、RSSI和RSRQ指标，应用连续小波变换提取半日潮分量，形成潮汐特征，结合轻量级神经网络学习水位变化。支持多基站协作模式提升稳定性。

Result: 在420米河流路径上，视距条件下达到0.8cm RMSE和0.5cm MAE；非视距条件下经微调后达到1.7cm RMSE和0.8cm MAE。

Conclusion: 该方法无需阵列校准，可在标准硬件上运行，支持多基站融合，为大规模水位监测提供了实用解决方案。

Abstract: Real-time water-level monitoring across many locations is vital for flood response, infrastructure management, and environmental forecasting. Yet many sensing methods rely on fixed instruments - acoustic, radar, camera, or pressure probes - that are costly to install and maintain and are vulnerable during extreme events. We propose a passive, low-cost water-level tracking scheme that uses only LTE downlink power metrics reported by commodity receivers. The method extracts per-antenna RSRP, RSSI, and RSRQ, applies a continuous wavelet transform (CWT) to the RSRP to isolate the semidiurnal tide component, and forms a summed-coefficient signature that simultaneously marks high/low tide (tide-turn times) and tracks the tide-rate (flow speed) over time. These wavelet features guide a lightweight neural network that learns water-level changes over time from a short training segment. Beyond a single serving base station, we also show a multi-base-station cooperative mode: independent CWTs are computed per carrier and fused by a robust median to produce one tide-band feature that improves stability and resilience to local disturbances. Experiments over a 420 m river path under line-of-sight conditions achieve root-mean-square and mean-absolute errors of 0.8 cm and 0.5 cm, respectively. Under a non-line-of-sight setting with vegetation and vessel traffic, the same model transfers successfully after brief fine-tuning, reaching 1.7 cm RMSE and 0.8 cm MAE. Unlike CSI-based methods, the approach needs no array calibration and runs on standard hardware, making wide deployment practical. When signals from multiple base stations are available, fusion further improves robustness.

</details>


### [4] [Evaluating the Performance of a Modified Skin Temperature Sensor for Lower Limb Prostheses: An Experimental Comparison](https://arxiv.org/abs/2511.21068)
*Anirshu Devroy,Gregor Fritz,Mathias Brandstoetter*

Main category: eess.SP

TL;DR: 本文研究用于假肢系统户外温度监测的改进型热敏电阻，通过实验比较常规热敏电阻和改进型热敏电阻的性能，旨在开发舒适的可穿戴传感器实时监测皮肤温度。


<details>
  <summary>Details</summary>
Motivation: 当前下肢假肢康复面临皮肤状况、刺激和不适等挑战，需要开发能够实时监测皮肤温度的舒适可穿戴传感器，为用户和矫形技师提供反馈以改进假肢适配。

Method: 进行一系列实验来理解和表征系统行为，比较常规热敏电阻和改进型热敏电阻作为假肢户外使用温度测量方法的性能。

Result: 初步结果显示部分改进型热敏电阻相比其他类型表现出更好的温度记录性能。

Conclusion: 改进型热敏电阻可以作为嵌入假肢系统的舒适温度测量的潜在替代方案，为温度分布提供有价值见解并作为皮肤问题的早期预警系统。

Abstract: Current rehabilitation of lower limb prostheses has significant challenges, especially with skin conditions, irritation and discomfort. Understanding the skin temperature and having comfortable wearable sensors that would monitor skin temperature in a real-time outdoor environment would be useful. The system would help the user and orthopedic technician to provide feedback and changes that might be required in the prosthesis. Hence in this paper, a series of experiments are conducted in order to understand and characterize the system behavior and compare a general thermistor and a modified thermistor as a potential method of temperature measurement for outdoor usage of prostheses. The paper goes on to compare the different modified thermistors behavior with their regular counterpart and highlights the challenges and improvement areas needed for such a modified thermistor for outdoor temperature monitoring in a prosthetic system. Initial results show that some of the modified thermistors showed better temperature recording compared to the rest. Finally, such modified thermistors can be a potential alternative for comfortable temperature measurement embedded in the prosthesis system. Such a system can provide valuable insights into temperature distribution and an early warning system for skin problems

</details>


### [5] [Data-Driven Assessment of Concrete Slab Integrity via Impact-Echo Signals and Neural Networks](https://arxiv.org/abs/2511.21080)
*Yeswanth Ravichandran,Duoduo Liao,Charan Teja Kurakula*

Main category: eess.SP

TL;DR: 提出基于机器学习的冲击回波框架，自动定位混凝土缺陷并进行多类别分类，包括浅层剥离、深层剥离、空洞和蜂窝状缺陷，准确率达73%。


<details>
  <summary>Details</summary>
Motivation: 混凝土桥面板中的地下缺陷（如剥离、空洞和蜂窝状缺陷）严重影响耐久性，但视觉检查或手动敲击难以可靠检测。

Method: 将原始冲击回波信号通过FFT转换为峰值频率特征并插值为空间图，使用k-means聚类突出缺陷区域，构建空间有序的峰值频率序列输入堆叠LSTM网络进行分类。

Result: 在桥面板上的现场验证表明，基于实验室数据训练的模型在真实耦合、噪声和环境变化下具有良好的泛化能力，整体分类准确率达73%。

Conclusion: 该框架提高了无损评估的客观性、可扩展性和可重复性，支持网络规模的智能、数据驱动的桥梁健康监测。

Abstract: Subsurface defects such as delamination, voids, and honeycombing critically affect the durability of concrete bridge decks but are difficult to detect reliably using visual inspection or manual sounding. This paper presents a machine learning based Impact Echo (IE) framework that automates both defect localization and multi-class classification of common concrete defects. Raw IE signals from Federal Highway Administration (FHWA) laboratory slabs and in-service bridge decks are transformed via Fast Fourier Transform (FFT) into dominant peak-frequency features and interpolated into spatial maps for defect zone visualization. Unsupervised k-means clustering highlights low-frequency, defect-prone regions, while Ground Truth Masks (GTMs) derived from seeded lab defects are used to validate spatial accuracy and generate high-confidence training labels. From these validated regions, spatially ordered peak-frequency sequences are constructed and fed into a stacked Long Short-Term Memory (LSTM) network that classifies four defect types shallow delamination, deep delamination, voids, and honeycombing with 73% overall accuracy. Field validation on the bridge deck demonstrates that models trained on laboratory data generalize under realistic coupling, noise, and environmental variability. The proposed framework enhances the objectivity, scalability, and repeatability of Non-Destructive Evaluation (NDE), supporting intelligent, data-driven bridge health monitoring at a network scale.

</details>


### [6] [2D Sparse Array Design via Reweighted L1 Second Order Cone Programming for 3D Ultrasound Imaging](https://arxiv.org/abs/2511.21133)
*Xi Zhang,Miguel Bernal,Wei-Ning Lee*

Main category: eess.SP

TL;DR: 提出一种基于二阶锥规划和重加权L1技术的稀疏阵列设计方法，用于减少3D超声成像中的通道数量，在保持分辨率的同时优化对比度性能。


<details>
  <summary>Details</summary>
Motivation: 传统全寻址2D阵列需要数千个独立通道，成本高昂。现有随机优化方法设计的稀疏阵列结果不稳定，需要更可靠的设计方法。

Method: 将稀疏阵列合成问题建模为二阶锥规划问题，并采用重加权L1技术进行序列优化，设计具有准平坦旁瓣的2D稀疏阵列。

Result: 成功设计出252个激活元素的Q-Flats阵列，旁瓣电平≤-21.26dB。与密集阵列、费马螺旋阵列和锥化螺旋阵列相比，Q-Flats在分辨率上优于螺旋阵列约3%，对比度略差。

Conclusion: 重加权L1 SOCP方法是一种有前景且灵活的方法，可在分辨率、对比度和激活元素数量之间寻求平衡。

Abstract: Two-dimensional (2D) fully-addressed arrays can conveniently realize three-dimensional (3D) ultrasound imaging while fully controlled such arrays usually demands thousands of independent channels, which is costly. Sparse array technique using stochastic optimization methods is one of promising techniques to reduce channel counts while due to the stochastic nature of these methods, the optimized results are usually unstable. In this work, we introduce a sparse array design approach that formulates the synthesis problem of sparse arrays as second-order cone programming (SOCP) and a re-weighted L1 technique is implemented to sequentially optimize the SOCP. Based on this method, an on-grid quasi-flatten side-lobe (Q-Flats) 2D sparse array with side-lobe level (SLL) no more than -21.26 dB and 252 activated elements is designed, which aims to achieve as high contrast performance as possible under the limits of resolution and maximum number of independent channels (i.e., 256). The imaging performance of the Q-Flats array was compared with those of a corresponding dense array (Dense), a Fermat spiral array (Spiral) and a spatially 50%-Tukey tapered spiral array (Spiral-Taper) using Field II simulations in a multi-angle steered diverging wave transmission scheme. It was demonstrated that the Dense achieved the best resolution and contrast and the Spiral-Taper the worst. The Q-Flats showed better resolution (about 3%) but slightly worse contrast than the Spiral. All the results indicate the re-weighted L1 SOCP method is a promising and flexible method for seeking trade-offs among resolution, contrast, and number of activated elements.

</details>


### [7] [Multiport Analytical Pixel Electromagnetic Simulator (MAPES) for AI-assisted RFIC and Microwave Circuit Design](https://arxiv.org/abs/2511.21274)
*Junhui Rao,Yi Liu,Jichen Zhang,Zhaoyang Ming,Tianrui Qiao,Yujie Zhang,Chi Yuk Chiu,Hua Wang,Ross Murch*

Main category: eess.SP

TL;DR: MAPES是一种新颖的多端口分析像素电磁模拟器，能够高效准确地预测任意基于像素的微波和RFIC结构的电磁性能，仅需约1%的全波仿真数据即可构建多端口阻抗矩阵，实现600-2000倍的速度提升。


<details>
  <summary>Details</summary>
Motivation: 传统AI辅助电磁设计需要大量数据集，存在过拟合问题且计算效率低，需要一种更高效准确的方法来预测像素基微波和RFIC结构的电磁性能。

Method: 基于集成内部多端口方法，引入虚拟像素和对角虚拟像素，在关键位置插入虚拟端口，通过少量全波仿真构建包含所有水平、垂直和对角电磁耦合的多端口阻抗矩阵。

Result: 在单层和双层CMOS工艺及PCB上验证，MAPES实现了高预测精度，相比CST仿真速度提升600-2000倍，消除了数据驱动的过拟合问题。

Conclusion: MAPES因其高效性、可扩展性和可靠性，为跨多种制造技术的AI辅助微波电路和RFIC设计提供了实用且通用的工具。

Abstract: This paper proposes a novel analytical framework, termed the Multiport Analytical Pixel Electromagnetic Simulator (MAPES). MAPES enables efficient and accurate prediction of the electromagnetic (EM) performance of arbitrary pixel-based microwave (MW) and RFIC structures. Inspired by the Integrated Internal Multiport Method (IMPM), MAPES extends the concept to the pixel presence/absence domain used in AI-assisted EM design. By introducing virtual pixels and diagonal virtual pixels and inserting virtual ports at critical positions, MAPES captures all horizontal, vertical, and diagonal electromagnetic couplings within a single multiport impedance matrix. Only a small set of full-wave simulations (typically about 1% of the datasets required by AI-assisted EM simulators) is needed to construct this matrix. Subsequently, any arbitrary pixel configuration can be evaluated analytically using a closed-form multiport relation without additional full-wave calculations. The proposed approach eliminates data-driven overfitting and ensures accurate results across all design variations. Comprehensive examples for single- and double-layer CMOS processes (180 nm and 65 nm) and PCBs confirm that MAPES achieves high prediction accuracy with 600- 2000x speed improvement compared to CST simulations. Owing to its efficiency, scalability and reliability, MAPES provides a practical and versatile tool for AI-assisted MW circuit and RFIC design across diverse fabrication technologies.

</details>


### [8] [Phase-Aware Code-Aided EM Algorithm for Blind Channel Estimation in PSK-Modulated OFDM](https://arxiv.org/abs/2511.21340)
*Chin-Hung Chen,Ivana Nikoloska,Wim van Houtum,Yan Wu,Alex Alvarado*

Main category: eess.SP

TL;DR: 提出了一种针对PSK调制OFDM系统的全盲相位感知EM算法，通过利用解码器的外部信息作为模型证据度量，解决传统盲EM估计器无法解决的相位模糊问题。


<details>
  <summary>Details</summary>
Motivation: 解决盲信道估计中EM算法的局部最大值问题，该问题主要由信道估计中的未知相位模糊引起，传统盲EM估计器无法解决这一限制。

Method: 基于PSK调制固有对称性生成有限候选模型集，利用解码器选择最可能的候选模型，在EM初始化阶段后仅调用一次该算法。

Result: 仿真结果表明，当与简单卷积码结合时，相位感知EM算法在初始化阶段可靠地解决相位模糊，在具有恒定相位模糊的频率选择性信道中，将局部收敛率从80%降低到接近0%。

Conclusion: 该算法在后续turbo迭代中仅产生可忽略的额外复杂度，有效解决了盲信道估计中的相位模糊问题。

Abstract: This paper presents a fully blind phase-aware expectation-maximization (EM) algorithm for OFDM systems with the phase-shift keying (PSK) modulation. We address the well-known local maximum problem of the EM algorithm for blind channel estimation. This is primarily caused by the unknown phase ambiguity in the channel estimates, which conventional blind EM estimators cannot resolve. To overcome this limitation, we propose to exploit the extrinsic information from the decoder as model evidence metrics. A finite set of candidate models is generated based on the inherent symmetries of PSK modulation, and the decoder selects the most likely candidate model. Simulation results demonstrate that, when combined with a simple convolutional code, the phase-aware EM algorithm reliably resolves phase ambiguity during the initialization stage and reduces the local convergence rate from 80% to nearly 0% in frequency-selective channels with a constant phase ambiguity. The algorithm is invoked only once after the EM initialization stage, resulting in negligible additional complexity during subsequent turbo iterations.

</details>


### [9] [Blind Turbo Demodulation for Differentially Encoded OFDM with 2D Trellis Decomposition](https://arxiv.org/abs/2511.21345)
*Chin-Hung Chen,Yan Wu,Wim van Houtum,Alex Alvarado*

Main category: eess.SP

TL;DR: 提出了一种完全盲的turbo-DE-PSK方案，无需导频即可联合估计信道相位、增益和噪声方差，在DAB类系统中实现了接近完美信道知识的性能。


<details>
  <summary>Details</summary>
Motivation: DAB类系统使用差分编码PSK传输，turbo-DE-PSK接收机通过迭代解码提供性能增益，但依赖无导频的准确信道估计，这是DAB类场景中的关键挑战。

Method: 开发了完全盲的turbo-DE-PSK方案，利用二维网格分解进行盲相位估计，辅以基于功率的信道增益和噪声方差估计器。

Result: 仿真结果显示，盲2D turbo解调器接近完美信道知识接收机的性能，并在实际传输条件下保持鲁棒性。

Conclusion: 所提出的盲turbo-DE-PSK方案有效解决了DAB类系统中无导频信道估计的挑战，实现了高性能的盲接收。

Abstract: Digital Audio Broadcasting (DAB)-like systems employ differentially encoded (DE) phase-shift keying (PSK) for transmission. While turbo-DE-PSK receivers offer substantial performance gains through iterative decoding by making the DE-PSK an inner code, they rely on accurate channel estimation without pilots, which is a key challenge in DAB-like scenarios. This paper develops a fully blind turbo-DE-PSK scheme that jointly estimates channel phase, channel gain, and noise variance directly from the received signal. The design leverages a two-dimensional (2D) trellis decomposition for blind phase estimation, complemented by power-based estimators for channel gain and noise variance. We provide a comprehensive system assessment across practical system parameters, including inner code length, phase quantization, and 2D block size. Simulation results show that the blind 2D turbo demodulator approaches the performance of receivers with perfect channel knowledge and remains robust under realistic transmission conditions.

</details>


### [10] [Group-wise Semantic Splitting Multiple Access for Multi-User Semantic Communication](https://arxiv.org/abs/2511.21411)
*Jungyeon Koh,Hyeonho Noh,Hyun Jong Yang*

Main category: eess.SP

TL;DR: 提出了一种基于用户语义特征相似度的分组多址接入框架，通过聚类提取组级公共特征和用户特定私有特征，分别采用组播和单播传输，结合重构损失和排斥损失的复合损失函数提升语义分离性和重建质量。


<details>
  <summary>Details</summary>
Motivation: 解决多用户语义通信中如何有效利用共享语义信息和用户特定信息的问题，提高语义传输效率和重建质量。

Method: 使用平衡聚类机制按语义特征相似度对用户分组，提取组级公共特征和用户私有特征；基站通过组播传输公共特征，单播传输私有特征；设计包含重构损失和排斥损失的复合损失函数。

Result: 在各种信道条件下，相比传统方案性能提升最高达3.26%，验证了方法的鲁棒性和语义效率。

Conclusion: 该框架能有效提升多用户语义通信的性能，适用于下一代无线网络。

Abstract: In this letter, we propose a group-wise semantic splitting multiple access framework for multi-user semantic communication in downlink scenarios. The framework begins by applying a balanced clustering mechanism that groups users based on the similarity of their semantic characteristics, enabling the extraction of group-level common features and user-specific private features. The base station then transmits the common features via multicast and the private features via unicast, effectively leveraging both shared and user-dependent semantic information. To further enhance semantic separability and reconstruction fidelity, we design a composite loss function that integrates a reconstruction loss with a repulsion loss, improving both the accuracy of semantic recovery and the distinctiveness of common embeddings in the latent space. Simulation results demonstrate that the proposed method achieves up to 3.26% performance improvement over conventional schemes across various channel conditions, validating its robustness and semantic efficiency for next-generation wireless networks.

</details>


### [11] [Design Of A Communication System To Send Text Using Lora At 400 MHz](https://arxiv.org/abs/2511.21434)
*Fabrizio André Farfán Prado,William César Pérez Campos,Steisy Anahi Carreño Tacuri,Favio David Cabrera Alva,Harold Jacobed Carhuas Lizarbe*

Main category: eess.SP

TL;DR: 基于ESP32和LoRa DXLR01模块的低功耗无线通信系统，用于在缺乏传统网络基础设施的区域传输文本数据


<details>
  <summary>Details</summary>
Motivation: 解决农村和某些城市环境中Wi-Fi或移动网络不可用或受限时的连接性和能效问题

Method: 集成LoRa技术（433MHz频段）与ESP32模块，使用Chirp扩频调制提高信号穿透性和抗干扰能力，并将接收数据发送到ThingSpeak平台

Result: 在受控环境中测试显示文本传输平均延迟为3.2秒，系统可应用于远程监控、基础设施管理和访问控制

Conclusion: 该系统提供了一种在传统网络基础设施不可用情况下的有效通信解决方案，具有长距离传输和低功耗的优势

Abstract: This work describes the design and implementation of a low-power wireless communication system for transmitting text using ESP32 modules and the LoRa DXLR01. The proposal arises as a solution to connectivity and energy-efficiency problems commonly found in rural areas and certain urban environments where Wi-Fi or mobile networks are unavailable or operate with limitations. To address this, LoRa technology known for its long-range capability and low power consumption is integrated with an ESP32 responsible for capturing, processing, and sending messages.
  The LoRa DXLR01 module, which operates in the 433 MHz band, is configured with parameters aimed at maximising both transmission range and efficient energy usage. Messages are sent using Chirp Spread Spectrum (CSS) modulation, improving signal penetration in obstructed areas and reducing the likelihood of errors. On the receiving end, the ESP32 interprets the data and displays it on an LCD screen. Additionally, the received information is sent to the ThingSpeak platform, allowing remote storage and visualisation without relying on conventional network infrastructure.
  Tests conducted in a controlled environment show an average latency of 3.2 seconds for text transmission. It was also verified that the system can be used in applications such as remote monitoring, infrastructure management, and access control.

</details>


### [12] [SIR Analysis for Affine Filter Bank Modulation](https://arxiv.org/abs/2511.21615)
*Henrique L. Senger,Gustavo P. Gonçalves,Bruno S. Chang,Hyeon Seok Rou,Kuranage Roche Rayan Ranasinghe,Giuseppe Thadeu Freitas de Abreu,Didier Le Ruyet*

Main category: eess.SP

TL;DR: 分析了AFBM波形在MMSE均衡下的SIR性能，发现在滤波时域中会出现干扰与正交近似误差的意外抵消现象，这解释了滤波时域检测方案相比仿射域等效方案的显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 研究AFBM波形在不同域（仿射域和滤波时域）中的SIR性能差异，特别是探索滤波时域中出现的干扰抵消现象及其对BER性能的影响。

Method: 在MMSE均衡条件下，分别在仿射域和滤波时域分析AFBM波形的SIR性能，结合DAFT和解扩/映射操作，研究干扰与正交近似误差的相互作用。

Result: 发现滤波时域中存在干扰与正交近似误差的意外抵消现象，这种现象在仿射域中不会发生，导致滤波时域检测方案具有显著的性能优势。

Conclusion: 滤波时域检测方案通过干扰抵消机制实现了比仿射域等效方案更好的BER性能，这为AFBM波形的优化设计提供了重要指导。

Abstract: The signal-to-interference ratio (SIR) of the Affine Filter Bank Modulation (AFBM) waveform is analyzed under minimum mean square error (MMSE) equalization in two domains; namely, the affine domain and the filtered time-domain (TD). Due to the incorporation of the discrete affine Fourier transform (DAFT) and despreading/mapping, an interesting and counter-intuitive cancellation of the unwanted combination of the channel induced interference with the orthogonality approximation error is seen in the filtered TD, a process which does not occur in the affine domain. The direct impact on bit error rate (BER) provides a thorough validation of the proposed analysis and explains the substantial gains in performance of the filtered TD detection scheme as opposed to its affine domain equivalent

</details>


### [13] [Optimal Bit Detection in Thermal Noise Communication Systems Under Rician Fading](https://arxiv.org/abs/2511.21649)
*Mohamed El Jbari,Fernando D. A. García,Hugerles S. Silva,Felipe A. P. de Figueiredo,Rausley A. A. de Souza*

Main category: eess.SP

TL;DR: 提出了在Rician衰落信道下热噪声通信系统的最优比特检测分析框架，使用卡方统计推导最大似然检测阈值和误码率表达式，相比基于高斯近似的次优检测显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有热噪声通信分析多依赖高斯近似且忽略衰落效应，限制了分析精度，需要建立更准确的分析框架来支持未来B5G/6G和大规模物联网系统的能效接收机设计。

Method: 使用卡方统计推导最优最大似然检测阈值，通过高斯-拉盖尔求积法获得误码率表达式，并采用蒙特卡洛仿真验证分析结果。

Result: 提出的模型消除了近似误差，能准确表征有限样本量下的性能，相比次优高斯检测显著改善误码率，并量化了样本量、电阻比和Rician K因子等关键参数的影响。

Conclusion: 该分析框架为未来B5G/6G和大规模物联网系统中设计能效热噪声通信接收机提供了坚实基础。

Abstract: Thermal noise communication (TNC) enables ultra-low-power wireless links for Internet of Things (IoT) devices by modulating the variance of thermal noise, rather than using active carriers. Existing analyses often rely on Gaussian approximations and overlook fading effects, which limits their accuracy. This paper presents an accurate analytical framework for optimal bit detection in TNC systems under Rician fading. Using chi-squared statistics, we derive the optimal maximum-likelihood detection threshold and an expression for the bit error probability (BEP) via Gauss-Laguerre quadrature. The proposed model eliminates approximation errors and accurately characterizes performance for finite sample sizes. Monte Carlo simulations confirm the analytical results and demonstrate significant improvements in BEP compared with suboptimal Gaussian-based detection. Furthermore, the influence of key parameters, sample size, resistance ratio, and Rician K-factor, is quantified. The proposed framework provides a solid foundation for designing energy-efficient TNC receivers in future B5G/6G and large-scale IoT systems.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [14] [Towards Audio Token Compression in Large Audio Language Models](https://arxiv.org/abs/2511.20973)
*Saurabhchand Bhati,Samuel Thomas,Hilde Kuehne,Rogerio Feris,James Glass*

Main category: eess.AS

TL;DR: 该论文提出通过无监督分割、均匀平均池化等技术压缩LALMs的音频token数量，并使用低秩适配器微调以缓解性能下降，在语音识别和语音翻译任务上实现接近帧级性能的同时将音频token数量减少三倍。


<details>
  <summary>Details</summary>
Motivation: 大型音频语言模型(LALMs)面临注意力机制二次复杂度和音频信号高token率的可扩展性限制，难以应用于长音频和在资源受限平台上部署。

Method: 采用无监督分割、均匀平均池化等技术减少音频编码器生成的token数量，使用低秩适配器进行微调以保持模型性能。

Result: 实验结果显示压缩后的LALMs在语音识别和语音翻译任务上能接近帧级LALMs的性能，同时将输入音频token数量减少三倍。

Conclusion: 通过token压缩和适配器微调，可以有效解决LALMs的可扩展性问题，在保持性能的同时显著减少计算需求。

Abstract: Large Audio Language Models (LALMs) demonstrate impressive performance across diverse tasks, ranging from speech recognition to general audio understanding. However, their scalability is limited by the quadratic complexity of attention and the high token rates of audio signals. These challenges make it difficult to extend LALMs to long-form audio and to deploy them on resource-constrained platforms such as edge devices.
  In this paper, we explore techniques such as unsupervised segmentation, uniform average pooling, etc., to reduce the number of audio tokens generated by the LALM's audio encoder but before they are consumed by the LLM decoder. To mitigate potential performance degradation introduced by the compressed representations, we employ low-rank adapters to finetune the model. We evaluate our proposed models on two tasks, automatic speech recognition and speech-to-speech translation tasks, that are dependent on effectively uncovering the underlying lexical content of the input signal and study the effect of downsampling on these tasks. Experimental results show that compressed LALMs can achieve performance closer to frame-level LALMs while reducing the input audio token count upto three times before the LLM backbone.

</details>


### [15] [RosettaSpeech: Zero-Shot Speech-to-Speech Translation from Monolingual Data](https://arxiv.org/abs/2511.20974)
*Zhisheng Zheng,Xiaohang Sun,Tuan Dinh,Abhishek Yanamandra,Abhinav Jain,Zhu Liu,Sunil Hadap,Vimal Bhat,Manoj Aggarwal,Gerard Medioni,David Harwath*

Main category: eess.AS

TL;DR: RosettaSpeech是一个新颖的零样本语音到语音翻译框架，仅使用单语语音-文本数据和机器翻译监督进行训练，无需平行语音对，在推理时实现端到端语音翻译。


<details>
  <summary>Details</summary>
Motivation: 平行语音语料库的稀缺严重阻碍了语音到语音翻译的发展，迫使依赖复杂多阶段流水线。本文旨在通过仅使用单语数据和机器翻译监督来简化S2ST流程。

Method: 使用文本作为训练时的中间桥梁，利用文本NMT模型的语言知识，但推理时作为直接的端到端语音到语音模型工作。通过机器翻译监督增强单语语音-文本数据。

Result: 在CVSS-C测试集上达到SOTA结果：德语到英语ASR-BLEU 25.17（相对提升27%），西班牙语到英语29.86（相对提升14%）。单个模型可实现多对一翻译（FR/ES/DE -> EN）。

Conclusion: 通过依赖丰富的平行文本而非难以获取的平行语音，RosettaSpeech为创建高质量、保留说话人特征的S2ST系统提供了可扩展的路径，适用于更广泛的语言。

Abstract: The scarcity of parallel speech corpora critically hampers speech-to-speech translation (S2ST), often forcing reliance on complex, multi-stage pipelines. This paper introduces RosettaSpeech, a novel and simplified framework for zero-shot S2ST that is trained on monolingual speech-text data augmented by machine translation supervision. While our method leverages the linguistic knowledge inherent in text-based NMT models, it strictly eliminates the need for parallel speech-to-speech pairs. Our model uniquely uses text as an intermediate bridge during training but functions as a direct, end-to-end speech-to-speech model at inference. This streamlined approach achieves state-of-the-art results on standard benchmarks. For instance, on the CVSS-C test set, RosettaSpeech outperforms leading systems, achieving an ASR-BLEU score of 25.17 for German-to-English and 29.86 for Spanish-to-English-relative gains of over 27% and 14%, respectively. Furthermore, we demonstrate that a single model can deliver strong many-to-one translation performance (FR/ES/DE -> EN). We also provide a foundational analysis of how training data scaling impacts model performance. By prioritizing reliance on abundant parallel text rather than difficult-to-acquire parallel speech, RosettaSpeech offers a scalable path to creating high-quality, speaker-preserving S2ST for a much broader array of languages.

</details>


### [16] [Evaluation of an ITD-to-ILD Transformation as a Method to Restore the Spatial Benefit in Speech Intelligibility in Hearing Impaired Listeners](https://arxiv.org/abs/2511.21222)
*Timm-Jonas Bäumer,Johannes W. de Vries,Stephan Töpken,Richard C. Hendriks,Peyman Goli,Steven van de Par*

Main category: eess.AS

TL;DR: 该研究探索了将低频ITD转换为ILD来恢复听力受损者的双耳听觉优势，实验表明这种转换方法能显著改善言语识别阈值，特别是在侧向说话者情况下。


<details>
  <summary>Details</summary>
Motivation: 听力受损者通常对ITD敏感度有限，导致言语理解能力下降。研究旨在通过将低频ITD转换为ILD，为听力受损者重新引入双耳听觉优势。

Method: 进行两个实验：1）使用不同频率的双耳相位偏移正弦波评估ITD敏感度阈值；2）通过操作HRTF在不同双耳配置下测量SRT，包括移除ITD、用ILD替代低频ITD等条件。

Result: 移除ITD使SRT降低约1dB；用ILD替代低频ITD改善了侧向说话者的表现；在保留ITD的同时添加低频ILD对所有方向的说话者都有显著改善。

Conclusion: ITD到ILD的转换方法能有效恢复听力受损者的双耳听觉优势，建议在助听器和人工耳蜗中实施此类转换技术。

Abstract: To improve speech intelligibility in complex everyday situations, the human auditory system partially relies on Interaural Time Differences (ITDs) and Interaural Level Differences (ILDs). However, hearing impaired (HI) listeners often exhibit limited sensitivity to ITDs, resulting in decreased speech intelligibility performance. This study aimed to investigate whether transforming low-frequency ITDs into ILDs could reintroduce a binaural benefit for HI listeners. We conducted two experiments with HI listeners. The first experiment used binaurally phase-shifted sinusoids at different frequencies to evaluate the HI listeners ITD sensitivity threshold. All subjects had an increased ITD threshold at higher frequencies, with different ITD sensitivities between the subjects in the lower frequencies. In the second experiment, Speech Reception Thresholds (SRTs) were measured in different binaural configurations by manipulating Head-Related Transfer Functions (HRTFs). The results showed that, despite the decreased ITD sensitivity, removing ITDs decreased SRTs by approximately 1 dB compared to the unprocessed baseline, where ITDs and ILDs are available. Furthermore, substituting low-frequency ITDs with ILDs yielded an improvement for a lateral target speaker. Adding the low-frequency ILDs while preserving the ITDs caused a significant improvement for speakers in all directions. These findings suggest that the proposed transformation method could be effective in restoring binaural benefits in HI listeners. The results of this study suggest the use of such transformation techniques to be implemented in hearing aids and cochlear implants, directly benefiting HI listeners.

</details>


### [17] [The Spheres Dataset: Multitrack Orchestral Recordings for Music Source Separation and Information Retrieval](https://arxiv.org/abs/2511.21247)
*Jaime Garcia-Martinez,David Diaz-Guerra,John Anderson,Ricardo Falcon-Perez,Pablo Cabañas-Molero,Tuomas Virtanen,Julio J. Carabias-Orti,Pedro Vera-Candeas*

Main category: eess.AS

TL;DR: Spheres数据集是一个用于古典音乐源分离研究的多轨管弦乐录音数据集，包含柴可夫斯基和莫扎特作品，提供23个麦克风录音和房间脉冲响应，支持源分离模型训练和评估。


<details>
  <summary>Details</summary>
Motivation: 为古典音乐领域的机器学习研究提供高质量的多轨录音数据集，解决复杂管弦乐场景中源分离的挑战。

Method: 使用23个麦克风录制Colibrì Ensemble的表演，包括近距离、主麦克风和环境麦克风，生成立体声混音和分离音轨，并估计房间脉冲响应。

Result: 基于X-UMX模型的基线评估显示了在复杂管弦乐场景中源分离的潜力和挑战，验证了数据集对分离、定位、去混响等任务的适用性。

Conclusion: Spheres数据集为古典音乐的源分离、定位、去混响和沉浸式渲染研究提供了有价值的基准测试平台。

Abstract: This paper introduces The Spheres dataset, multitrack orchestral recordings designed to advance machine learning research in music source separation and related MIR tasks within the classical music domain. The dataset is composed of over one hour recordings of musical pieces performed by the Colibrì Ensemble at The Spheres recording studio, capturing two canonical works - Tchaikovsky's Romeo and Juliet and Mozart's Symphony No. 40 - along with chromatic scales and solo excerpts for each instrument. The recording setup employed 23 microphones, including close spot, main, and ambient microphones, enabling the creation of realistic stereo mixes with controlled bleeding and providing isolated stems for supervised training of source separation models. In addition, room impulse responses were estimated for each instrument position, offering valuable acoustic characterization of the recording space. We present the dataset structure, acoustic analysis, and baseline evaluations using X-UMX based models for orchestral family separation and microphone debleeding. Results highlight both the potential and the challenges of source separation in complex orchestral scenarios, underscoring the dataset's value for benchmarking and for exploring new approaches to separation, localization, dereverberation, and immersive rendering of classical music.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [18] [Seeing Beyond Sound: Visualization and Abstraction in Audio Data Representation](https://arxiv.org/abs/2511.20658)
*Ashlae Blum'e*

Main category: cs.SD

TL;DR: 该论文探讨了在音频信息研究中通过增加视觉化工具的维度和交互性来改善复杂工作流程的潜力，使用Jellyfish Dynamite软件作为案例。


<details>
  <summary>Details</summary>
Motivation: 传统音频处理软件携带的历史背景假设可能与现代工作流程不匹配，而创建与新兴需求对齐的工具可以提高分析性和创造性输出。

Method: 通过增加视觉化工具的维度和交互性，使用Jellyfish Dynamite软件来探索复杂音频信息研究工作流程的改进。

Result: 论文展示了增强视觉化工具如何通过更好地与人类感知系统对齐来改善模式识别和复杂工作流程。

Conclusion: 创建与新兴需求对齐的视觉化工具，特别是通过增加维度和交互性，可以显著提高音频信息研究的分析性和创造性成果。

Abstract: In audio signal processing, the interpretation of complex information using visual representation enhances pattern recognition through its alignment with human perceptual systems. Software tools that carry hidden assumptions inherited from their historical contexts risk misalignment with modern workflows as design origins become obscured. We argue that creating tools that align with emergent needs improves analytical and creative outputs due to an increased affinity for using them. This paper explores the potentials associated with adding dimensionality and interactivity into visualization tools to facilitate complex workflows in audio information research using the Jellyfish Dynamite software.

</details>


### [19] [Musical Score Understanding Benchmark: Evaluating Large Language Models' Comprehension of Complete Musical Scores](https://arxiv.org/abs/2511.20697)
*Congren Dai,Yue Yang,Krinos Li,Huichi Zhou,Shijie Liang,Zhang Bo,Enyang Liu,Ge Jin,Hongran An,Haosen Zhang,Peiyuan Jing,KinHei Lee,Zhenxuan Zhang,Xiaobing Li,Maosong Sun*

Main category: cs.SD

TL;DR: MSU-Bench是首个大规模人工策划的音乐乐谱理解基准，评估AI模型在文本和视觉模态下对音乐符号的理解能力，包含1800个生成式问答对，涵盖从基础音符到复杂和声与曲式的四个理解层级。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM和VLM在自然语言和多模态任务上进展迅速，但它们在理解音乐乐谱方面的能力仍未充分探索。需要建立专门的基准来评估AI对音乐符号结构的理解。

Method: 创建MSU-Bench基准，包含1800个QA对，涵盖文本（ABC记谱法）和视觉（PDF）两种模态，分为四个渐进理解层级：起始信息、记谱与音符、和弦与和声、织体与曲式。评估了15+个SOTA模型的零样本和微调性能。

Result: 评估揭示了明显的模态差距、脆弱的层级成功率以及维持多层级正确性的困难。微调显著提高了两种模态的性能，同时保留了一般知识。

Conclusion: MSU-Bench为AI、音乐学和多模态推理交叉领域的未来研究建立了严谨的基础，表明微调是提升音乐乐谱理解能力的有效方法。

Abstract: Understanding complete musical scores requires reasoning over symbolic structures such as pitch, rhythm, harmony, and form. Despite the rapid progress of Large Language Models (LLMs) and Vision-Language Models (VLMs) in natural language and multimodal tasks, their ability to comprehend musical notation remains underexplored. We introduce Musical Score Understanding Benchmark (MSU-Bench), the first large-scale, human-curated benchmark for evaluating score-level musical understanding across both textual (ABC notation) and visual (PDF) modalities. MSU-Bench comprises 1,800 generative question-answer (QA) pairs drawn from works spanning Bach, Beethoven, Chopin, Debussy, and others, organised into four progressive levels of comprehension: Onset Information, Notation & Note, Chord & Harmony, and Texture & Form. Through extensive zero-shot and fine-tuned evaluations of over 15+ state-of-the-art (SOTA) models, we reveal sharp modality gaps, fragile level-wise success rates, and the difficulty of sustaining multilevel correctness. Fine-tuning markedly improves performance in both modalities while preserving general knowledge, establishing MSU-Bench as a rigorous foundation for future research at the intersection of Artificial Intelligence (AI), musicological, and multimodal reasoning.

</details>


### [20] [Acoustic neural networks: Identifying design principles and exploring physical feasibility](https://arxiv.org/abs/2511.21313)
*Ivan Kalthoff,Marcel Rey,Raphael Wittkowski*

Main category: cs.SD

TL;DR: 提出了声学神经网络的设计框架，通过声波传播进行计算，使用数字孪生方法训练受物理约束的神经网络，实现了语音分类任务并达到95%准确率。


<details>
  <summary>Details</summary>
Motivation: 波导物理系统为超越传统电子学的节能模拟计算提供了有前景的途径，声学神经网络在电子效率低或受限的环境中实现低功耗计算，但其系统化设计尚未充分探索。

Method: 采用数字孪生方法，在物理约束下训练传统神经网络架构，包括非负信号和权重、无偏置项，以及与基于强度的非负声学信号兼容的非线性。提出了SincHSRNN混合模型，结合可学习的声学带通滤波器和分层时间处理。

Result: 约束的循环和分层架构能够准确执行语音分类，SincHSRNN在AudioMNIST数据集上达到95%准确率，同时保持与被动声学组件的兼容性。学习参数对应可测量的材料和几何特性。

Conclusion: 建立了物理可实现的声学神经网络的通用设计原则，为低功耗、基于波的神经计算开辟了途径。

Abstract: Wave-guide-based physical systems provide a promising route toward energy-efficient analog computing beyond traditional electronics. Within this landscape, acoustic neural networks represent a promising approach for achieving low-power computation in environments where electronics are inefficient or limited, yet their systematic design has remained largely unexplored. Here we introduce a framework for designing and simulating acoustic neural networks, which perform computation through the propagation of sound waves. Using a digital-twin approach, we train conventional neural network architectures under physically motivated constraints including non-negative signals and weights, the absence of bias terms, and nonlinearities compatible with intensity-based, non-negative acoustic signals. Our work provides a general framework for acoustic neural networks that connects learnable network components directly to physically measurable acoustic properties, enabling the systematic design of realizable acoustic computing systems. We demonstrate that constrained recurrent and hierarchical architectures can perform accurate speech classification, and we propose the SincHSRNN, a hybrid model that combines learnable acoustic bandpass filters with hierarchical temporal processing. The SincHSRNN achieves up to 95% accuracy on the AudioMNIST dataset while remaining compatible with passive acoustic components. Beyond computational performance, the learned parameters correspond to measurable material and geometric properties such as attenuation and transmission. Our results establish general design principles for physically realizable acoustic neural networks and outline a pathway toward low-power, wave-based neural computing.

</details>


### [21] [SingingSDS: A Singing-Capable Spoken Dialogue System for Conversational Roleplay Applications](https://arxiv.org/abs/2511.20972)
*Jionghao Han,Jiatong Shi,Masao Someki,Yuxun Tang,Lan Liu,Yiwen Zhao,Wenhao Feng,Shinji Watanabe*

Main category: cs.SD

TL;DR: SingingSDS是一个通过唱歌而非说话来回应的口语对话系统，采用模块化ASR-LLM-SVS流水线，支持角色扮演和互动娱乐场景中的情感化交互。


<details>
  <summary>Details</summary>
Motivation: 现有口语对话系统大多局限于传统语音回应，缺乏情感表达和记忆性。SingingSDS旨在通过唱歌回应创造更富有情感、更难忘、更愉悦的交互体验。

Method: 采用模块化ASR-LLM-SVS流水线架构，支持多种配置：角色设定、ASR和LLM后端、SVS模型、旋律来源和声音配置文件，可根据延迟、质量和音乐风格需求进行定制。

Result: 开发了即插即用的Web演示系统，提供模块化开源代码支持定制和扩展。

Conclusion: SingingSDS为基于角色的角色扮演和互动娱乐场景提供了一种新颖的情感化交互方式，通过唱歌回应增强了用户体验。

Abstract: With recent advances in automatic speech recognition (ASR), large language models (LLMs), and text-to-speech (TTS) technologies, spoken dialogue systems (SDS) have become widely accessible. However, most existing SDS are limited to conventional spoken responses. We present SingingSDS, a cascaded SDS that responds through singing rather than speaking, fostering more affective, memorable, and pleasurable interactions in character-based roleplay and interactive entertainment scenarios. SingingSDS employs a modular ASR-LLM-SVS pipeline and supports a wide range of configurations across character personas, ASR and LLM backends, SVS models, melody sources, and voice profiles, tailored to different needs in terms of latency, quality, and musical style. SingingSDS is available as a plug-and-play web demo, featuring modular, open-source code that supports customization and extension. Demo: https://huggingface.co/spaces/espnet/SingingSDS. Code: https://github.com/SingingSDS/SingingSDS.

</details>


### [22] [CartoonSing: Unifying Human and Nonhuman Timbres in Singing Generation](https://arxiv.org/abs/2511.21045)
*Jionghao Han,Jiatong Shi,Zhuoyan Tao,Yuxun Tang,Yiwen Zhao,Gus Xia,Shinji Watanabe*

Main category: cs.SD

TL;DR: 提出了非人类歌声生成（NHSG）任务，包括非人类歌声合成（NHSVS）和非人类歌声转换（NHSVC），旨在生成具有非人类音色特征的歌声。


<details>
  <summary>Details</summary>
Motivation: 现有歌声合成系统仅限于人类音色，无法生成超出人类范围的歌声，而视频游戏、电影和虚拟角色等创意应用对此类声音的需求日益增长。

Method: 提出CartoonSing框架，采用两阶段流程：使用标注人类歌声训练的音符表示编码器，以及能够重建人类和非人类音频波形的音色感知声码器。

Result: 实验表明CartoonSing成功生成了非人类歌声，能够泛化到新音色，并将传统SVS和SVC扩展到创意性非人类歌声生成。

Conclusion: CartoonSing统一了歌声合成与转换，弥合了人类与非人类歌声生成之间的差距，为创意应用提供了有效解决方案。

Abstract: Singing voice synthesis (SVS) and singing voice conversion (SVC) have achieved remarkable progress in generating natural-sounding human singing. However, existing systems are restricted to human timbres and have limited ability to synthesize voices outside the human range, which are increasingly demanded in creative applications such as video games, movies, and virtual characters. We introduce Non-Human Singing Generation (NHSG), covering non-human singing voice synthesis (NHSVS) and non-human singing voice conversion (NHSVC), as a novel machine learning task for generating musically coherent singing with non-human timbral characteristics. NHSG is particularly challenging due to the scarcity of non-human singing data, the lack of symbolic alignment, and the wide timbral gap between human and non-human voices. To address these challenges, we propose CartoonSing, a unified framework that integrates singing voice synthesis and conversion while bridging human and non-human singing generation. CartoonSing employs a two-stage pipeline: a score representation encoder trained with annotated human singing and a timbre-aware vocoder that reconstructs waveforms for both human and non-human audio. Experiments demonstrate that CartoonSing successfully generates non-human singing voices, generalizes to novel timbres, and extends conventional SVS and SVC toward creative, non-human singing generation.

</details>


### [23] [Multi-Reward GRPO for Stable and Prosodic Single-Codebook TTS LLMs at Scale](https://arxiv.org/abs/2511.21270)
*Yicheng Zhong,Peiji Yang,Zhisheng Wang*

Main category: cs.SD

TL;DR: 提出了多奖励组相对策略优化(GRPO)框架，通过强化学习直接优化单码本TTS大语言模型的token生成策略，解决韵律不稳定、说话人漂移和自然度下降问题。


<details>
  <summary>Details</summary>
Motivation: 单码本TTS LLMs虽然高效紧凑，但存在韵律不稳定、说话人漂移和自然度下降的问题，需要改进其token生成策略。

Method: 使用多奖励GRPO框架，整合长度惩罚、熵正则化和LLM标注的韵律对齐奖励，通过外部推理LLM预测停顿结构来提供监督信号。

Result: 该方法显著提升了韵律稳定性、说话人相似性和整体语音自然度，在流匹配解码器上也能获得一致增益。

Conclusion: GRPO框架有效提升了单码本TTS LLMs的性能，增强了内在自回归策略，在不同数据规模和模型尺度下都具有良好的可扩展性。

Abstract: Recent advances in Large Language Models (LLMs) have transformed text-to-speech (TTS) synthesis, inspiring autoregressive frameworks that represent speech as sequences of discrete codec tokens. Among them, single-codebook TTS LLMs have emerged as compact and streamable architectures that jointly model semantic and acoustic integration. However, despite their efficiency, these models often exhibit unstable prosody, speaker drift, and degraded naturalness. To address these issues, we propose a multi-reward Group Relative Policy Optimization (GRPO) framework that directly optimizes the token generation policy of single-codebook TTS LLMs. Beyond standard intelligibility and speaker similarity objectives, our design integrates three rule-based rewards: a length penalty for duration consistency, an entropy regularization reward for decoding stability, and an LLM-annotated prosody alignment reward that explicitly supervises rhythm. In this prosody reward, an external reasoning LLM predicts multiple plausible pause structures via in-context learning, providing a human-preference-aligned supervisory signal for GRPO training. To assess universality, we further attach a flow-matching (FM) decoder on top of the GRPO-optimized AR backbone and observe consistent additional gains, indicating that our reinforcement optimization enhances the intrinsic AR policy. We further conduct a scalability analysis across data sizes and model scales, revealing that the proposed method consistently enhances prosodic stability, speaker similarity, and overall speech naturalness in single-codebook TTS LLMs.

</details>


### [24] [SONAR: Spectral-Contrastive Audio Residuals for Generalizable Deepfake Detection](https://arxiv.org/abs/2511.21325)
*Ido Nitzan HIdekel,Gal lifshitz,Khen Cohen,Dan Raviv*

Main category: cs.SD

TL;DR: SONAR是一个频率引导的对比学习框架，通过显式分离音频信号为互补表示来检测Deepfake音频。它使用XLSR编码器捕获低频内容，同时通过可学习SRM和高通滤波器提取高频残差，然后通过频率交叉注意力重新结合两种视图，实现最先进的检测性能。


<details>
  <summary>Details</summary>
Motivation: 解决Deepfake音频检测器在分布外输入上泛化能力差的问题。主要原因是频谱偏差导致神经网络优先学习低频结构，使得DF生成器遗留高频伪影，而这些伪影在常见检测器中未被充分利用。

Method: 提出SONAR框架：1）使用XLSR编码器捕获主导低频内容；2）通过可学习SRM和值约束高通滤波器提取微弱高频残差；3）频率交叉注意力融合两种视图；4）频率感知Jensen-Shannon对比损失拉近真实音频对、推开伪造嵌入。

Result: 在ASVspoof 2021和真实世界基准测试中达到最先进性能，收敛速度比强基线快4倍。将潜在空间分为两个不相交的流形：自然高频对应真实音频，失真高频对应合成音频。

Conclusion: SONAR通过将微弱高频残差提升为一级学习信号，提供了一个完全数据驱动、频率引导的对比框架。该方案在表示层面运行，与架构无关，未来可无缝集成到任何模型或模态中，其中微妙的高频线索具有决定性作用。

Abstract: Deepfake (DF) audio detectors still struggle to generalize to out of distribution inputs. A central reason is spectral bias, the tendency of neural networks to learn low-frequency structure before high-frequency (HF) details, which both causes DF generators to leave HF artifacts and leaves those same artifacts under-exploited by common detectors. To address this gap, we propose Spectral-cONtrastive Audio Residuals (SONAR), a frequency-guided framework that explicitly disentangles an audio signal into complementary representations. An XLSR encoder captures the dominant low-frequency content, while the same cloned path, preceded by learnable SRM, value-constrained high-pass filters, distills faint HF residuals. Frequency cross-attention reunites the two views for long- and short-range frequency dependencies, and a frequency-aware Jensen-Shannon contrastive loss pulls real content-noise pairs together while pushing fake embeddings apart, accelerating optimization and sharpening decision boundaries. Evaluated on the ASVspoof 2021 and in-the-wild benchmarks, SONAR attains state-of-the-art performance and converges four times faster than strong baselines. By elevating faint high-frequency residuals to first-class learning signals, SONAR unveils a fully data-driven, frequency-guided contrastive framework that splits the latent space into two disjoint manifolds: natural-HF for genuine audio and distorted-HF for synthetic audio, thereby sharpening decision boundaries. Because the scheme operates purely at the representation level, it is architecture-agnostic and, in future work, can be seamlessly integrated into any model or modality where subtle high-frequency cues are decisive.

</details>


### [25] [Generating Separated Singing Vocals Using a Diffusion Model Conditioned on Music Mixtures](https://arxiv.org/abs/2511.21342)
*Genís Plaja-Roglans,Yun-Ning Hung,Xavier Serra,Igor Pereira*

Main category: cs.SD

TL;DR: 本文提出了一种基于扩散模型的歌声分离方法，通过训练模型在给定音乐混合物的条件下生成独唱人声，在生成式系统中表现优异，并与非生成式基线方法竞争。


<details>
  <summary>Details</summary>
Motivation: 音乐混合物中分离单个元素对音乐分析和实践至关重要。传统方法使用神经网络进行时频表示掩码或变换，而扩散模型的灵活性和泛化能力为这一复杂任务提供了新的解决方案。

Method: 使用扩散模型，训练其在对应混合物的条件下生成独唱人声。通过迭代采样过程，用户可以控制质量-效率权衡，并在需要时细化输出。

Result: 该方法在生成式系统中有所改进，当使用补充数据训练时，在客观评分上与非生成式基线方法竞争。消融研究展示了采样算法中用户可配置参数的影响。

Conclusion: 扩散模型为歌声分离任务提供了一种有效的生成式解决方案，具有用户可控的采样过程和竞争性的性能表现。

Abstract: Separating the individual elements in a musical mixture is an essential process for music analysis and practice. While this is generally addressed using neural networks optimized to mask or transform the time-frequency representation of a mixture to extract the target sources, the flexibility and generalization capabilities of generative diffusion models are giving rise to a novel class of solutions for this complicated task. In this work, we explore singing voice separation from real music recordings using a diffusion model which is trained to generate the solo vocals conditioned on the corresponding mixture. Our approach improves upon prior generative systems and achieves competitive objective scores against non-generative baselines when trained with supplementary data. The iterative nature of diffusion sampling enables the user to control the quality-efficiency trade-off, and also refine the output when needed. We present an ablation study of the sampling algorithm, highlighting the effects of the user-configurable parameters.

</details>


### [26] [HarmonicAttack: An Adaptive Cross-Domain Audio Watermark Removal](https://arxiv.org/abs/2511.21577)
*Kexin Li,Xiao Hu,Ilya Grishchenko,David Lie*

Main category: cs.SD

TL;DR: HarmonicAttack是一种高效的音频水印去除方法，仅需目标水印方案的基本生成能力，就能训练通用水印去除模型，在多个先进水印方案上表现出色且接近实时性能。


<details>
  <summary>Details</summary>
Motivation: AI生成音频的滥用带来安全挑战，水印技术是重要防御手段。研究有效的水印去除方法对于客观评估水印方案的鲁棒性至关重要，现有方法要么假设不切实际的知识，要么计算成本高昂。

Method: 采用双路径卷积自编码器，在时域和频域同时操作，结合GAN风格训练，将水印从原始音频中分离出来。

Result: 在AudioSeal、WavMark和Silentcipher等先进水印方案上，HarmonicAttack表现出比先前方法更强的水印去除能力，且性能接近实时。

Conclusion: HarmonicAttack是一种高效通用的水印去除方法，即使需要训练，也能在分布外样本上保持良好的性能转移能力。

Abstract: The availability of high-quality, AI-generated audio raises security challenges such as misinformation campaigns and voice-cloning fraud. A key defense against the misuse of AI-generated audio is by watermarking it, so that it can be easily distinguished from genuine audio. As those seeking to misuse AI-generated audio may thus seek to remove audio watermarks, studying effective watermark removal techniques is critical to being able to objectively evaluate the robustness of audio watermarks against removal. Previous watermark removal schemes either assume impractical knowledge of the watermarks they are designed to remove or are computationally expensive, potentially generating a false sense of confidence in current watermark schemes.
  We introduce HarmonicAttack, an efficient audio watermark removal method that only requires the basic ability to generate the watermarks from the targeted scheme and nothing else. With this, we are able to train a general watermark removal model that is able to remove the watermarks generated by the targeted scheme from any watermarked audio sample. HarmonicAttack employs a dual-path convolutional autoencoder that operates in both temporal and frequency domains, along with GAN-style training, to separate the watermark from the original audio. When evaluated against state-of-the-art watermark schemes AudioSeal, WavMark, and Silentcipher, HarmonicAttack demonstrates greater watermark removal ability than previous watermark removal methods with near real-time performance. Moreover, while HarmonicAttack requires training, we find that it is able to transfer to out-of-distribution samples with minimal degradation in performance.

</details>


### [27] [Harmonic-Percussive Disentangled Neural Audio Codec for Bandwidth Extension](https://arxiv.org/abs/2511.21580)
*Benoît Giniès,Xiaoyu Bie,Olivier Fercoq,Gaël Richard*

Main category: cs.SD

TL;DR: 本文提出了一种基于神经音频编解码器和Transformer的带宽扩展方法，将带宽扩展问题构建为音频token预测任务，通过谐波-打击乐分解引导的分离式编解码器设计实现高质量音频重建。


<details>
  <summary>Details</summary>
Motivation: 带宽扩展是音频处理中的长期难题，传统方法性能有限。随着神经架构在音频任务中的显著进步，作者希望将这些进展应用于带宽扩展，通过将问题重新构建为音频token预测来提升性能。

Method: 使用基于Transformer的语言模型，在由分离式神经音频编解码器产生的离散表示上进行训练。编解码器设计采用谐波-打击乐分解引导的分离策略，专门针对下游token预测任务进行优化，实现编解码器结构与Transformer建模的有效耦合。

Result: 该方法在客观指标和主观评价中均实现了原始信号的高质量重建，证明了联合设计的有效性。

Conclusion: 研究表明，将编解码器分离和表示学习与生成建模阶段对齐至关重要，全局的、表示感知的设计在推进带宽扩展方面具有巨大潜力。

Abstract: Bandwidth extension, the task of reconstructing the high-frequency components of an audio signal from its low-pass counterpart, is a long-standing problem in audio processing. While traditional approaches have evolved alongside the broader trends in signal processing, recent advances in neural architectures have significantly improved performance across a wide range of audio tasks, In this work, we extend these advances by framing bandwidth extension as an audio token prediction problem. Specifically, we train a transformer-based language model on the discrete representations produced by a disentangled neural audio codec, where the disentanglement is guided by a Harmonic-Percussive decomposition of the input signals, highlighting spectral structures particularly relevant for bandwidth extension. Our approach introduces a novel codec design that explicitly accounts for the downstream token prediction task, enabling a more effective coupling between codec structure and transformer modeling. This joint design yields high-quality reconstructions of the original signal, as measured by both objective metrics and subjective evaluations. These results highlight the importance of aligning codec disentanglement and representation learning with the generative modeling stage, and demonstrate the potential of global, representation-aware design for advancing bandwidth extension.

</details>

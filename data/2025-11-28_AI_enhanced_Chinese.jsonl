{"id": "2511.20973", "categories": ["eess.AS", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.20973", "abs": "https://arxiv.org/abs/2511.20973", "authors": ["Saurabhchand Bhati", "Samuel Thomas", "Hilde Kuehne", "Rogerio Feris", "James Glass"], "title": "Towards Audio Token Compression in Large Audio Language Models", "comment": null, "summary": "Large Audio Language Models (LALMs) demonstrate impressive performance across diverse tasks, ranging from speech recognition to general audio understanding. However, their scalability is limited by the quadratic complexity of attention and the high token rates of audio signals. These challenges make it difficult to extend LALMs to long-form audio and to deploy them on resource-constrained platforms such as edge devices.\n  In this paper, we explore techniques such as unsupervised segmentation, uniform average pooling, etc., to reduce the number of audio tokens generated by the LALM's audio encoder but before they are consumed by the LLM decoder. To mitigate potential performance degradation introduced by the compressed representations, we employ low-rank adapters to finetune the model. We evaluate our proposed models on two tasks, automatic speech recognition and speech-to-speech translation tasks, that are dependent on effectively uncovering the underlying lexical content of the input signal and study the effect of downsampling on these tasks. Experimental results show that compressed LALMs can achieve performance closer to frame-level LALMs while reducing the input audio token count upto three times before the LLM backbone.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u901a\u8fc7\u65e0\u76d1\u7763\u5206\u5272\u3001\u5747\u5300\u5e73\u5747\u6c60\u5316\u7b49\u6280\u672f\u538b\u7f29LALMs\u7684\u97f3\u9891token\u6570\u91cf\uff0c\u5e76\u4f7f\u7528\u4f4e\u79e9\u9002\u914d\u5668\u5fae\u8c03\u4ee5\u7f13\u89e3\u6027\u80fd\u4e0b\u964d\uff0c\u5728\u8bed\u97f3\u8bc6\u522b\u548c\u8bed\u97f3\u7ffb\u8bd1\u4efb\u52a1\u4e0a\u5b9e\u73b0\u63a5\u8fd1\u5e27\u7ea7\u6027\u80fd\u7684\u540c\u65f6\u5c06\u97f3\u9891token\u6570\u91cf\u51cf\u5c11\u4e09\u500d\u3002", "motivation": "\u5927\u578b\u97f3\u9891\u8bed\u8a00\u6a21\u578b(LALMs)\u9762\u4e34\u6ce8\u610f\u529b\u673a\u5236\u4e8c\u6b21\u590d\u6742\u5ea6\u548c\u97f3\u9891\u4fe1\u53f7\u9ad8token\u7387\u7684\u53ef\u6269\u5c55\u6027\u9650\u5236\uff0c\u96be\u4ee5\u5e94\u7528\u4e8e\u957f\u97f3\u9891\u548c\u5728\u8d44\u6e90\u53d7\u9650\u5e73\u53f0\u4e0a\u90e8\u7f72\u3002", "method": "\u91c7\u7528\u65e0\u76d1\u7763\u5206\u5272\u3001\u5747\u5300\u5e73\u5747\u6c60\u5316\u7b49\u6280\u672f\u51cf\u5c11\u97f3\u9891\u7f16\u7801\u5668\u751f\u6210\u7684token\u6570\u91cf\uff0c\u4f7f\u7528\u4f4e\u79e9\u9002\u914d\u5668\u8fdb\u884c\u5fae\u8c03\u4ee5\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u538b\u7f29\u540e\u7684LALMs\u5728\u8bed\u97f3\u8bc6\u522b\u548c\u8bed\u97f3\u7ffb\u8bd1\u4efb\u52a1\u4e0a\u80fd\u63a5\u8fd1\u5e27\u7ea7LALMs\u7684\u6027\u80fd\uff0c\u540c\u65f6\u5c06\u8f93\u5165\u97f3\u9891token\u6570\u91cf\u51cf\u5c11\u4e09\u500d\u3002", "conclusion": "\u901a\u8fc7token\u538b\u7f29\u548c\u9002\u914d\u5668\u5fae\u8c03\uff0c\u53ef\u4ee5\u6709\u6548\u89e3\u51b3LALMs\u7684\u53ef\u6269\u5c55\u6027\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11\u8ba1\u7b97\u9700\u6c42\u3002"}}
{"id": "2511.20974", "categories": ["eess.AS", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.20974", "abs": "https://arxiv.org/abs/2511.20974", "authors": ["Zhisheng Zheng", "Xiaohang Sun", "Tuan Dinh", "Abhishek Yanamandra", "Abhinav Jain", "Zhu Liu", "Sunil Hadap", "Vimal Bhat", "Manoj Aggarwal", "Gerard Medioni", "David Harwath"], "title": "RosettaSpeech: Zero-Shot Speech-to-Speech Translation from Monolingual Data", "comment": "Work in progress", "summary": "The scarcity of parallel speech corpora critically hampers speech-to-speech translation (S2ST), often forcing reliance on complex, multi-stage pipelines. This paper introduces RosettaSpeech, a novel and simplified framework for zero-shot S2ST that is trained on monolingual speech-text data augmented by machine translation supervision. While our method leverages the linguistic knowledge inherent in text-based NMT models, it strictly eliminates the need for parallel speech-to-speech pairs. Our model uniquely uses text as an intermediate bridge during training but functions as a direct, end-to-end speech-to-speech model at inference. This streamlined approach achieves state-of-the-art results on standard benchmarks. For instance, on the CVSS-C test set, RosettaSpeech outperforms leading systems, achieving an ASR-BLEU score of 25.17 for German-to-English and 29.86 for Spanish-to-English-relative gains of over 27% and 14%, respectively. Furthermore, we demonstrate that a single model can deliver strong many-to-one translation performance (FR/ES/DE -> EN). We also provide a foundational analysis of how training data scaling impacts model performance. By prioritizing reliance on abundant parallel text rather than difficult-to-acquire parallel speech, RosettaSpeech offers a scalable path to creating high-quality, speaker-preserving S2ST for a much broader array of languages.", "AI": {"tldr": "RosettaSpeech\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u96f6\u6837\u672c\u8bed\u97f3\u5230\u8bed\u97f3\u7ffb\u8bd1\u6846\u67b6\uff0c\u4ec5\u4f7f\u7528\u5355\u8bed\u8bed\u97f3-\u6587\u672c\u6570\u636e\u548c\u673a\u5668\u7ffb\u8bd1\u76d1\u7763\u8fdb\u884c\u8bad\u7ec3\uff0c\u65e0\u9700\u5e73\u884c\u8bed\u97f3\u5bf9\uff0c\u5728\u63a8\u7406\u65f6\u5b9e\u73b0\u7aef\u5230\u7aef\u8bed\u97f3\u7ffb\u8bd1\u3002", "motivation": "\u5e73\u884c\u8bed\u97f3\u8bed\u6599\u5e93\u7684\u7a00\u7f3a\u4e25\u91cd\u963b\u788d\u4e86\u8bed\u97f3\u5230\u8bed\u97f3\u7ffb\u8bd1\u7684\u53d1\u5c55\uff0c\u8feb\u4f7f\u4f9d\u8d56\u590d\u6742\u591a\u9636\u6bb5\u6d41\u6c34\u7ebf\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u4ec5\u4f7f\u7528\u5355\u8bed\u6570\u636e\u548c\u673a\u5668\u7ffb\u8bd1\u76d1\u7763\u6765\u7b80\u5316S2ST\u6d41\u7a0b\u3002", "method": "\u4f7f\u7528\u6587\u672c\u4f5c\u4e3a\u8bad\u7ec3\u65f6\u7684\u4e2d\u95f4\u6865\u6881\uff0c\u5229\u7528\u6587\u672cNMT\u6a21\u578b\u7684\u8bed\u8a00\u77e5\u8bc6\uff0c\u4f46\u63a8\u7406\u65f6\u4f5c\u4e3a\u76f4\u63a5\u7684\u7aef\u5230\u7aef\u8bed\u97f3\u5230\u8bed\u97f3\u6a21\u578b\u5de5\u4f5c\u3002\u901a\u8fc7\u673a\u5668\u7ffb\u8bd1\u76d1\u7763\u589e\u5f3a\u5355\u8bed\u8bed\u97f3-\u6587\u672c\u6570\u636e\u3002", "result": "\u5728CVSS-C\u6d4b\u8bd5\u96c6\u4e0a\u8fbe\u5230SOTA\u7ed3\u679c\uff1a\u5fb7\u8bed\u5230\u82f1\u8bedASR-BLEU 25.17\uff08\u76f8\u5bf9\u63d0\u534727%\uff09\uff0c\u897f\u73ed\u7259\u8bed\u5230\u82f1\u8bed29.86\uff08\u76f8\u5bf9\u63d0\u534714%\uff09\u3002\u5355\u4e2a\u6a21\u578b\u53ef\u5b9e\u73b0\u591a\u5bf9\u4e00\u7ffb\u8bd1\uff08FR/ES/DE -> EN\uff09\u3002", "conclusion": "\u901a\u8fc7\u4f9d\u8d56\u4e30\u5bcc\u7684\u5e73\u884c\u6587\u672c\u800c\u975e\u96be\u4ee5\u83b7\u53d6\u7684\u5e73\u884c\u8bed\u97f3\uff0cRosettaSpeech\u4e3a\u521b\u5efa\u9ad8\u8d28\u91cf\u3001\u4fdd\u7559\u8bf4\u8bdd\u4eba\u7279\u5f81\u7684S2ST\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u8def\u5f84\uff0c\u9002\u7528\u4e8e\u66f4\u5e7f\u6cdb\u7684\u8bed\u8a00\u3002"}}
{"id": "2511.21222", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2511.21222", "abs": "https://arxiv.org/abs/2511.21222", "authors": ["Timm-Jonas B\u00e4umer", "Johannes W. de Vries", "Stephan T\u00f6pken", "Richard C. Hendriks", "Peyman Goli", "Steven van de Par"], "title": "Evaluation of an ITD-to-ILD Transformation as a Method to Restore the Spatial Benefit in Speech Intelligibility in Hearing Impaired Listeners", "comment": "12 pages, 11 figues. Submitted to the special issue for the International Symposium on Hearing 2025 in Trends in Hearing", "summary": "To improve speech intelligibility in complex everyday situations, the human auditory system partially relies on Interaural Time Differences (ITDs) and Interaural Level Differences (ILDs). However, hearing impaired (HI) listeners often exhibit limited sensitivity to ITDs, resulting in decreased speech intelligibility performance. This study aimed to investigate whether transforming low-frequency ITDs into ILDs could reintroduce a binaural benefit for HI listeners. We conducted two experiments with HI listeners. The first experiment used binaurally phase-shifted sinusoids at different frequencies to evaluate the HI listeners ITD sensitivity threshold. All subjects had an increased ITD threshold at higher frequencies, with different ITD sensitivities between the subjects in the lower frequencies. In the second experiment, Speech Reception Thresholds (SRTs) were measured in different binaural configurations by manipulating Head-Related Transfer Functions (HRTFs). The results showed that, despite the decreased ITD sensitivity, removing ITDs decreased SRTs by approximately 1 dB compared to the unprocessed baseline, where ITDs and ILDs are available. Furthermore, substituting low-frequency ITDs with ILDs yielded an improvement for a lateral target speaker. Adding the low-frequency ILDs while preserving the ITDs caused a significant improvement for speakers in all directions. These findings suggest that the proposed transformation method could be effective in restoring binaural benefits in HI listeners. The results of this study suggest the use of such transformation techniques to be implemented in hearing aids and cochlear implants, directly benefiting HI listeners.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u7d22\u4e86\u5c06\u4f4e\u9891ITD\u8f6c\u6362\u4e3aILD\u6765\u6062\u590d\u542c\u529b\u53d7\u635f\u8005\u7684\u53cc\u8033\u542c\u89c9\u4f18\u52bf\uff0c\u5b9e\u9a8c\u8868\u660e\u8fd9\u79cd\u8f6c\u6362\u65b9\u6cd5\u80fd\u663e\u8457\u6539\u5584\u8a00\u8bed\u8bc6\u522b\u9608\u503c\uff0c\u7279\u522b\u662f\u5728\u4fa7\u5411\u8bf4\u8bdd\u8005\u60c5\u51b5\u4e0b\u3002", "motivation": "\u542c\u529b\u53d7\u635f\u8005\u901a\u5e38\u5bf9ITD\u654f\u611f\u5ea6\u6709\u9650\uff0c\u5bfc\u81f4\u8a00\u8bed\u7406\u89e3\u80fd\u529b\u4e0b\u964d\u3002\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u5c06\u4f4e\u9891ITD\u8f6c\u6362\u4e3aILD\uff0c\u4e3a\u542c\u529b\u53d7\u635f\u8005\u91cd\u65b0\u5f15\u5165\u53cc\u8033\u542c\u89c9\u4f18\u52bf\u3002", "method": "\u8fdb\u884c\u4e24\u4e2a\u5b9e\u9a8c\uff1a1\uff09\u4f7f\u7528\u4e0d\u540c\u9891\u7387\u7684\u53cc\u8033\u76f8\u4f4d\u504f\u79fb\u6b63\u5f26\u6ce2\u8bc4\u4f30ITD\u654f\u611f\u5ea6\u9608\u503c\uff1b2\uff09\u901a\u8fc7\u64cd\u4f5cHRTF\u5728\u4e0d\u540c\u53cc\u8033\u914d\u7f6e\u4e0b\u6d4b\u91cfSRT\uff0c\u5305\u62ec\u79fb\u9664ITD\u3001\u7528ILD\u66ff\u4ee3\u4f4e\u9891ITD\u7b49\u6761\u4ef6\u3002", "result": "\u79fb\u9664ITD\u4f7fSRT\u964d\u4f4e\u7ea61dB\uff1b\u7528ILD\u66ff\u4ee3\u4f4e\u9891ITD\u6539\u5584\u4e86\u4fa7\u5411\u8bf4\u8bdd\u8005\u7684\u8868\u73b0\uff1b\u5728\u4fdd\u7559ITD\u7684\u540c\u65f6\u6dfb\u52a0\u4f4e\u9891ILD\u5bf9\u6240\u6709\u65b9\u5411\u7684\u8bf4\u8bdd\u8005\u90fd\u6709\u663e\u8457\u6539\u5584\u3002", "conclusion": "ITD\u5230ILD\u7684\u8f6c\u6362\u65b9\u6cd5\u80fd\u6709\u6548\u6062\u590d\u542c\u529b\u53d7\u635f\u8005\u7684\u53cc\u8033\u542c\u89c9\u4f18\u52bf\uff0c\u5efa\u8bae\u5728\u52a9\u542c\u5668\u548c\u4eba\u5de5\u8033\u8717\u4e2d\u5b9e\u65bd\u6b64\u7c7b\u8f6c\u6362\u6280\u672f\u3002"}}
{"id": "2511.21247", "categories": ["eess.AS", "cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2511.21247", "abs": "https://arxiv.org/abs/2511.21247", "authors": ["Jaime Garcia-Martinez", "David Diaz-Guerra", "John Anderson", "Ricardo Falcon-Perez", "Pablo Caba\u00f1as-Molero", "Tuomas Virtanen", "Julio J. Carabias-Orti", "Pedro Vera-Candeas"], "title": "The Spheres Dataset: Multitrack Orchestral Recordings for Music Source Separation and Information Retrieval", "comment": null, "summary": "This paper introduces The Spheres dataset, multitrack orchestral recordings designed to advance machine learning research in music source separation and related MIR tasks within the classical music domain. The dataset is composed of over one hour recordings of musical pieces performed by the Colibr\u00ec Ensemble at The Spheres recording studio, capturing two canonical works - Tchaikovsky's Romeo and Juliet and Mozart's Symphony No. 40 - along with chromatic scales and solo excerpts for each instrument. The recording setup employed 23 microphones, including close spot, main, and ambient microphones, enabling the creation of realistic stereo mixes with controlled bleeding and providing isolated stems for supervised training of source separation models. In addition, room impulse responses were estimated for each instrument position, offering valuable acoustic characterization of the recording space. We present the dataset structure, acoustic analysis, and baseline evaluations using X-UMX based models for orchestral family separation and microphone debleeding. Results highlight both the potential and the challenges of source separation in complex orchestral scenarios, underscoring the dataset's value for benchmarking and for exploring new approaches to separation, localization, dereverberation, and immersive rendering of classical music.", "AI": {"tldr": "Spheres\u6570\u636e\u96c6\u662f\u4e00\u4e2a\u7528\u4e8e\u53e4\u5178\u97f3\u4e50\u6e90\u5206\u79bb\u7814\u7a76\u7684\u591a\u8f68\u7ba1\u5f26\u4e50\u5f55\u97f3\u6570\u636e\u96c6\uff0c\u5305\u542b\u67f4\u53ef\u592b\u65af\u57fa\u548c\u83ab\u624e\u7279\u4f5c\u54c1\uff0c\u63d0\u4f9b23\u4e2a\u9ea6\u514b\u98ce\u5f55\u97f3\u548c\u623f\u95f4\u8109\u51b2\u54cd\u5e94\uff0c\u652f\u6301\u6e90\u5206\u79bb\u6a21\u578b\u8bad\u7ec3\u548c\u8bc4\u4f30\u3002", "motivation": "\u4e3a\u53e4\u5178\u97f3\u4e50\u9886\u57df\u7684\u673a\u5668\u5b66\u4e60\u7814\u7a76\u63d0\u4f9b\u9ad8\u8d28\u91cf\u7684\u591a\u8f68\u5f55\u97f3\u6570\u636e\u96c6\uff0c\u89e3\u51b3\u590d\u6742\u7ba1\u5f26\u4e50\u573a\u666f\u4e2d\u6e90\u5206\u79bb\u7684\u6311\u6218\u3002", "method": "\u4f7f\u752823\u4e2a\u9ea6\u514b\u98ce\u5f55\u5236Colibr\u00ec Ensemble\u7684\u8868\u6f14\uff0c\u5305\u62ec\u8fd1\u8ddd\u79bb\u3001\u4e3b\u9ea6\u514b\u98ce\u548c\u73af\u5883\u9ea6\u514b\u98ce\uff0c\u751f\u6210\u7acb\u4f53\u58f0\u6df7\u97f3\u548c\u5206\u79bb\u97f3\u8f68\uff0c\u5e76\u4f30\u8ba1\u623f\u95f4\u8109\u51b2\u54cd\u5e94\u3002", "result": "\u57fa\u4e8eX-UMX\u6a21\u578b\u7684\u57fa\u7ebf\u8bc4\u4f30\u663e\u793a\u4e86\u5728\u590d\u6742\u7ba1\u5f26\u4e50\u573a\u666f\u4e2d\u6e90\u5206\u79bb\u7684\u6f5c\u529b\u548c\u6311\u6218\uff0c\u9a8c\u8bc1\u4e86\u6570\u636e\u96c6\u5bf9\u5206\u79bb\u3001\u5b9a\u4f4d\u3001\u53bb\u6df7\u54cd\u7b49\u4efb\u52a1\u7684\u9002\u7528\u6027\u3002", "conclusion": "Spheres\u6570\u636e\u96c6\u4e3a\u53e4\u5178\u97f3\u4e50\u7684\u6e90\u5206\u79bb\u3001\u5b9a\u4f4d\u3001\u53bb\u6df7\u54cd\u548c\u6c89\u6d78\u5f0f\u6e32\u67d3\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u57fa\u51c6\u6d4b\u8bd5\u5e73\u53f0\u3002"}}
{"id": "2511.20671", "categories": ["eess.SP", "cs.IT"], "pdf": "https://arxiv.org/pdf/2511.20671", "abs": "https://arxiv.org/abs/2511.20671", "authors": ["Zhaoxin Chang", "Shuguang Xiao", "Fusang Zhang", "Xujun Ma", "Badii Jouaber", "Qingfeng Zhang", "Daqing Zhang"], "title": "WiRainbow: Single-Antenna Direction-Aware Wi-Fi Sensing via Dispersion Effect", "comment": null, "summary": "Recently, Wi-Fi signals have emerged as a powerful tool for contactless sensing. During the sensing process, obtaining target direction information can provide valuable contextual insights for various applications. Existing direction estimation methods typically rely on antenna arrays, which are costly and complex to deploy in real-world scenarios. In this paper, we present WiRainbow, a novel approach that enables single-antenna-based direction awareness for Wi-Fi sensing by leveraging the dispersion effect of frequency-scanning antennas (FSAs), which can naturally steer Wi-Fi subcarriers toward distinct angles during signal transmission. To address key challenges in antenna design and signal processing, we propose a coupled-resonator-based antenna architecture that significantly expands the narrow Field-of-View inherent in conventional FSAs, improving sensing coverage. Additionally, we develop a sensing signal-to-noise-ratio-based signal processing framework that reliably estimates target direction in multipath-rich environments. We prototype WiRainbow and evaluate its performance through benchmark experiments and real-world case studies, demonstrating its ability to achieve accurate, robust, and cost-effective direction awareness for diverse Wi-Fi sensing applications.", "AI": {"tldr": "WiRainbow\u662f\u4e00\u79cd\u5229\u7528\u9891\u7387\u626b\u63cf\u5929\u7ebf(FSA)\u8272\u6563\u6548\u5e94\u7684\u5355\u5929\u7ebfWi-Fi\u65b9\u5411\u611f\u77e5\u65b9\u6cd5\uff0c\u901a\u8fc7\u8026\u5408\u8c10\u632f\u5668\u5929\u7ebf\u67b6\u6784\u6269\u5c55\u89c6\u91ce\u8303\u56f4\uff0c\u5728\u590d\u6742\u591a\u5f84\u73af\u5883\u4e2d\u5b9e\u73b0\u51c6\u786e\u3001\u9c81\u68d2\u4e14\u7ecf\u6d4e\u7684\u65b9\u5411\u611f\u77e5\u3002", "motivation": "\u73b0\u6709Wi-Fi\u65b9\u5411\u4f30\u8ba1\u65b9\u6cd5\u4f9d\u8d56\u6602\u8d35\u590d\u6742\u7684\u5929\u7ebf\u9635\u5217\uff0c\u96be\u4ee5\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u90e8\u7f72\u3002\u9700\u8981\u5f00\u53d1\u4f4e\u6210\u672c\u3001\u5355\u5929\u7ebf\u89e3\u51b3\u65b9\u6848\u6765\u83b7\u53d6\u76ee\u6807\u65b9\u5411\u4fe1\u606f\uff0c\u4e3a\u5404\u79cd\u5e94\u7528\u63d0\u4f9b\u6709\u4ef7\u503c\u7684\u4e0a\u4e0b\u6587\u6d1e\u5bdf\u3002", "method": "\u5229\u7528\u9891\u7387\u626b\u63cf\u5929\u7ebf\u7684\u8272\u6563\u6548\u5e94\uff0c\u4f7fWi-Fi\u5b50\u8f7d\u6ce2\u5728\u4f20\u8f93\u65f6\u81ea\u7136\u6307\u5411\u4e0d\u540c\u89d2\u5ea6\uff1b\u63d0\u51fa\u8026\u5408\u8c10\u632f\u5668\u5929\u7ebf\u67b6\u6784\u6269\u5c55\u4f20\u7edfFSA\u7684\u7a84\u89c6\u91ce\uff1b\u5f00\u53d1\u57fa\u4e8e\u4fe1\u566a\u6bd4\u7684\u4fe1\u53f7\u5904\u7406\u6846\u67b6\u5728\u590d\u6742\u591a\u5f84\u73af\u5883\u4e2d\u53ef\u9760\u4f30\u8ba1\u76ee\u6807\u65b9\u5411\u3002", "result": "\u901a\u8fc7\u57fa\u51c6\u5b9e\u9a8c\u548c\u771f\u5b9e\u6848\u4f8b\u7814\u7a76\u9a8c\u8bc1\uff0cWiRainbow\u80fd\u591f\u4e3a\u591a\u79cdWi-Fi\u4f20\u611f\u5e94\u7528\u5b9e\u73b0\u51c6\u786e\u3001\u9c81\u68d2\u4e14\u7ecf\u6d4e\u9ad8\u6548\u7684\u65b9\u5411\u611f\u77e5\u3002", "conclusion": "WiRainbow\u63d0\u4f9b\u4e86\u4e00\u79cd\u521b\u65b0\u7684\u5355\u5929\u7ebfWi-Fi\u65b9\u5411\u611f\u77e5\u89e3\u51b3\u65b9\u6848\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u5929\u7ebf\u9635\u5217\u7684\u6210\u672c\u548c\u90e8\u7f72\u590d\u6742\u6027\u9650\u5236\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2511.20658", "categories": ["cs.SD", "cs.HC", "eess.AS"], "pdf": "https://arxiv.org/pdf/2511.20658", "abs": "https://arxiv.org/abs/2511.20658", "authors": ["Ashlae Blum'e"], "title": "Seeing Beyond Sound: Visualization and Abstraction in Audio Data Representation", "comment": "23 pages, 3 figures", "summary": "In audio signal processing, the interpretation of complex information using visual representation enhances pattern recognition through its alignment with human perceptual systems. Software tools that carry hidden assumptions inherited from their historical contexts risk misalignment with modern workflows as design origins become obscured. We argue that creating tools that align with emergent needs improves analytical and creative outputs due to an increased affinity for using them. This paper explores the potentials associated with adding dimensionality and interactivity into visualization tools to facilitate complex workflows in audio information research using the Jellyfish Dynamite software.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u5728\u97f3\u9891\u4fe1\u606f\u7814\u7a76\u4e2d\u901a\u8fc7\u589e\u52a0\u89c6\u89c9\u5316\u5de5\u5177\u7684\u7ef4\u5ea6\u548c\u4ea4\u4e92\u6027\u6765\u6539\u5584\u590d\u6742\u5de5\u4f5c\u6d41\u7a0b\u7684\u6f5c\u529b\uff0c\u4f7f\u7528Jellyfish Dynamite\u8f6f\u4ef6\u4f5c\u4e3a\u6848\u4f8b\u3002", "motivation": "\u4f20\u7edf\u97f3\u9891\u5904\u7406\u8f6f\u4ef6\u643a\u5e26\u7684\u5386\u53f2\u80cc\u666f\u5047\u8bbe\u53ef\u80fd\u4e0e\u73b0\u4ee3\u5de5\u4f5c\u6d41\u7a0b\u4e0d\u5339\u914d\uff0c\u800c\u521b\u5efa\u4e0e\u65b0\u5174\u9700\u6c42\u5bf9\u9f50\u7684\u5de5\u5177\u53ef\u4ee5\u63d0\u9ad8\u5206\u6790\u6027\u548c\u521b\u9020\u6027\u8f93\u51fa\u3002", "method": "\u901a\u8fc7\u589e\u52a0\u89c6\u89c9\u5316\u5de5\u5177\u7684\u7ef4\u5ea6\u548c\u4ea4\u4e92\u6027\uff0c\u4f7f\u7528Jellyfish Dynamite\u8f6f\u4ef6\u6765\u63a2\u7d22\u590d\u6742\u97f3\u9891\u4fe1\u606f\u7814\u7a76\u5de5\u4f5c\u6d41\u7a0b\u7684\u6539\u8fdb\u3002", "result": "\u8bba\u6587\u5c55\u793a\u4e86\u589e\u5f3a\u89c6\u89c9\u5316\u5de5\u5177\u5982\u4f55\u901a\u8fc7\u66f4\u597d\u5730\u4e0e\u4eba\u7c7b\u611f\u77e5\u7cfb\u7edf\u5bf9\u9f50\u6765\u6539\u5584\u6a21\u5f0f\u8bc6\u522b\u548c\u590d\u6742\u5de5\u4f5c\u6d41\u7a0b\u3002", "conclusion": "\u521b\u5efa\u4e0e\u65b0\u5174\u9700\u6c42\u5bf9\u9f50\u7684\u89c6\u89c9\u5316\u5de5\u5177\uff0c\u7279\u522b\u662f\u901a\u8fc7\u589e\u52a0\u7ef4\u5ea6\u548c\u4ea4\u4e92\u6027\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u97f3\u9891\u4fe1\u606f\u7814\u7a76\u7684\u5206\u6790\u6027\u548c\u521b\u9020\u6027\u6210\u679c\u3002"}}
{"id": "2511.20831", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.20831", "abs": "https://arxiv.org/abs/2511.20831", "authors": ["Khuram Naveed", "Naveed ur Rehman"], "title": "A Fully Multivariate Multifractal Detrended Fluctuation Analysis Method for Fault Diagnosis", "comment": null, "summary": "We propose a fully multivariate generalization of multifractal detrended fluctuation analysis (MFDFA) and leverage it to develop a fault diagnosis framework for multichannel machine vibration data. We introduce a novel covariance-weighted $L_{pq}$ matrix norm based on Mahalanobis distance to define a fully multivariate fluctuation function that uniquely captures cross-channel dependencies and variance biases in multichannel vibration data. This formulation, termed FM-MFDFA, allows for a more accurate characterization of the multiscale structure of multivariate signals. To enhance feature relevance, the proposed framework integrates multivariate variational mode decomposition (MVMD) to isolate fault-relevant components before applying FM-MFDFA. Results on wind turbine gearbox data demonstrate that the proposed method outperforms conventional MFDFA approaches by effectively distinguishing between healthy and faulty machine states, even under noisy conditions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u5b8c\u5168\u591a\u53d8\u91cfMFDFA\u65b9\u6cd5(FM-MFDFA)\uff0c\u57fa\u4e8e\u9a6c\u6c0f\u8ddd\u79bb\u7684\u534f\u65b9\u5dee\u52a0\u6743Lpq\u77e9\u9635\u8303\u6570\u5b9a\u4e49\u591a\u53d8\u91cf\u6ce2\u52a8\u51fd\u6570\uff0c\u7ed3\u5408MVMD\u8fdb\u884c\u6545\u969c\u8bca\u65ad\uff0c\u5728\u98ce\u529b\u6da1\u8f6e\u673a\u53d8\u901f\u7bb1\u6570\u636e\u4e0a\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edfMFDFA\u65b9\u6cd5\u65e0\u6cd5\u6709\u6548\u6355\u6349\u591a\u901a\u9053\u632f\u52a8\u6570\u636e\u4e2d\u7684\u8de8\u901a\u9053\u4f9d\u8d56\u5173\u7cfb\u548c\u65b9\u5dee\u504f\u5dee\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u51c6\u786e\u8868\u5f81\u591a\u53d8\u91cf\u4fe1\u53f7\u591a\u5c3a\u5ea6\u7ed3\u6784\u7684\u5b8c\u5168\u591a\u53d8\u91cf\u65b9\u6cd5\u3002", "method": "\u5f15\u5165\u57fa\u4e8e\u9a6c\u6c0f\u8ddd\u79bb\u7684\u534f\u65b9\u5dee\u52a0\u6743Lpq\u77e9\u9635\u8303\u6570\u5b9a\u4e49\u5b8c\u5168\u591a\u53d8\u91cf\u6ce2\u52a8\u51fd\u6570\uff0c\u7ed3\u5408\u591a\u53d8\u91cf\u53d8\u5206\u6a21\u6001\u5206\u89e3(MVMD)\u5206\u79bb\u6545\u969c\u76f8\u5173\u5206\u91cf\uff0c\u7136\u540e\u5e94\u7528FM-MFDFA\u8fdb\u884c\u7279\u5f81\u63d0\u53d6\u3002", "result": "\u5728\u98ce\u529b\u6da1\u8f6e\u673a\u53d8\u901f\u7bb1\u6570\u636e\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u533a\u5206\u5065\u5eb7\u548c\u6545\u969c\u72b6\u6001\uff0c\u5373\u4f7f\u5728\u566a\u58f0\u6761\u4ef6\u4e0b\u4e5f\u4f18\u4e8e\u4f20\u7edfMFDFA\u65b9\u6cd5\u3002", "conclusion": "FM-MFDFA\u6846\u67b6\u901a\u8fc7\u6355\u83b7\u8de8\u901a\u9053\u4f9d\u8d56\u5173\u7cfb\uff0c\u63d0\u4f9b\u4e86\u66f4\u51c6\u786e\u7684\u591a\u53d8\u91cf\u4fe1\u53f7\u591a\u5c3a\u5ea6\u7ed3\u6784\u8868\u5f81\uff0c\u5728\u591a\u901a\u9053\u673a\u68b0\u632f\u52a8\u6545\u969c\u8bca\u65ad\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2511.20697", "categories": ["cs.SD", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.20697", "abs": "https://arxiv.org/abs/2511.20697", "authors": ["Congren Dai", "Yue Yang", "Krinos Li", "Huichi Zhou", "Shijie Liang", "Zhang Bo", "Enyang Liu", "Ge Jin", "Hongran An", "Haosen Zhang", "Peiyuan Jing", "KinHei Lee", "Zhenxuan Zhang", "Xiaobing Li", "Maosong Sun"], "title": "Musical Score Understanding Benchmark: Evaluating Large Language Models' Comprehension of Complete Musical Scores", "comment": null, "summary": "Understanding complete musical scores requires reasoning over symbolic structures such as pitch, rhythm, harmony, and form. Despite the rapid progress of Large Language Models (LLMs) and Vision-Language Models (VLMs) in natural language and multimodal tasks, their ability to comprehend musical notation remains underexplored. We introduce Musical Score Understanding Benchmark (MSU-Bench), the first large-scale, human-curated benchmark for evaluating score-level musical understanding across both textual (ABC notation) and visual (PDF) modalities. MSU-Bench comprises 1,800 generative question-answer (QA) pairs drawn from works spanning Bach, Beethoven, Chopin, Debussy, and others, organised into four progressive levels of comprehension: Onset Information, Notation & Note, Chord & Harmony, and Texture & Form. Through extensive zero-shot and fine-tuned evaluations of over 15+ state-of-the-art (SOTA) models, we reveal sharp modality gaps, fragile level-wise success rates, and the difficulty of sustaining multilevel correctness. Fine-tuning markedly improves performance in both modalities while preserving general knowledge, establishing MSU-Bench as a rigorous foundation for future research at the intersection of Artificial Intelligence (AI), musicological, and multimodal reasoning.", "AI": {"tldr": "MSU-Bench\u662f\u9996\u4e2a\u5927\u89c4\u6a21\u4eba\u5de5\u7b56\u5212\u7684\u97f3\u4e50\u4e50\u8c31\u7406\u89e3\u57fa\u51c6\uff0c\u8bc4\u4f30AI\u6a21\u578b\u5728\u6587\u672c\u548c\u89c6\u89c9\u6a21\u6001\u4e0b\u5bf9\u97f3\u4e50\u7b26\u53f7\u7684\u7406\u89e3\u80fd\u529b\uff0c\u5305\u542b1800\u4e2a\u751f\u6210\u5f0f\u95ee\u7b54\u5bf9\uff0c\u6db5\u76d6\u4ece\u57fa\u7840\u97f3\u7b26\u5230\u590d\u6742\u548c\u58f0\u4e0e\u66f2\u5f0f\u7684\u56db\u4e2a\u7406\u89e3\u5c42\u7ea7\u3002", "motivation": "\u5c3d\u7ba1LLM\u548cVLM\u5728\u81ea\u7136\u8bed\u8a00\u548c\u591a\u6a21\u6001\u4efb\u52a1\u4e0a\u8fdb\u5c55\u8fc5\u901f\uff0c\u4f46\u5b83\u4eec\u5728\u7406\u89e3\u97f3\u4e50\u4e50\u8c31\u65b9\u9762\u7684\u80fd\u529b\u4ecd\u672a\u5145\u5206\u63a2\u7d22\u3002\u9700\u8981\u5efa\u7acb\u4e13\u95e8\u7684\u57fa\u51c6\u6765\u8bc4\u4f30AI\u5bf9\u97f3\u4e50\u7b26\u53f7\u7ed3\u6784\u7684\u7406\u89e3\u3002", "method": "\u521b\u5efaMSU-Bench\u57fa\u51c6\uff0c\u5305\u542b1800\u4e2aQA\u5bf9\uff0c\u6db5\u76d6\u6587\u672c\uff08ABC\u8bb0\u8c31\u6cd5\uff09\u548c\u89c6\u89c9\uff08PDF\uff09\u4e24\u79cd\u6a21\u6001\uff0c\u5206\u4e3a\u56db\u4e2a\u6e10\u8fdb\u7406\u89e3\u5c42\u7ea7\uff1a\u8d77\u59cb\u4fe1\u606f\u3001\u8bb0\u8c31\u4e0e\u97f3\u7b26\u3001\u548c\u5f26\u4e0e\u548c\u58f0\u3001\u7ec7\u4f53\u4e0e\u66f2\u5f0f\u3002\u8bc4\u4f30\u4e8615+\u4e2aSOTA\u6a21\u578b\u7684\u96f6\u6837\u672c\u548c\u5fae\u8c03\u6027\u80fd\u3002", "result": "\u8bc4\u4f30\u63ed\u793a\u4e86\u660e\u663e\u7684\u6a21\u6001\u5dee\u8ddd\u3001\u8106\u5f31\u7684\u5c42\u7ea7\u6210\u529f\u7387\u4ee5\u53ca\u7ef4\u6301\u591a\u5c42\u7ea7\u6b63\u786e\u6027\u7684\u56f0\u96be\u3002\u5fae\u8c03\u663e\u8457\u63d0\u9ad8\u4e86\u4e24\u79cd\u6a21\u6001\u7684\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u7559\u4e86\u4e00\u822c\u77e5\u8bc6\u3002", "conclusion": "MSU-Bench\u4e3aAI\u3001\u97f3\u4e50\u5b66\u548c\u591a\u6a21\u6001\u63a8\u7406\u4ea4\u53c9\u9886\u57df\u7684\u672a\u6765\u7814\u7a76\u5efa\u7acb\u4e86\u4e25\u8c28\u7684\u57fa\u7840\uff0c\u8868\u660e\u5fae\u8c03\u662f\u63d0\u5347\u97f3\u4e50\u4e50\u8c31\u7406\u89e3\u80fd\u529b\u7684\u6709\u6548\u65b9\u6cd5\u3002"}}
{"id": "2511.21313", "categories": ["cs.SD", "cond-mat.dis-nn", "cs.NE", "eess.AS", "physics.app-ph"], "pdf": "https://arxiv.org/pdf/2511.21313", "abs": "https://arxiv.org/abs/2511.21313", "authors": ["Ivan Kalthoff", "Marcel Rey", "Raphael Wittkowski"], "title": "Acoustic neural networks: Identifying design principles and exploring physical feasibility", "comment": "13 pages, 4 figures, 8 tables", "summary": "Wave-guide-based physical systems provide a promising route toward energy-efficient analog computing beyond traditional electronics. Within this landscape, acoustic neural networks represent a promising approach for achieving low-power computation in environments where electronics are inefficient or limited, yet their systematic design has remained largely unexplored. Here we introduce a framework for designing and simulating acoustic neural networks, which perform computation through the propagation of sound waves. Using a digital-twin approach, we train conventional neural network architectures under physically motivated constraints including non-negative signals and weights, the absence of bias terms, and nonlinearities compatible with intensity-based, non-negative acoustic signals. Our work provides a general framework for acoustic neural networks that connects learnable network components directly to physically measurable acoustic properties, enabling the systematic design of realizable acoustic computing systems. We demonstrate that constrained recurrent and hierarchical architectures can perform accurate speech classification, and we propose the SincHSRNN, a hybrid model that combines learnable acoustic bandpass filters with hierarchical temporal processing. The SincHSRNN achieves up to 95% accuracy on the AudioMNIST dataset while remaining compatible with passive acoustic components. Beyond computational performance, the learned parameters correspond to measurable material and geometric properties such as attenuation and transmission. Our results establish general design principles for physically realizable acoustic neural networks and outline a pathway toward low-power, wave-based neural computing.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u58f0\u5b66\u795e\u7ecf\u7f51\u7edc\u7684\u8bbe\u8ba1\u6846\u67b6\uff0c\u901a\u8fc7\u58f0\u6ce2\u4f20\u64ad\u8fdb\u884c\u8ba1\u7b97\uff0c\u4f7f\u7528\u6570\u5b57\u5b6a\u751f\u65b9\u6cd5\u8bad\u7ec3\u53d7\u7269\u7406\u7ea6\u675f\u7684\u795e\u7ecf\u7f51\u7edc\uff0c\u5b9e\u73b0\u4e86\u8bed\u97f3\u5206\u7c7b\u4efb\u52a1\u5e76\u8fbe\u523095%\u51c6\u786e\u7387\u3002", "motivation": "\u6ce2\u5bfc\u7269\u7406\u7cfb\u7edf\u4e3a\u8d85\u8d8a\u4f20\u7edf\u7535\u5b50\u5b66\u7684\u8282\u80fd\u6a21\u62df\u8ba1\u7b97\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u9014\u5f84\uff0c\u58f0\u5b66\u795e\u7ecf\u7f51\u7edc\u5728\u7535\u5b50\u6548\u7387\u4f4e\u6216\u53d7\u9650\u7684\u73af\u5883\u4e2d\u5b9e\u73b0\u4f4e\u529f\u8017\u8ba1\u7b97\uff0c\u4f46\u5176\u7cfb\u7edf\u5316\u8bbe\u8ba1\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u91c7\u7528\u6570\u5b57\u5b6a\u751f\u65b9\u6cd5\uff0c\u5728\u7269\u7406\u7ea6\u675f\u4e0b\u8bad\u7ec3\u4f20\u7edf\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u5305\u62ec\u975e\u8d1f\u4fe1\u53f7\u548c\u6743\u91cd\u3001\u65e0\u504f\u7f6e\u9879\uff0c\u4ee5\u53ca\u4e0e\u57fa\u4e8e\u5f3a\u5ea6\u7684\u975e\u8d1f\u58f0\u5b66\u4fe1\u53f7\u517c\u5bb9\u7684\u975e\u7ebf\u6027\u3002\u63d0\u51fa\u4e86SincHSRNN\u6df7\u5408\u6a21\u578b\uff0c\u7ed3\u5408\u53ef\u5b66\u4e60\u7684\u58f0\u5b66\u5e26\u901a\u6ee4\u6ce2\u5668\u548c\u5206\u5c42\u65f6\u95f4\u5904\u7406\u3002", "result": "\u7ea6\u675f\u7684\u5faa\u73af\u548c\u5206\u5c42\u67b6\u6784\u80fd\u591f\u51c6\u786e\u6267\u884c\u8bed\u97f3\u5206\u7c7b\uff0cSincHSRNN\u5728AudioMNIST\u6570\u636e\u96c6\u4e0a\u8fbe\u523095%\u51c6\u786e\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u88ab\u52a8\u58f0\u5b66\u7ec4\u4ef6\u7684\u517c\u5bb9\u6027\u3002\u5b66\u4e60\u53c2\u6570\u5bf9\u5e94\u53ef\u6d4b\u91cf\u7684\u6750\u6599\u548c\u51e0\u4f55\u7279\u6027\u3002", "conclusion": "\u5efa\u7acb\u4e86\u7269\u7406\u53ef\u5b9e\u73b0\u7684\u58f0\u5b66\u795e\u7ecf\u7f51\u7edc\u7684\u901a\u7528\u8bbe\u8ba1\u539f\u5219\uff0c\u4e3a\u4f4e\u529f\u8017\u3001\u57fa\u4e8e\u6ce2\u7684\u795e\u7ecf\u8ba1\u7b97\u5f00\u8f9f\u4e86\u9014\u5f84\u3002"}}
{"id": "2511.20936", "categories": ["eess.SP", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.20936", "abs": "https://arxiv.org/abs/2511.20936", "authors": ["Ayoob Salari", "Kai Wu", "Khawaja Fahad Masood", "Y. Jay Guo", "J. Andrew Zhang"], "title": "Wavelet-Guided Water-Level Estimation for ISAC", "comment": null, "summary": "Real-time water-level monitoring across many locations is vital for flood response, infrastructure management, and environmental forecasting. Yet many sensing methods rely on fixed instruments - acoustic, radar, camera, or pressure probes - that are costly to install and maintain and are vulnerable during extreme events. We propose a passive, low-cost water-level tracking scheme that uses only LTE downlink power metrics reported by commodity receivers. The method extracts per-antenna RSRP, RSSI, and RSRQ, applies a continuous wavelet transform (CWT) to the RSRP to isolate the semidiurnal tide component, and forms a summed-coefficient signature that simultaneously marks high/low tide (tide-turn times) and tracks the tide-rate (flow speed) over time. These wavelet features guide a lightweight neural network that learns water-level changes over time from a short training segment. Beyond a single serving base station, we also show a multi-base-station cooperative mode: independent CWTs are computed per carrier and fused by a robust median to produce one tide-band feature that improves stability and resilience to local disturbances. Experiments over a 420 m river path under line-of-sight conditions achieve root-mean-square and mean-absolute errors of 0.8 cm and 0.5 cm, respectively. Under a non-line-of-sight setting with vegetation and vessel traffic, the same model transfers successfully after brief fine-tuning, reaching 1.7 cm RMSE and 0.8 cm MAE. Unlike CSI-based methods, the approach needs no array calibration and runs on standard hardware, making wide deployment practical. When signals from multiple base stations are available, fusion further improves robustness.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eLTE\u4e0b\u884c\u529f\u7387\u6307\u6807\u7684\u88ab\u52a8\u5f0f\u4f4e\u6210\u672c\u6c34\u4f4d\u76d1\u6d4b\u65b9\u6cd5\uff0c\u5229\u7528\u8fde\u7eed\u5c0f\u6ce2\u53d8\u6362\u63d0\u53d6\u6f6e\u6c50\u7279\u5f81\uff0c\u7ed3\u5408\u8f7b\u91cf\u7ea7\u795e\u7ecf\u7f51\u7edc\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u6c34\u4f4d\u8ddf\u8e2a\u3002", "motivation": "\u4f20\u7edf\u6c34\u4f4d\u76d1\u6d4b\u65b9\u6cd5\u4f9d\u8d56\u56fa\u5b9a\u8bbe\u5907\uff0c\u6210\u672c\u9ad8\u4e14\u6613\u53d7\u6781\u7aef\u4e8b\u4ef6\u5f71\u54cd\uff0c\u9700\u8981\u4e00\u79cd\u4f4e\u6210\u672c\u3001\u6613\u90e8\u7f72\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u4f7f\u7528LTE\u63a5\u6536\u5668\u62a5\u544a\u7684RSRP\u3001RSSI\u548cRSRQ\u6307\u6807\uff0c\u5e94\u7528\u8fde\u7eed\u5c0f\u6ce2\u53d8\u6362\u63d0\u53d6\u534a\u65e5\u6f6e\u5206\u91cf\uff0c\u5f62\u6210\u6f6e\u6c50\u7279\u5f81\uff0c\u7ed3\u5408\u8f7b\u91cf\u7ea7\u795e\u7ecf\u7f51\u7edc\u5b66\u4e60\u6c34\u4f4d\u53d8\u5316\u3002\u652f\u6301\u591a\u57fa\u7ad9\u534f\u4f5c\u6a21\u5f0f\u63d0\u5347\u7a33\u5b9a\u6027\u3002", "result": "\u5728420\u7c73\u6cb3\u6d41\u8def\u5f84\u4e0a\uff0c\u89c6\u8ddd\u6761\u4ef6\u4e0b\u8fbe\u52300.8cm RMSE\u548c0.5cm MAE\uff1b\u975e\u89c6\u8ddd\u6761\u4ef6\u4e0b\u7ecf\u5fae\u8c03\u540e\u8fbe\u52301.7cm RMSE\u548c0.8cm MAE\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u65e0\u9700\u9635\u5217\u6821\u51c6\uff0c\u53ef\u5728\u6807\u51c6\u786c\u4ef6\u4e0a\u8fd0\u884c\uff0c\u652f\u6301\u591a\u57fa\u7ad9\u878d\u5408\uff0c\u4e3a\u5927\u89c4\u6a21\u6c34\u4f4d\u76d1\u6d4b\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.20972", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2511.20972", "abs": "https://arxiv.org/abs/2511.20972", "authors": ["Jionghao Han", "Jiatong Shi", "Masao Someki", "Yuxun Tang", "Lan Liu", "Yiwen Zhao", "Wenhao Feng", "Shinji Watanabe"], "title": "SingingSDS: A Singing-Capable Spoken Dialogue System for Conversational Roleplay Applications", "comment": null, "summary": "With recent advances in automatic speech recognition (ASR), large language models (LLMs), and text-to-speech (TTS) technologies, spoken dialogue systems (SDS) have become widely accessible. However, most existing SDS are limited to conventional spoken responses. We present SingingSDS, a cascaded SDS that responds through singing rather than speaking, fostering more affective, memorable, and pleasurable interactions in character-based roleplay and interactive entertainment scenarios. SingingSDS employs a modular ASR-LLM-SVS pipeline and supports a wide range of configurations across character personas, ASR and LLM backends, SVS models, melody sources, and voice profiles, tailored to different needs in terms of latency, quality, and musical style. SingingSDS is available as a plug-and-play web demo, featuring modular, open-source code that supports customization and extension. Demo: https://huggingface.co/spaces/espnet/SingingSDS. Code: https://github.com/SingingSDS/SingingSDS.", "AI": {"tldr": "SingingSDS\u662f\u4e00\u4e2a\u901a\u8fc7\u5531\u6b4c\u800c\u975e\u8bf4\u8bdd\u6765\u56de\u5e94\u7684\u53e3\u8bed\u5bf9\u8bdd\u7cfb\u7edf\uff0c\u91c7\u7528\u6a21\u5757\u5316ASR-LLM-SVS\u6d41\u6c34\u7ebf\uff0c\u652f\u6301\u89d2\u8272\u626e\u6f14\u548c\u4e92\u52a8\u5a31\u4e50\u573a\u666f\u4e2d\u7684\u60c5\u611f\u5316\u4ea4\u4e92\u3002", "motivation": "\u73b0\u6709\u53e3\u8bed\u5bf9\u8bdd\u7cfb\u7edf\u5927\u591a\u5c40\u9650\u4e8e\u4f20\u7edf\u8bed\u97f3\u56de\u5e94\uff0c\u7f3a\u4e4f\u60c5\u611f\u8868\u8fbe\u548c\u8bb0\u5fc6\u6027\u3002SingingSDS\u65e8\u5728\u901a\u8fc7\u5531\u6b4c\u56de\u5e94\u521b\u9020\u66f4\u5bcc\u6709\u60c5\u611f\u3001\u66f4\u96be\u5fd8\u3001\u66f4\u6109\u60a6\u7684\u4ea4\u4e92\u4f53\u9a8c\u3002", "method": "\u91c7\u7528\u6a21\u5757\u5316ASR-LLM-SVS\u6d41\u6c34\u7ebf\u67b6\u6784\uff0c\u652f\u6301\u591a\u79cd\u914d\u7f6e\uff1a\u89d2\u8272\u8bbe\u5b9a\u3001ASR\u548cLLM\u540e\u7aef\u3001SVS\u6a21\u578b\u3001\u65cb\u5f8b\u6765\u6e90\u548c\u58f0\u97f3\u914d\u7f6e\u6587\u4ef6\uff0c\u53ef\u6839\u636e\u5ef6\u8fdf\u3001\u8d28\u91cf\u548c\u97f3\u4e50\u98ce\u683c\u9700\u6c42\u8fdb\u884c\u5b9a\u5236\u3002", "result": "\u5f00\u53d1\u4e86\u5373\u63d2\u5373\u7528\u7684Web\u6f14\u793a\u7cfb\u7edf\uff0c\u63d0\u4f9b\u6a21\u5757\u5316\u5f00\u6e90\u4ee3\u7801\u652f\u6301\u5b9a\u5236\u548c\u6269\u5c55\u3002", "conclusion": "SingingSDS\u4e3a\u57fa\u4e8e\u89d2\u8272\u7684\u89d2\u8272\u626e\u6f14\u548c\u4e92\u52a8\u5a31\u4e50\u573a\u666f\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u60c5\u611f\u5316\u4ea4\u4e92\u65b9\u5f0f\uff0c\u901a\u8fc7\u5531\u6b4c\u56de\u5e94\u589e\u5f3a\u4e86\u7528\u6237\u4f53\u9a8c\u3002"}}
{"id": "2511.21068", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.21068", "abs": "https://arxiv.org/abs/2511.21068", "authors": ["Anirshu Devroy", "Gregor Fritz", "Mathias Brandstoetter"], "title": "Evaluating the Performance of a Modified Skin Temperature Sensor for Lower Limb Prostheses: An Experimental Comparison", "comment": "6 pages, 10 figures,", "summary": "Current rehabilitation of lower limb prostheses has significant challenges, especially with skin conditions, irritation and discomfort. Understanding the skin temperature and having comfortable wearable sensors that would monitor skin temperature in a real-time outdoor environment would be useful. The system would help the user and orthopedic technician to provide feedback and changes that might be required in the prosthesis. Hence in this paper, a series of experiments are conducted in order to understand and characterize the system behavior and compare a general thermistor and a modified thermistor as a potential method of temperature measurement for outdoor usage of prostheses. The paper goes on to compare the different modified thermistors behavior with their regular counterpart and highlights the challenges and improvement areas needed for such a modified thermistor for outdoor temperature monitoring in a prosthetic system. Initial results show that some of the modified thermistors showed better temperature recording compared to the rest. Finally, such modified thermistors can be a potential alternative for comfortable temperature measurement embedded in the prosthesis system. Such a system can provide valuable insights into temperature distribution and an early warning system for skin problems", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u7528\u4e8e\u5047\u80a2\u7cfb\u7edf\u6237\u5916\u6e29\u5ea6\u76d1\u6d4b\u7684\u6539\u8fdb\u578b\u70ed\u654f\u7535\u963b\uff0c\u901a\u8fc7\u5b9e\u9a8c\u6bd4\u8f83\u5e38\u89c4\u70ed\u654f\u7535\u963b\u548c\u6539\u8fdb\u578b\u70ed\u654f\u7535\u963b\u7684\u6027\u80fd\uff0c\u65e8\u5728\u5f00\u53d1\u8212\u9002\u7684\u53ef\u7a7f\u6234\u4f20\u611f\u5668\u5b9e\u65f6\u76d1\u6d4b\u76ae\u80a4\u6e29\u5ea6\u3002", "motivation": "\u5f53\u524d\u4e0b\u80a2\u5047\u80a2\u5eb7\u590d\u9762\u4e34\u76ae\u80a4\u72b6\u51b5\u3001\u523a\u6fc0\u548c\u4e0d\u9002\u7b49\u6311\u6218\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u5b9e\u65f6\u76d1\u6d4b\u76ae\u80a4\u6e29\u5ea6\u7684\u8212\u9002\u53ef\u7a7f\u6234\u4f20\u611f\u5668\uff0c\u4e3a\u7528\u6237\u548c\u77eb\u5f62\u6280\u5e08\u63d0\u4f9b\u53cd\u9988\u4ee5\u6539\u8fdb\u5047\u80a2\u9002\u914d\u3002", "method": "\u8fdb\u884c\u4e00\u7cfb\u5217\u5b9e\u9a8c\u6765\u7406\u89e3\u548c\u8868\u5f81\u7cfb\u7edf\u884c\u4e3a\uff0c\u6bd4\u8f83\u5e38\u89c4\u70ed\u654f\u7535\u963b\u548c\u6539\u8fdb\u578b\u70ed\u654f\u7535\u963b\u4f5c\u4e3a\u5047\u80a2\u6237\u5916\u4f7f\u7528\u6e29\u5ea6\u6d4b\u91cf\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "result": "\u521d\u6b65\u7ed3\u679c\u663e\u793a\u90e8\u5206\u6539\u8fdb\u578b\u70ed\u654f\u7535\u963b\u76f8\u6bd4\u5176\u4ed6\u7c7b\u578b\u8868\u73b0\u51fa\u66f4\u597d\u7684\u6e29\u5ea6\u8bb0\u5f55\u6027\u80fd\u3002", "conclusion": "\u6539\u8fdb\u578b\u70ed\u654f\u7535\u963b\u53ef\u4ee5\u4f5c\u4e3a\u5d4c\u5165\u5047\u80a2\u7cfb\u7edf\u7684\u8212\u9002\u6e29\u5ea6\u6d4b\u91cf\u7684\u6f5c\u5728\u66ff\u4ee3\u65b9\u6848\uff0c\u4e3a\u6e29\u5ea6\u5206\u5e03\u63d0\u4f9b\u6709\u4ef7\u503c\u89c1\u89e3\u5e76\u4f5c\u4e3a\u76ae\u80a4\u95ee\u9898\u7684\u65e9\u671f\u9884\u8b66\u7cfb\u7edf\u3002"}}
{"id": "2511.21045", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2511.21045", "abs": "https://arxiv.org/abs/2511.21045", "authors": ["Jionghao Han", "Jiatong Shi", "Zhuoyan Tao", "Yuxun Tang", "Yiwen Zhao", "Gus Xia", "Shinji Watanabe"], "title": "CartoonSing: Unifying Human and Nonhuman Timbres in Singing Generation", "comment": null, "summary": "Singing voice synthesis (SVS) and singing voice conversion (SVC) have achieved remarkable progress in generating natural-sounding human singing. However, existing systems are restricted to human timbres and have limited ability to synthesize voices outside the human range, which are increasingly demanded in creative applications such as video games, movies, and virtual characters. We introduce Non-Human Singing Generation (NHSG), covering non-human singing voice synthesis (NHSVS) and non-human singing voice conversion (NHSVC), as a novel machine learning task for generating musically coherent singing with non-human timbral characteristics. NHSG is particularly challenging due to the scarcity of non-human singing data, the lack of symbolic alignment, and the wide timbral gap between human and non-human voices. To address these challenges, we propose CartoonSing, a unified framework that integrates singing voice synthesis and conversion while bridging human and non-human singing generation. CartoonSing employs a two-stage pipeline: a score representation encoder trained with annotated human singing and a timbre-aware vocoder that reconstructs waveforms for both human and non-human audio. Experiments demonstrate that CartoonSing successfully generates non-human singing voices, generalizes to novel timbres, and extends conventional SVS and SVC toward creative, non-human singing generation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u975e\u4eba\u7c7b\u6b4c\u58f0\u751f\u6210\uff08NHSG\uff09\u4efb\u52a1\uff0c\u5305\u62ec\u975e\u4eba\u7c7b\u6b4c\u58f0\u5408\u6210\uff08NHSVS\uff09\u548c\u975e\u4eba\u7c7b\u6b4c\u58f0\u8f6c\u6362\uff08NHSVC\uff09\uff0c\u65e8\u5728\u751f\u6210\u5177\u6709\u975e\u4eba\u7c7b\u97f3\u8272\u7279\u5f81\u7684\u6b4c\u58f0\u3002", "motivation": "\u73b0\u6709\u6b4c\u58f0\u5408\u6210\u7cfb\u7edf\u4ec5\u9650\u4e8e\u4eba\u7c7b\u97f3\u8272\uff0c\u65e0\u6cd5\u751f\u6210\u8d85\u51fa\u4eba\u7c7b\u8303\u56f4\u7684\u6b4c\u58f0\uff0c\u800c\u89c6\u9891\u6e38\u620f\u3001\u7535\u5f71\u548c\u865a\u62df\u89d2\u8272\u7b49\u521b\u610f\u5e94\u7528\u5bf9\u6b64\u7c7b\u58f0\u97f3\u7684\u9700\u6c42\u65e5\u76ca\u589e\u957f\u3002", "method": "\u63d0\u51faCartoonSing\u6846\u67b6\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u6d41\u7a0b\uff1a\u4f7f\u7528\u6807\u6ce8\u4eba\u7c7b\u6b4c\u58f0\u8bad\u7ec3\u7684\u97f3\u7b26\u8868\u793a\u7f16\u7801\u5668\uff0c\u4ee5\u53ca\u80fd\u591f\u91cd\u5efa\u4eba\u7c7b\u548c\u975e\u4eba\u7c7b\u97f3\u9891\u6ce2\u5f62\u7684\u97f3\u8272\u611f\u77e5\u58f0\u7801\u5668\u3002", "result": "\u5b9e\u9a8c\u8868\u660eCartoonSing\u6210\u529f\u751f\u6210\u4e86\u975e\u4eba\u7c7b\u6b4c\u58f0\uff0c\u80fd\u591f\u6cdb\u5316\u5230\u65b0\u97f3\u8272\uff0c\u5e76\u5c06\u4f20\u7edfSVS\u548cSVC\u6269\u5c55\u5230\u521b\u610f\u6027\u975e\u4eba\u7c7b\u6b4c\u58f0\u751f\u6210\u3002", "conclusion": "CartoonSing\u7edf\u4e00\u4e86\u6b4c\u58f0\u5408\u6210\u4e0e\u8f6c\u6362\uff0c\u5f25\u5408\u4e86\u4eba\u7c7b\u4e0e\u975e\u4eba\u7c7b\u6b4c\u58f0\u751f\u6210\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4e3a\u521b\u610f\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.21080", "categories": ["eess.SP", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.21080", "abs": "https://arxiv.org/abs/2511.21080", "authors": ["Yeswanth Ravichandran", "Duoduo Liao", "Charan Teja Kurakula"], "title": "Data-Driven Assessment of Concrete Slab Integrity via Impact-Echo Signals and Neural Networks", "comment": "Accepted by IEEE Big Data 2025", "summary": "Subsurface defects such as delamination, voids, and honeycombing critically affect the durability of concrete bridge decks but are difficult to detect reliably using visual inspection or manual sounding. This paper presents a machine learning based Impact Echo (IE) framework that automates both defect localization and multi-class classification of common concrete defects. Raw IE signals from Federal Highway Administration (FHWA) laboratory slabs and in-service bridge decks are transformed via Fast Fourier Transform (FFT) into dominant peak-frequency features and interpolated into spatial maps for defect zone visualization. Unsupervised k-means clustering highlights low-frequency, defect-prone regions, while Ground Truth Masks (GTMs) derived from seeded lab defects are used to validate spatial accuracy and generate high-confidence training labels. From these validated regions, spatially ordered peak-frequency sequences are constructed and fed into a stacked Long Short-Term Memory (LSTM) network that classifies four defect types shallow delamination, deep delamination, voids, and honeycombing with 73% overall accuracy. Field validation on the bridge deck demonstrates that models trained on laboratory data generalize under realistic coupling, noise, and environmental variability. The proposed framework enhances the objectivity, scalability, and repeatability of Non-Destructive Evaluation (NDE), supporting intelligent, data-driven bridge health monitoring at a network scale.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u51b2\u51fb\u56de\u6ce2\u6846\u67b6\uff0c\u81ea\u52a8\u5b9a\u4f4d\u6df7\u51dd\u571f\u7f3a\u9677\u5e76\u8fdb\u884c\u591a\u7c7b\u522b\u5206\u7c7b\uff0c\u5305\u62ec\u6d45\u5c42\u5265\u79bb\u3001\u6df1\u5c42\u5265\u79bb\u3001\u7a7a\u6d1e\u548c\u8702\u7a9d\u72b6\u7f3a\u9677\uff0c\u51c6\u786e\u7387\u8fbe73%\u3002", "motivation": "\u6df7\u51dd\u571f\u6865\u9762\u677f\u4e2d\u7684\u5730\u4e0b\u7f3a\u9677\uff08\u5982\u5265\u79bb\u3001\u7a7a\u6d1e\u548c\u8702\u7a9d\u72b6\u7f3a\u9677\uff09\u4e25\u91cd\u5f71\u54cd\u8010\u4e45\u6027\uff0c\u4f46\u89c6\u89c9\u68c0\u67e5\u6216\u624b\u52a8\u6572\u51fb\u96be\u4ee5\u53ef\u9760\u68c0\u6d4b\u3002", "method": "\u5c06\u539f\u59cb\u51b2\u51fb\u56de\u6ce2\u4fe1\u53f7\u901a\u8fc7FFT\u8f6c\u6362\u4e3a\u5cf0\u503c\u9891\u7387\u7279\u5f81\u5e76\u63d2\u503c\u4e3a\u7a7a\u95f4\u56fe\uff0c\u4f7f\u7528k-means\u805a\u7c7b\u7a81\u51fa\u7f3a\u9677\u533a\u57df\uff0c\u6784\u5efa\u7a7a\u95f4\u6709\u5e8f\u7684\u5cf0\u503c\u9891\u7387\u5e8f\u5217\u8f93\u5165\u5806\u53e0LSTM\u7f51\u7edc\u8fdb\u884c\u5206\u7c7b\u3002", "result": "\u5728\u6865\u9762\u677f\u4e0a\u7684\u73b0\u573a\u9a8c\u8bc1\u8868\u660e\uff0c\u57fa\u4e8e\u5b9e\u9a8c\u5ba4\u6570\u636e\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u771f\u5b9e\u8026\u5408\u3001\u566a\u58f0\u548c\u73af\u5883\u53d8\u5316\u4e0b\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u6574\u4f53\u5206\u7c7b\u51c6\u786e\u7387\u8fbe73%\u3002", "conclusion": "\u8be5\u6846\u67b6\u63d0\u9ad8\u4e86\u65e0\u635f\u8bc4\u4f30\u7684\u5ba2\u89c2\u6027\u3001\u53ef\u6269\u5c55\u6027\u548c\u53ef\u91cd\u590d\u6027\uff0c\u652f\u6301\u7f51\u7edc\u89c4\u6a21\u7684\u667a\u80fd\u3001\u6570\u636e\u9a71\u52a8\u7684\u6865\u6881\u5065\u5eb7\u76d1\u6d4b\u3002"}}
{"id": "2511.21270", "categories": ["cs.SD", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.21270", "abs": "https://arxiv.org/abs/2511.21270", "authors": ["Yicheng Zhong", "Peiji Yang", "Zhisheng Wang"], "title": "Multi-Reward GRPO for Stable and Prosodic Single-Codebook TTS LLMs at Scale", "comment": "4 pages, 2 figures", "summary": "Recent advances in Large Language Models (LLMs) have transformed text-to-speech (TTS) synthesis, inspiring autoregressive frameworks that represent speech as sequences of discrete codec tokens. Among them, single-codebook TTS LLMs have emerged as compact and streamable architectures that jointly model semantic and acoustic integration. However, despite their efficiency, these models often exhibit unstable prosody, speaker drift, and degraded naturalness. To address these issues, we propose a multi-reward Group Relative Policy Optimization (GRPO) framework that directly optimizes the token generation policy of single-codebook TTS LLMs. Beyond standard intelligibility and speaker similarity objectives, our design integrates three rule-based rewards: a length penalty for duration consistency, an entropy regularization reward for decoding stability, and an LLM-annotated prosody alignment reward that explicitly supervises rhythm. In this prosody reward, an external reasoning LLM predicts multiple plausible pause structures via in-context learning, providing a human-preference-aligned supervisory signal for GRPO training. To assess universality, we further attach a flow-matching (FM) decoder on top of the GRPO-optimized AR backbone and observe consistent additional gains, indicating that our reinforcement optimization enhances the intrinsic AR policy. We further conduct a scalability analysis across data sizes and model scales, revealing that the proposed method consistently enhances prosodic stability, speaker similarity, and overall speech naturalness in single-codebook TTS LLMs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u591a\u5956\u52b1\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316(GRPO)\u6846\u67b6\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u76f4\u63a5\u4f18\u5316\u5355\u7801\u672cTTS\u5927\u8bed\u8a00\u6a21\u578b\u7684token\u751f\u6210\u7b56\u7565\uff0c\u89e3\u51b3\u97f5\u5f8b\u4e0d\u7a33\u5b9a\u3001\u8bf4\u8bdd\u4eba\u6f02\u79fb\u548c\u81ea\u7136\u5ea6\u4e0b\u964d\u95ee\u9898\u3002", "motivation": "\u5355\u7801\u672cTTS LLMs\u867d\u7136\u9ad8\u6548\u7d27\u51d1\uff0c\u4f46\u5b58\u5728\u97f5\u5f8b\u4e0d\u7a33\u5b9a\u3001\u8bf4\u8bdd\u4eba\u6f02\u79fb\u548c\u81ea\u7136\u5ea6\u4e0b\u964d\u7684\u95ee\u9898\uff0c\u9700\u8981\u6539\u8fdb\u5176token\u751f\u6210\u7b56\u7565\u3002", "method": "\u4f7f\u7528\u591a\u5956\u52b1GRPO\u6846\u67b6\uff0c\u6574\u5408\u957f\u5ea6\u60e9\u7f5a\u3001\u71b5\u6b63\u5219\u5316\u548cLLM\u6807\u6ce8\u7684\u97f5\u5f8b\u5bf9\u9f50\u5956\u52b1\uff0c\u901a\u8fc7\u5916\u90e8\u63a8\u7406LLM\u9884\u6d4b\u505c\u987f\u7ed3\u6784\u6765\u63d0\u4f9b\u76d1\u7763\u4fe1\u53f7\u3002", "result": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u97f5\u5f8b\u7a33\u5b9a\u6027\u3001\u8bf4\u8bdd\u4eba\u76f8\u4f3c\u6027\u548c\u6574\u4f53\u8bed\u97f3\u81ea\u7136\u5ea6\uff0c\u5728\u6d41\u5339\u914d\u89e3\u7801\u5668\u4e0a\u4e5f\u80fd\u83b7\u5f97\u4e00\u81f4\u589e\u76ca\u3002", "conclusion": "GRPO\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86\u5355\u7801\u672cTTS LLMs\u7684\u6027\u80fd\uff0c\u589e\u5f3a\u4e86\u5185\u5728\u81ea\u56de\u5f52\u7b56\u7565\uff0c\u5728\u4e0d\u540c\u6570\u636e\u89c4\u6a21\u548c\u6a21\u578b\u5c3a\u5ea6\u4e0b\u90fd\u5177\u6709\u826f\u597d\u7684\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2511.21133", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.21133", "abs": "https://arxiv.org/abs/2511.21133", "authors": ["Xi Zhang", "Miguel Bernal", "Wei-Ning Lee"], "title": "2D Sparse Array Design via Reweighted L1 Second Order Cone Programming for 3D Ultrasound Imaging", "comment": null, "summary": "Two-dimensional (2D) fully-addressed arrays can conveniently realize three-dimensional (3D) ultrasound imaging while fully controlled such arrays usually demands thousands of independent channels, which is costly. Sparse array technique using stochastic optimization methods is one of promising techniques to reduce channel counts while due to the stochastic nature of these methods, the optimized results are usually unstable. In this work, we introduce a sparse array design approach that formulates the synthesis problem of sparse arrays as second-order cone programming (SOCP) and a re-weighted L1 technique is implemented to sequentially optimize the SOCP. Based on this method, an on-grid quasi-flatten side-lobe (Q-Flats) 2D sparse array with side-lobe level (SLL) no more than -21.26 dB and 252 activated elements is designed, which aims to achieve as high contrast performance as possible under the limits of resolution and maximum number of independent channels (i.e., 256). The imaging performance of the Q-Flats array was compared with those of a corresponding dense array (Dense), a Fermat spiral array (Spiral) and a spatially 50%-Tukey tapered spiral array (Spiral-Taper) using Field II simulations in a multi-angle steered diverging wave transmission scheme. It was demonstrated that the Dense achieved the best resolution and contrast and the Spiral-Taper the worst. The Q-Flats showed better resolution (about 3%) but slightly worse contrast than the Spiral. All the results indicate the re-weighted L1 SOCP method is a promising and flexible method for seeking trade-offs among resolution, contrast, and number of activated elements.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u4e8c\u9636\u9525\u89c4\u5212\u548c\u91cd\u52a0\u6743L1\u6280\u672f\u7684\u7a00\u758f\u9635\u5217\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u7528\u4e8e\u51cf\u5c113D\u8d85\u58f0\u6210\u50cf\u4e2d\u7684\u901a\u9053\u6570\u91cf\uff0c\u5728\u4fdd\u6301\u5206\u8fa8\u7387\u7684\u540c\u65f6\u4f18\u5316\u5bf9\u6bd4\u5ea6\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u5168\u5bfb\u57402D\u9635\u5217\u9700\u8981\u6570\u5343\u4e2a\u72ec\u7acb\u901a\u9053\uff0c\u6210\u672c\u9ad8\u6602\u3002\u73b0\u6709\u968f\u673a\u4f18\u5316\u65b9\u6cd5\u8bbe\u8ba1\u7684\u7a00\u758f\u9635\u5217\u7ed3\u679c\u4e0d\u7a33\u5b9a\uff0c\u9700\u8981\u66f4\u53ef\u9760\u7684\u8bbe\u8ba1\u65b9\u6cd5\u3002", "method": "\u5c06\u7a00\u758f\u9635\u5217\u5408\u6210\u95ee\u9898\u5efa\u6a21\u4e3a\u4e8c\u9636\u9525\u89c4\u5212\u95ee\u9898\uff0c\u5e76\u91c7\u7528\u91cd\u52a0\u6743L1\u6280\u672f\u8fdb\u884c\u5e8f\u5217\u4f18\u5316\uff0c\u8bbe\u8ba1\u5177\u6709\u51c6\u5e73\u5766\u65c1\u74e3\u76842D\u7a00\u758f\u9635\u5217\u3002", "result": "\u6210\u529f\u8bbe\u8ba1\u51fa252\u4e2a\u6fc0\u6d3b\u5143\u7d20\u7684Q-Flats\u9635\u5217\uff0c\u65c1\u74e3\u7535\u5e73\u2264-21.26dB\u3002\u4e0e\u5bc6\u96c6\u9635\u5217\u3001\u8d39\u9a6c\u87ba\u65cb\u9635\u5217\u548c\u9525\u5316\u87ba\u65cb\u9635\u5217\u76f8\u6bd4\uff0cQ-Flats\u5728\u5206\u8fa8\u7387\u4e0a\u4f18\u4e8e\u87ba\u65cb\u9635\u5217\u7ea63%\uff0c\u5bf9\u6bd4\u5ea6\u7565\u5dee\u3002", "conclusion": "\u91cd\u52a0\u6743L1 SOCP\u65b9\u6cd5\u662f\u4e00\u79cd\u6709\u524d\u666f\u4e14\u7075\u6d3b\u7684\u65b9\u6cd5\uff0c\u53ef\u5728\u5206\u8fa8\u7387\u3001\u5bf9\u6bd4\u5ea6\u548c\u6fc0\u6d3b\u5143\u7d20\u6570\u91cf\u4e4b\u95f4\u5bfb\u6c42\u5e73\u8861\u3002"}}
{"id": "2511.21274", "categories": ["eess.SP", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.21274", "abs": "https://arxiv.org/abs/2511.21274", "authors": ["Junhui Rao", "Yi Liu", "Jichen Zhang", "Zhaoyang Ming", "Tianrui Qiao", "Yujie Zhang", "Chi Yuk Chiu", "Hua Wang", "Ross Murch"], "title": "Multiport Analytical Pixel Electromagnetic Simulator (MAPES) for AI-assisted RFIC and Microwave Circuit Design", "comment": null, "summary": "This paper proposes a novel analytical framework, termed the Multiport Analytical Pixel Electromagnetic Simulator (MAPES). MAPES enables efficient and accurate prediction of the electromagnetic (EM) performance of arbitrary pixel-based microwave (MW) and RFIC structures. Inspired by the Integrated Internal Multiport Method (IMPM), MAPES extends the concept to the pixel presence/absence domain used in AI-assisted EM design. By introducing virtual pixels and diagonal virtual pixels and inserting virtual ports at critical positions, MAPES captures all horizontal, vertical, and diagonal electromagnetic couplings within a single multiport impedance matrix. Only a small set of full-wave simulations (typically about 1% of the datasets required by AI-assisted EM simulators) is needed to construct this matrix. Subsequently, any arbitrary pixel configuration can be evaluated analytically using a closed-form multiport relation without additional full-wave calculations. The proposed approach eliminates data-driven overfitting and ensures accurate results across all design variations. Comprehensive examples for single- and double-layer CMOS processes (180 nm and 65 nm) and PCBs confirm that MAPES achieves high prediction accuracy with 600- 2000x speed improvement compared to CST simulations. Owing to its efficiency, scalability and reliability, MAPES provides a practical and versatile tool for AI-assisted MW circuit and RFIC design across diverse fabrication technologies.", "AI": {"tldr": "MAPES\u662f\u4e00\u79cd\u65b0\u9896\u7684\u591a\u7aef\u53e3\u5206\u6790\u50cf\u7d20\u7535\u78c1\u6a21\u62df\u5668\uff0c\u80fd\u591f\u9ad8\u6548\u51c6\u786e\u5730\u9884\u6d4b\u4efb\u610f\u57fa\u4e8e\u50cf\u7d20\u7684\u5fae\u6ce2\u548cRFIC\u7ed3\u6784\u7684\u7535\u78c1\u6027\u80fd\uff0c\u4ec5\u9700\u7ea61%\u7684\u5168\u6ce2\u4eff\u771f\u6570\u636e\u5373\u53ef\u6784\u5efa\u591a\u7aef\u53e3\u963b\u6297\u77e9\u9635\uff0c\u5b9e\u73b0600-2000\u500d\u7684\u901f\u5ea6\u63d0\u5347\u3002", "motivation": "\u4f20\u7edfAI\u8f85\u52a9\u7535\u78c1\u8bbe\u8ba1\u9700\u8981\u5927\u91cf\u6570\u636e\u96c6\uff0c\u5b58\u5728\u8fc7\u62df\u5408\u95ee\u9898\u4e14\u8ba1\u7b97\u6548\u7387\u4f4e\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u51c6\u786e\u7684\u65b9\u6cd5\u6765\u9884\u6d4b\u50cf\u7d20\u57fa\u5fae\u6ce2\u548cRFIC\u7ed3\u6784\u7684\u7535\u78c1\u6027\u80fd\u3002", "method": "\u57fa\u4e8e\u96c6\u6210\u5185\u90e8\u591a\u7aef\u53e3\u65b9\u6cd5\uff0c\u5f15\u5165\u865a\u62df\u50cf\u7d20\u548c\u5bf9\u89d2\u865a\u62df\u50cf\u7d20\uff0c\u5728\u5173\u952e\u4f4d\u7f6e\u63d2\u5165\u865a\u62df\u7aef\u53e3\uff0c\u901a\u8fc7\u5c11\u91cf\u5168\u6ce2\u4eff\u771f\u6784\u5efa\u5305\u542b\u6240\u6709\u6c34\u5e73\u3001\u5782\u76f4\u548c\u5bf9\u89d2\u7535\u78c1\u8026\u5408\u7684\u591a\u7aef\u53e3\u963b\u6297\u77e9\u9635\u3002", "result": "\u5728\u5355\u5c42\u548c\u53cc\u5c42CMOS\u5de5\u827a\u53caPCB\u4e0a\u9a8c\u8bc1\uff0cMAPES\u5b9e\u73b0\u4e86\u9ad8\u9884\u6d4b\u7cbe\u5ea6\uff0c\u76f8\u6bd4CST\u4eff\u771f\u901f\u5ea6\u63d0\u5347600-2000\u500d\uff0c\u6d88\u9664\u4e86\u6570\u636e\u9a71\u52a8\u7684\u8fc7\u62df\u5408\u95ee\u9898\u3002", "conclusion": "MAPES\u56e0\u5176\u9ad8\u6548\u6027\u3001\u53ef\u6269\u5c55\u6027\u548c\u53ef\u9760\u6027\uff0c\u4e3a\u8de8\u591a\u79cd\u5236\u9020\u6280\u672f\u7684AI\u8f85\u52a9\u5fae\u6ce2\u7535\u8def\u548cRFIC\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u901a\u7528\u7684\u5de5\u5177\u3002"}}
{"id": "2511.21325", "categories": ["cs.SD", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.21325", "abs": "https://arxiv.org/abs/2511.21325", "authors": ["Ido Nitzan HIdekel", "Gal lifshitz", "Khen Cohen", "Dan Raviv"], "title": "SONAR: Spectral-Contrastive Audio Residuals for Generalizable Deepfake Detection", "comment": null, "summary": "Deepfake (DF) audio detectors still struggle to generalize to out of distribution inputs. A central reason is spectral bias, the tendency of neural networks to learn low-frequency structure before high-frequency (HF) details, which both causes DF generators to leave HF artifacts and leaves those same artifacts under-exploited by common detectors. To address this gap, we propose Spectral-cONtrastive Audio Residuals (SONAR), a frequency-guided framework that explicitly disentangles an audio signal into complementary representations. An XLSR encoder captures the dominant low-frequency content, while the same cloned path, preceded by learnable SRM, value-constrained high-pass filters, distills faint HF residuals. Frequency cross-attention reunites the two views for long- and short-range frequency dependencies, and a frequency-aware Jensen-Shannon contrastive loss pulls real content-noise pairs together while pushing fake embeddings apart, accelerating optimization and sharpening decision boundaries. Evaluated on the ASVspoof 2021 and in-the-wild benchmarks, SONAR attains state-of-the-art performance and converges four times faster than strong baselines. By elevating faint high-frequency residuals to first-class learning signals, SONAR unveils a fully data-driven, frequency-guided contrastive framework that splits the latent space into two disjoint manifolds: natural-HF for genuine audio and distorted-HF for synthetic audio, thereby sharpening decision boundaries. Because the scheme operates purely at the representation level, it is architecture-agnostic and, in future work, can be seamlessly integrated into any model or modality where subtle high-frequency cues are decisive.", "AI": {"tldr": "SONAR\u662f\u4e00\u4e2a\u9891\u7387\u5f15\u5bfc\u7684\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u663e\u5f0f\u5206\u79bb\u97f3\u9891\u4fe1\u53f7\u4e3a\u4e92\u8865\u8868\u793a\u6765\u68c0\u6d4bDeepfake\u97f3\u9891\u3002\u5b83\u4f7f\u7528XLSR\u7f16\u7801\u5668\u6355\u83b7\u4f4e\u9891\u5185\u5bb9\uff0c\u540c\u65f6\u901a\u8fc7\u53ef\u5b66\u4e60SRM\u548c\u9ad8\u901a\u6ee4\u6ce2\u5668\u63d0\u53d6\u9ad8\u9891\u6b8b\u5dee\uff0c\u7136\u540e\u901a\u8fc7\u9891\u7387\u4ea4\u53c9\u6ce8\u610f\u529b\u91cd\u65b0\u7ed3\u5408\u4e24\u79cd\u89c6\u56fe\uff0c\u5b9e\u73b0\u6700\u5148\u8fdb\u7684\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3Deepfake\u97f3\u9891\u68c0\u6d4b\u5668\u5728\u5206\u5e03\u5916\u8f93\u5165\u4e0a\u6cdb\u5316\u80fd\u529b\u5dee\u7684\u95ee\u9898\u3002\u4e3b\u8981\u539f\u56e0\u662f\u9891\u8c31\u504f\u5dee\u5bfc\u81f4\u795e\u7ecf\u7f51\u7edc\u4f18\u5148\u5b66\u4e60\u4f4e\u9891\u7ed3\u6784\uff0c\u4f7f\u5f97DF\u751f\u6210\u5668\u9057\u7559\u9ad8\u9891\u4f2a\u5f71\uff0c\u800c\u8fd9\u4e9b\u4f2a\u5f71\u5728\u5e38\u89c1\u68c0\u6d4b\u5668\u4e2d\u672a\u88ab\u5145\u5206\u5229\u7528\u3002", "method": "\u63d0\u51faSONAR\u6846\u67b6\uff1a1\uff09\u4f7f\u7528XLSR\u7f16\u7801\u5668\u6355\u83b7\u4e3b\u5bfc\u4f4e\u9891\u5185\u5bb9\uff1b2\uff09\u901a\u8fc7\u53ef\u5b66\u4e60SRM\u548c\u503c\u7ea6\u675f\u9ad8\u901a\u6ee4\u6ce2\u5668\u63d0\u53d6\u5fae\u5f31\u9ad8\u9891\u6b8b\u5dee\uff1b3\uff09\u9891\u7387\u4ea4\u53c9\u6ce8\u610f\u529b\u878d\u5408\u4e24\u79cd\u89c6\u56fe\uff1b4\uff09\u9891\u7387\u611f\u77e5Jensen-Shannon\u5bf9\u6bd4\u635f\u5931\u62c9\u8fd1\u771f\u5b9e\u97f3\u9891\u5bf9\u3001\u63a8\u5f00\u4f2a\u9020\u5d4c\u5165\u3002", "result": "\u5728ASVspoof 2021\u548c\u771f\u5b9e\u4e16\u754c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u6536\u655b\u901f\u5ea6\u6bd4\u5f3a\u57fa\u7ebf\u5feb4\u500d\u3002\u5c06\u6f5c\u5728\u7a7a\u95f4\u5206\u4e3a\u4e24\u4e2a\u4e0d\u76f8\u4ea4\u7684\u6d41\u5f62\uff1a\u81ea\u7136\u9ad8\u9891\u5bf9\u5e94\u771f\u5b9e\u97f3\u9891\uff0c\u5931\u771f\u9ad8\u9891\u5bf9\u5e94\u5408\u6210\u97f3\u9891\u3002", "conclusion": "SONAR\u901a\u8fc7\u5c06\u5fae\u5f31\u9ad8\u9891\u6b8b\u5dee\u63d0\u5347\u4e3a\u4e00\u7ea7\u5b66\u4e60\u4fe1\u53f7\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b8c\u5168\u6570\u636e\u9a71\u52a8\u3001\u9891\u7387\u5f15\u5bfc\u7684\u5bf9\u6bd4\u6846\u67b6\u3002\u8be5\u65b9\u6848\u5728\u8868\u793a\u5c42\u9762\u8fd0\u884c\uff0c\u4e0e\u67b6\u6784\u65e0\u5173\uff0c\u672a\u6765\u53ef\u65e0\u7f1d\u96c6\u6210\u5230\u4efb\u4f55\u6a21\u578b\u6216\u6a21\u6001\u4e2d\uff0c\u5176\u4e2d\u5fae\u5999\u7684\u9ad8\u9891\u7ebf\u7d22\u5177\u6709\u51b3\u5b9a\u6027\u4f5c\u7528\u3002"}}
{"id": "2511.21340", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.21340", "abs": "https://arxiv.org/abs/2511.21340", "authors": ["Chin-Hung Chen", "Ivana Nikoloska", "Wim van Houtum", "Yan Wu", "Alex Alvarado"], "title": "Phase-Aware Code-Aided EM Algorithm for Blind Channel Estimation in PSK-Modulated OFDM", "comment": "preprint", "summary": "This paper presents a fully blind phase-aware expectation-maximization (EM) algorithm for OFDM systems with the phase-shift keying (PSK) modulation. We address the well-known local maximum problem of the EM algorithm for blind channel estimation. This is primarily caused by the unknown phase ambiguity in the channel estimates, which conventional blind EM estimators cannot resolve. To overcome this limitation, we propose to exploit the extrinsic information from the decoder as model evidence metrics. A finite set of candidate models is generated based on the inherent symmetries of PSK modulation, and the decoder selects the most likely candidate model. Simulation results demonstrate that, when combined with a simple convolutional code, the phase-aware EM algorithm reliably resolves phase ambiguity during the initialization stage and reduces the local convergence rate from 80% to nearly 0% in frequency-selective channels with a constant phase ambiguity. The algorithm is invoked only once after the EM initialization stage, resulting in negligible additional complexity during subsequent turbo iterations.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9PSK\u8c03\u5236OFDM\u7cfb\u7edf\u7684\u5168\u76f2\u76f8\u4f4d\u611f\u77e5EM\u7b97\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u89e3\u7801\u5668\u7684\u5916\u90e8\u4fe1\u606f\u4f5c\u4e3a\u6a21\u578b\u8bc1\u636e\u5ea6\u91cf\uff0c\u89e3\u51b3\u4f20\u7edf\u76f2EM\u4f30\u8ba1\u5668\u65e0\u6cd5\u89e3\u51b3\u7684\u76f8\u4f4d\u6a21\u7cca\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u76f2\u4fe1\u9053\u4f30\u8ba1\u4e2dEM\u7b97\u6cd5\u7684\u5c40\u90e8\u6700\u5927\u503c\u95ee\u9898\uff0c\u8be5\u95ee\u9898\u4e3b\u8981\u7531\u4fe1\u9053\u4f30\u8ba1\u4e2d\u7684\u672a\u77e5\u76f8\u4f4d\u6a21\u7cca\u5f15\u8d77\uff0c\u4f20\u7edf\u76f2EM\u4f30\u8ba1\u5668\u65e0\u6cd5\u89e3\u51b3\u8fd9\u4e00\u9650\u5236\u3002", "method": "\u57fa\u4e8ePSK\u8c03\u5236\u56fa\u6709\u5bf9\u79f0\u6027\u751f\u6210\u6709\u9650\u5019\u9009\u6a21\u578b\u96c6\uff0c\u5229\u7528\u89e3\u7801\u5668\u9009\u62e9\u6700\u53ef\u80fd\u7684\u5019\u9009\u6a21\u578b\uff0c\u5728EM\u521d\u59cb\u5316\u9636\u6bb5\u540e\u4ec5\u8c03\u7528\u4e00\u6b21\u8be5\u7b97\u6cd5\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u5f53\u4e0e\u7b80\u5355\u5377\u79ef\u7801\u7ed3\u5408\u65f6\uff0c\u76f8\u4f4d\u611f\u77e5EM\u7b97\u6cd5\u5728\u521d\u59cb\u5316\u9636\u6bb5\u53ef\u9760\u5730\u89e3\u51b3\u76f8\u4f4d\u6a21\u7cca\uff0c\u5728\u5177\u6709\u6052\u5b9a\u76f8\u4f4d\u6a21\u7cca\u7684\u9891\u7387\u9009\u62e9\u6027\u4fe1\u9053\u4e2d\uff0c\u5c06\u5c40\u90e8\u6536\u655b\u7387\u4ece80%\u964d\u4f4e\u5230\u63a5\u8fd10%\u3002", "conclusion": "\u8be5\u7b97\u6cd5\u5728\u540e\u7eedturbo\u8fed\u4ee3\u4e2d\u4ec5\u4ea7\u751f\u53ef\u5ffd\u7565\u7684\u989d\u5916\u590d\u6742\u5ea6\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u76f2\u4fe1\u9053\u4f30\u8ba1\u4e2d\u7684\u76f8\u4f4d\u6a21\u7cca\u95ee\u9898\u3002"}}
{"id": "2511.21342", "categories": ["cs.SD", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.21342", "abs": "https://arxiv.org/abs/2511.21342", "authors": ["Gen\u00eds Plaja-Roglans", "Yun-Ning Hung", "Xavier Serra", "Igor Pereira"], "title": "Generating Separated Singing Vocals Using a Diffusion Model Conditioned on Music Mixtures", "comment": "Accepted for publication at WASPAA 2025", "summary": "Separating the individual elements in a musical mixture is an essential process for music analysis and practice. While this is generally addressed using neural networks optimized to mask or transform the time-frequency representation of a mixture to extract the target sources, the flexibility and generalization capabilities of generative diffusion models are giving rise to a novel class of solutions for this complicated task. In this work, we explore singing voice separation from real music recordings using a diffusion model which is trained to generate the solo vocals conditioned on the corresponding mixture. Our approach improves upon prior generative systems and achieves competitive objective scores against non-generative baselines when trained with supplementary data. The iterative nature of diffusion sampling enables the user to control the quality-efficiency trade-off, and also refine the output when needed. We present an ablation study of the sampling algorithm, highlighting the effects of the user-configurable parameters.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u6b4c\u58f0\u5206\u79bb\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bad\u7ec3\u6a21\u578b\u5728\u7ed9\u5b9a\u97f3\u4e50\u6df7\u5408\u7269\u7684\u6761\u4ef6\u4e0b\u751f\u6210\u72ec\u5531\u4eba\u58f0\uff0c\u5728\u751f\u6210\u5f0f\u7cfb\u7edf\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u4e0e\u975e\u751f\u6210\u5f0f\u57fa\u7ebf\u65b9\u6cd5\u7ade\u4e89\u3002", "motivation": "\u97f3\u4e50\u6df7\u5408\u7269\u4e2d\u5206\u79bb\u5355\u4e2a\u5143\u7d20\u5bf9\u97f3\u4e50\u5206\u6790\u548c\u5b9e\u8df5\u81f3\u5173\u91cd\u8981\u3002\u4f20\u7edf\u65b9\u6cd5\u4f7f\u7528\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u65f6\u9891\u8868\u793a\u63a9\u7801\u6216\u53d8\u6362\uff0c\u800c\u6269\u6563\u6a21\u578b\u7684\u7075\u6d3b\u6027\u548c\u6cdb\u5316\u80fd\u529b\u4e3a\u8fd9\u4e00\u590d\u6742\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u6269\u6563\u6a21\u578b\uff0c\u8bad\u7ec3\u5176\u5728\u5bf9\u5e94\u6df7\u5408\u7269\u7684\u6761\u4ef6\u4e0b\u751f\u6210\u72ec\u5531\u4eba\u58f0\u3002\u901a\u8fc7\u8fed\u4ee3\u91c7\u6837\u8fc7\u7a0b\uff0c\u7528\u6237\u53ef\u4ee5\u63a7\u5236\u8d28\u91cf-\u6548\u7387\u6743\u8861\uff0c\u5e76\u5728\u9700\u8981\u65f6\u7ec6\u5316\u8f93\u51fa\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u751f\u6210\u5f0f\u7cfb\u7edf\u4e2d\u6709\u6240\u6539\u8fdb\uff0c\u5f53\u4f7f\u7528\u8865\u5145\u6570\u636e\u8bad\u7ec3\u65f6\uff0c\u5728\u5ba2\u89c2\u8bc4\u5206\u4e0a\u4e0e\u975e\u751f\u6210\u5f0f\u57fa\u7ebf\u65b9\u6cd5\u7ade\u4e89\u3002\u6d88\u878d\u7814\u7a76\u5c55\u793a\u4e86\u91c7\u6837\u7b97\u6cd5\u4e2d\u7528\u6237\u53ef\u914d\u7f6e\u53c2\u6570\u7684\u5f71\u54cd\u3002", "conclusion": "\u6269\u6563\u6a21\u578b\u4e3a\u6b4c\u58f0\u5206\u79bb\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u751f\u6210\u5f0f\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u7528\u6237\u53ef\u63a7\u7684\u91c7\u6837\u8fc7\u7a0b\u548c\u7ade\u4e89\u6027\u7684\u6027\u80fd\u8868\u73b0\u3002"}}
{"id": "2511.21345", "categories": ["eess.SP", "cs.IT"], "pdf": "https://arxiv.org/pdf/2511.21345", "abs": "https://arxiv.org/abs/2511.21345", "authors": ["Chin-Hung Chen", "Yan Wu", "Wim van Houtum", "Alex Alvarado"], "title": "Blind Turbo Demodulation for Differentially Encoded OFDM with 2D Trellis Decomposition", "comment": "preprint", "summary": "Digital Audio Broadcasting (DAB)-like systems employ differentially encoded (DE) phase-shift keying (PSK) for transmission. While turbo-DE-PSK receivers offer substantial performance gains through iterative decoding by making the DE-PSK an inner code, they rely on accurate channel estimation without pilots, which is a key challenge in DAB-like scenarios. This paper develops a fully blind turbo-DE-PSK scheme that jointly estimates channel phase, channel gain, and noise variance directly from the received signal. The design leverages a two-dimensional (2D) trellis decomposition for blind phase estimation, complemented by power-based estimators for channel gain and noise variance. We provide a comprehensive system assessment across practical system parameters, including inner code length, phase quantization, and 2D block size. Simulation results show that the blind 2D turbo demodulator approaches the performance of receivers with perfect channel knowledge and remains robust under realistic transmission conditions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5b8c\u5168\u76f2\u7684turbo-DE-PSK\u65b9\u6848\uff0c\u65e0\u9700\u5bfc\u9891\u5373\u53ef\u8054\u5408\u4f30\u8ba1\u4fe1\u9053\u76f8\u4f4d\u3001\u589e\u76ca\u548c\u566a\u58f0\u65b9\u5dee\uff0c\u5728DAB\u7c7b\u7cfb\u7edf\u4e2d\u5b9e\u73b0\u4e86\u63a5\u8fd1\u5b8c\u7f8e\u4fe1\u9053\u77e5\u8bc6\u7684\u6027\u80fd\u3002", "motivation": "DAB\u7c7b\u7cfb\u7edf\u4f7f\u7528\u5dee\u5206\u7f16\u7801PSK\u4f20\u8f93\uff0cturbo-DE-PSK\u63a5\u6536\u673a\u901a\u8fc7\u8fed\u4ee3\u89e3\u7801\u63d0\u4f9b\u6027\u80fd\u589e\u76ca\uff0c\u4f46\u4f9d\u8d56\u65e0\u5bfc\u9891\u7684\u51c6\u786e\u4fe1\u9053\u4f30\u8ba1\uff0c\u8fd9\u662fDAB\u7c7b\u573a\u666f\u4e2d\u7684\u5173\u952e\u6311\u6218\u3002", "method": "\u5f00\u53d1\u4e86\u5b8c\u5168\u76f2\u7684turbo-DE-PSK\u65b9\u6848\uff0c\u5229\u7528\u4e8c\u7ef4\u7f51\u683c\u5206\u89e3\u8fdb\u884c\u76f2\u76f8\u4f4d\u4f30\u8ba1\uff0c\u8f85\u4ee5\u57fa\u4e8e\u529f\u7387\u7684\u4fe1\u9053\u589e\u76ca\u548c\u566a\u58f0\u65b9\u5dee\u4f30\u8ba1\u5668\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u663e\u793a\uff0c\u76f22D turbo\u89e3\u8c03\u5668\u63a5\u8fd1\u5b8c\u7f8e\u4fe1\u9053\u77e5\u8bc6\u63a5\u6536\u673a\u7684\u6027\u80fd\uff0c\u5e76\u5728\u5b9e\u9645\u4f20\u8f93\u6761\u4ef6\u4e0b\u4fdd\u6301\u9c81\u68d2\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u76f2turbo-DE-PSK\u65b9\u6848\u6709\u6548\u89e3\u51b3\u4e86DAB\u7c7b\u7cfb\u7edf\u4e2d\u65e0\u5bfc\u9891\u4fe1\u9053\u4f30\u8ba1\u7684\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fd\u7684\u76f2\u63a5\u6536\u3002"}}
{"id": "2511.21577", "categories": ["cs.SD", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.21577", "abs": "https://arxiv.org/abs/2511.21577", "authors": ["Kexin Li", "Xiao Hu", "Ilya Grishchenko", "David Lie"], "title": "HarmonicAttack: An Adaptive Cross-Domain Audio Watermark Removal", "comment": null, "summary": "The availability of high-quality, AI-generated audio raises security challenges such as misinformation campaigns and voice-cloning fraud. A key defense against the misuse of AI-generated audio is by watermarking it, so that it can be easily distinguished from genuine audio. As those seeking to misuse AI-generated audio may thus seek to remove audio watermarks, studying effective watermark removal techniques is critical to being able to objectively evaluate the robustness of audio watermarks against removal. Previous watermark removal schemes either assume impractical knowledge of the watermarks they are designed to remove or are computationally expensive, potentially generating a false sense of confidence in current watermark schemes.\n  We introduce HarmonicAttack, an efficient audio watermark removal method that only requires the basic ability to generate the watermarks from the targeted scheme and nothing else. With this, we are able to train a general watermark removal model that is able to remove the watermarks generated by the targeted scheme from any watermarked audio sample. HarmonicAttack employs a dual-path convolutional autoencoder that operates in both temporal and frequency domains, along with GAN-style training, to separate the watermark from the original audio. When evaluated against state-of-the-art watermark schemes AudioSeal, WavMark, and Silentcipher, HarmonicAttack demonstrates greater watermark removal ability than previous watermark removal methods with near real-time performance. Moreover, while HarmonicAttack requires training, we find that it is able to transfer to out-of-distribution samples with minimal degradation in performance.", "AI": {"tldr": "HarmonicAttack\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u97f3\u9891\u6c34\u5370\u53bb\u9664\u65b9\u6cd5\uff0c\u4ec5\u9700\u76ee\u6807\u6c34\u5370\u65b9\u6848\u7684\u57fa\u672c\u751f\u6210\u80fd\u529b\uff0c\u5c31\u80fd\u8bad\u7ec3\u901a\u7528\u6c34\u5370\u53bb\u9664\u6a21\u578b\uff0c\u5728\u591a\u4e2a\u5148\u8fdb\u6c34\u5370\u65b9\u6848\u4e0a\u8868\u73b0\u51fa\u8272\u4e14\u63a5\u8fd1\u5b9e\u65f6\u6027\u80fd\u3002", "motivation": "AI\u751f\u6210\u97f3\u9891\u7684\u6ee5\u7528\u5e26\u6765\u5b89\u5168\u6311\u6218\uff0c\u6c34\u5370\u6280\u672f\u662f\u91cd\u8981\u9632\u5fa1\u624b\u6bb5\u3002\u7814\u7a76\u6709\u6548\u7684\u6c34\u5370\u53bb\u9664\u65b9\u6cd5\u5bf9\u4e8e\u5ba2\u89c2\u8bc4\u4f30\u6c34\u5370\u65b9\u6848\u7684\u9c81\u68d2\u6027\u81f3\u5173\u91cd\u8981\uff0c\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u5047\u8bbe\u4e0d\u5207\u5b9e\u9645\u7684\u77e5\u8bc6\uff0c\u8981\u4e48\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u3002", "method": "\u91c7\u7528\u53cc\u8def\u5f84\u5377\u79ef\u81ea\u7f16\u7801\u5668\uff0c\u5728\u65f6\u57df\u548c\u9891\u57df\u540c\u65f6\u64cd\u4f5c\uff0c\u7ed3\u5408GAN\u98ce\u683c\u8bad\u7ec3\uff0c\u5c06\u6c34\u5370\u4ece\u539f\u59cb\u97f3\u9891\u4e2d\u5206\u79bb\u51fa\u6765\u3002", "result": "\u5728AudioSeal\u3001WavMark\u548cSilentcipher\u7b49\u5148\u8fdb\u6c34\u5370\u65b9\u6848\u4e0a\uff0cHarmonicAttack\u8868\u73b0\u51fa\u6bd4\u5148\u524d\u65b9\u6cd5\u66f4\u5f3a\u7684\u6c34\u5370\u53bb\u9664\u80fd\u529b\uff0c\u4e14\u6027\u80fd\u63a5\u8fd1\u5b9e\u65f6\u3002", "conclusion": "HarmonicAttack\u662f\u4e00\u79cd\u9ad8\u6548\u901a\u7528\u7684\u6c34\u5370\u53bb\u9664\u65b9\u6cd5\uff0c\u5373\u4f7f\u9700\u8981\u8bad\u7ec3\uff0c\u4e5f\u80fd\u5728\u5206\u5e03\u5916\u6837\u672c\u4e0a\u4fdd\u6301\u826f\u597d\u7684\u6027\u80fd\u8f6c\u79fb\u80fd\u529b\u3002"}}
{"id": "2511.21411", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.21411", "abs": "https://arxiv.org/abs/2511.21411", "authors": ["Jungyeon Koh", "Hyeonho Noh", "Hyun Jong Yang"], "title": "Group-wise Semantic Splitting Multiple Access for Multi-User Semantic Communication", "comment": null, "summary": "In this letter, we propose a group-wise semantic splitting multiple access framework for multi-user semantic communication in downlink scenarios. The framework begins by applying a balanced clustering mechanism that groups users based on the similarity of their semantic characteristics, enabling the extraction of group-level common features and user-specific private features. The base station then transmits the common features via multicast and the private features via unicast, effectively leveraging both shared and user-dependent semantic information. To further enhance semantic separability and reconstruction fidelity, we design a composite loss function that integrates a reconstruction loss with a repulsion loss, improving both the accuracy of semantic recovery and the distinctiveness of common embeddings in the latent space. Simulation results demonstrate that the proposed method achieves up to 3.26% performance improvement over conventional schemes across various channel conditions, validating its robustness and semantic efficiency for next-generation wireless networks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7528\u6237\u8bed\u4e49\u7279\u5f81\u76f8\u4f3c\u5ea6\u7684\u5206\u7ec4\u591a\u5740\u63a5\u5165\u6846\u67b6\uff0c\u901a\u8fc7\u805a\u7c7b\u63d0\u53d6\u7ec4\u7ea7\u516c\u5171\u7279\u5f81\u548c\u7528\u6237\u7279\u5b9a\u79c1\u6709\u7279\u5f81\uff0c\u5206\u522b\u91c7\u7528\u7ec4\u64ad\u548c\u5355\u64ad\u4f20\u8f93\uff0c\u7ed3\u5408\u91cd\u6784\u635f\u5931\u548c\u6392\u65a5\u635f\u5931\u7684\u590d\u5408\u635f\u5931\u51fd\u6570\u63d0\u5347\u8bed\u4e49\u5206\u79bb\u6027\u548c\u91cd\u5efa\u8d28\u91cf\u3002", "motivation": "\u89e3\u51b3\u591a\u7528\u6237\u8bed\u4e49\u901a\u4fe1\u4e2d\u5982\u4f55\u6709\u6548\u5229\u7528\u5171\u4eab\u8bed\u4e49\u4fe1\u606f\u548c\u7528\u6237\u7279\u5b9a\u4fe1\u606f\u7684\u95ee\u9898\uff0c\u63d0\u9ad8\u8bed\u4e49\u4f20\u8f93\u6548\u7387\u548c\u91cd\u5efa\u8d28\u91cf\u3002", "method": "\u4f7f\u7528\u5e73\u8861\u805a\u7c7b\u673a\u5236\u6309\u8bed\u4e49\u7279\u5f81\u76f8\u4f3c\u5ea6\u5bf9\u7528\u6237\u5206\u7ec4\uff0c\u63d0\u53d6\u7ec4\u7ea7\u516c\u5171\u7279\u5f81\u548c\u7528\u6237\u79c1\u6709\u7279\u5f81\uff1b\u57fa\u7ad9\u901a\u8fc7\u7ec4\u64ad\u4f20\u8f93\u516c\u5171\u7279\u5f81\uff0c\u5355\u64ad\u4f20\u8f93\u79c1\u6709\u7279\u5f81\uff1b\u8bbe\u8ba1\u5305\u542b\u91cd\u6784\u635f\u5931\u548c\u6392\u65a5\u635f\u5931\u7684\u590d\u5408\u635f\u5931\u51fd\u6570\u3002", "result": "\u5728\u5404\u79cd\u4fe1\u9053\u6761\u4ef6\u4e0b\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6848\u6027\u80fd\u63d0\u5347\u6700\u9ad8\u8fbe3.26%\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u9c81\u68d2\u6027\u548c\u8bed\u4e49\u6548\u7387\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u6709\u6548\u63d0\u5347\u591a\u7528\u6237\u8bed\u4e49\u901a\u4fe1\u7684\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u4e0b\u4e00\u4ee3\u65e0\u7ebf\u7f51\u7edc\u3002"}}
{"id": "2511.21580", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2511.21580", "abs": "https://arxiv.org/abs/2511.21580", "authors": ["Beno\u00eet Gini\u00e8s", "Xiaoyu Bie", "Olivier Fercoq", "Ga\u00ebl Richard"], "title": "Harmonic-Percussive Disentangled Neural Audio Codec for Bandwidth Extension", "comment": null, "summary": "Bandwidth extension, the task of reconstructing the high-frequency components of an audio signal from its low-pass counterpart, is a long-standing problem in audio processing. While traditional approaches have evolved alongside the broader trends in signal processing, recent advances in neural architectures have significantly improved performance across a wide range of audio tasks, In this work, we extend these advances by framing bandwidth extension as an audio token prediction problem. Specifically, we train a transformer-based language model on the discrete representations produced by a disentangled neural audio codec, where the disentanglement is guided by a Harmonic-Percussive decomposition of the input signals, highlighting spectral structures particularly relevant for bandwidth extension. Our approach introduces a novel codec design that explicitly accounts for the downstream token prediction task, enabling a more effective coupling between codec structure and transformer modeling. This joint design yields high-quality reconstructions of the original signal, as measured by both objective metrics and subjective evaluations. These results highlight the importance of aligning codec disentanglement and representation learning with the generative modeling stage, and demonstrate the potential of global, representation-aware design for advancing bandwidth extension.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u97f3\u9891\u7f16\u89e3\u7801\u5668\u548cTransformer\u7684\u5e26\u5bbd\u6269\u5c55\u65b9\u6cd5\uff0c\u5c06\u5e26\u5bbd\u6269\u5c55\u95ee\u9898\u6784\u5efa\u4e3a\u97f3\u9891token\u9884\u6d4b\u4efb\u52a1\uff0c\u901a\u8fc7\u8c10\u6ce2-\u6253\u51fb\u4e50\u5206\u89e3\u5f15\u5bfc\u7684\u5206\u79bb\u5f0f\u7f16\u89e3\u7801\u5668\u8bbe\u8ba1\u5b9e\u73b0\u9ad8\u8d28\u91cf\u97f3\u9891\u91cd\u5efa\u3002", "motivation": "\u5e26\u5bbd\u6269\u5c55\u662f\u97f3\u9891\u5904\u7406\u4e2d\u7684\u957f\u671f\u96be\u9898\uff0c\u4f20\u7edf\u65b9\u6cd5\u6027\u80fd\u6709\u9650\u3002\u968f\u7740\u795e\u7ecf\u67b6\u6784\u5728\u97f3\u9891\u4efb\u52a1\u4e2d\u7684\u663e\u8457\u8fdb\u6b65\uff0c\u4f5c\u8005\u5e0c\u671b\u5c06\u8fd9\u4e9b\u8fdb\u5c55\u5e94\u7528\u4e8e\u5e26\u5bbd\u6269\u5c55\uff0c\u901a\u8fc7\u5c06\u95ee\u9898\u91cd\u65b0\u6784\u5efa\u4e3a\u97f3\u9891token\u9884\u6d4b\u6765\u63d0\u5347\u6027\u80fd\u3002", "method": "\u4f7f\u7528\u57fa\u4e8eTransformer\u7684\u8bed\u8a00\u6a21\u578b\uff0c\u5728\u7531\u5206\u79bb\u5f0f\u795e\u7ecf\u97f3\u9891\u7f16\u89e3\u7801\u5668\u4ea7\u751f\u7684\u79bb\u6563\u8868\u793a\u4e0a\u8fdb\u884c\u8bad\u7ec3\u3002\u7f16\u89e3\u7801\u5668\u8bbe\u8ba1\u91c7\u7528\u8c10\u6ce2-\u6253\u51fb\u4e50\u5206\u89e3\u5f15\u5bfc\u7684\u5206\u79bb\u7b56\u7565\uff0c\u4e13\u95e8\u9488\u5bf9\u4e0b\u6e38token\u9884\u6d4b\u4efb\u52a1\u8fdb\u884c\u4f18\u5316\uff0c\u5b9e\u73b0\u7f16\u89e3\u7801\u5668\u7ed3\u6784\u4e0eTransformer\u5efa\u6a21\u7684\u6709\u6548\u8026\u5408\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u5ba2\u89c2\u6307\u6807\u548c\u4e3b\u89c2\u8bc4\u4ef7\u4e2d\u5747\u5b9e\u73b0\u4e86\u539f\u59cb\u4fe1\u53f7\u7684\u9ad8\u8d28\u91cf\u91cd\u5efa\uff0c\u8bc1\u660e\u4e86\u8054\u5408\u8bbe\u8ba1\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u5c06\u7f16\u89e3\u7801\u5668\u5206\u79bb\u548c\u8868\u793a\u5b66\u4e60\u4e0e\u751f\u6210\u5efa\u6a21\u9636\u6bb5\u5bf9\u9f50\u81f3\u5173\u91cd\u8981\uff0c\u5168\u5c40\u7684\u3001\u8868\u793a\u611f\u77e5\u7684\u8bbe\u8ba1\u5728\u63a8\u8fdb\u5e26\u5bbd\u6269\u5c55\u65b9\u9762\u5177\u6709\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2511.21434", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.21434", "abs": "https://arxiv.org/abs/2511.21434", "authors": ["Fabrizio Andr\u00e9 Farf\u00e1n Prado", "William C\u00e9sar P\u00e9rez Campos", "Steisy Anahi Carre\u00f1o Tacuri", "Favio David Cabrera Alva", "Harold Jacobed Carhuas Lizarbe"], "title": "Design Of A Communication System To Send Text Using Lora At 400 MHz", "comment": null, "summary": "This work describes the design and implementation of a low-power wireless communication system for transmitting text using ESP32 modules and the LoRa DXLR01. The proposal arises as a solution to connectivity and energy-efficiency problems commonly found in rural areas and certain urban environments where Wi-Fi or mobile networks are unavailable or operate with limitations. To address this, LoRa technology known for its long-range capability and low power consumption is integrated with an ESP32 responsible for capturing, processing, and sending messages.\n  The LoRa DXLR01 module, which operates in the 433 MHz band, is configured with parameters aimed at maximising both transmission range and efficient energy usage. Messages are sent using Chirp Spread Spectrum (CSS) modulation, improving signal penetration in obstructed areas and reducing the likelihood of errors. On the receiving end, the ESP32 interprets the data and displays it on an LCD screen. Additionally, the received information is sent to the ThingSpeak platform, allowing remote storage and visualisation without relying on conventional network infrastructure.\n  Tests conducted in a controlled environment show an average latency of 3.2 seconds for text transmission. It was also verified that the system can be used in applications such as remote monitoring, infrastructure management, and access control.", "AI": {"tldr": "\u57fa\u4e8eESP32\u548cLoRa DXLR01\u6a21\u5757\u7684\u4f4e\u529f\u8017\u65e0\u7ebf\u901a\u4fe1\u7cfb\u7edf\uff0c\u7528\u4e8e\u5728\u7f3a\u4e4f\u4f20\u7edf\u7f51\u7edc\u57fa\u7840\u8bbe\u65bd\u7684\u533a\u57df\u4f20\u8f93\u6587\u672c\u6570\u636e", "motivation": "\u89e3\u51b3\u519c\u6751\u548c\u67d0\u4e9b\u57ce\u5e02\u73af\u5883\u4e2dWi-Fi\u6216\u79fb\u52a8\u7f51\u7edc\u4e0d\u53ef\u7528\u6216\u53d7\u9650\u65f6\u7684\u8fde\u63a5\u6027\u548c\u80fd\u6548\u95ee\u9898", "method": "\u96c6\u6210LoRa\u6280\u672f\uff08433MHz\u9891\u6bb5\uff09\u4e0eESP32\u6a21\u5757\uff0c\u4f7f\u7528Chirp\u6269\u9891\u8c03\u5236\u63d0\u9ad8\u4fe1\u53f7\u7a7f\u900f\u6027\u548c\u6297\u5e72\u6270\u80fd\u529b\uff0c\u5e76\u5c06\u63a5\u6536\u6570\u636e\u53d1\u9001\u5230ThingSpeak\u5e73\u53f0", "result": "\u5728\u53d7\u63a7\u73af\u5883\u4e2d\u6d4b\u8bd5\u663e\u793a\u6587\u672c\u4f20\u8f93\u5e73\u5747\u5ef6\u8fdf\u4e3a3.2\u79d2\uff0c\u7cfb\u7edf\u53ef\u5e94\u7528\u4e8e\u8fdc\u7a0b\u76d1\u63a7\u3001\u57fa\u7840\u8bbe\u65bd\u7ba1\u7406\u548c\u8bbf\u95ee\u63a7\u5236", "conclusion": "\u8be5\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u5728\u4f20\u7edf\u7f51\u7edc\u57fa\u7840\u8bbe\u65bd\u4e0d\u53ef\u7528\u60c5\u51b5\u4e0b\u7684\u6709\u6548\u901a\u4fe1\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u957f\u8ddd\u79bb\u4f20\u8f93\u548c\u4f4e\u529f\u8017\u7684\u4f18\u52bf"}}
{"id": "2511.21615", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.21615", "abs": "https://arxiv.org/abs/2511.21615", "authors": ["Henrique L. Senger", "Gustavo P. Gon\u00e7alves", "Bruno S. Chang", "Hyeon Seok Rou", "Kuranage Roche Rayan Ranasinghe", "Giuseppe Thadeu Freitas de Abreu", "Didier Le Ruyet"], "title": "SIR Analysis for Affine Filter Bank Modulation", "comment": "Submitted to an IEEE Conference", "summary": "The signal-to-interference ratio (SIR) of the Affine Filter Bank Modulation (AFBM) waveform is analyzed under minimum mean square error (MMSE) equalization in two domains; namely, the affine domain and the filtered time-domain (TD). Due to the incorporation of the discrete affine Fourier transform (DAFT) and despreading/mapping, an interesting and counter-intuitive cancellation of the unwanted combination of the channel induced interference with the orthogonality approximation error is seen in the filtered TD, a process which does not occur in the affine domain. The direct impact on bit error rate (BER) provides a thorough validation of the proposed analysis and explains the substantial gains in performance of the filtered TD detection scheme as opposed to its affine domain equivalent", "AI": {"tldr": "\u5206\u6790\u4e86AFBM\u6ce2\u5f62\u5728MMSE\u5747\u8861\u4e0b\u7684SIR\u6027\u80fd\uff0c\u53d1\u73b0\u5728\u6ee4\u6ce2\u65f6\u57df\u4e2d\u4f1a\u51fa\u73b0\u5e72\u6270\u4e0e\u6b63\u4ea4\u8fd1\u4f3c\u8bef\u5dee\u7684\u610f\u5916\u62b5\u6d88\u73b0\u8c61\uff0c\u8fd9\u89e3\u91ca\u4e86\u6ee4\u6ce2\u65f6\u57df\u68c0\u6d4b\u65b9\u6848\u76f8\u6bd4\u4eff\u5c04\u57df\u7b49\u6548\u65b9\u6848\u7684\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u7814\u7a76AFBM\u6ce2\u5f62\u5728\u4e0d\u540c\u57df\uff08\u4eff\u5c04\u57df\u548c\u6ee4\u6ce2\u65f6\u57df\uff09\u4e2d\u7684SIR\u6027\u80fd\u5dee\u5f02\uff0c\u7279\u522b\u662f\u63a2\u7d22\u6ee4\u6ce2\u65f6\u57df\u4e2d\u51fa\u73b0\u7684\u5e72\u6270\u62b5\u6d88\u73b0\u8c61\u53ca\u5176\u5bf9BER\u6027\u80fd\u7684\u5f71\u54cd\u3002", "method": "\u5728MMSE\u5747\u8861\u6761\u4ef6\u4e0b\uff0c\u5206\u522b\u5728\u4eff\u5c04\u57df\u548c\u6ee4\u6ce2\u65f6\u57df\u5206\u6790AFBM\u6ce2\u5f62\u7684SIR\u6027\u80fd\uff0c\u7ed3\u5408DAFT\u548c\u89e3\u6269/\u6620\u5c04\u64cd\u4f5c\uff0c\u7814\u7a76\u5e72\u6270\u4e0e\u6b63\u4ea4\u8fd1\u4f3c\u8bef\u5dee\u7684\u76f8\u4e92\u4f5c\u7528\u3002", "result": "\u53d1\u73b0\u6ee4\u6ce2\u65f6\u57df\u4e2d\u5b58\u5728\u5e72\u6270\u4e0e\u6b63\u4ea4\u8fd1\u4f3c\u8bef\u5dee\u7684\u610f\u5916\u62b5\u6d88\u73b0\u8c61\uff0c\u8fd9\u79cd\u73b0\u8c61\u5728\u4eff\u5c04\u57df\u4e2d\u4e0d\u4f1a\u53d1\u751f\uff0c\u5bfc\u81f4\u6ee4\u6ce2\u65f6\u57df\u68c0\u6d4b\u65b9\u6848\u5177\u6709\u663e\u8457\u7684\u6027\u80fd\u4f18\u52bf\u3002", "conclusion": "\u6ee4\u6ce2\u65f6\u57df\u68c0\u6d4b\u65b9\u6848\u901a\u8fc7\u5e72\u6270\u62b5\u6d88\u673a\u5236\u5b9e\u73b0\u4e86\u6bd4\u4eff\u5c04\u57df\u7b49\u6548\u65b9\u6848\u66f4\u597d\u7684BER\u6027\u80fd\uff0c\u8fd9\u4e3aAFBM\u6ce2\u5f62\u7684\u4f18\u5316\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u91cd\u8981\u6307\u5bfc\u3002"}}
{"id": "2511.21649", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2511.21649", "abs": "https://arxiv.org/abs/2511.21649", "authors": ["Mohamed El Jbari", "Fernando D. A. Garc\u00eda", "Hugerles S. Silva", "Felipe A. P. de Figueiredo", "Rausley A. A. de Souza"], "title": "Optimal Bit Detection in Thermal Noise Communication Systems Under Rician Fading", "comment": null, "summary": "Thermal noise communication (TNC) enables ultra-low-power wireless links for Internet of Things (IoT) devices by modulating the variance of thermal noise, rather than using active carriers. Existing analyses often rely on Gaussian approximations and overlook fading effects, which limits their accuracy. This paper presents an accurate analytical framework for optimal bit detection in TNC systems under Rician fading. Using chi-squared statistics, we derive the optimal maximum-likelihood detection threshold and an expression for the bit error probability (BEP) via Gauss-Laguerre quadrature. The proposed model eliminates approximation errors and accurately characterizes performance for finite sample sizes. Monte Carlo simulations confirm the analytical results and demonstrate significant improvements in BEP compared with suboptimal Gaussian-based detection. Furthermore, the influence of key parameters, sample size, resistance ratio, and Rician K-factor, is quantified. The proposed framework provides a solid foundation for designing energy-efficient TNC receivers in future B5G/6G and large-scale IoT systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u5728Rician\u8870\u843d\u4fe1\u9053\u4e0b\u70ed\u566a\u58f0\u901a\u4fe1\u7cfb\u7edf\u7684\u6700\u4f18\u6bd4\u7279\u68c0\u6d4b\u5206\u6790\u6846\u67b6\uff0c\u4f7f\u7528\u5361\u65b9\u7edf\u8ba1\u63a8\u5bfc\u6700\u5927\u4f3c\u7136\u68c0\u6d4b\u9608\u503c\u548c\u8bef\u7801\u7387\u8868\u8fbe\u5f0f\uff0c\u76f8\u6bd4\u57fa\u4e8e\u9ad8\u65af\u8fd1\u4f3c\u7684\u6b21\u4f18\u68c0\u6d4b\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u70ed\u566a\u58f0\u901a\u4fe1\u5206\u6790\u591a\u4f9d\u8d56\u9ad8\u65af\u8fd1\u4f3c\u4e14\u5ffd\u7565\u8870\u843d\u6548\u5e94\uff0c\u9650\u5236\u4e86\u5206\u6790\u7cbe\u5ea6\uff0c\u9700\u8981\u5efa\u7acb\u66f4\u51c6\u786e\u7684\u5206\u6790\u6846\u67b6\u6765\u652f\u6301\u672a\u6765B5G/6G\u548c\u5927\u89c4\u6a21\u7269\u8054\u7f51\u7cfb\u7edf\u7684\u80fd\u6548\u63a5\u6536\u673a\u8bbe\u8ba1\u3002", "method": "\u4f7f\u7528\u5361\u65b9\u7edf\u8ba1\u63a8\u5bfc\u6700\u4f18\u6700\u5927\u4f3c\u7136\u68c0\u6d4b\u9608\u503c\uff0c\u901a\u8fc7\u9ad8\u65af-\u62c9\u76d6\u5c14\u6c42\u79ef\u6cd5\u83b7\u5f97\u8bef\u7801\u7387\u8868\u8fbe\u5f0f\uff0c\u5e76\u91c7\u7528\u8499\u7279\u5361\u6d1b\u4eff\u771f\u9a8c\u8bc1\u5206\u6790\u7ed3\u679c\u3002", "result": "\u63d0\u51fa\u7684\u6a21\u578b\u6d88\u9664\u4e86\u8fd1\u4f3c\u8bef\u5dee\uff0c\u80fd\u51c6\u786e\u8868\u5f81\u6709\u9650\u6837\u672c\u91cf\u4e0b\u7684\u6027\u80fd\uff0c\u76f8\u6bd4\u6b21\u4f18\u9ad8\u65af\u68c0\u6d4b\u663e\u8457\u6539\u5584\u8bef\u7801\u7387\uff0c\u5e76\u91cf\u5316\u4e86\u6837\u672c\u91cf\u3001\u7535\u963b\u6bd4\u548cRician K\u56e0\u5b50\u7b49\u5173\u952e\u53c2\u6570\u7684\u5f71\u54cd\u3002", "conclusion": "\u8be5\u5206\u6790\u6846\u67b6\u4e3a\u672a\u6765B5G/6G\u548c\u5927\u89c4\u6a21\u7269\u8054\u7f51\u7cfb\u7edf\u4e2d\u8bbe\u8ba1\u80fd\u6548\u70ed\u566a\u58f0\u901a\u4fe1\u63a5\u6536\u673a\u63d0\u4f9b\u4e86\u575a\u5b9e\u57fa\u7840\u3002"}}

<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 13]
- [eess.AS](#eess.AS) [Total: 4]
- [cs.SD](#cs.SD) [Total: 10]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [WiRainbow: Single-Antenna Direction-Aware Wi-Fi Sensing via Dispersion Effect](https://arxiv.org/abs/2511.20671)
*Zhaoxin Chang,Shuguang Xiao,Fusang Zhang,Xujun Ma,Badii Jouaber,Qingfeng Zhang,Daqing Zhang*

Main category: eess.SP

TL;DR: WiRainbow是一种利用频率扫描天线实现单天线Wi-Fi方向感知的新方法，通过扩展视场角和改进信号处理，在复杂环境中实现准确的方向估计。


<details>
  <summary>Details</summary>
Motivation: 现有Wi-Fi方向估计方法依赖昂贵复杂的天线阵列，难以在实际场景中部署，需要开发成本更低、部署更简单的单天线方向感知方案。

Method: 提出基于耦合谐振器的天线架构扩展传统频率扫描天线的窄视场角，并开发基于信噪比的信号处理框架，在丰富多径环境中可靠估计目标方向。

Result: 原型系统通过基准实验和真实案例研究验证了WiRainbow能够为各种Wi-Fi传感应用提供准确、鲁棒且经济高效的方向感知能力。

Conclusion: WiRainbow通过创新的天线设计和信号处理，实现了单天线Wi-Fi方向感知，解决了传统天线阵列部署成本高和复杂度大的问题。

Abstract: Recently, Wi-Fi signals have emerged as a powerful tool for contactless sensing. During the sensing process, obtaining target direction information can provide valuable contextual insights for various applications. Existing direction estimation methods typically rely on antenna arrays, which are costly and complex to deploy in real-world scenarios. In this paper, we present WiRainbow, a novel approach that enables single-antenna-based direction awareness for Wi-Fi sensing by leveraging the dispersion effect of frequency-scanning antennas (FSAs), which can naturally steer Wi-Fi subcarriers toward distinct angles during signal transmission. To address key challenges in antenna design and signal processing, we propose a coupled-resonator-based antenna architecture that significantly expands the narrow Field-of-View inherent in conventional FSAs, improving sensing coverage. Additionally, we develop a sensing signal-to-noise-ratio-based signal processing framework that reliably estimates target direction in multipath-rich environments. We prototype WiRainbow and evaluate its performance through benchmark experiments and real-world case studies, demonstrating its ability to achieve accurate, robust, and cost-effective direction awareness for diverse Wi-Fi sensing applications.

</details>


### [2] [A Fully Multivariate Multifractal Detrended Fluctuation Analysis Method for Fault Diagnosis](https://arxiv.org/abs/2511.20831)
*Khuram Naveed,Naveed ur Rehman*

Main category: eess.SP

TL;DR: 提出完全多变量MFDFA方法，结合MVMD预处理，用于多通道机器振动数据的故障诊断，能有效区分健康与故障状态。


<details>
  <summary>Details</summary>
Motivation: 传统MFDFA方法无法充分捕捉多通道振动数据中的跨通道依赖关系和方差偏差，需要开发能更好表征多变量信号多尺度结构的方法。

Method: 引入基于马氏距离的协方差加权Lpq矩阵范数定义完全多变量波动函数，结合多元变分模态分解(MVMD)进行故障相关分量分离。

Result: 在风力涡轮机齿轮箱数据上的实验表明，该方法优于传统MFDFA方法，能有效区分健康与故障状态，即使在噪声条件下也表现良好。

Conclusion: 提出的FM-MFDFA框架为多通道机器振动数据的故障诊断提供了更准确的多尺度特征表征方法。

Abstract: We propose a fully multivariate generalization of multifractal detrended fluctuation analysis (MFDFA) and leverage it to develop a fault diagnosis framework for multichannel machine vibration data. We introduce a novel covariance-weighted $L_{pq}$ matrix norm based on Mahalanobis distance to define a fully multivariate fluctuation function that uniquely captures cross-channel dependencies and variance biases in multichannel vibration data. This formulation, termed FM-MFDFA, allows for a more accurate characterization of the multiscale structure of multivariate signals. To enhance feature relevance, the proposed framework integrates multivariate variational mode decomposition (MVMD) to isolate fault-relevant components before applying FM-MFDFA. Results on wind turbine gearbox data demonstrate that the proposed method outperforms conventional MFDFA approaches by effectively distinguishing between healthy and faulty machine states, even under noisy conditions.

</details>


### [3] [Wavelet-Guided Water-Level Estimation for ISAC](https://arxiv.org/abs/2511.20936)
*Ayoob Salari,Kai Wu,Khawaja Fahad Masood,Y. Jay Guo,J. Andrew Zhang*

Main category: eess.SP

TL;DR: 提出一种利用LTE下行链路功率指标的低成本水位监测方法，通过连续小波变换提取潮汐特征，结合轻量神经网络实现厘米级精度水位跟踪。


<details>
  <summary>Details</summary>
Motivation: 传统水位监测方法依赖固定仪器，成本高且易受极端事件影响，需要一种低成本、被动式的实时水位监测方案。

Method: 使用LTE下行链路的RSRP、RSSI和RSRQ指标，应用连续小波变换提取半日潮分量，形成特征向量训练轻量神经网络；支持多基站协作模式提高稳定性。

Result: 在420米河流路径上，视距条件下RMSE为0.8cm，MAE为0.5cm；非视距条件下经过微调后RMSE为1.7cm，MAE为0.8cm。

Conclusion: 该方法无需阵列校准，可在标准硬件上运行，相比基于CSI的方法更实用，多基站融合进一步提高了鲁棒性，适合大规模部署。

Abstract: Real-time water-level monitoring across many locations is vital for flood response, infrastructure management, and environmental forecasting. Yet many sensing methods rely on fixed instruments - acoustic, radar, camera, or pressure probes - that are costly to install and maintain and are vulnerable during extreme events. We propose a passive, low-cost water-level tracking scheme that uses only LTE downlink power metrics reported by commodity receivers. The method extracts per-antenna RSRP, RSSI, and RSRQ, applies a continuous wavelet transform (CWT) to the RSRP to isolate the semidiurnal tide component, and forms a summed-coefficient signature that simultaneously marks high/low tide (tide-turn times) and tracks the tide-rate (flow speed) over time. These wavelet features guide a lightweight neural network that learns water-level changes over time from a short training segment. Beyond a single serving base station, we also show a multi-base-station cooperative mode: independent CWTs are computed per carrier and fused by a robust median to produce one tide-band feature that improves stability and resilience to local disturbances. Experiments over a 420 m river path under line-of-sight conditions achieve root-mean-square and mean-absolute errors of 0.8 cm and 0.5 cm, respectively. Under a non-line-of-sight setting with vegetation and vessel traffic, the same model transfers successfully after brief fine-tuning, reaching 1.7 cm RMSE and 0.8 cm MAE. Unlike CSI-based methods, the approach needs no array calibration and runs on standard hardware, making wide deployment practical. When signals from multiple base stations are available, fusion further improves robustness.

</details>


### [4] [Evaluating the Performance of a Modified Skin Temperature Sensor for Lower Limb Prostheses: An Experimental Comparison](https://arxiv.org/abs/2511.21068)
*Anirshu Devroy,Gregor Fritz,Mathias Brandstoetter*

Main category: eess.SP

TL;DR: 本研究开发了一种用于假肢系统户外温度监测的改进型热敏电阻，通过实验比较了普通热敏电阻和改进型热敏电阻的性能，旨在解决假肢康复中的皮肤温度和舒适度监测问题。


<details>
  <summary>Details</summary>
Motivation: 当前下肢假肢康复面临皮肤状况、刺激和不适等重大挑战，需要实时监测皮肤温度的系统来为用户和矫形技师提供反馈，以便对假肢进行必要调整。

Method: 进行了一系列实验来理解和表征系统行为，比较了普通热敏电阻和改进型热敏电阻作为假肢户外使用温度测量方法的性能。

Result: 初步结果显示，某些改进型热敏电阻相比其他类型表现出更好的温度记录性能。

Conclusion: 改进型热敏电阻可以作为嵌入假肢系统的舒适温度测量的潜在替代方案，为温度分布提供有价值的见解，并作为皮肤问题的早期预警系统。

Abstract: Current rehabilitation of lower limb prostheses has significant challenges, especially with skin conditions, irritation and discomfort. Understanding the skin temperature and having comfortable wearable sensors that would monitor skin temperature in a real-time outdoor environment would be useful. The system would help the user and orthopedic technician to provide feedback and changes that might be required in the prosthesis. Hence in this paper, a series of experiments are conducted in order to understand and characterize the system behavior and compare a general thermistor and a modified thermistor as a potential method of temperature measurement for outdoor usage of prostheses. The paper goes on to compare the different modified thermistors behavior with their regular counterpart and highlights the challenges and improvement areas needed for such a modified thermistor for outdoor temperature monitoring in a prosthetic system. Initial results show that some of the modified thermistors showed better temperature recording compared to the rest. Finally, such modified thermistors can be a potential alternative for comfortable temperature measurement embedded in the prosthesis system. Such a system can provide valuable insights into temperature distribution and an early warning system for skin problems

</details>


### [5] [Data-Driven Assessment of Concrete Slab Integrity via Impact-Echo Signals and Neural Networks](https://arxiv.org/abs/2511.21080)
*Yeswanth Ravichandran,Duoduo Liao,Charan Teja Kurakula*

Main category: eess.SP

TL;DR: 提出基于机器学习的冲击回波框架，自动定位混凝土缺陷并进行多类别分类，包括浅层脱层、深层脱层、空洞和蜂窝状缺陷，准确率达73%。


<details>
  <summary>Details</summary>
Motivation: 混凝土桥面板的亚表面缺陷（如脱层、空洞、蜂窝）严重影响耐久性，但传统视觉检测和手动敲击难以可靠检测。

Method: 将原始冲击回波信号通过FFT转换为峰值频率特征，插值为空间图进行缺陷区域可视化；使用k-means聚类识别缺陷区域，构建空间有序峰值频率序列，输入堆叠LSTM网络进行分类。

Result: 在实验室数据上实现73%的整体分类准确率，现场验证表明模型在真实耦合、噪声和环境变化下具有良好的泛化能力。

Conclusion: 该框架提高了无损评估的客观性、可扩展性和可重复性，支持网络规模的智能桥梁健康监测。

Abstract: Subsurface defects such as delamination, voids, and honeycombing critically affect the durability of concrete bridge decks but are difficult to detect reliably using visual inspection or manual sounding. This paper presents a machine learning based Impact Echo (IE) framework that automates both defect localization and multi-class classification of common concrete defects. Raw IE signals from Federal Highway Administration (FHWA) laboratory slabs and in-service bridge decks are transformed via Fast Fourier Transform (FFT) into dominant peak-frequency features and interpolated into spatial maps for defect zone visualization. Unsupervised k-means clustering highlights low-frequency, defect-prone regions, while Ground Truth Masks (GTMs) derived from seeded lab defects are used to validate spatial accuracy and generate high-confidence training labels. From these validated regions, spatially ordered peak-frequency sequences are constructed and fed into a stacked Long Short-Term Memory (LSTM) network that classifies four defect types shallow delamination, deep delamination, voids, and honeycombing with 73% overall accuracy. Field validation on the bridge deck demonstrates that models trained on laboratory data generalize under realistic coupling, noise, and environmental variability. The proposed framework enhances the objectivity, scalability, and repeatability of Non-Destructive Evaluation (NDE), supporting intelligent, data-driven bridge health monitoring at a network scale.

</details>


### [6] [2D Sparse Array Design via Reweighted L1 Second Order Cone Programming for 3D Ultrasound Imaging](https://arxiv.org/abs/2511.21133)
*Xi Zhang,Miguel Bernal,Wei-Ning Lee*

Main category: eess.SP

TL;DR: 提出了一种基于二阶锥规划（SOCP）和重加权L1技术的稀疏阵列设计方法，设计了具有-21.26 dB旁瓣水平和252个激活元素的准平坦旁瓣2D稀疏阵列，在256通道限制下实现高对比度性能。


<details>
  <summary>Details</summary>
Motivation: 传统全寻址2D阵列需要数千个独立通道，成本高昂。现有随机优化方法设计的稀疏阵列结果不稳定，需要更可靠的稀疏阵列设计方法。

Method: 将稀疏阵列合成问题表述为二阶锥规划（SOCP），并实施重加权L1技术来顺序优化SOCP。使用Field II仿真在多角度转向发散波传输方案中评估成像性能。

Result: 设计的Q-Flats阵列与密集阵列、费马螺旋阵列和空间50%-Tukey锥化螺旋阵列相比，分辨率比螺旋阵列好约3%，对比度略差，但整体性能均衡。密集阵列性能最佳，螺旋锥化阵列最差。

Conclusion: 重加权L1 SOCP方法是一种有前景且灵活的方法，可以在分辨率、对比度和激活元素数量之间寻求平衡。

Abstract: Two-dimensional (2D) fully-addressed arrays can conveniently realize three-dimensional (3D) ultrasound imaging while fully controlled such arrays usually demands thousands of independent channels, which is costly. Sparse array technique using stochastic optimization methods is one of promising techniques to reduce channel counts while due to the stochastic nature of these methods, the optimized results are usually unstable. In this work, we introduce a sparse array design approach that formulates the synthesis problem of sparse arrays as second-order cone programming (SOCP) and a re-weighted L1 technique is implemented to sequentially optimize the SOCP. Based on this method, an on-grid quasi-flatten side-lobe (Q-Flats) 2D sparse array with side-lobe level (SLL) no more than -21.26 dB and 252 activated elements is designed, which aims to achieve as high contrast performance as possible under the limits of resolution and maximum number of independent channels (i.e., 256). The imaging performance of the Q-Flats array was compared with those of a corresponding dense array (Dense), a Fermat spiral array (Spiral) and a spatially 50%-Tukey tapered spiral array (Spiral-Taper) using Field II simulations in a multi-angle steered diverging wave transmission scheme. It was demonstrated that the Dense achieved the best resolution and contrast and the Spiral-Taper the worst. The Q-Flats showed better resolution (about 3%) but slightly worse contrast than the Spiral. All the results indicate the re-weighted L1 SOCP method is a promising and flexible method for seeking trade-offs among resolution, contrast, and number of activated elements.

</details>


### [7] [Multiport Analytical Pixel Electromagnetic Simulator (MAPES) for AI-assisted RFIC and Microwave Circuit Design](https://arxiv.org/abs/2511.21274)
*Junhui Rao,Yi Liu,Jichen Zhang,Zhaoyang Ming,Tianrui Qiao,Yujie Zhang,Chi Yuk Chiu,Hua Wang,Ross Murch*

Main category: eess.SP

TL;DR: MAPES是一种新颖的多端口分析像素电磁模拟器，能够高效准确地预测任意基于像素的微波和RFIC结构的电磁性能，仅需约1%的全波仿真数据即可构建多端口阻抗矩阵，实现600-2000倍的速度提升。


<details>
  <summary>Details</summary>
Motivation: 传统AI辅助电磁设计需要大量全波仿真数据集，存在数据驱动过拟合问题，且计算效率低下。需要一种能够准确捕获像素间电磁耦合的高效分析方法。

Method: 基于集成内部多端口方法，引入虚拟像素和对角虚拟像素，在关键位置插入虚拟端口，构建包含所有水平、垂直和对角电磁耦合的单多端口阻抗矩阵。

Result: 在单层和双层CMOS工艺（180nm和65nm）及PCB上的综合示例表明，MAPES实现了高预测精度，相比CST仿真速度提升600-2000倍。

Conclusion: MAPES提供了一种高效、可扩展且可靠的实用工具，适用于各种制造技术中的AI辅助微波电路和RFIC设计。

Abstract: This paper proposes a novel analytical framework, termed the Multiport Analytical Pixel Electromagnetic Simulator (MAPES). MAPES enables efficient and accurate prediction of the electromagnetic (EM) performance of arbitrary pixel-based microwave (MW) and RFIC structures. Inspired by the Integrated Internal Multiport Method (IMPM), MAPES extends the concept to the pixel presence/absence domain used in AI-assisted EM design. By introducing virtual pixels and diagonal virtual pixels and inserting virtual ports at critical positions, MAPES captures all horizontal, vertical, and diagonal electromagnetic couplings within a single multiport impedance matrix. Only a small set of full-wave simulations (typically about 1% of the datasets required by AI-assisted EM simulators) is needed to construct this matrix. Subsequently, any arbitrary pixel configuration can be evaluated analytically using a closed-form multiport relation without additional full-wave calculations. The proposed approach eliminates data-driven overfitting and ensures accurate results across all design variations. Comprehensive examples for single- and double-layer CMOS processes (180 nm and 65 nm) and PCBs confirm that MAPES achieves high prediction accuracy with 600- 2000x speed improvement compared to CST simulations. Owing to its efficiency, scalability and reliability, MAPES provides a practical and versatile tool for AI-assisted MW circuit and RFIC design across diverse fabrication technologies.

</details>


### [8] [Phase-Aware Code-Aided EM Algorithm for Blind Channel Estimation in PSK-Modulated OFDM](https://arxiv.org/abs/2511.21340)
*Chin-Hung Chen,Ivana Nikoloska,Wim van Houtum,Yan Wu,Alex Alvarado*

Main category: eess.SP

TL;DR: 提出了一种用于PSK调制OFDM系统的全盲相位感知EM算法，通过利用解码器外部信息解决EM算法中的相位模糊问题，显著降低了局部收敛率。


<details>
  <summary>Details</summary>
Motivation: 解决传统盲EM估计算法中由于信道估计的未知相位模糊导致的局部最大值问题，该问题限制了EM算法的性能。

Method: 利用解码器的外部信息作为模型证据指标，基于PSK调制的固有对称性生成候选模型集，解码器选择最可能的候选模型。

Result: 仿真结果表明，结合简单卷积码，相位感知EM算法在初始化阶段可靠地解决了相位模糊，在具有恒定相位模糊的频率选择性信道中，将局部收敛率从80%降低到接近0%。

Conclusion: 该算法仅在EM初始化阶段调用一次，在后续turbo迭代中带来可忽略的额外复杂度，有效解决了盲信道估计中的相位模糊问题。

Abstract: This paper presents a fully blind phase-aware expectation-maximization (EM) algorithm for OFDM systems with the phase-shift keying (PSK) modulation. We address the well-known local maximum problem of the EM algorithm for blind channel estimation. This is primarily caused by the unknown phase ambiguity in the channel estimates, which conventional blind EM estimators cannot resolve. To overcome this limitation, we propose to exploit the extrinsic information from the decoder as model evidence metrics. A finite set of candidate models is generated based on the inherent symmetries of PSK modulation, and the decoder selects the most likely candidate model. Simulation results demonstrate that, when combined with a simple convolutional code, the phase-aware EM algorithm reliably resolves phase ambiguity during the initialization stage and reduces the local convergence rate from 80% to nearly 0% in frequency-selective channels with a constant phase ambiguity. The algorithm is invoked only once after the EM initialization stage, resulting in negligible additional complexity during subsequent turbo iterations.

</details>


### [9] [Blind Turbo Demodulation for Differentially Encoded OFDM with 2D Trellis Decomposition](https://arxiv.org/abs/2511.21345)
*Chin-Hung Chen,Yan Wu,Wim van Houtum,Alex Alvarado*

Main category: eess.SP

TL;DR: 开发了一种完全盲的turbo-DE-PSK方案，无需导频即可联合估计信道相位、信道增益和噪声方差，在DAB-like系统中实现了接近完美信道知识的性能。


<details>
  <summary>Details</summary>
Motivation: 在DAB-like系统中，turbo-DE-PSK接收机虽然通过迭代解码提供显著性能增益，但依赖无导频的准确信道估计，这是关键挑战。

Method: 利用二维网格分解进行盲相位估计，辅以基于功率的信道增益和噪声方差估计器，开发了完全盲的turbo-DE-PSK方案。

Result: 仿真结果表明，盲2D turbo解调器接近完美信道知识接收机的性能，并在实际传输条件下保持鲁棒性。

Conclusion: 所提出的盲turbo-DE-PSK方案有效解决了DAB-like系统中无导频信道估计的挑战，具有实际应用价值。

Abstract: Digital Audio Broadcasting (DAB)-like systems employ differentially encoded (DE) phase-shift keying (PSK) for transmission. While turbo-DE-PSK receivers offer substantial performance gains through iterative decoding by making the DE-PSK an inner code, they rely on accurate channel estimation without pilots, which is a key challenge in DAB-like scenarios. This paper develops a fully blind turbo-DE-PSK scheme that jointly estimates channel phase, channel gain, and noise variance directly from the received signal. The design leverages a two-dimensional (2D) trellis decomposition for blind phase estimation, complemented by power-based estimators for channel gain and noise variance. We provide a comprehensive system assessment across practical system parameters, including inner code length, phase quantization, and 2D block size. Simulation results show that the blind 2D turbo demodulator approaches the performance of receivers with perfect channel knowledge and remains robust under realistic transmission conditions.

</details>


### [10] [Group-wise Semantic Splitting Multiple Access for Multi-User Semantic Communication](https://arxiv.org/abs/2511.21411)
*Jungyeon Koh,Hyeonho Noh,Hyun Jong Yang*

Main category: eess.SP

TL;DR: 提出了一种基于用户语义特征相似度的分组语义分割多址接入框架，通过聚类提取组级公共特征和用户特定私有特征，分别采用组播和单播传输，结合重建损失和排斥损失的复合损失函数提升语义分离性和重建质量。


<details>
  <summary>Details</summary>
Motivation: 在多用户语义通信下行场景中，传统方法未能充分利用用户间的语义相似性，导致传输效率低下。需要一种能够有效利用共享语义信息和用户特定信息的框架来提高通信效率。

Method: 采用平衡聚类机制根据用户语义特征相似度进行分组，提取组级公共特征和用户特定私有特征；基站通过组播传输公共特征，单播传输私有特征；设计包含重建损失和排斥损失的复合损失函数来增强语义分离性和重建保真度。

Result: 仿真结果表明，该方法在各种信道条件下相比传统方案实现了最高3.26%的性能提升，验证了其在下一代无线网络中的鲁棒性和语义效率。

Conclusion: 所提出的分组语义分割多址接入框架通过有效利用用户语义相似性，结合组播和单播传输策略，显著提高了多用户语义通信的性能和效率。

Abstract: In this letter, we propose a group-wise semantic splitting multiple access framework for multi-user semantic communication in downlink scenarios. The framework begins by applying a balanced clustering mechanism that groups users based on the similarity of their semantic characteristics, enabling the extraction of group-level common features and user-specific private features. The base station then transmits the common features via multicast and the private features via unicast, effectively leveraging both shared and user-dependent semantic information. To further enhance semantic separability and reconstruction fidelity, we design a composite loss function that integrates a reconstruction loss with a repulsion loss, improving both the accuracy of semantic recovery and the distinctiveness of common embeddings in the latent space. Simulation results demonstrate that the proposed method achieves up to 3.26% performance improvement over conventional schemes across various channel conditions, validating its robustness and semantic efficiency for next-generation wireless networks.

</details>


### [11] [Design Of A Communication System To Send Text Using Lora At 400 MHz](https://arxiv.org/abs/2511.21434)
*Fabrizio André Farfán Prado,William César Pérez Campos,Steisy Anahi Carreño Tacuri,Favio David Cabrera Alva,Harold Jacobed Carhuas Lizarbe*

Main category: eess.SP

TL;DR: 基于ESP32和LoRa DXLR01的低功耗无线文本传输系统，用于解决农村和城市边缘地区的网络连接问题，支持远程监控和基础设施管理应用。


<details>
  <summary>Details</summary>
Motivation: 解决农村地区和某些城市环境中Wi-Fi或移动网络不可用或受限时的连接和能效问题。

Method: 集成LoRa技术（使用433MHz频段和CSS调制）与ESP32模块，ESP32负责捕获、处理和发送消息，接收端显示在LCD屏并上传至ThingSpeak平台。

Result: 在受控环境中平均传输延迟为3.2秒，系统可用于远程监控、基础设施管理和门禁控制等应用。

Conclusion: 该系统证明了LoRa技术在解决网络覆盖盲区通信问题的有效性，具有低功耗和长距离传输的优势。

Abstract: This work describes the design and implementation of a low-power wireless communication system for transmitting text using ESP32 modules and the LoRa DXLR01. The proposal arises as a solution to connectivity and energy-efficiency problems commonly found in rural areas and certain urban environments where Wi-Fi or mobile networks are unavailable or operate with limitations. To address this, LoRa technology known for its long-range capability and low power consumption is integrated with an ESP32 responsible for capturing, processing, and sending messages.
  The LoRa DXLR01 module, which operates in the 433 MHz band, is configured with parameters aimed at maximising both transmission range and efficient energy usage. Messages are sent using Chirp Spread Spectrum (CSS) modulation, improving signal penetration in obstructed areas and reducing the likelihood of errors. On the receiving end, the ESP32 interprets the data and displays it on an LCD screen. Additionally, the received information is sent to the ThingSpeak platform, allowing remote storage and visualisation without relying on conventional network infrastructure.
  Tests conducted in a controlled environment show an average latency of 3.2 seconds for text transmission. It was also verified that the system can be used in applications such as remote monitoring, infrastructure management, and access control.

</details>


### [12] [SIR Analysis for Affine Filter Bank Modulation](https://arxiv.org/abs/2511.21615)
*Henrique L. Senger,Gustavo P. Gonçalves,Bruno S. Chang,Hyeon Seok Rou,Kuranage Roche Rayan Ranasinghe,Giuseppe Thadeu Freitas de Abreu,Didier Le Ruyet*

Main category: eess.SP

TL;DR: 分析了AFBM波形在MMSE均衡下两个域中的SIR性能，发现滤波时域检测方案相比仿射域等效方案有显著性能提升


<details>
  <summary>Details</summary>
Motivation: 研究AFBM波形在不同域中的SIR性能差异，特别是滤波时域检测相比仿射域检测的性能优势

Method: 在仿射域和滤波时域两个域中分析AFBM波形的SIR性能，使用MMSE均衡，结合DAFT和去扩频/映射操作

Result: 在滤波时域中观察到信道干扰与正交性近似误差的意外抵消现象，这种现象在仿射域中不会发生，导致滤波时域检测方案的BER性能显著优于仿射域等效方案

Conclusion: 滤波时域检测方案相比仿射域检测具有实质性性能增益，该分析为AFBM波形的性能评估提供了验证

Abstract: The signal-to-interference ratio (SIR) of the Affine Filter Bank Modulation (AFBM) waveform is analyzed under minimum mean square error (MMSE) equalization in two domains; namely, the affine domain and the filtered time-domain (TD). Due to the incorporation of the discrete affine Fourier transform (DAFT) and despreading/mapping, an interesting and counter-intuitive cancellation of the unwanted combination of the channel induced interference with the orthogonality approximation error is seen in the filtered TD, a process which does not occur in the affine domain. The direct impact on bit error rate (BER) provides a thorough validation of the proposed analysis and explains the substantial gains in performance of the filtered TD detection scheme as opposed to its affine domain equivalent

</details>


### [13] [Optimal Bit Detection in Thermal Noise Communication Systems Under Rician Fading](https://arxiv.org/abs/2511.21649)
*Mohamed El Jbari,Fernando D. A. García,Hugerles S. Silva,Felipe A. P. de Figueiredo,Rausley A. A. de Souza*

Main category: eess.SP

TL;DR: 本文提出了一个精确的Rician衰落信道下热噪声通信系统的最优比特检测分析框架，使用卡方统计推导最大似然检测阈值和误码率表达式，相比传统高斯近似方法显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有热噪声通信分析多依赖高斯近似且忽略衰落效应，限制了分析精度。需要建立准确的分析框架来支持未来B5G/6G和大规模物联网系统中能效型TNC接收器的设计。

Method: 使用卡方统计推导最优最大似然检测阈值，通过Gauss-Laguerre求积法得到误码率表达式，消除近似误差并准确表征有限样本量下的性能。

Result: 蒙特卡洛仿真验证了分析结果的准确性，相比次优的高斯基检测方法，误码率有显著改善。量化了样本量、电阻比和Rician K因子等关键参数的影响。

Conclusion: 所提出的框架为未来B5G/6G和大规模物联网系统中设计能效型热噪声通信接收器提供了坚实基础。

Abstract: Thermal noise communication (TNC) enables ultra-low-power wireless links for Internet of Things (IoT) devices by modulating the variance of thermal noise, rather than using active carriers. Existing analyses often rely on Gaussian approximations and overlook fading effects, which limits their accuracy. This paper presents an accurate analytical framework for optimal bit detection in TNC systems under Rician fading. Using chi-squared statistics, we derive the optimal maximum-likelihood detection threshold and an expression for the bit error probability (BEP) via Gauss-Laguerre quadrature. The proposed model eliminates approximation errors and accurately characterizes performance for finite sample sizes. Monte Carlo simulations confirm the analytical results and demonstrate significant improvements in BEP compared with suboptimal Gaussian-based detection. Furthermore, the influence of key parameters, sample size, resistance ratio, and Rician K-factor, is quantified. The proposed framework provides a solid foundation for designing energy-efficient TNC receivers in future B5G/6G and large-scale IoT systems.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [14] [Towards Audio Token Compression in Large Audio Language Models](https://arxiv.org/abs/2511.20973)
*Saurabhchand Bhati,Samuel Thomas,Hilde Kuehne,Rogerio Feris,James Glass*

Main category: eess.AS

TL;DR: 本文提出通过无监督分割、均匀平均池化等技术压缩LALM音频编码器的输出token数量，并使用低秩适配器微调模型，在ASR和语音翻译任务上实现接近帧级LALM性能的同时将输入音频token数量减少三倍。


<details>
  <summary>Details</summary>
Motivation: 大型音频语言模型在处理长音频和部署到边缘设备时面临注意力二次复杂度和音频信号高token率的可扩展性限制。

Method: 采用无监督分割、均匀平均池化等技术压缩音频编码器输出token，使用低秩适配器微调模型以缓解性能下降。

Result: 实验结果显示压缩后的LALM在自动语音识别和语音翻译任务上性能接近帧级LALM，同时输入音频token数量减少三倍。

Conclusion: 提出的压缩方法有效解决了LALM的可扩展性问题，在保持性能的同时显著减少了计算需求。

Abstract: Large Audio Language Models (LALMs) demonstrate impressive performance across diverse tasks, ranging from speech recognition to general audio understanding. However, their scalability is limited by the quadratic complexity of attention and the high token rates of audio signals. These challenges make it difficult to extend LALMs to long-form audio and to deploy them on resource-constrained platforms such as edge devices.
  In this paper, we explore techniques such as unsupervised segmentation, uniform average pooling, etc., to reduce the number of audio tokens generated by the LALM's audio encoder but before they are consumed by the LLM decoder. To mitigate potential performance degradation introduced by the compressed representations, we employ low-rank adapters to finetune the model. We evaluate our proposed models on two tasks, automatic speech recognition and speech-to-speech translation tasks, that are dependent on effectively uncovering the underlying lexical content of the input signal and study the effect of downsampling on these tasks. Experimental results show that compressed LALMs can achieve performance closer to frame-level LALMs while reducing the input audio token count upto three times before the LLM backbone.

</details>


### [15] [RosettaSpeech: Zero-Shot Speech-to-Speech Translation from Monolingual Data](https://arxiv.org/abs/2511.20974)
*Zhisheng Zheng,Xiaohang Sun,Tuan Dinh,Abhishek Yanamandra,Abhinav Jain,Zhu Liu,Sunil Hadap,Vimal Bhat,Manoj Aggarwal,Gerard Medioni,David Harwath*

Main category: eess.AS

TL;DR: RosettaSpeech是一个创新的零样本语音到语音翻译框架，仅使用单语语音-文本数据和机器翻译监督进行训练，无需平行语音对，在推理时实现端到端的直接语音翻译。


<details>
  <summary>Details</summary>
Motivation: 平行语音语料库的稀缺严重阻碍了语音到语音翻译的发展，通常需要依赖复杂多阶段的流水线方法。

Method: 使用文本作为训练时的中间桥梁，利用文本NMT模型的语言知识，但推理时作为直接的端到端语音到语音模型运行。

Result: 在CVSS-C测试集上取得最先进结果：德语到英语ASR-BLEU 25.17（相对提升27%），西班牙语到英语29.86（相对提升14%），单个模型可实现多对一翻译。

Conclusion: 通过依赖丰富的平行文本而非难以获取的平行语音，RosettaSpeech为创建高质量、保留说话者特征的语音翻译提供了可扩展的路径。

Abstract: The scarcity of parallel speech corpora critically hampers speech-to-speech translation (S2ST), often forcing reliance on complex, multi-stage pipelines. This paper introduces RosettaSpeech, a novel and simplified framework for zero-shot S2ST that is trained on monolingual speech-text data augmented by machine translation supervision. While our method leverages the linguistic knowledge inherent in text-based NMT models, it strictly eliminates the need for parallel speech-to-speech pairs. Our model uniquely uses text as an intermediate bridge during training but functions as a direct, end-to-end speech-to-speech model at inference. This streamlined approach achieves state-of-the-art results on standard benchmarks. For instance, on the CVSS-C test set, RosettaSpeech outperforms leading systems, achieving an ASR-BLEU score of 25.17 for German-to-English and 29.86 for Spanish-to-English-relative gains of over 27% and 14%, respectively. Furthermore, we demonstrate that a single model can deliver strong many-to-one translation performance (FR/ES/DE -> EN). We also provide a foundational analysis of how training data scaling impacts model performance. By prioritizing reliance on abundant parallel text rather than difficult-to-acquire parallel speech, RosettaSpeech offers a scalable path to creating high-quality, speaker-preserving S2ST for a much broader array of languages.

</details>


### [16] [Evaluation of an ITD-to-ILD Transformation as a Method to Restore the Spatial Benefit in Speech Intelligibility in Hearing Impaired Listeners](https://arxiv.org/abs/2511.21222)
*Timm-Jonas Bäumer,Johannes W. de Vries,Stephan Töpken,Richard C. Hendriks,Peyman Goli,Steven van de Par*

Main category: eess.AS

TL;DR: 该研究探索了将低频ITD转换为ILD来恢复听力受损者的双耳听觉优势，实验表明这种转换方法能显著改善言语识别阈值。


<details>
  <summary>Details</summary>
Motivation: 听力受损者通常对ITD敏感性有限，导致言语识别能力下降。研究旨在通过将ITD转换为ILD来重新引入双耳听觉优势。

Method: 进行两个实验：第一个实验使用不同频率的双耳相位偏移正弦波评估ITD敏感性阈值；第二个实验通过操作HRTF在不同双耳配置下测量SRT。

Result: 移除ITD使SRT降低约1dB；将低频ITD替换为ILD可改善侧向目标说话者的识别；在保留ITD的同时添加低频ILD对所有方向的说话者都有显著改善。

Conclusion: 提出的转换方法能有效恢复听力受损者的双耳听觉优势，建议在助听器和人工耳蜗中实施此类技术。

Abstract: To improve speech intelligibility in complex everyday situations, the human auditory system partially relies on Interaural Time Differences (ITDs) and Interaural Level Differences (ILDs). However, hearing impaired (HI) listeners often exhibit limited sensitivity to ITDs, resulting in decreased speech intelligibility performance. This study aimed to investigate whether transforming low-frequency ITDs into ILDs could reintroduce a binaural benefit for HI listeners. We conducted two experiments with HI listeners. The first experiment used binaurally phase-shifted sinusoids at different frequencies to evaluate the HI listeners ITD sensitivity threshold. All subjects had an increased ITD threshold at higher frequencies, with different ITD sensitivities between the subjects in the lower frequencies. In the second experiment, Speech Reception Thresholds (SRTs) were measured in different binaural configurations by manipulating Head-Related Transfer Functions (HRTFs). The results showed that, despite the decreased ITD sensitivity, removing ITDs decreased SRTs by approximately 1 dB compared to the unprocessed baseline, where ITDs and ILDs are available. Furthermore, substituting low-frequency ITDs with ILDs yielded an improvement for a lateral target speaker. Adding the low-frequency ILDs while preserving the ITDs caused a significant improvement for speakers in all directions. These findings suggest that the proposed transformation method could be effective in restoring binaural benefits in HI listeners. The results of this study suggest the use of such transformation techniques to be implemented in hearing aids and cochlear implants, directly benefiting HI listeners.

</details>


### [17] [The Spheres Dataset: Multitrack Orchestral Recordings for Music Source Separation and Information Retrieval](https://arxiv.org/abs/2511.21247)
*Jaime Garcia-Martinez,David Diaz-Guerra,John Anderson,Ricardo Falcon-Perez,Pablo Cabañas-Molero,Tuomas Virtanen,Julio J. Carabias-Orti,Pedro Vera-Candeas*

Main category: eess.AS

TL;DR: The Spheres数据集是一个用于古典音乐源分离研究的多轨管弦乐录音数据集，包含柴可夫斯基和莫扎特作品，提供23个麦克风录音和房间脉冲响应，用于训练源分离模型。


<details>
  <summary>Details</summary>
Motivation: 推动古典音乐领域的机器学习研究，特别是音乐源分离和相关MIR任务，解决复杂管弦乐场景中的分离挑战。

Method: 使用23个麦克风录制超过1小时的管弦乐作品，包括近距离、主麦克风和环境麦克风，创建真实立体声混音并提供分离音轨用于监督训练。

Result: 基于X-UMX模型的基线评估显示了在复杂管弦乐场景中源分离的潜力和挑战，验证了数据集在分离、定位、去混响和沉浸式渲染方面的价值。

Conclusion: The Spheres数据集为古典音乐的源分离研究提供了有价值的基准测试平台，有助于探索新的分离方法和相关音频处理技术。

Abstract: This paper introduces The Spheres dataset, multitrack orchestral recordings designed to advance machine learning research in music source separation and related MIR tasks within the classical music domain. The dataset is composed of over one hour recordings of musical pieces performed by the Colibrì Ensemble at The Spheres recording studio, capturing two canonical works - Tchaikovsky's Romeo and Juliet and Mozart's Symphony No. 40 - along with chromatic scales and solo excerpts for each instrument. The recording setup employed 23 microphones, including close spot, main, and ambient microphones, enabling the creation of realistic stereo mixes with controlled bleeding and providing isolated stems for supervised training of source separation models. In addition, room impulse responses were estimated for each instrument position, offering valuable acoustic characterization of the recording space. We present the dataset structure, acoustic analysis, and baseline evaluations using X-UMX based models for orchestral family separation and microphone debleeding. Results highlight both the potential and the challenges of source separation in complex orchestral scenarios, underscoring the dataset's value for benchmarking and for exploring new approaches to separation, localization, dereverberation, and immersive rendering of classical music.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [18] [Seeing Beyond Sound: Visualization and Abstraction in Audio Data Representation](https://arxiv.org/abs/2511.20658)
*Ashlae Blum'e*

Main category: cs.SD

TL;DR: 本文探讨了在音频可视化工具中增加维度和交互性以支持复杂音频信息研究工作的潜力。


<details>
  <summary>Details</summary>
Motivation: 传统音频处理软件携带的历史背景假设可能与现代工作流程不匹配，创建符合新兴需求的工具能提高分析性和创造性产出。

Method: 使用Jellyfish Dynamite软件探索在可视化工具中添加维度和交互性的方法。

Result: 通过增强视觉表示的维度和交互性，可以更好地与人类感知系统对齐，改善复杂音频信息的模式识别。

Conclusion: 开发与新兴需求对齐的音频可视化工具，特别是增加维度和交互性，能够促进复杂工作流程并提高工具使用亲和力。

Abstract: In audio signal processing, the interpretation of complex information using visual representation enhances pattern recognition through its alignment with human perceptual systems. Software tools that carry hidden assumptions inherited from their historical contexts risk misalignment with modern workflows as design origins become obscured. We argue that creating tools that align with emergent needs improves analytical and creative outputs due to an increased affinity for using them. This paper explores the potentials associated with adding dimensionality and interactivity into visualization tools to facilitate complex workflows in audio information research using the Jellyfish Dynamite software.

</details>


### [19] [Musical Score Understanding Benchmark: Evaluating Large Language Models' Comprehension of Complete Musical Scores](https://arxiv.org/abs/2511.20697)
*Congren Dai,Yue Yang,Krinos Li,Huichi Zhou,Shijie Liang,Zhang Bo,Enyang Liu,Ge Jin,Hongran An,Haosen Zhang,Peiyuan Jing,KinHei Lee,Zhenxuan Zhang,Xiaobing Li,Maosong Sun*

Main category: cs.SD

TL;DR: MSU-Bench是首个大规模人工策划的音乐乐谱理解基准，评估文本和视觉模态下的乐谱理解能力，包含1800个生成式问答对，涵盖从基础音符到复杂和声与形式的四个理解层级。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型和视觉语言模型在自然语言和多模态任务上进展迅速，但它们在理解音乐记谱方面的能力仍未充分探索。需要建立基准来评估AI对乐谱符号结构的理解能力。

Method: 构建包含1800个生成式问答对的MSU-Bench基准，涵盖巴赫、贝多芬、肖邦等作曲家作品，分为四个渐进理解层级：起始信息、记谱与音符、和弦与和声、织体与形式。评估了15+个SOTA模型的零样本和微调性能。

Result: 评估揭示了显著的模态差距、脆弱的层级成功率以及维持多层级正确性的困难。微调显著提高了两种模态的性能，同时保留了一般知识。

Conclusion: MSU-Bench为AI、音乐学和多模态推理交叉领域的未来研究建立了严格基础，表明微调能有效提升乐谱理解能力。

Abstract: Understanding complete musical scores requires reasoning over symbolic structures such as pitch, rhythm, harmony, and form. Despite the rapid progress of Large Language Models (LLMs) and Vision-Language Models (VLMs) in natural language and multimodal tasks, their ability to comprehend musical notation remains underexplored. We introduce Musical Score Understanding Benchmark (MSU-Bench), the first large-scale, human-curated benchmark for evaluating score-level musical understanding across both textual (ABC notation) and visual (PDF) modalities. MSU-Bench comprises 1,800 generative question-answer (QA) pairs drawn from works spanning Bach, Beethoven, Chopin, Debussy, and others, organised into four progressive levels of comprehension: Onset Information, Notation & Note, Chord & Harmony, and Texture & Form. Through extensive zero-shot and fine-tuned evaluations of over 15+ state-of-the-art (SOTA) models, we reveal sharp modality gaps, fragile level-wise success rates, and the difficulty of sustaining multilevel correctness. Fine-tuning markedly improves performance in both modalities while preserving general knowledge, establishing MSU-Bench as a rigorous foundation for future research at the intersection of Artificial Intelligence (AI), musicological, and multimodal reasoning.

</details>


### [20] [SingingSDS: A Singing-Capable Spoken Dialogue System for Conversational Roleplay Applications](https://arxiv.org/abs/2511.20972)
*Jionghao Han,Jiatong Shi,Masao Someki,Yuxun Tang,Lan Liu,Yiwen Zhao,Wenhao Feng,Shinji Watanabe*

Main category: cs.SD

TL;DR: SingingSDS是一个通过唱歌而非说话来回应的口语对话系统，采用模块化的ASR-LLM-SVS流水线，支持角色扮演和互动娱乐场景中的情感化、难忘和愉悦的交互。


<details>
  <summary>Details</summary>
Motivation: 现有口语对话系统大多局限于传统的语音回应，缺乏情感表达和娱乐性。SingingSDS旨在通过唱歌回应来创造更具情感、更难忘和更愉悦的交互体验。

Method: 采用模块化的ASR-LLM-SVS流水线架构，支持多种配置选项，包括角色人物、ASR和LLM后端、SVS模型、旋律来源和声音配置文件，可根据延迟、质量和音乐风格需求进行定制。

Result: 开发了一个即插即用的网页演示系统，提供模块化、开源的代码，支持定制和扩展。

Conclusion: SingingSDS展示了通过唱歌回应来增强口语对话系统情感表达和娱乐价值的可行性，为角色扮演和互动娱乐场景提供了新的交互方式。

Abstract: With recent advances in automatic speech recognition (ASR), large language models (LLMs), and text-to-speech (TTS) technologies, spoken dialogue systems (SDS) have become widely accessible. However, most existing SDS are limited to conventional spoken responses. We present SingingSDS, a cascaded SDS that responds through singing rather than speaking, fostering more affective, memorable, and pleasurable interactions in character-based roleplay and interactive entertainment scenarios. SingingSDS employs a modular ASR-LLM-SVS pipeline and supports a wide range of configurations across character personas, ASR and LLM backends, SVS models, melody sources, and voice profiles, tailored to different needs in terms of latency, quality, and musical style. SingingSDS is available as a plug-and-play web demo, featuring modular, open-source code that supports customization and extension. Demo: https://huggingface.co/spaces/espnet/SingingSDS. Code: https://github.com/SingingSDS/SingingSDS.

</details>


### [21] [CartoonSing: Unifying Human and Nonhuman Timbres in Singing Generation](https://arxiv.org/abs/2511.21045)
*Jionghao Han,Jiatong Shi,Zhuoyan Tao,Yuxun Tang,Yiwen Zhao,Gus Xia,Shinji Watanabe*

Main category: cs.SD

TL;DR: 本文提出了非人类歌声生成(NHSG)任务，包括非人类歌声合成(NHSVS)和非人类歌声转换(NHSVC)，旨在生成具有非人类音色特征的歌声。为解决非人类歌声数据稀缺等挑战，作者开发了CartoonSing统一框架。


<details>
  <summary>Details</summary>
Motivation: 现有歌声合成系统仅限于人类音色，无法生成超出人类范围的歌声，而视频游戏、电影等创意应用对此类非人类歌声的需求日益增长。

Method: 提出CartoonSing框架，采用两阶段流程：使用标注人类歌声训练的音符表示编码器，以及能够重建人类和非人类音频波形的音色感知声码器。

Result: 实验表明CartoonSing成功生成了非人类歌声，能够泛化到新音色，并将传统SVS和SVC扩展到创意性非人类歌声生成。

Conclusion: CartoonSing框架有效解决了非人类歌声生成中的挑战，为创意应用提供了新的可能性，推动了歌声生成技术向非人类领域的发展。

Abstract: Singing voice synthesis (SVS) and singing voice conversion (SVC) have achieved remarkable progress in generating natural-sounding human singing. However, existing systems are restricted to human timbres and have limited ability to synthesize voices outside the human range, which are increasingly demanded in creative applications such as video games, movies, and virtual characters. We introduce Non-Human Singing Generation (NHSG), covering non-human singing voice synthesis (NHSVS) and non-human singing voice conversion (NHSVC), as a novel machine learning task for generating musically coherent singing with non-human timbral characteristics. NHSG is particularly challenging due to the scarcity of non-human singing data, the lack of symbolic alignment, and the wide timbral gap between human and non-human voices. To address these challenges, we propose CartoonSing, a unified framework that integrates singing voice synthesis and conversion while bridging human and non-human singing generation. CartoonSing employs a two-stage pipeline: a score representation encoder trained with annotated human singing and a timbre-aware vocoder that reconstructs waveforms for both human and non-human audio. Experiments demonstrate that CartoonSing successfully generates non-human singing voices, generalizes to novel timbres, and extends conventional SVS and SVC toward creative, non-human singing generation.

</details>


### [22] [Multi-Reward GRPO for Stable and Prosodic Single-Codebook TTS LLMs at Scale](https://arxiv.org/abs/2511.21270)
*Yicheng Zhong,Peiji Yang,Zhisheng Wang*

Main category: cs.SD

TL;DR: 提出了一种多奖励组相对策略优化(GRPO)框架，通过强化学习直接优化单码本TTS大语言模型的token生成策略，解决了韵律不稳定、说话人漂移和自然度下降等问题。


<details>
  <summary>Details</summary>
Motivation: 单码本TTS LLMs虽然紧凑且可流式传输，但存在韵律不稳定、说话人漂移和自然度下降的问题，需要改进其token生成策略。

Method: 使用多奖励GRPO框架，整合了长度惩罚、熵正则化奖励和LLM标注的韵律对齐奖励，其中韵律奖励通过上下文学习预测多个可能的停顿结构。

Result: 该方法在韵律稳定性、说话人相似性和整体语音自然度方面均有提升，在附加流匹配解码器后还能获得额外增益。

Conclusion: 提出的GRPO框架能有效增强单码本TTS LLMs的内在自回归策略，在不同数据规模和模型尺度下均表现出一致的改进效果。

Abstract: Recent advances in Large Language Models (LLMs) have transformed text-to-speech (TTS) synthesis, inspiring autoregressive frameworks that represent speech as sequences of discrete codec tokens. Among them, single-codebook TTS LLMs have emerged as compact and streamable architectures that jointly model semantic and acoustic integration. However, despite their efficiency, these models often exhibit unstable prosody, speaker drift, and degraded naturalness. To address these issues, we propose a multi-reward Group Relative Policy Optimization (GRPO) framework that directly optimizes the token generation policy of single-codebook TTS LLMs. Beyond standard intelligibility and speaker similarity objectives, our design integrates three rule-based rewards: a length penalty for duration consistency, an entropy regularization reward for decoding stability, and an LLM-annotated prosody alignment reward that explicitly supervises rhythm. In this prosody reward, an external reasoning LLM predicts multiple plausible pause structures via in-context learning, providing a human-preference-aligned supervisory signal for GRPO training. To assess universality, we further attach a flow-matching (FM) decoder on top of the GRPO-optimized AR backbone and observe consistent additional gains, indicating that our reinforcement optimization enhances the intrinsic AR policy. We further conduct a scalability analysis across data sizes and model scales, revealing that the proposed method consistently enhances prosodic stability, speaker similarity, and overall speech naturalness in single-codebook TTS LLMs.

</details>


### [23] [Acoustic neural networks: Identifying design principles and exploring physical feasibility](https://arxiv.org/abs/2511.21313)
*Ivan Kalthoff,Marcel Rey,Raphael Wittkowski*

Main category: cs.SD

TL;DR: 该论文提出了声学神经网络的设计框架，通过声波传播进行计算，在物理约束下训练神经网络，实现了语音分类任务，并开发了兼容无源声学组件的混合模型。


<details>
  <summary>Details</summary>
Motivation: 波导物理系统为实现超越传统电子器件的节能模拟计算提供了有前景的途径，声学神经网络在电子器件效率低下或受限的环境中具有低功耗计算潜力，但其系统化设计尚未充分探索。

Method: 采用数字孪生方法，在物理约束条件下训练传统神经网络架构，包括非负信号和权重、无偏置项，以及与基于强度的非负声学信号兼容的非线性。开发了SincHSRNN混合模型，结合可学习的声学带通滤波器和分层时间处理。

Result: 约束循环和分层架构能够准确执行语音分类，SincHSRNN在AudioMNIST数据集上达到95%的准确率，同时保持与无源声学组件的兼容性。学习参数对应可测量的材料和几何特性。

Conclusion: 建立了物理可实现的声学神经网络的通用设计原则，为低功耗、基于波的神经计算开辟了路径。

Abstract: Wave-guide-based physical systems provide a promising route toward energy-efficient analog computing beyond traditional electronics. Within this landscape, acoustic neural networks represent a promising approach for achieving low-power computation in environments where electronics are inefficient or limited, yet their systematic design has remained largely unexplored. Here we introduce a framework for designing and simulating acoustic neural networks, which perform computation through the propagation of sound waves. Using a digital-twin approach, we train conventional neural network architectures under physically motivated constraints including non-negative signals and weights, the absence of bias terms, and nonlinearities compatible with intensity-based, non-negative acoustic signals. Our work provides a general framework for acoustic neural networks that connects learnable network components directly to physically measurable acoustic properties, enabling the systematic design of realizable acoustic computing systems. We demonstrate that constrained recurrent and hierarchical architectures can perform accurate speech classification, and we propose the SincHSRNN, a hybrid model that combines learnable acoustic bandpass filters with hierarchical temporal processing. The SincHSRNN achieves up to 95% accuracy on the AudioMNIST dataset while remaining compatible with passive acoustic components. Beyond computational performance, the learned parameters correspond to measurable material and geometric properties such as attenuation and transmission. Our results establish general design principles for physically realizable acoustic neural networks and outline a pathway toward low-power, wave-based neural computing.

</details>


### [24] [SONAR: Spectral-Contrastive Audio Residuals for Generalizable Deepfake Detection](https://arxiv.org/abs/2511.21325)
*Ido Nitzan HIdekel,Gal lifshitz,Khen Cohen,Dan Raviv*

Main category: cs.SD

TL;DR: SONAR是一个频率引导的对比学习框架，通过显式分离音频信号的互补表示来检测Deepfake音频，特别关注高频残差特征，在ASVspoof 2021和真实场景基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 解决Deepfake音频检测器在分布外输入上泛化能力差的问题，主要原因是频谱偏差导致神经网络优先学习低频结构而忽略高频细节，使得检测器未能充分利用DF生成器留下的高频伪影。

Method: 提出SONAR框架：1) 使用XLSR编码器捕获主导低频内容；2) 通过可学习SRM和值约束高通滤波器提取微弱高频残差；3) 频率交叉注意力融合两种视图；4) 频率感知Jensen-Shannon对比损失优化嵌入空间。

Result: 在ASVspoof 2021和真实场景基准测试中达到最先进性能，收敛速度比强基线快4倍，将潜在空间分离为自然高频和失真高频两个不相交流形。

Conclusion: SONAR通过将微弱高频残差提升为一级学习信号，提供了一个完全数据驱动、频率引导的对比框架，可无缝集成到任何模型或模态中，特别适用于需要细微高频线索的场景。

Abstract: Deepfake (DF) audio detectors still struggle to generalize to out of distribution inputs. A central reason is spectral bias, the tendency of neural networks to learn low-frequency structure before high-frequency (HF) details, which both causes DF generators to leave HF artifacts and leaves those same artifacts under-exploited by common detectors. To address this gap, we propose Spectral-cONtrastive Audio Residuals (SONAR), a frequency-guided framework that explicitly disentangles an audio signal into complementary representations. An XLSR encoder captures the dominant low-frequency content, while the same cloned path, preceded by learnable SRM, value-constrained high-pass filters, distills faint HF residuals. Frequency cross-attention reunites the two views for long- and short-range frequency dependencies, and a frequency-aware Jensen-Shannon contrastive loss pulls real content-noise pairs together while pushing fake embeddings apart, accelerating optimization and sharpening decision boundaries. Evaluated on the ASVspoof 2021 and in-the-wild benchmarks, SONAR attains state-of-the-art performance and converges four times faster than strong baselines. By elevating faint high-frequency residuals to first-class learning signals, SONAR unveils a fully data-driven, frequency-guided contrastive framework that splits the latent space into two disjoint manifolds: natural-HF for genuine audio and distorted-HF for synthetic audio, thereby sharpening decision boundaries. Because the scheme operates purely at the representation level, it is architecture-agnostic and, in future work, can be seamlessly integrated into any model or modality where subtle high-frequency cues are decisive.

</details>


### [25] [Generating Separated Singing Vocals Using a Diffusion Model Conditioned on Music Mixtures](https://arxiv.org/abs/2511.21342)
*Genís Plaja-Roglans,Yun-Ning Hung,Xavier Serra,Igor Pereira*

Main category: cs.SD

TL;DR: 本文提出了一种基于扩散模型的歌唱声音分离方法，通过训练模型在给定音乐混合物的条件下生成独唱人声，在生成式系统中表现优异，并可与非生成式基线方法竞争。


<details>
  <summary>Details</summary>
Motivation: 音乐混合物中分离各个元素对音乐分析和实践至关重要。虽然通常使用神经网络通过掩蔽或变换时频表示来提取目标源，但生成扩散模型的灵活性和泛化能力为这一复杂任务提供了新的解决方案。

Method: 使用扩散模型从真实音乐录音中分离歌唱声音，训练模型在对应混合物的条件下生成独唱人声。通过扩散采样的迭代性质，用户可以控制质量-效率权衡并在需要时细化输出。

Result: 该方法改进了先前的生成系统，在使用补充数据训练时，相对于非生成式基线获得了有竞争力的客观评分。消融研究展示了采样算法中用户可配置参数的影响。

Conclusion: 扩散模型为歌唱声音分离提供了一种有效的生成式方法，具有用户可控的质量-效率权衡能力，在客观指标上表现优异。

Abstract: Separating the individual elements in a musical mixture is an essential process for music analysis and practice. While this is generally addressed using neural networks optimized to mask or transform the time-frequency representation of a mixture to extract the target sources, the flexibility and generalization capabilities of generative diffusion models are giving rise to a novel class of solutions for this complicated task. In this work, we explore singing voice separation from real music recordings using a diffusion model which is trained to generate the solo vocals conditioned on the corresponding mixture. Our approach improves upon prior generative systems and achieves competitive objective scores against non-generative baselines when trained with supplementary data. The iterative nature of diffusion sampling enables the user to control the quality-efficiency trade-off, and also refine the output when needed. We present an ablation study of the sampling algorithm, highlighting the effects of the user-configurable parameters.

</details>


### [26] [HarmonicAttack: An Adaptive Cross-Domain Audio Watermark Removal](https://arxiv.org/abs/2511.21577)
*Kexin Li,Xiao Hu,Ilya Grishchenko,David Lie*

Main category: cs.SD

TL;DR: HarmonicAttack是一种高效的音频水印移除方法，仅需目标水印方案的基本生成能力，就能训练通用水印移除模型，在时域和频域同时操作，具有接近实时的性能。


<details>
  <summary>Details</summary>
Motivation: AI生成音频的滥用带来安全挑战，水印技术是重要防御手段。研究有效的水印移除方法对于客观评估水印方案的鲁棒性至关重要。现有方法要么需要不切实际的水印知识，要么计算成本高。

Method: 采用双路径卷积自编码器在时域和频域同时操作，结合GAN风格训练来分离水印和原始音频。仅需目标水印方案的生成能力即可训练通用移除模型。

Result: 在AudioSeal、WavMark和Silentcipher等最先进水印方案上，HarmonicAttack表现出比先前方法更强的水印移除能力，且性能接近实时。即使对分布外样本也能保持良好性能。

Conclusion: HarmonicAttack提供了一种高效实用的水印移除方法，有助于更客观地评估音频水印方案的鲁棒性，揭示现有方案可能存在的安全漏洞。

Abstract: The availability of high-quality, AI-generated audio raises security challenges such as misinformation campaigns and voice-cloning fraud. A key defense against the misuse of AI-generated audio is by watermarking it, so that it can be easily distinguished from genuine audio. As those seeking to misuse AI-generated audio may thus seek to remove audio watermarks, studying effective watermark removal techniques is critical to being able to objectively evaluate the robustness of audio watermarks against removal. Previous watermark removal schemes either assume impractical knowledge of the watermarks they are designed to remove or are computationally expensive, potentially generating a false sense of confidence in current watermark schemes.
  We introduce HarmonicAttack, an efficient audio watermark removal method that only requires the basic ability to generate the watermarks from the targeted scheme and nothing else. With this, we are able to train a general watermark removal model that is able to remove the watermarks generated by the targeted scheme from any watermarked audio sample. HarmonicAttack employs a dual-path convolutional autoencoder that operates in both temporal and frequency domains, along with GAN-style training, to separate the watermark from the original audio. When evaluated against state-of-the-art watermark schemes AudioSeal, WavMark, and Silentcipher, HarmonicAttack demonstrates greater watermark removal ability than previous watermark removal methods with near real-time performance. Moreover, while HarmonicAttack requires training, we find that it is able to transfer to out-of-distribution samples with minimal degradation in performance.

</details>


### [27] [Harmonic-Percussive Disentangled Neural Audio Codec for Bandwidth Extension](https://arxiv.org/abs/2511.21580)
*Benoît Giniès,Xiaoyu Bie,Olivier Fercoq,Gaël Richard*

Main category: cs.SD

TL;DR: 本文提出了一种基于Transformer语言模型的带宽扩展方法，通过神经音频编解码器的离散表示来重构音频信号的高频成分。


<details>
  <summary>Details</summary>
Motivation: 带宽扩展是音频处理中的长期问题，传统方法性能有限。近年来神经架构在音频任务中取得显著进展，但需要更好的编解码器与生成模型之间的协同设计。

Method: 将带宽扩展构建为音频token预测问题，使用基于Transformer的语言模型，训练在解缠神经音频编解码器产生的离散表示上，编解码器通过谐波-打击乐分解引导解缠。

Result: 该方法在客观指标和主观评估中都实现了高质量的信号重构，证明了编解码器解缠与生成模型阶段对齐的重要性。

Conclusion: 全局的、表示感知的设计对于推进带宽扩展具有潜力，强调了编解码器结构与Transformer建模之间有效耦合的重要性。

Abstract: Bandwidth extension, the task of reconstructing the high-frequency components of an audio signal from its low-pass counterpart, is a long-standing problem in audio processing. While traditional approaches have evolved alongside the broader trends in signal processing, recent advances in neural architectures have significantly improved performance across a wide range of audio tasks, In this work, we extend these advances by framing bandwidth extension as an audio token prediction problem. Specifically, we train a transformer-based language model on the discrete representations produced by a disentangled neural audio codec, where the disentanglement is guided by a Harmonic-Percussive decomposition of the input signals, highlighting spectral structures particularly relevant for bandwidth extension. Our approach introduces a novel codec design that explicitly accounts for the downstream token prediction task, enabling a more effective coupling between codec structure and transformer modeling. This joint design yields high-quality reconstructions of the original signal, as measured by both objective metrics and subjective evaluations. These results highlight the importance of aligning codec disentanglement and representation learning with the generative modeling stage, and demonstrate the potential of global, representation-aware design for advancing bandwidth extension.

</details>

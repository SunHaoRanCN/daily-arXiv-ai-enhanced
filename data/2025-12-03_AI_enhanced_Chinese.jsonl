{"id": "2512.02025", "categories": ["eess.SP", "cs.AI", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.02025", "abs": "https://arxiv.org/abs/2512.02025", "authors": ["Aditya Sneh", "Nilesh Kumar Sahu", "Snehil Gupta", "Haroon R. Lone"], "title": "DySTAN: Joint Modeling of Sedentary Activity and Social Context from Smartphone Sensors", "comment": null, "summary": "Accurately recognizing human context from smartphone sensor data remains a significant challenge, especially in sedentary settings where activities such as studying, attending lectures, relaxing, and eating exhibit highly similar inertial patterns. Furthermore, social context plays a critical role in understanding user behavior, yet is often overlooked in mobile sensing research. To address these gaps, we introduce LogMe, a mobile sensing application that passively collects smartphone sensor data (accelerometer, gyroscope, magnetometer, and rotation vector) and prompts users for hourly self-reports capturing both sedentary activity and social context. Using this dual-label dataset, we propose DySTAN (Dynamic Cross-Stitch with Task Attention Network), a multi-task learning framework that jointly classifies both context dimensions from shared sensor inputs. It integrates task-specific layers with cross-task attention to model subtle distinctions effectively. DySTAN improves sedentary activity macro F1 scores by 21.8% over a single-task CNN-BiLSTM-GRU (CBG) model and by 8.2% over the strongest multi-task baseline, Sluice Network (SN). These results demonstrate the importance of modeling multiple, co-occurring context dimensions to improve the accuracy and robustness of mobile context recognition.", "AI": {"tldr": "LogMe\u5e94\u7528\u6536\u96c6\u667a\u80fd\u624b\u673a\u4f20\u611f\u5668\u6570\u636e\u548c\u7528\u6237\u81ea\u6211\u62a5\u544a\uff0cDySTAN\u591a\u4efb\u52a1\u5b66\u4e60\u6846\u67b6\u8054\u5408\u5206\u7c7b\u4e45\u5750\u6d3b\u52a8\u548c\u793e\u4f1a\u60c5\u5883\uff0c\u663e\u8457\u63d0\u5347\u8bc6\u522b\u51c6\u786e\u7387\u3002", "motivation": "\u73b0\u6709\u79fb\u52a8\u611f\u77e5\u7814\u7a76\u5728\u8bc6\u522b\u4e45\u5750\u6d3b\u52a8\uff08\u5982\u5b66\u4e60\u3001\u542c\u8bfe\u3001\u653e\u677e\u3001\u8fdb\u98df\uff09\u65f6\u9762\u4e34\u6311\u6218\uff0c\u8fd9\u4e9b\u6d3b\u52a8\u5177\u6709\u9ad8\u5ea6\u76f8\u4f3c\u7684\u60ef\u6027\u6a21\u5f0f\u3002\u6b64\u5916\uff0c\u793e\u4f1a\u60c5\u5883\u5bf9\u7406\u89e3\u7528\u6237\u884c\u4e3a\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5728\u79fb\u52a8\u611f\u77e5\u7814\u7a76\u4e2d\u5e38\u88ab\u5ffd\u89c6\u3002", "method": "\u5f00\u53d1LogMe\u79fb\u52a8\u611f\u77e5\u5e94\u7528\uff0c\u88ab\u52a8\u6536\u96c6\u667a\u80fd\u624b\u673a\u4f20\u611f\u5668\u6570\u636e\uff08\u52a0\u901f\u5ea6\u8ba1\u3001\u9640\u87ba\u4eea\u3001\u78c1\u529b\u8ba1\u3001\u65cb\u8f6c\u77e2\u91cf\uff09\uff0c\u5e76\u6bcf\u5c0f\u65f6\u63d0\u793a\u7528\u6237\u81ea\u6211\u62a5\u544a\u4e45\u5750\u6d3b\u52a8\u548c\u793e\u4f1a\u60c5\u5883\u3002\u63d0\u51faDySTAN\uff08\u52a8\u6001\u4ea4\u53c9\u7f1d\u5408\u4efb\u52a1\u6ce8\u610f\u529b\u7f51\u7edc\uff09\uff0c\u8fd9\u662f\u4e00\u4e2a\u591a\u4efb\u52a1\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u4efb\u52a1\u7279\u5b9a\u5c42\u548c\u8de8\u4efb\u52a1\u6ce8\u610f\u529b\u673a\u5236\uff0c\u4ece\u5171\u4eab\u4f20\u611f\u5668\u8f93\u5165\u4e2d\u8054\u5408\u5206\u7c7b\u4e24\u4e2a\u60c5\u5883\u7ef4\u5ea6\u3002", "result": "DySTAN\u5c06\u4e45\u5750\u6d3b\u52a8\u7684\u5b8f\u89c2F1\u5206\u6570\u6bd4\u5355\u4efb\u52a1CNN-BiLSTM-GRU\u6a21\u578b\u63d0\u9ad8\u4e8621.8%\uff0c\u6bd4\u6700\u5f3a\u7684\u591a\u4efb\u52a1\u57fa\u7ebfSluice Network\u63d0\u9ad8\u4e868.2%\u3002", "conclusion": "\u5efa\u6a21\u591a\u4e2a\u5171\u73b0\u7684\u60c5\u5883\u7ef4\u5ea6\u5bf9\u4e8e\u63d0\u9ad8\u79fb\u52a8\u60c5\u5883\u8bc6\u522b\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u81f3\u5173\u91cd\u8981\u3002DySTAN\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u4e45\u5750\u6d3b\u52a8\u8bc6\u522b\u548c\u793e\u4f1a\u60c5\u5883\u5efa\u6a21\u7684\u6311\u6218\u3002"}}
{"id": "2512.02026", "categories": ["eess.SP", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.02026", "abs": "https://arxiv.org/abs/2512.02026", "authors": ["Luis Correas-Naranjo", "Miguel Camacho-S\u00e1nchez", "La\u00ebtitia Launet", "Milena Zuric", "Valery Naranjo"], "title": "Towards Sustainable Precision: Machine Learning for Laser Micromachining Optimization", "comment": null, "summary": "In the pursuit of sustainable manufacturing, ultra-short pulse laser micromachining stands out as a promising solution while also offering high-precision and qualitative laser processing. However, unlocking the full potential of ultra-short pulse lasers requires an optimized monitoring system capable of early detection of defective workpieces, regardless of the preprocessing technique employed. While advances in machine learning can help predict process quality features, the complexity of monitoring data necessitates reducing both model size and data dimensionality to enable real-time analysis. To address these challenges, this paper introduces a machine learning framework designed to enhance surface quality assessment across diverse preprocessing techniques. To facilitate real-time laser processing monitoring, our solution aims to optimize the computational requirements of the machine learning model. Experimental results show that the proposed model not only outperforms the generalizability achieved by previous works across diverse preprocessing techniques but also significantly reduces the computational requirements for training. Through these advancements, we aim to establish the baseline for a more sustainable manufacturing process.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u673a\u5668\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u8d85\u77ed\u8109\u51b2\u6fc0\u5149\u5fae\u52a0\u5de5\u7684\u8868\u9762\u8d28\u91cf\u8bc4\u4f30\uff0c\u901a\u8fc7\u4f18\u5316\u8ba1\u7b97\u9700\u6c42\u5b9e\u73b0\u5b9e\u65f6\u76d1\u6d4b", "motivation": "\u8d85\u77ed\u8109\u51b2\u6fc0\u5149\u5fae\u52a0\u5de5\u5728\u53ef\u6301\u7eed\u5236\u9020\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u9700\u8981\u4f18\u5316\u7684\u76d1\u6d4b\u7cfb\u7edf\u6765\u65e9\u671f\u68c0\u6d4b\u7f3a\u9677\u5de5\u4ef6\u3002\u73b0\u6709\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u867d\u7136\u80fd\u9884\u6d4b\u5de5\u827a\u8d28\u91cf\u7279\u5f81\uff0c\u4f46\u76d1\u6d4b\u6570\u636e\u590d\u6742\uff0c\u9700\u8981\u51cf\u5c11\u6a21\u578b\u5927\u5c0f\u548c\u6570\u636e\u7ef4\u5ea6\u4ee5\u5b9e\u73b0\u5b9e\u65f6\u5206\u6790\u3002", "method": "\u5f15\u5165\u4e00\u4e2a\u673a\u5668\u5b66\u4e60\u6846\u67b6\uff0c\u65e8\u5728\u4f18\u5316\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u8ba1\u7b97\u9700\u6c42\uff0c\u4ee5\u4fc3\u8fdb\u5b9e\u65f6\u6fc0\u5149\u52a0\u5de5\u76d1\u6d4b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u6a21\u578b\u4e0d\u4ec5\u5728\u4e0d\u540c\u9884\u5904\u7406\u6280\u672f\u4e0a\u7684\u6cdb\u5316\u80fd\u529b\u4f18\u4e8e\u5148\u524d\u5de5\u4f5c\uff0c\u800c\u4e14\u663e\u8457\u51cf\u5c11\u4e86\u8bad\u7ec3\u7684\u8ba1\u7b97\u9700\u6c42\u3002", "conclusion": "\u901a\u8fc7\u8fd9\u4e9b\u8fdb\u5c55\uff0c\u65e8\u5728\u4e3a\u66f4\u53ef\u6301\u7eed\u7684\u5236\u9020\u8fc7\u7a0b\u5efa\u7acb\u57fa\u51c6\u3002"}}
{"id": "2512.02028", "categories": ["eess.SP", "cs.LG", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2512.02028", "abs": "https://arxiv.org/abs/2512.02028", "authors": ["Yiping Wang", "Peiren Wang", "Zhenye Li", "Fang Liu", "Jinguo Huang"], "title": "Seizure-NGCLNet: Representation Learning of SEEG Spatial Pathological Patterns for Epileptic Seizure Detection via Node-Graph Dual Contrastive Learning", "comment": null, "summary": "Complex spatial connectivity patterns, such as interictal suppression and ictal propagation, complicate accurate drug-resistant epilepsy (DRE) seizure detection using stereotactic electroencephalography (SEEG) and traditional machine learning methods. Two critical challenges remain:(1)a low signal-to-noise ratio in functional connectivity estimates, making it difficult to learn seizure-related interactions; and (2)expert labels for spatial pathological connectivity patterns are difficult to obtain, meanwhile lacking the patterns' representation to improve seizure detection. To address these issues, we propose a novel node-graph dual contrastive learning framework, Seizure-NGCLNet, to learn SEEG interictal suppression and ictal propagation patterns for detecting DRE seizures with high precision. First, an adaptive graph augmentation strategy guided by centrality metrics is developed to generate seizure-related brain networks. Second, a dual-contrastive learning approach is integrated, combining global graph-level contrast with local node-graph contrast, to encode both spatial structural and semantic epileptogenic features. Third, the pretrained embeddings are fine-tuned via a top-k localized graph attention network to perform the final classification. Extensive experiments on a large-scale public SEEG dataset from 33 DRE patients demonstrate that Seizure-NGCLNet achieves state-of-the-art performance, with an average accuracy of 95.93%, sensitivity of 96.25%, and specificity of 94.12%. Visualizations confirm that the learned embeddings clearly separate ictal from interictal states, reflecting suppression and propagation patterns that correspond to the clinical mechanisms. These results highlight Seizure-NGCLNet's ability to learn interpretable spatial pathological patterns, enhancing both seizure detection and seizure onset zone localization.", "AI": {"tldr": "\u63d0\u51faSeizure-NGCLNet\uff0c\u4e00\u79cd\u8282\u70b9-\u56fe\u53cc\u91cd\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u7acb\u4f53\u8111\u7535\u56fe(SEEG)\u4e2d\u5b66\u4e60\u766b\u75eb\u53d1\u4f5c\u76f8\u5173\u7684\u5927\u8111\u7f51\u7edc\u6a21\u5f0f\uff0c\u63d0\u9ad8\u8010\u836f\u6027\u766b\u75eb\u53d1\u4f5c\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u8010\u836f\u6027\u766b\u75eb(DRE)\u7684\u590d\u6742\u7a7a\u95f4\u8fde\u63a5\u6a21\u5f0f\uff08\u5982\u53d1\u4f5c\u95f4\u671f\u6291\u5236\u548c\u53d1\u4f5c\u671f\u4f20\u64ad\uff09\u4f7f\u5f97\u4f7f\u7528\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u8fdb\u884c\u766b\u75eb\u53d1\u4f5c\u68c0\u6d4b\u53d8\u5f97\u56f0\u96be\u3002\u4e3b\u8981\u6311\u6218\u5305\u62ec\uff1a1\uff09\u529f\u80fd\u8fde\u63a5\u4f30\u8ba1\u7684\u4fe1\u566a\u6bd4\u4f4e\uff0c\u96be\u4ee5\u5b66\u4e60\u53d1\u4f5c\u76f8\u5173\u4ea4\u4e92\uff1b2\uff09\u4e13\u5bb6\u6807\u6ce8\u7684\u7a7a\u95f4\u75c5\u7406\u8fde\u63a5\u6a21\u5f0f\u96be\u4ee5\u83b7\u53d6\uff0c\u4e14\u7f3a\u4e4f\u8fd9\u4e9b\u6a21\u5f0f\u7684\u8868\u793a\u6765\u6539\u5584\u53d1\u4f5c\u68c0\u6d4b\u3002", "method": "\u63d0\u51fa\u8282\u70b9-\u56fe\u53cc\u91cd\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6Seizure-NGCLNet\uff1a1\uff09\u57fa\u4e8e\u4e2d\u5fc3\u6027\u5ea6\u91cf\u7684\u81ea\u9002\u5e94\u56fe\u589e\u5f3a\u7b56\u7565\u751f\u6210\u766b\u75eb\u76f8\u5173\u5927\u8111\u7f51\u7edc\uff1b2\uff09\u96c6\u6210\u53cc\u91cd\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\uff0c\u7ed3\u5408\u5168\u5c40\u56fe\u7ea7\u5bf9\u6bd4\u548c\u5c40\u90e8\u8282\u70b9-\u56fe\u5bf9\u6bd4\uff0c\u7f16\u7801\u7a7a\u95f4\u7ed3\u6784\u548c\u8bed\u4e49\u81f4\u75eb\u7279\u5f81\uff1b3\uff09\u901a\u8fc7top-k\u5c40\u90e8\u56fe\u6ce8\u610f\u529b\u7f51\u7edc\u5fae\u8c03\u9884\u8bad\u7ec3\u5d4c\u5165\u8fdb\u884c\u6700\u7ec8\u5206\u7c7b\u3002", "result": "\u572833\u540dDRE\u60a3\u8005\u7684\u5927\u89c4\u6a21\u516c\u5171SEEG\u6570\u636e\u96c6\u4e0a\uff0cSeizure-NGCLNet\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff1a\u5e73\u5747\u51c6\u786e\u738795.93%\uff0c\u7075\u654f\u5ea696.25%\uff0c\u7279\u5f02\u602794.12%\u3002\u53ef\u89c6\u5316\u663e\u793a\u5b66\u4e60\u5230\u7684\u5d4c\u5165\u80fd\u6e05\u6670\u533a\u5206\u53d1\u4f5c\u671f\u548c\u53d1\u4f5c\u95f4\u671f\u72b6\u6001\uff0c\u53cd\u6620\u4e86\u4e0e\u4e34\u5e8a\u673a\u5236\u5bf9\u5e94\u7684\u6291\u5236\u548c\u4f20\u64ad\u6a21\u5f0f\u3002", "conclusion": "Seizure-NGCLNet\u80fd\u591f\u5b66\u4e60\u53ef\u89e3\u91ca\u7684\u7a7a\u95f4\u75c5\u7406\u6a21\u5f0f\uff0c\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u766b\u75eb\u53d1\u4f5c\u68c0\u6d4b\u6027\u80fd\uff0c\u8fd8\u6709\u52a9\u4e8e\u766b\u75eb\u53d1\u4f5c\u8d77\u59cb\u533a\u7684\u5b9a\u4f4d\uff0c\u4e3a\u8010\u836f\u6027\u766b\u75eb\u7684\u7cbe\u51c6\u8bca\u65ad\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2512.02153", "categories": ["eess.SP", "cs.IT"], "pdf": "https://arxiv.org/pdf/2512.02153", "abs": "https://arxiv.org/abs/2512.02153", "authors": ["Murat Babek Salman", "Emil Bj\u00f6rnson", "\u00d6zlem Tugfe Demir"], "title": "Hardware Distortion Aware Precoding for ISAC Systems", "comment": "5 pages, 4 figures, Asilomar Conference on Signals, Systems, and Computers, 2025", "summary": "The impact of hardware impairments on the spectral efficiency of communication systems is well studied, but their effect on sensing performance remains unexplored. In this paper, we analyze the influence of hardware impairments on integrated sensing and communication (ISAC) systems in cluttered environments. We derive the sensing signal-to-clutter-plus-noise ratio (SCNR) and show that hardware distortions significantly degrade sensing performance by enhancing clutter-induced noise, which masks target echoes. The isotropic nature of transmit distortion due to multiple stream transmission further complicates clutter suppression. To address this, we propose a distortion- and clutter-aware precoding strategy that minimizes the deviation from the communication-optimized precoder while improving sensing robustness. We also propose an alternative power allocation-based approach that reduces computational complexity. Numerical results confirm the effectiveness of the proposed approaches in overcoming hardware- and clutter-induced limitations, demonstrating significant performance gains over distortion-unaware designs.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u786c\u4ef6\u635f\u4f24\u5bf9\u96c6\u6210\u611f\u77e5\u4e0e\u901a\u4fe1\u7cfb\u7edf\u5728\u6742\u6ce2\u73af\u5883\u4e2d\u7684\u5f71\u54cd\uff0c\u63d0\u51fa\u4e86\u4e24\u79cd\u65b9\u6cd5\u6765\u51cf\u8f7b\u786c\u4ef6\u635f\u4f24\u548c\u6742\u6ce2\u5bf9\u611f\u77e5\u6027\u80fd\u7684\u6076\u5316\u3002", "motivation": "\u786c\u4ef6\u635f\u4f24\u5bf9\u901a\u4fe1\u7cfb\u7edf\u9891\u8c31\u6548\u7387\u7684\u5f71\u54cd\u5df2\u6709\u5145\u5206\u7814\u7a76\uff0c\u4f46\u5176\u5bf9\u611f\u77e5\u6027\u80fd\u7684\u5f71\u54cd\u5c1a\u672a\u63a2\u7d22\u3002\u672c\u6587\u65e8\u5728\u5206\u6790\u786c\u4ef6\u635f\u4f24\u5728\u6742\u6ce2\u73af\u5883\u4e2d\u5bf9\u96c6\u6210\u611f\u77e5\u4e0e\u901a\u4fe1\u7cfb\u7edf\u7684\u5f71\u54cd\u3002", "method": "\u63a8\u5bfc\u4e86\u611f\u77e5\u4fe1\u53f7\u4e0e\u6742\u6ce2\u52a0\u566a\u58f0\u6bd4\uff0c\u5206\u6790\u4e86\u786c\u4ef6\u5931\u771f\u5982\u4f55\u589e\u5f3a\u6742\u6ce2\u566a\u58f0\u5e76\u63a9\u76d6\u76ee\u6807\u56de\u6ce2\u3002\u63d0\u51fa\u4e86\u4e24\u79cd\u65b9\u6cd5\uff1a1\uff09\u5931\u771f\u548c\u6742\u6ce2\u611f\u77e5\u7684\u9884\u7f16\u7801\u7b56\u7565\uff0c\u5728\u6700\u5c0f\u5316\u4e0e\u901a\u4fe1\u4f18\u5316\u9884\u7f16\u7801\u5668\u504f\u5dee\u7684\u540c\u65f6\u63d0\u9ad8\u611f\u77e5\u9c81\u68d2\u6027\uff1b2\uff09\u57fa\u4e8e\u529f\u7387\u5206\u914d\u7684\u66ff\u4ee3\u65b9\u6cd5\u4ee5\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "result": "\u6570\u503c\u7ed3\u679c\u8bc1\u5b9e\u4e86\u6240\u63d0\u65b9\u6cd5\u5728\u514b\u670d\u786c\u4ef6\u548c\u6742\u6ce2\u5f15\u8d77\u7684\u9650\u5236\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u76f8\u6bd4\u4e0d\u8003\u8651\u5931\u771f\u7684\u8bbe\u8ba1\u663e\u793a\u51fa\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u786c\u4ef6\u5931\u771f\u663e\u8457\u6076\u5316\u611f\u77e5\u6027\u80fd\uff0c\u7279\u522b\u662f\u901a\u8fc7\u589e\u5f3a\u6742\u6ce2\u566a\u58f0\u3002\u63d0\u51fa\u7684\u5931\u771f\u548c\u6742\u6ce2\u611f\u77e5\u65b9\u6cd5\u80fd\u6709\u6548\u6539\u5584\u96c6\u6210\u611f\u77e5\u4e0e\u901a\u4fe1\u7cfb\u7edf\u5728\u786c\u4ef6\u635f\u4f24\u548c\u6742\u6ce2\u73af\u5883\u4e0b\u7684\u6027\u80fd\u3002"}}
{"id": "2512.02027", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.02027", "abs": "https://arxiv.org/abs/2512.02027", "authors": ["Kashaf Gulzar", "Dominik Wagner", "Sebastian P. Bayerl", "Florian H\u00f6nig", "Tobias Bocklet", "Korbinian Riedhammer"], "title": "On the Difficulty of Token-Level Modeling of Dysfluency and Fluency Shaping Artifacts", "comment": "6 pages, 1 figure. Accepted to ASRU 2025. This is the arXiv preprint of the accepted paper", "summary": "Automatic transcription of stuttered speech remains a challenge, even for modern end-to-end (E2E) automatic speech recognition (ASR) frameworks. Dysfluencies and fluency-shaping artifacts are often overlooked, resulting in non-verbatim transcriptions with limited clinical and research value. We propose a parameter-efficient adaptation method to decode dysfluencies and fluency modifications as special tokens within transcriptions, evaluated on simulated (LibriStutter, English) and natural (KSoF, German) stuttered speech datasets. To mitigate ASR performance disparities and bias towards English, we introduce a multi-step fine-tuning strategy with language-adaptive pretraining. Tokenization analysis further highlights the tokenizer's English-centric bias, which poses challenges for improving performance on German data. Our findings demonstrate the effectiveness of lightweight adaptation techniques for dysfluency-aware ASR while exposing key limitations in multilingual E2E systems.", "AI": {"tldr": "\u63d0\u51fa\u53c2\u6570\u9ad8\u6548\u7684\u81ea\u9002\u5e94\u65b9\u6cd5\uff0c\u5c06\u53e3\u5403\u8bed\u97f3\u4e2d\u7684\u4e0d\u6d41\u7545\u548c\u6d41\u7545\u5ea6\u4fee\u9970\u4f5c\u4e3a\u7279\u6b8a\u6807\u8bb0\u89e3\u7801\uff0c\u5728\u82f1\u8bed\u548c\u5fb7\u8bed\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u63ed\u793a\u4e86\u7aef\u5230\u7aefASR\u7cfb\u7edf\u7684\u591a\u8bed\u8a00\u5c40\u9650\u6027", "motivation": "\u73b0\u4ee3\u7aef\u5230\u7aefASR\u7cfb\u7edf\u5728\u53e3\u5403\u8bed\u97f3\u8f6c\u5f55\u65b9\u9762\u4ecd\u9762\u4e34\u6311\u6218\uff0c\u5f80\u5f80\u5ffd\u7565\u4e0d\u6d41\u7545\u548c\u6d41\u7545\u5ea6\u4fee\u9970\uff0c\u5bfc\u81f4\u8f6c\u5f55\u7ed3\u679c\u7f3a\u4e4f\u4e34\u5e8a\u548c\u7814\u7a76\u4ef7\u503c", "method": "\u63d0\u51fa\u53c2\u6570\u9ad8\u6548\u7684\u81ea\u9002\u5e94\u65b9\u6cd5\uff0c\u5c06\u53e3\u5403\u8bed\u97f3\u4e2d\u7684\u4e0d\u6d41\u7545\u548c\u6d41\u7545\u5ea6\u4fee\u9970\u89e3\u7801\u4e3a\u7279\u6b8a\u6807\u8bb0\uff1b\u91c7\u7528\u591a\u6b65\u5fae\u8c03\u7b56\u7565\u548c\u8bed\u8a00\u81ea\u9002\u5e94\u9884\u8bad\u7ec3\uff1b\u5728\u6a21\u62df\uff08LibriStutter\uff0c\u82f1\u8bed\uff09\u548c\u81ea\u7136\uff08KSoF\uff0c\u5fb7\u8bed\uff09\u53e3\u5403\u8bed\u97f3\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30", "result": "\u8f7b\u91cf\u7ea7\u81ea\u9002\u5e94\u6280\u672f\u5bf9\u4e0d\u6d41\u7545\u611f\u77e5ASR\u6709\u6548\uff0c\u4f46\u6807\u8bb0\u5316\u5206\u6790\u63ed\u793a\u4e86\u6807\u8bb0\u5668\u7684\u82f1\u8bed\u4e2d\u5fc3\u504f\u89c1\uff0c\u8fd9\u5bf9\u63d0\u5347\u5fb7\u8bed\u6570\u636e\u6027\u80fd\u6784\u6210\u6311\u6218", "conclusion": "\u5c55\u793a\u4e86\u8f7b\u91cf\u7ea7\u81ea\u9002\u5e94\u6280\u672f\u5728\u4e0d\u6d41\u7545\u611f\u77e5ASR\u4e2d\u7684\u6709\u6548\u6027\uff0c\u540c\u65f6\u66b4\u9732\u4e86\u7aef\u5230\u7aef\u591a\u8bed\u8a00\u7cfb\u7edf\u7684\u5173\u952e\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f\u6807\u8bb0\u5668\u7684\u82f1\u8bed\u4e2d\u5fc3\u504f\u89c1\u95ee\u9898"}}
{"id": "2512.02192", "categories": ["cs.SD", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.02192", "abs": "https://arxiv.org/abs/2512.02192", "authors": ["Mohammad Shokri", "Alexandra C. Salem", "Gabriel Levine", "Johanna Devaney", "Sarah Ita Levitan"], "title": "Story2MIDI: Emotionally Aligned Music Generation from Text", "comment": "8 pages (6 pages of main text + 2 pages of references and appendices), 4 figures, 1 table. Presented at IEEE Big Data 2025 3rd Workshop on AI Music Generation (AIMG 2025)", "summary": "In this paper, we introduce Story2MIDI, a sequence-to-sequence Transformer-based model for generating emotion-aligned music from a given piece of text. To develop this model, we construct the Story2MIDI dataset by merging existing datasets for sentiment analysis from text and emotion classification in music. The resulting dataset contains pairs of text blurbs and music pieces that evoke the same emotions in the reader or listener. Despite the small scale of our dataset and limited computational resources, our results indicate that our model effectively learns emotion-relevant features in music and incorporates them into its generation process, producing samples with diverse emotional responses. We evaluate the generated outputs using objective musical metrics and a human listening study, confirming the model's ability to capture intended emotional cues.", "AI": {"tldr": "Story2MIDI\uff1a\u57fa\u4e8eTransformer\u7684\u5e8f\u5217\u5230\u5e8f\u5217\u6a21\u578b\uff0c\u53ef\u4ece\u6587\u672c\u751f\u6210\u60c5\u611f\u5bf9\u9f50\u7684\u97f3\u4e50", "motivation": "\u5f00\u53d1\u80fd\u591f\u6839\u636e\u6587\u672c\u5185\u5bb9\u751f\u6210\u76f8\u5e94\u60c5\u611f\u97f3\u4e50\u7684AI\u7cfb\u7edf\uff0c\u5b9e\u73b0\u8de8\u6a21\u6001\u60c5\u611f\u8868\u8fbe", "method": "\u6784\u5efaStory2MIDI\u6570\u636e\u96c6\uff08\u5408\u5e76\u6587\u672c\u60c5\u611f\u5206\u6790\u548c\u97f3\u4e50\u60c5\u611f\u5206\u7c7b\u6570\u636e\u96c6\uff09\uff0c\u4f7f\u7528\u5e8f\u5217\u5230\u5e8f\u5217Transformer\u6a21\u578b\uff0c\u5728\u5c0f\u6570\u636e\u96c6\u548c\u6709\u9650\u8ba1\u7b97\u8d44\u6e90\u4e0b\u8bad\u7ec3", "result": "\u6a21\u578b\u80fd\u6709\u6548\u5b66\u4e60\u97f3\u4e50\u4e2d\u7684\u60c5\u611f\u76f8\u5173\u7279\u5f81\uff0c\u751f\u6210\u5177\u6709\u591a\u6837\u60c5\u611f\u54cd\u5e94\u7684\u97f3\u4e50\u6837\u672c\uff0c\u5ba2\u89c2\u97f3\u4e50\u6307\u6807\u548c\u4eba\u7c7b\u542c\u97f3\u6d4b\u8bd5\u5747\u8bc1\u5b9e\u5176\u6355\u6349\u60c5\u611f\u7ebf\u7d22\u7684\u80fd\u529b", "conclusion": "Story2MIDI\u6a21\u578b\u6210\u529f\u5b9e\u73b0\u4e86\u4ece\u6587\u672c\u5230\u60c5\u611f\u5bf9\u9f50\u97f3\u4e50\u7684\u751f\u6210\uff0c\u5373\u4f7f\u5728\u8d44\u6e90\u6709\u9650\u6761\u4ef6\u4e0b\u4e5f\u80fd\u6709\u6548\u5b66\u4e60\u60c5\u611f\u7279\u5f81"}}
{"id": "2512.02245", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2512.02245", "abs": "https://arxiv.org/abs/2512.02245", "authors": ["Ashutosh Prajapati", "Prathapasinghe Dharmawansa", "Marco Di Renzo", "Italo Atzeni"], "title": "Wavenumber-Division Multiplexing in Holographic MIMO with NLoS Channels", "comment": "Presented at the Asilomar Conference on Signals, Systems, and Computers 2025", "summary": "Wavenumber-division multiplexing (WDM) was introduced as a counterpart of orthogonal frequency-division multiplexing in the spatial-frequency domain for line-of-sight holographic multiple-input multiple-output (MIMO) systems. In this paper, we extend WDM to holographic MIMO channels with non-line-of-sight (NLoS) propagation. We show that applying WDM to the NLoS channel yields the corresponding angular-domain representation, which we characterize through the power spectral factor and power spectral density. We further obtain a closed-form characterization for the case of isotropic scattering, recovering Jakes' isotropic model. The analysis is complemented by numerical results evaluating the degrees of freedom and ergodic capacity under both isotropic and non-isotropic scattering.", "AI": {"tldr": "\u8bba\u6587\u5c06\u6ce2\u6570\u5206\u590d\u7528\u6280\u672f\u4ece\u89c6\u8ddd\u4f20\u64ad\u6269\u5c55\u5230\u975e\u89c6\u8ddd\u4f20\u64ad\u7684\u5168\u606fMIMO\u7cfb\u7edf\uff0c\u5206\u6790\u4e86\u89d2\u5ea6\u57df\u8868\u5f81\u3001\u5404\u5411\u540c\u6027\u6563\u5c04\u4e0b\u7684\u95ed\u5f0f\u89e3\uff0c\u5e76\u8bc4\u4f30\u4e86\u81ea\u7531\u5ea6\u548c\u904d\u5386\u5bb9\u91cf\u3002", "motivation": "\u6ce2\u6570\u5206\u590d\u7528\u6280\u672f\u6700\u521d\u662f\u4e3a\u89c6\u8ddd\u4f20\u64ad\u7684\u5168\u606fMIMO\u7cfb\u7edf\u8bbe\u8ba1\u7684\uff0c\u4f46\u5b9e\u9645\u901a\u4fe1\u573a\u666f\u4e2d\u975e\u89c6\u8ddd\u4f20\u64ad\u66f4\u4e3a\u5e38\u89c1\u3002\u56e0\u6b64\u9700\u8981\u5c06WDM\u6280\u672f\u6269\u5c55\u5230\u975e\u89c6\u8ddd\u4f20\u64ad\u7684\u5168\u606fMIMO\u4fe1\u9053\u4e2d\uff0c\u4ee5\u66f4\u597d\u5730\u7406\u89e3\u548c\u5229\u7528\u7a7a\u95f4\u9891\u7387\u57df\u7684\u7279\u6027\u3002", "method": "\u5c06WDM\u6280\u672f\u5e94\u7528\u4e8e\u975e\u89c6\u8ddd\u4fe1\u9053\uff0c\u5f97\u5230\u5bf9\u5e94\u7684\u89d2\u5ea6\u57df\u8868\u5f81\uff0c\u901a\u8fc7\u529f\u7387\u8c31\u56e0\u5b50\u548c\u529f\u7387\u8c31\u5bc6\u5ea6\u8fdb\u884c\u7279\u5f81\u5206\u6790\u3002\u9488\u5bf9\u5404\u5411\u540c\u6027\u6563\u5c04\u60c5\u51b5\uff0c\u63a8\u5bfc\u51fa\u95ed\u5f0f\u89e3\uff0c\u6062\u590dJakes\u7684\u5404\u5411\u540c\u6027\u6a21\u578b\u3002\u901a\u8fc7\u6570\u503c\u7ed3\u679c\u8bc4\u4f30\u5404\u5411\u540c\u6027\u548c\u975e\u5404\u5411\u540c\u6027\u6563\u5c04\u4e0b\u7684\u81ea\u7531\u5ea6\u548c\u904d\u5386\u5bb9\u91cf\u3002", "result": "\u6210\u529f\u5c06WDM\u6269\u5c55\u5230\u975e\u89c6\u8ddd\u4f20\u64ad\u7684\u5168\u606fMIMO\u4fe1\u9053\uff0c\u83b7\u5f97\u4e86\u89d2\u5ea6\u57df\u8868\u5f81\u7684\u6570\u5b66\u63cf\u8ff0\u3002\u5728\u5404\u5411\u540c\u6027\u6563\u5c04\u60c5\u51b5\u4e0b\u5f97\u5230\u4e86\u95ed\u5f0f\u89e3\uff0c\u4e0eJakes\u6a21\u578b\u4e00\u81f4\u3002\u6570\u503c\u5206\u6790\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u8bc4\u4f30\u4fe1\u9053\u81ea\u7531\u5ea6\u5e76\u8ba1\u7b97\u904d\u5386\u5bb9\u91cf\u3002", "conclusion": "WDM\u6280\u672f\u53ef\u4ee5\u6210\u529f\u6269\u5c55\u5230\u975e\u89c6\u8ddd\u4f20\u64ad\u7684\u5168\u606fMIMO\u7cfb\u7edf\uff0c\u4e3a\u7a7a\u95f4\u9891\u7387\u57df\u7684\u4fe1\u9053\u5206\u6790\u63d0\u4f9b\u4e86\u7406\u8bba\u6846\u67b6\uff0c\u7279\u522b\u662f\u5728\u5404\u5411\u540c\u6027\u6563\u5c04\u6761\u4ef6\u4e0b\u80fd\u591f\u83b7\u5f97\u7b80\u6d01\u7684\u95ed\u5f0f\u89e3\uff0c\u4e3a\u5b9e\u9645\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u7406\u8bba\u6307\u5bfc\u3002"}}
{"id": "2512.02759", "categories": ["eess.AS", "cs.SD", "eess.IV"], "pdf": "https://arxiv.org/pdf/2512.02759", "abs": "https://arxiv.org/abs/2512.02759", "authors": ["Aref Farhadipour", "Teodora Vukovic", "Volker Dellwo"], "title": "Towards Language-Independent Face-Voice Association with Multimodal Foundation Models", "comment": "This paper presents the system description of the UZH-CL team for the FAME2026 Challenge at ICASSP 2026. Our model achieved second place in the final ranking", "summary": "This paper describes the UZH-CL system submitted to the FAME2026 Challenge. The challenge focuses on cross-modal verification under unique multilingual conditions, specifically unseen and unheard languages. Our approach investigates two distinct architectures, consisting of a baseline dual-encoder system trained from scratch using contrastive and orthogonal projection losses, and a foundation model approach leveraging ImageBind with LoRA. To address the data scarcity and language constraints of the challenge, we curated an external Arabic dataset from VoxBlink. Our best-performing system, ImageBind-LoRA, demonstrates remarkable cross-lingual generalization: despite being fine-tuned exclusively on Arabic audio, it achieved an EER of 24.73% on the evaluation set (English and German), securing 2nd place in the competition.", "AI": {"tldr": "UZH-CL\u7cfb\u7edf\u53c2\u52a0FAME2026\u6311\u6218\u8d5b\uff0c\u91c7\u7528ImageBind-LoRA\u65b9\u6cd5\uff0c\u4ec5\u7528\u963f\u62c9\u4f2f\u8bed\u97f3\u9891\u5fae\u8c03\u5c31\u5728\u82f1\u8bed\u548c\u5fb7\u8bed\u8bc4\u4f30\u96c6\u4e0a\u53d6\u5f9724.73% EER\uff0c\u83b7\u5f97\u7b2c\u4e8c\u540d", "motivation": "\u89e3\u51b3FAME2026\u6311\u6218\u8d5b\u4e2d\u8de8\u6a21\u6001\u9a8c\u8bc1\u5728\u591a\u8bed\u8a00\u6761\u4ef6\u4e0b\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u672a\u89c1\u548c\u672a\u542c\u8fc7\u7684\u8bed\u8a00\uff0c\u5e94\u5bf9\u6570\u636e\u7a00\u7f3a\u548c\u8bed\u8a00\u9650\u5236\u7684\u6311\u6218", "method": "\u7814\u7a76\u4e24\u79cd\u67b6\u6784\uff1a1\uff09\u4ece\u5934\u8bad\u7ec3\u7684\u57fa\u7ebf\u53cc\u7f16\u7801\u5668\u7cfb\u7edf\uff0c\u4f7f\u7528\u5bf9\u6bd4\u635f\u5931\u548c\u6b63\u4ea4\u6295\u5f71\u635f\u5931\uff1b2\uff09\u57fa\u4e8eImageBind\u7ed3\u5408LoRA\u7684\u57fa\u7840\u6a21\u578b\u65b9\u6cd5\u3002\u4eceVoxBlink\u6536\u96c6\u5916\u90e8\u963f\u62c9\u4f2f\u8bed\u6570\u636e\u96c6\u89e3\u51b3\u6570\u636e\u7a00\u7f3a\u95ee\u9898", "result": "\u6700\u4f73\u7cfb\u7edfImageBind-LoRA\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u8de8\u8bed\u8a00\u6cdb\u5316\u80fd\u529b\uff1a\u4ec5\u7528\u963f\u62c9\u4f2f\u8bed\u97f3\u9891\u5fae\u8c03\uff0c\u5728\u82f1\u8bed\u548c\u5fb7\u8bed\u8bc4\u4f30\u96c6\u4e0a\u8fbe\u523024.73% EER\uff0c\u5728\u6bd4\u8d5b\u4e2d\u83b7\u5f97\u7b2c\u4e8c\u540d", "conclusion": "ImageBind-LoRA\u65b9\u6cd5\u5728\u8de8\u8bed\u8a00\u8de8\u6a21\u6001\u9a8c\u8bc1\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5373\u4f7f\u8bad\u7ec3\u6570\u636e\u6709\u9650\u4e14\u8bed\u8a00\u4e0d\u5339\u914d\uff0c\u4e5f\u80fd\u5b9e\u73b0\u6709\u6548\u7684\u6cdb\u5316\uff0c\u4e3a\u591a\u8bed\u8a00\u97f3\u9891-\u6587\u672c\u9a8c\u8bc1\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2512.02432", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2512.02432", "abs": "https://arxiv.org/abs/2512.02432", "authors": ["Ankur Gupta", "Anshul Rai", "Archit Bansal", "Vipul Arora"], "title": "Continual Learning for Singing Voice Separation with Human in the Loop Adaptation", "comment": "Proceedings of the 26th International Symposium on Frontiers of Research in Speech and Music, 2021", "summary": "Deep learning-based works for singing voice separation have performed exceptionally well in the recent past. However, most of these works do not focus on allowing users to interact with the model to improve performance. This can be crucial when deploying the model in real-world scenarios where music tracks can vary from the original training data in both genre and instruments. In this paper, we present a deep learning-based interactive continual learning framework for singing voice separation that allows users to fine-tune the vocal separation model to conform it to new target songs. We use a U-Net-based base model architecture that produces a mask for separating vocals from the spectrogram, followed by a human-in-the-loop task where the user provides feedback by marking a few false positives, i.e., regions in the extracted vocals that should have been silence. We propose two continual learning algorithms. Experiments substantiate the improvement in singing voice separation performance by the proposed algorithms over the base model in intra-dataset and inter-dataset settings.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u4ea4\u4e92\u5f0f\u6301\u7eed\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u6b4c\u5531\u4eba\u58f0\u5206\u79bb\uff0c\u5141\u8bb8\u7528\u6237\u901a\u8fc7\u6807\u8bb0\u8bef\u62a5\u533a\u57df\u6765\u5fae\u8c03\u6a21\u578b\uff0c\u9002\u5e94\u65b0\u6b4c\u66f2\u3002", "motivation": "\u73b0\u6709\u6b4c\u5531\u4eba\u58f0\u5206\u79bb\u6a21\u578b\u7f3a\u4e4f\u7528\u6237\u4ea4\u4e92\u80fd\u529b\uff0c\u65e0\u6cd5\u9002\u5e94\u5b9e\u9645\u573a\u666f\u4e2d\u97f3\u4e50\u98ce\u683c\u548c\u4e50\u5668\u7684\u53d8\u5316\uff0c\u9700\u8981\u7528\u6237\u53c2\u4e0e\u6765\u63d0\u5347\u6a21\u578b\u5728\u65b0\u6570\u636e\u4e0a\u7684\u8868\u73b0\u3002", "method": "\u4f7f\u7528U-Net\u67b6\u6784\u4f5c\u4e3a\u57fa\u7840\u6a21\u578b\u751f\u6210\u9891\u8c31\u56fe\u63a9\u7801\uff0c\u5f15\u5165\u4eba\u673a\u4ea4\u4e92\u73af\u8282\u8ba9\u7528\u6237\u6807\u8bb0\u8bef\u62a5\u533a\u57df\uff08\u672c\u5e94\u662f\u9759\u97f3\u7684\u4eba\u58f0\u533a\u57df\uff09\uff0c\u63d0\u51fa\u4e24\u79cd\u6301\u7eed\u5b66\u4e60\u7b97\u6cd5\u6765\u6574\u5408\u7528\u6237\u53cd\u9988\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u63d0\u51fa\u7684\u7b97\u6cd5\u5728\u6570\u636e\u96c6\u5185\u548c\u8de8\u6570\u636e\u96c6\u8bbe\u7f6e\u4e2d\uff0c\u76f8\u6bd4\u57fa\u7840\u6a21\u578b\u663e\u8457\u63d0\u5347\u4e86\u6b4c\u5531\u4eba\u58f0\u5206\u79bb\u6027\u80fd\u3002", "conclusion": "\u4ea4\u4e92\u5f0f\u6301\u7eed\u5b66\u4e60\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86\u6b4c\u5531\u4eba\u58f0\u5206\u79bb\u6a21\u578b\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u7684\u9002\u5e94\u6027\u548c\u6027\u80fd\uff0c\u901a\u8fc7\u7528\u6237\u53cd\u9988\u4f7f\u6a21\u578b\u80fd\u66f4\u597d\u5730\u5904\u7406\u4e0e\u8bad\u7ec3\u6570\u636e\u4e0d\u540c\u7684\u65b0\u6b4c\u66f2\u3002"}}
{"id": "2512.02462", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2512.02462", "abs": "https://arxiv.org/abs/2512.02462", "authors": ["Shengheng Liu", "Xingkang Li", "Yongming Huang", "Yuan Fang", "Qingji Jiang", "Dazhuan Xu", "Ziguo Zhong", "Dongming Wang", "Xiaohu You"], "title": "Bayesian Probability Fusion for Multi-AP Collaborative Sensing in Mobile Networks", "comment": "16 pages, 10 figures, submitted to Science China Information Sciences", "summary": "Integrated sensing and communication is widely acknowledged as a foundational technology for next-generation mobile networks. Compared with monostatic sensing, multi-access point (AP) collaborative sensing endows mobile networks with broader, more accurate, and resilient sensing capabilities, which are critical for diverse location-based sectors. This paper focuses on collaborative sensing in multi-AP networks and proposes a Bayesian probability fusion framework for target parameter estimation using orthogonal frequency-division multiplexing waveform. The framework models multi-AP received signals as probability distributions to capture stochastic observations from channel noise and scattering coefficients. Prior information is then incorporated into the joint probability density function to cast the problem as a constrained maximum a posteriori estimation. To address the high-dimensional optimization, we develop a prior-constrained gradient ascent (PCGA) algorithm that decouples correlated parameters and performs efficient gradient updates guided by the target prior. Theoretical analysis covers optimal fusion weights for global signal-to-noise ratio maximization, PCGA convergence, and the Cramer-Rao lower bound of the estimator, with insights applicable to broader fusion schemes. Extensive numerical simulations and real-world experiments with commercial devices show the framework reduces transmission overhead by 90% versus signal fusion and lowers estimation error by 41% relative to parameter fusion. Notably, field tests achieve submeter accuracy with 50% probability in typical coverage of mmWave APs. These improvements highlight a favorable balance between communication efficiency and estimation accuracy for practical multi-AP sensing deployment.\n  The dataset is released for research purposes and is publicly available at: http://pmldatanet.com.cn/dataapp/multimodal", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7528\u4e8e\u591aAP\u534f\u4f5c\u611f\u77e5\u7684\u8d1d\u53f6\u65af\u6982\u7387\u878d\u5408\u6846\u67b6\uff0c\u91c7\u7528PCGA\u7b97\u6cd5\u8fdb\u884c\u53c2\u6570\u4f30\u8ba1\uff0c\u5728\u964d\u4f4e\u4f20\u8f93\u5f00\u9500\u7684\u540c\u65f6\u63d0\u5347\u4f30\u8ba1\u7cbe\u5ea6", "motivation": "\u591a\u63a5\u5165\u70b9\u534f\u4f5c\u611f\u77e5\u76f8\u6bd4\u5355\u7ad9\u611f\u77e5\u80fd\u63d0\u4f9b\u66f4\u5e7f\u6cdb\u3001\u66f4\u51c6\u786e\u3001\u66f4\u9c81\u68d2\u7684\u611f\u77e5\u80fd\u529b\uff0c\u5bf9\u4e0b\u4e00\u4ee3\u79fb\u52a8\u7f51\u7edc\u4e2d\u7684\u4f4d\u7f6e\u670d\u52a1\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u4f20\u8f93\u5f00\u9500\u548c\u4f30\u8ba1\u7cbe\u5ea6\u4e4b\u95f4\u5b58\u5728\u6743\u8861\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u8d1d\u53f6\u65af\u6982\u7387\u878d\u5408\u6846\u67b6\uff0c\u5c06\u591aAP\u63a5\u6536\u4fe1\u53f7\u5efa\u6a21\u4e3a\u6982\u7387\u5206\u5e03\u4ee5\u6355\u83b7\u4fe1\u9053\u566a\u58f0\u548c\u6563\u5c04\u7cfb\u6570\u7684\u968f\u673a\u6027\u3002\u91c7\u7528\u5148\u9a8c\u7ea6\u675f\u68af\u5ea6\u4e0a\u5347\u7b97\u6cd5\u89e3\u51b3\u9ad8\u7ef4\u4f18\u5316\u95ee\u9898\uff0c\u5e76\u63a8\u5bfc\u4e86\u6700\u4f18\u878d\u5408\u6743\u91cd\u3001\u6536\u655b\u6027\u548cCramer-Rao\u4e0b\u754c\u3002", "result": "\u76f8\u6bd4\u4fe1\u53f7\u878d\u5408\u51cf\u5c1190%\u4f20\u8f93\u5f00\u9500\uff0c\u76f8\u6bd4\u53c2\u6570\u878d\u5408\u964d\u4f4e41%\u4f30\u8ba1\u8bef\u5dee\u3002\u5b9e\u9645\u6beb\u7c73\u6ce2AP\u8986\u76d6\u8303\u56f4\u5185\uff0c50%\u6982\u7387\u8fbe\u5230\u4e9a\u7c73\u7ea7\u7cbe\u5ea6\u3002\u53d1\u5e03\u4e86\u516c\u5f00\u6570\u636e\u96c6\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u5b9e\u9645\u591aAP\u611f\u77e5\u90e8\u7f72\u4e2d\u5b9e\u73b0\u4e86\u901a\u4fe1\u6548\u7387\u548c\u4f30\u8ba1\u7cbe\u5ea6\u4e4b\u95f4\u7684\u826f\u597d\u5e73\u8861\uff0c\u4e3a\u4e0b\u4e00\u4ee3\u79fb\u52a8\u7f51\u7edc\u7684\u96c6\u6210\u611f\u77e5\u4e0e\u901a\u4fe1\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.02891", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2512.02891", "abs": "https://arxiv.org/abs/2512.02891", "authors": ["Stefan Fichna", "Steven van de Par", "Bernhard U. Seeber", "Stephan D. Ewert"], "title": "Perceptual evaluation of Acoustic Level of Detail in Virtual Acoustic Environments", "comment": "This work has been submitted to Acoustics for possible publication. Template provided by MDPI", "summary": "Virtual acoustic environments enable the creation and simulation of realistic and eco-logically valid daily-life situations vital for hearing research and audiology. Reverberant indoor environments are particularly important. For real-time applications, room acous-tics simulation requires simplifications, however, the necessary acoustic level of detail (ALOD) remains unclear in order to capture all perceptually relevant effects. This study examines the impact of varying ALOD in simulations of three real environments: a living room with a coupled kitchen, a pub, and an underground station. ALOD was varied by generating different numbers of image sources for early reflections, or by excluding geo-metrical room details specific for each environment. Simulations were perceptually eval-uated using headphones in comparison to binaural room impulse responses measured with a dummy head in the corresponding real environments, or by using loudspeakers. The study assessed the perceived overall difference for a pulse stimulus, a played electric bass and a speech token. Additionally, plausibility, speech intelligibility, and externaliza-tion were evaluated. Results indicate that a strong reduction in ALOD is feasible while maintaining similar plausibility, speech intelligibility, and externalization as with dummy head recordings. The number and accuracy of early reflections appear less relevant, pro-vided diffuse late reverberation is appropriately represented.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u865a\u62df\u58f0\u5b66\u73af\u5883\u4e2d\u58f0\u5b66\u7ec6\u8282\u6c34\u5e73(ALOD)\u5bf9\u611f\u77e5\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u5373\u4f7f\u5927\u5e45\u964d\u4f4eALOD\u4ecd\u80fd\u4fdd\u6301\u4e0e\u771f\u5b9e\u5f55\u97f3\u76f8\u4f3c\u7684\u611f\u77e5\u8d28\u91cf\uff0c\u524d\u63d0\u662f\u9002\u5f53\u8868\u73b0\u6269\u6563\u540e\u671f\u6df7\u54cd\u3002", "motivation": "\u865a\u62df\u58f0\u5b66\u73af\u5883\u5bf9\u542c\u529b\u7814\u7a76\u548c\u542c\u529b\u5b66\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5b9e\u65f6\u5e94\u7528\u9700\u8981\u7b80\u5316\u623f\u95f4\u58f0\u5b66\u6a21\u62df\u3002\u76ee\u524d\u5c1a\u4e0d\u6e05\u695a\u9700\u8981\u591a\u5c11\u58f0\u5b66\u7ec6\u8282\u6c34\u5e73(ALOD)\u624d\u80fd\u6355\u6349\u6240\u6709\u611f\u77e5\u76f8\u5173\u6548\u679c\u3002", "method": "\u7814\u7a76\u5728\u4e09\u4e2a\u771f\u5b9e\u73af\u5883\uff08\u5ba2\u5385+\u53a8\u623f\u3001\u9152\u5427\u3001\u5730\u94c1\u7ad9\uff09\u4e2d\u53d8\u5316ALOD\uff1a\u901a\u8fc7\u751f\u6210\u4e0d\u540c\u6570\u91cf\u7684\u955c\u50cf\u6e90\u6765\u6539\u53d8\u65e9\u671f\u53cd\u5c04\uff0c\u6216\u6392\u9664\u7279\u5b9a\u51e0\u4f55\u7ec6\u8282\u3002\u4f7f\u7528\u8033\u673a\u4e0e\u771f\u5b9e\u73af\u5883\u4e2d\u7684\u5047\u4eba\u5934\u53cc\u8033\u5f55\u97f3\u8fdb\u884c\u6bd4\u8f83\u8bc4\u4f30\uff0c\u6216\u4f7f\u7528\u626c\u58f0\u5668\u3002\u8bc4\u4f30\u5185\u5bb9\u5305\u62ec\u8109\u51b2\u523a\u6fc0\u3001\u7535\u8d1d\u65af\u548c\u8bed\u97f3\u7247\u6bb5\u7684\u6574\u4f53\u5dee\u5f02\u611f\u77e5\uff0c\u4ee5\u53ca\u5408\u7406\u6027\u3001\u8bed\u97f3\u6e05\u6670\u5ea6\u548c\u5916\u90e8\u5316\u611f\u77e5\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u5373\u4f7f\u5927\u5e45\u964d\u4f4eALOD\uff0c\u4ecd\u80fd\u4fdd\u6301\u4e0e\u5047\u4eba\u5934\u5f55\u97f3\u76f8\u4f3c\u7684\u5408\u7406\u6027\u3001\u8bed\u97f3\u6e05\u6670\u5ea6\u548c\u5916\u90e8\u5316\u611f\u77e5\u3002\u65e9\u671f\u53cd\u5c04\u7684\u6570\u91cf\u548c\u51c6\u786e\u6027\u4f3c\u4e4e\u4e0d\u90a3\u4e48\u91cd\u8981\uff0c\u524d\u63d0\u662f\u6269\u6563\u540e\u671f\u6df7\u54cd\u5f97\u5230\u9002\u5f53\u8868\u73b0\u3002", "conclusion": "\u865a\u62df\u58f0\u5b66\u73af\u5883\u6a21\u62df\u53ef\u4ee5\u5728\u4fdd\u6301\u826f\u597d\u611f\u77e5\u8d28\u91cf\u7684\u540c\u65f6\u5927\u5e45\u7b80\u5316\uff0c\u5173\u952e\u5728\u4e8e\u9002\u5f53\u8868\u73b0\u6269\u6563\u540e\u671f\u6df7\u54cd\u800c\u975e\u7cbe\u786e\u7684\u65e9\u671f\u53cd\u5c04\u7ec6\u8282\u3002"}}
{"id": "2512.02515", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2512.02515", "abs": "https://arxiv.org/abs/2512.02515", "authors": ["Lixing He", "Yunqi Guo", "Haozheng Hou", "Zhenyu Yan"], "title": "VibOmni: Towards Scalable Bone-conduction Speech Enhancement on Earables", "comment": "Submitted to TMC", "summary": "Earables, such as True Wireless Stereo earphones and VR/AR headsets, are increasingly popular, yet their compact design poses challenges for robust voice-related applications like telecommunication and voice assistant interactions in noisy environments. Existing speech enhancement systems, reliant solely on omnidirectional microphones, struggle with ambient noise like competing speakers. To address these issues, we propose VibOmni, a lightweight, end-to-end multi-modal speech enhancement system for earables that leverages bone-conducted vibrations captured by widely available Inertial Measurement Units (IMUs). VibOmni integrates a two-branch encoder-decoder deep neural network to fuse audio and vibration features. To overcome the scarcity of paired audio-vibration datasets, we introduce a novel data augmentation technique that models Bone Conduction Functions (BCFs) from limited recordings, enabling synthetic vibration data generation with only 4.5% spectrogram similarity error. Additionally, a multi-modal SNR estimator facilitates continual learning and adaptive inference, optimizing performance in dynamic, noisy settings without on-device back-propagation. Evaluated on real-world datasets from 32 volunteers with different devices, VibOmni achieves up to 21% improvement in Perceptual Evaluation of Speech Quality (PESQ), 26% in Signal-to-Noise Ratio (SNR), and about 40% WER reduction with much less latency on mobile devices. A user study with 35 participants showed 87% preferred VibOmni over baselines, demonstrating its effectiveness for deployment in diverse acoustic environments.", "AI": {"tldr": "VibOmni\uff1a\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7aef\u5230\u7aef\u591a\u6a21\u6001\u8bed\u97f3\u589e\u5f3a\u7cfb\u7edf\uff0c\u5229\u7528IMU\u91c7\u96c6\u7684\u9aa8\u4f20\u5bfc\u632f\u52a8\u6765\u63d0\u5347\u8033\u6234\u8bbe\u5907\u5728\u5608\u6742\u73af\u5883\u4e2d\u7684\u8bed\u97f3\u8d28\u91cf", "motivation": "\u8033\u6234\u8bbe\u5907\uff08\u5982TWS\u8033\u673a\u548cVR/AR\u5934\u663e\uff09\u5728\u5608\u6742\u73af\u5883\u4e2d\u8bed\u97f3\u5e94\u7528\u6548\u679c\u4e0d\u4f73\uff0c\u73b0\u6709\u4ec5\u4f9d\u8d56\u5168\u5411\u9ea6\u514b\u98ce\u7684\u7cfb\u7edf\u96be\u4ee5\u5904\u7406\u73af\u5883\u566a\u58f0\u548c\u7ade\u4e89\u8bf4\u8bdd\u8005\u5e72\u6270", "method": "\u63d0\u51faVibOmni\u7cfb\u7edf\uff0c\u91c7\u7528\u53cc\u5206\u652f\u7f16\u7801\u5668-\u89e3\u7801\u5668\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u878d\u5408\u97f3\u9891\u548c\u632f\u52a8\u7279\u5f81\uff1b\u5f15\u5165BCF\u5efa\u6a21\u7684\u6570\u636e\u589e\u5f3a\u6280\u672f\u751f\u6210\u5408\u6210\u632f\u52a8\u6570\u636e\uff1b\u4f7f\u7528\u591a\u6a21\u6001SNR\u4f30\u8ba1\u5668\u5b9e\u73b0\u6301\u7eed\u5b66\u4e60\u548c\u81ea\u9002\u5e94\u63a8\u7406", "result": "\u572832\u540d\u5fd7\u613f\u8005\u7684\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\uff0cVibOmni\u5728PESQ\u4e0a\u63d0\u534721%\uff0cSNR\u63d0\u534726%\uff0cWER\u964d\u4f4e\u7ea640%\uff0c\u79fb\u52a8\u8bbe\u5907\u5ef6\u8fdf\u66f4\u4f4e\uff1b35\u540d\u53c2\u4e0e\u8005\u7684\u7528\u6237\u7814\u7a76\u663e\u793a87%\u504f\u597dVibOmni", "conclusion": "VibOmni\u901a\u8fc7\u6709\u6548\u878d\u5408\u9aa8\u4f20\u5bfc\u632f\u52a8\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8033\u6234\u8bbe\u5907\u5728\u5608\u6742\u73af\u5883\u4e2d\u7684\u8bed\u97f3\u589e\u5f3a\u6027\u80fd\uff0c\u5177\u6709\u5b9e\u9645\u90e8\u7f72\u4ef7\u503c"}}
{"id": "2512.02464", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2512.02464", "abs": "https://arxiv.org/abs/2512.02464", "authors": ["Jiaxuan Li", "Yilong Chen", "Fan Liu", "Jie Xu"], "title": "Channel Knowledge Map Enabled Low-Altitude ISAC Networks: Joint Air Corridor Planning and Base Station Deployment", "comment": null, "summary": "This letter addresses the joint air corridor planning and base station (BS) deployment problem for low-altitude integrated sensing and communication (ISAC) networks. In the considered system, unmanned aerial vehicles (UAVs) operate within a structured air corridor composed of connected cubic segments, and multiple BSs need to be selectively deployed at a set of candidate locations to ensure both sensing and communication coverage throughout the corridor. In particular, we leverage the channel knowledge map (CKM) to characterize wireless channels for candidate BS sites prior to deployment, thereby facilitating the offline planning. Under this setup, we minimize the system cost in terms of the weighted sum of the air corridor length and the number of deployed BSs, subject to the constraints on both sensing and communication performance across the corridor. To solve the formulated large-scale nonconvex integer programming problem, we develop a hierarchical coarse-to-fine grid decomposition algorithm. Simulation results demonstrate the benefit of the proposed joint design in reducing the overall deployment cost while ensuring the coverage of the low-altitude ISAC networks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u4f4e\u7a7aISAC\u7f51\u7edc\u7684\u8054\u5408\u7a7a\u4e2d\u8d70\u5eca\u89c4\u5212\u548c\u57fa\u7ad9\u90e8\u7f72\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u5c42\u7c97\u5230\u7ec6\u7f51\u683c\u5206\u89e3\u7b97\u6cd5\u6700\u5c0f\u5316\u7cfb\u7edf\u6210\u672c\uff0c\u786e\u4fdd\u4f20\u611f\u548c\u901a\u4fe1\u8986\u76d6\u3002", "motivation": "\u5728\u4f4e\u7a7a\u96c6\u6210\u4f20\u611f\u4e0e\u901a\u4fe1\u7f51\u7edc\u4e2d\uff0c\u9700\u8981\u540c\u65f6\u89c4\u5212\u65e0\u4eba\u673a\u7a7a\u4e2d\u8d70\u5eca\u548c\u90e8\u7f72\u57fa\u7ad9\uff0c\u786e\u4fdd\u6574\u4e2a\u8d70\u5eca\u7684\u4f20\u611f\u548c\u901a\u4fe1\u8986\u76d6\uff0c\u540c\u65f6\u6700\u5c0f\u5316\u90e8\u7f72\u6210\u672c\u3002", "method": "\u5229\u7528\u4fe1\u9053\u77e5\u8bc6\u56fe\uff08CKM\uff09\u5728\u90e8\u7f72\u524d\u8868\u5f81\u5019\u9009\u57fa\u7ad9\u7ad9\u70b9\u7684\u65e0\u7ebf\u4fe1\u9053\uff0c\u63d0\u51fa\u5206\u5c42\u7c97\u5230\u7ec6\u7f51\u683c\u5206\u89e3\u7b97\u6cd5\u6765\u89e3\u51b3\u5927\u89c4\u6a21\u975e\u51f8\u6574\u6570\u89c4\u5212\u95ee\u9898\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u8054\u5408\u8bbe\u8ba1\u80fd\u591f\u5728\u786e\u4fdd\u4f4e\u7a7aISAC\u7f51\u7edc\u8986\u76d6\u7684\u540c\u65f6\uff0c\u6709\u6548\u964d\u4f4e\u6574\u4f53\u90e8\u7f72\u6210\u672c\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u4f4e\u7a7aISAC\u7f51\u7edc\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u8054\u5408\u7a7a\u4e2d\u8d70\u5eca\u89c4\u5212\u548c\u57fa\u7ad9\u90e8\u7f72\u65b9\u6848\uff0c\u901a\u8fc7\u79bb\u7ebf\u89c4\u5212\u4f18\u5316\u7cfb\u7edf\u6210\u672c\uff0c\u786e\u4fdd\u7f51\u7edc\u6027\u80fd\u3002"}}
{"id": "2512.02523", "categories": ["cs.SD", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.02523", "abs": "https://arxiv.org/abs/2512.02523", "authors": ["Xueyan Li", "Yuxin Wang", "Mengjie Jiang", "Qingzi Zhu", "Jiang Zhang", "Zoey Kim", "Yazhe Niu"], "title": "Generative Multi-modal Feedback for Singing Voice Synthesis Evaluation", "comment": "16 pages, 5 figures", "summary": "Singing voice synthesis (SVS) has advanced significantly, enabling models to generate vocals with accurate pitch and consistent style. As these capabilities improve, the need for reliable evaluation and optimization becomes increasingly critical. However, current methods like reward systems often rely on single numerical scores, struggle to capture various dimensions such as phrasing or expressiveness, and require costly annotations, limiting interpretability and generalization. To address these issues, we propose a generative feedback (i.e., reward model) framework that provides multi-dimensional language and audio feedback for SVS assessment. Our approach leverages an audio-language model to generate text and audio critiques-covering aspects such as melody, content, and auditory quality. The model is fine-tuned on a hybrid dataset combining human music reactions and synthetic critiques from a MLLMs, enhancing diversity and linguistic richness. Quantitative experiments validate the effectiveness of the proposed dataset and training strategy, demonstrating that the framework produces musically accurate and interpretable evaluations suitable for guiding generative model improvement. The code is at [https://github.com/opendilab/VocalCritic](https://github.com/opendilab/VocalCritic)", "AI": {"tldr": "\u63d0\u51faVocalCritic\u6846\u67b6\uff0c\u4f7f\u7528\u97f3\u9891-\u8bed\u8a00\u6a21\u578b\u4e3a\u6b4c\u5531\u5408\u6210\u63d0\u4f9b\u591a\u7ef4\u8bed\u8a00\u548c\u97f3\u9891\u53cd\u9988\uff0c\u89e3\u51b3\u4f20\u7edf\u5956\u52b1\u6a21\u578b\u4f9d\u8d56\u5355\u4e00\u5206\u6570\u3001\u96be\u4ee5\u6355\u6349\u8868\u8fbe\u7ef4\u5ea6\u7684\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u6b4c\u5531\u5408\u6210\u8bc4\u4f30\u65b9\u6cd5\u4f9d\u8d56\u5355\u4e00\u6570\u503c\u8bc4\u5206\uff0c\u96be\u4ee5\u6355\u6349\u97f3\u8272\u3001\u8868\u8fbe\u7b49\u591a\u7ef4\u5ea6\u7279\u5f81\uff0c\u4e14\u9700\u8981\u6602\u8d35\u7684\u4eba\u5de5\u6807\u6ce8\uff0c\u9650\u5236\u4e86\u53ef\u89e3\u91ca\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u751f\u6210\u5f0f\u53cd\u9988\u6846\u67b6\uff0c\u4f7f\u7528\u97f3\u9891-\u8bed\u8a00\u6a21\u578b\u751f\u6210\u6587\u672c\u548c\u97f3\u9891\u5f62\u5f0f\u7684\u6279\u8bc4\u53cd\u9988\uff0c\u6db5\u76d6\u65cb\u5f8b\u3001\u5185\u5bb9\u3001\u542c\u89c9\u8d28\u91cf\u7b49\u65b9\u9762\u3002\u901a\u8fc7\u6df7\u5408\u4eba\u7c7b\u97f3\u4e50\u53cd\u5e94\u548cMLLMs\u751f\u6210\u7684\u5408\u6210\u6279\u8bc4\u6570\u636e\u8fdb\u884c\u5fae\u8c03\u3002", "result": "\u5b9a\u91cf\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6570\u636e\u96c6\u548c\u8bad\u7ec3\u7b56\u7565\u7684\u6709\u6548\u6027\uff0c\u6846\u67b6\u80fd\u591f\u4ea7\u751f\u97f3\u4e50\u51c6\u786e\u4e14\u53ef\u89e3\u91ca\u7684\u8bc4\u4f30\uff0c\u9002\u5408\u6307\u5bfc\u751f\u6210\u6a21\u578b\u7684\u6539\u8fdb\u3002", "conclusion": "VocalCritic\u6846\u67b6\u4e3a\u6b4c\u5531\u5408\u6210\u63d0\u4f9b\u4e86\u591a\u7ef4\u3001\u53ef\u89e3\u91ca\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u5956\u52b1\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u6709\u52a9\u4e8e\u63d0\u5347\u751f\u6210\u6a21\u578b\u7684\u8d28\u91cf\u3002"}}
{"id": "2512.02501", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2512.02501", "abs": "https://arxiv.org/abs/2512.02501", "authors": ["Qi Zhen", "Pan Tang", "Haiyang Miao", "Enrui Liu", "Ximan Liu", "Zihang Ding", "Jianhua Zhang"], "title": "Cell-free versus Conventional Massive MIMO : An Analysis of Channel Capacity based on Channel Measurement in the FR3 Band", "comment": null, "summary": "Cell-free massive MIMO (CF-mMIMO) has emerged as a promising technology for next generation wireless systems, combining the benefits of distributed antenna systems (DAS) and traditional MIMO technology. In this work, we present the first extensive channel measurements for CF-mMIMO in the mid-band (FR3, 6-24 GHz), using a virtual widely distributed antenna array comprising 512 elements in the urban Macrocell (UMa) environment. Based on the measurement data, this paper compares the channel capacity of CF-mMIMO and Conventional mMIMO under both line-of-sight (LOS) and non-line-of-sight (NLOS) conditions across a range of signal-to-noise ratios (SNRs). We then analyze how channel capacity varies with Rx positions from the perspectives of the full array and of individual subarrays. Finally, we conclude that the 64-element array configuration yields the greatest advantage in channel capacity for CF-mMIMO in the measurement environment considered, with gains of 14.02\\% under LOS and 24.61\\% under NLOS conditions. This in-depth analysis of channel capacity in the FR3 band provides critical insights for optimizing CF-mMIMO systems in next generation wireless networks.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u5728FR3\u9891\u6bb5\uff086-24 GHz\uff09\u5bf9\u65e0\u8702\u7a9d\u5927\u89c4\u6a21MIMO\u8fdb\u884c\u5e7f\u6cdb\u4fe1\u9053\u6d4b\u91cf\uff0c\u6bd4\u8f83\u4e86CF-mMIMO\u4e0e\u4f20\u7edfmMIMO\u5728LOS\u548cNLOS\u6761\u4ef6\u4e0b\u7684\u4fe1\u9053\u5bb9\u91cf\uff0c\u53d1\u73b064\u5929\u7ebf\u914d\u7f6e\u5728\u6d4b\u91cf\u73af\u5883\u4e2d\u80fd\u5e26\u6765\u6700\u5927\u5bb9\u91cf\u589e\u76ca\u3002", "motivation": "\u65e0\u8702\u7a9d\u5927\u89c4\u6a21MIMO\u7ed3\u5408\u4e86\u5206\u5e03\u5f0f\u5929\u7ebf\u7cfb\u7edf\u548c\u4f20\u7edfMIMO\u6280\u672f\u7684\u4f18\u52bf\uff0c\u662f\u4e0b\u4e00\u4ee3\u65e0\u7ebf\u7cfb\u7edf\u7684\u5173\u952e\u6280\u672f\u3002\u7136\u800c\u5728FR3\u4e2d\u9891\u6bb5\uff086-24 GHz\uff09\u7f3a\u4e4f\u5b9e\u9645\u4fe1\u9053\u6d4b\u91cf\u6570\u636e\uff0c\u9700\u8981\u4e86\u89e3\u5176\u5b9e\u9645\u6027\u80fd\u8868\u73b0\u3002", "method": "\u4f7f\u7528\u5305\u542b512\u4e2a\u5929\u7ebf\u7684\u865a\u62df\u5206\u5e03\u5f0f\u5929\u7ebf\u9635\u5217\uff0c\u5728\u57ce\u533a\u5b8f\u8702\u7a9d\u73af\u5883\u4e2d\u8fdb\u884c\u4fe1\u9053\u6d4b\u91cf\u3002\u57fa\u4e8e\u6d4b\u91cf\u6570\u636e\uff0c\u6bd4\u8f83CF-mMIMO\u4e0e\u4f20\u7edfmMIMO\u5728\u4e0d\u540cSNR\u3001LOS/NLOS\u6761\u4ef6\u4e0b\u7684\u4fe1\u9053\u5bb9\u91cf\uff0c\u5e76\u4ece\u5168\u9635\u5217\u548c\u5b50\u9635\u5217\u89d2\u5ea6\u5206\u6790\u5bb9\u91cf\u968f\u63a5\u6536\u4f4d\u7f6e\u7684\u53d8\u5316\u3002", "result": "\u6d4b\u91cf\u7ed3\u679c\u663e\u793a\uff0c\u5728\u8003\u8651\u7684\u6d4b\u91cf\u73af\u5883\u4e2d\uff0c64\u5929\u7ebf\u914d\u7f6e\u4e3aCF-mMIMO\u5e26\u6765\u6700\u5927\u4fe1\u9053\u5bb9\u91cf\u589e\u76ca\uff1aLOS\u6761\u4ef6\u4e0b\u589e\u76ca14.02%\uff0cNLOS\u6761\u4ef6\u4e0b\u589e\u76ca24.61%\u3002", "conclusion": "FR3\u9891\u6bb5CF-mMIMO\u4fe1\u9053\u5bb9\u91cf\u7684\u6df1\u5165\u5206\u6790\u4e3a\u4f18\u5316\u4e0b\u4e00\u4ee3\u65e0\u7ebf\u7f51\u7edc\u4e2d\u7684\u65e0\u8702\u7a9d\u5927\u89c4\u6a21MIMO\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5173\u952e\u89c1\u89e3\uff0c64\u5929\u7ebf\u914d\u7f6e\u5728\u6d4b\u91cf\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u6700\u4f73\u6027\u80fd\u3002"}}
{"id": "2512.02652", "categories": ["cs.SD", "cs.AI", "cs.MM"], "pdf": "https://arxiv.org/pdf/2512.02652", "abs": "https://arxiv.org/abs/2512.02652", "authors": ["Hong-Jie You", "Jie-Jing Shao", "Xiao-Wen Yang", "Lin-Han Jia", "Lan-Zhe Guo", "Yu-Feng Li"], "title": "Pianist Transformer: Towards Expressive Piano Performance Rendering via Scalable Self-Supervised Pre-Training", "comment": null, "summary": "Existing methods for expressive music performance rendering rely on supervised learning over small labeled datasets, which limits scaling of both data volume and model size, despite the availability of vast unlabeled music, as in vision and language. To address this gap, we introduce Pianist Transformer, with four key contributions: 1) a unified Musical Instrument Digital Interface (MIDI) data representation for learning the shared principles of musical structure and expression without explicit annotation; 2) an efficient asymmetric architecture, enabling longer contexts and faster inference without sacrificing rendering quality; 3) a self-supervised pre-training pipeline with 10B tokens and 135M-parameter model, unlocking data and model scaling advantages for expressive performance rendering; 4) a state-of-the-art performance model, which achieves strong objective metrics and human-level subjective ratings. Overall, Pianist Transformer establishes a scalable path toward human-like performance synthesis in the music domain.", "AI": {"tldr": "Pianist Transformer\uff1a\u9996\u4e2a\u57fa\u4e8e\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u7684\u5927\u89c4\u6a21\u97f3\u4e50\u8868\u73b0\u529b\u6e32\u67d3\u6a21\u578b\uff0c\u4f7f\u7528\u7edf\u4e00MIDI\u8868\u793a\u548c\u9ad8\u6548\u975e\u5bf9\u79f0\u67b6\u6784\uff0c\u572810B token\u4e0a\u8bad\u7ec3135M\u53c2\u6570\u6a21\u578b\uff0c\u5b9e\u73b0\u4eba\u7c7b\u6c34\u5e73\u7684\u97f3\u4e50\u8868\u73b0\u529b\u5408\u6210\u3002", "motivation": "\u73b0\u6709\u97f3\u4e50\u8868\u73b0\u529b\u6e32\u67d3\u65b9\u6cd5\u4f9d\u8d56\u5c0f\u89c4\u6a21\u6807\u6ce8\u6570\u636e\u96c6\uff0c\u9650\u5236\u4e86\u6570\u636e\u548c\u6a21\u578b\u89c4\u6a21\u7684\u6269\u5c55\uff0c\u5c3d\u7ba1\u5b58\u5728\u5927\u91cf\u672a\u6807\u6ce8\u97f3\u4e50\u6570\u636e\u3002\u9700\u8981\u89e3\u51b3\u8fd9\u4e00\u6269\u5c55\u74f6\u9888\u4ee5\u5b9e\u73b0\u66f4\u9ad8\u8d28\u91cf\u7684\u97f3\u4e50\u8868\u73b0\u529b\u5408\u6210\u3002", "method": "1) \u7edf\u4e00\u7684MIDI\u6570\u636e\u8868\u793a\uff0c\u65e0\u9700\u663e\u5f0f\u6807\u6ce8\u5373\u53ef\u5b66\u4e60\u97f3\u4e50\u7ed3\u6784\u548c\u8868\u73b0\u529b\u7684\u5171\u4eab\u539f\u5219\uff1b2) \u9ad8\u6548\u7684\u975e\u5bf9\u79f0\u67b6\u6784\uff0c\u652f\u6301\u66f4\u957f\u4e0a\u4e0b\u6587\u548c\u66f4\u5feb\u63a8\u7406\uff1b3) \u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u6d41\u7a0b\uff0c\u4f7f\u752810B token\u8bad\u7ec3135M\u53c2\u6570\u6a21\u578b\uff1b4) \u6700\u5148\u8fdb\u7684\u6027\u80fd\u6a21\u578b\u3002", "result": "\u6a21\u578b\u5728\u5ba2\u89c2\u6307\u6807\u548c\u4e3b\u89c2\u8bc4\u5206\u4e0a\u5747\u8fbe\u5230\u4eba\u7c7b\u6c34\u5e73\uff0c\u5efa\u7acb\u4e86\u97f3\u4e50\u8868\u73b0\u529b\u5408\u6210\u7684\u53ef\u6269\u5c55\u8def\u5f84\uff0c\u5b9e\u73b0\u4e86\u6570\u636e\u89c4\u6a21\u548c\u6a21\u578b\u89c4\u6a21\u7684\u53cc\u91cd\u7a81\u7834\u3002", "conclusion": "Pianist Transformer\u4e3a\u97f3\u4e50\u9886\u57df\u7684\u4eba\u7c7b\u6c34\u5e73\u8868\u73b0\u529b\u5408\u6210\u5efa\u7acb\u4e86\u53ef\u6269\u5c55\u7684\u8def\u5f84\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u6570\u636e\u548c\u6a21\u578b\u89c4\u6a21\u6269\u5c55\u4e0a\u7684\u9650\u5236\uff0c\u4e3a\u97f3\u4e50AI\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2512.02557", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2512.02557", "abs": "https://arxiv.org/abs/2512.02557", "authors": ["Xuan He", "Hongwei Hou", "Yafei Wang", "Wenjin Wang", "Shi Jin", "Symeon Chatzinotas", "Bj\u00f6rn Ottersten"], "title": "Deep Learning-Based Joint Uplink-Downlink CSI Acquisition for Next-Generation Upper Mid-Band Systems", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "In next-generation wireless communication systems, the newly designated upper mid-band has attracted considerable attention, also called frequency range 3 (FR3), highlighting the need for downlink (DL) transmission design, which fundamentally relies on accurate CSI. However, CSI acquisition in FR3 systems faces significant challenges: the increased number of antennas and wider transmission bandwidth introduces prohibitive training overhead with traditional estimation approaches, as each probing captures only incomplete spatial-frequency observation, while higher carrier frequencies lead to faster temporal channel variation. To address these challenges, we propose a novel CSI acquisition framework that integrates CSI feedback, uplink (UL) and DL channel estimation, as well as channel prediction in the FR3 TDD massive MIMO systems. Specifically, we first develop the Joint UL and DL Channel Estimation Network (JUDCEN) to fuse incomplete observations based on the SRSs and CSI-RSs. By exploiting the complementary characteristics of preliminary UL and DL estimation features, obtained through initial UL estimation and quantized-feedback-assisted DL estimation, it enables full CSI reconstruction in the spatial domain. To mitigate the performance degradation in the feedback process, we propose the Transformer-MLP CSI Feedback Network (TMCFN), employing an MLP-based module to jointly exploit angle- and delay-domain features. Building upon the reconstructed full CSI, we further develop the Mamba-based Channel Prediction Network (MCPN), which exploits selective state-space model (SSM) mechanism to capture long-range temporal dynamics in the angle-delay domain for future CSI prediction. Simulation results demonstrate that the proposed framework consistently outperforms benchmarks in both CSI acquisition accuracy and transmission spectral efficiency with lower computational complexity.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7528\u4e8eFR3\u9891\u6bb5\u5927\u89c4\u6a21MIMO\u7cfb\u7edf\u7684CSI\u83b7\u53d6\u6846\u67b6\uff0c\u6574\u5408CSI\u53cd\u9988\u3001\u4e0a\u4e0b\u884c\u4fe1\u9053\u4f30\u8ba1\u548c\u4fe1\u9053\u9884\u6d4b\uff0c\u901a\u8fc7\u8054\u5408\u4f30\u8ba1\u7f51\u7edc\u3001Transformer-MLP\u53cd\u9988\u7f51\u7edc\u548cMamba\u9884\u6d4b\u7f51\u7edc\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u4e0b\u4e00\u4ee3\u65e0\u7ebf\u901a\u4fe1\u7cfb\u7edf\u4e2d\u7684FR3\u9891\u6bb5\u9762\u4e34CSI\u83b7\u53d6\u6311\u6218\uff1a\u5929\u7ebf\u6570\u91cf\u589e\u52a0\u548c\u4f20\u8f93\u5e26\u5bbd\u6269\u5927\u5bfc\u81f4\u4f20\u7edf\u4f30\u8ba1\u65b9\u6cd5\u8bad\u7ec3\u5f00\u9500\u8fc7\u5927\uff0c\u800c\u66f4\u9ad8\u8f7d\u9891\u5bfc\u81f4\u4fe1\u9053\u65f6\u53d8\u66f4\u5feb\uff0c\u9700\u8981\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "1) \u63d0\u51faJUDCEN\u8054\u5408\u4e0a\u4e0b\u884c\u4fe1\u9053\u4f30\u8ba1\u7f51\u7edc\uff0c\u878d\u5408SRS\u548cCSI-RS\u7684\u4e0d\u5b8c\u6574\u89c2\u6d4b\uff1b2) \u8bbe\u8ba1TMCFN Transformer-MLP CSI\u53cd\u9988\u7f51\u7edc\uff0c\u8054\u5408\u5229\u7528\u89d2\u5ea6\u57df\u548c\u65f6\u5ef6\u57df\u7279\u5f81\uff1b3) \u5f00\u53d1MCPN Mamba\u4fe1\u9053\u9884\u6d4b\u7f51\u7edc\uff0c\u5229\u7528\u9009\u62e9\u6027\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u6355\u6349\u65f6\u57df\u52a8\u6001\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u6846\u67b6\u5728CSI\u83b7\u53d6\u7cbe\u5ea6\u548c\u4f20\u8f93\u9891\u8c31\u6548\u7387\u65b9\u9762\u5747\u4f18\u4e8e\u57fa\u51c6\u65b9\u6cd5\uff0c\u540c\u65f6\u5177\u6709\u66f4\u4f4e\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86FR3\u9891\u6bb5\u5927\u89c4\u6a21MIMO\u7cfb\u7edf\u7684CSI\u83b7\u53d6\u6311\u6218\uff0c\u901a\u8fc7\u6574\u5408\u53cd\u9988\u3001\u4f30\u8ba1\u548c\u9884\u6d4b\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fd\u3001\u4f4e\u590d\u6742\u5ea6\u7684CSI\u83b7\u53d6\u65b9\u6848\u3002"}}
{"id": "2512.02669", "categories": ["cs.SD", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.02669", "abs": "https://arxiv.org/abs/2512.02669", "authors": ["Gauri Deshpande", "Harish Battula", "Ashish Panda", "Sunil Kumar Kopparapu"], "title": "SAND Challenge: Four Approaches for Dysartria Severity Classification", "comment": "7 pages, 5 figures", "summary": "This paper presents a unified study of four distinct modeling approaches for classifying dysarthria severity in the Speech Analysis for Neurodegenerative Diseases (SAND) challenge. All models tackle the same five class classification task using a common dataset of speech recordings. We investigate: (1) a ViT-OF method leveraging a Vision Transformer on spectrogram images, (2) a 1D-CNN approach using eight 1-D CNN's with majority-vote fusion, (3) a BiLSTM-OF approach using nine BiLSTM models with majority vote fusion, and (4) a Hierarchical XGBoost ensemble that combines glottal and formant features through a two stage learning framework. Each method is described, and their performances on a validation set of 53 speakers are compared. Results show that while the feature-engineered XGBoost ensemble achieves the highest macro-F1 (0.86), the deep learning models (ViT, CNN, BiLSTM) attain competitive F1-scores (0.70) and offer complementary insights into the problem.", "AI": {"tldr": "\u672c\u6587\u5bf9SAND\u6311\u6218\u8d5b\u4e2d\u56db\u79cd\u4e0d\u540c\u7684\u6784\u97f3\u969c\u788d\u4e25\u91cd\u7a0b\u5ea6\u5206\u7c7b\u5efa\u6a21\u65b9\u6cd5\u8fdb\u884c\u4e86\u7edf\u4e00\u7814\u7a76\uff0c\u6bd4\u8f83\u4e86\u57fa\u4e8e\u89c6\u89c9Transformer\u7684ViT-OF\u30011D-CNN\u3001BiLSTM-OF\u548c\u5206\u5c42XGBoost\u96c6\u6210\u56db\u79cd\u65b9\u6cd5\u5728\u76f8\u540c\u6570\u636e\u96c6\u4e0a\u7684\u6027\u80fd\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u7cfb\u7edf\u6bd4\u8f83\u4e0d\u540c\u5efa\u6a21\u65b9\u6cd5\u5728\u6784\u97f3\u969c\u788d\u4e25\u91cd\u7a0b\u5ea6\u5206\u7c7b\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u63a2\u7d22\u7279\u5f81\u5de5\u7a0b\u65b9\u6cd5\u4e0e\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u7684\u4f18\u52a3\uff0c\u4e3a\u795e\u7ecf\u9000\u884c\u6027\u75be\u75c5\u8bed\u97f3\u5206\u6790\u63d0\u4f9b\u53c2\u8003\u3002", "method": "\u4f7f\u7528\u56db\u79cd\u4e0d\u540c\u65b9\u6cd5\uff1a1\uff09\u57fa\u4e8e\u8c31\u56fe\u56fe\u50cf\u7684Vision Transformer\uff08ViT-OF\uff09\uff1b2\uff09\u4f7f\u7528\u516b\u4e2a1D-CNN\u5e76\u8fdb\u884c\u591a\u6570\u6295\u7968\u878d\u5408\uff1b3\uff09\u4f7f\u7528\u4e5d\u4e2aBiLSTM\u6a21\u578b\u5e76\u8fdb\u884c\u591a\u6570\u6295\u7968\u878d\u5408\uff1b4\uff09\u91c7\u7528\u4e24\u9636\u6bb5\u5b66\u4e60\u6846\u67b6\u7ed3\u5408\u58f0\u95e8\u548c\u5171\u632f\u5cf0\u7279\u5f81\u7684\u5206\u5c42XGBoost\u96c6\u6210\u3002", "result": "\u572853\u540d\u8bf4\u8bdd\u8005\u7684\u9a8c\u8bc1\u96c6\u4e0a\uff0c\u7279\u5f81\u5de5\u7a0b\u7684XGBoost\u96c6\u6210\u65b9\u6cd5\u83b7\u5f97\u6700\u9ad8\u7684macro-F1\u5206\u6570\uff080.86\uff09\uff0c\u800c\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff08ViT\u3001CNN\u3001BiLSTM\uff09\u83b7\u5f97\u7ea60.70\u7684F1\u5206\u6570\uff0c\u8868\u73b0\u51fa\u7ade\u4e89\u6027\u4f46\u7565\u900a\u4e8e\u7279\u5f81\u5de5\u7a0b\u65b9\u6cd5\u3002", "conclusion": "\u7279\u5f81\u5de5\u7a0b\u7684XGBoost\u96c6\u6210\u65b9\u6cd5\u5728\u6784\u97f3\u969c\u788d\u4e25\u91cd\u7a0b\u5ea6\u5206\u7c7b\u4efb\u52a1\u4e0a\u8868\u73b0\u6700\u4f73\uff0c\u4f46\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u4e5f\u5c55\u73b0\u51fa\u7ade\u4e89\u6027\u6027\u80fd\uff0c\u5e76\u4e3a\u95ee\u9898\u63d0\u4f9b\u4e86\u4e92\u8865\u7684\u89c6\u89d2\uff0c\u8868\u660e\u4e0d\u540c\u65b9\u6cd5\u5404\u6709\u4f18\u52bf\u3002"}}
{"id": "2512.02563", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2512.02563", "abs": "https://arxiv.org/abs/2512.02563", "authors": ["Xiaotong Zhao", "Yuanhao Cui", "Weijie Yuan", "Ziye Jia", "Heng Liu", "Chengwen Xing"], "title": "Predictive Beamforming in Low-Altitude Wireless Networks: A Cross-Attention Approach", "comment": null, "summary": "Accurate beam prediction is essential for maintaining reliable links and high spectral efficiency in dynamic low-altitude wireless networks. However, existing approaches often fail to capture the deep correlations across heterogeneous sensing modalities, limiting their adaptability in complex three-dimensional environments. To overcome these challenges, we propose a multi-modal predictive beamforming method based on a cross-attention fusion mechanism that jointly leverages visual and structured sensor data. The proposed model utilizes a Convolutional Neural Network (CNN) to learn multi-scale spatial feature hierarchies from visual images and a Transformer encoder to capture cross-dimensional dependencies within sensor data. Then, a cross-attention fusion module is introduced to integrate complementary information between the two modalities, generating a unified and discriminative representation for accurate beam prediction. Through experimental evaluations conducted on a real-world dataset, our method reaches 79.7% Top-1 accuracy and 99.3% Top-3 accuracy, surpassing the 3D ResNet-Transformer baseline by 4.4%-23.2% across Top-1 to Top-5 metrics. These results verify that multi-modal cross-attention fusion is effective for intelligent beam selection in dynamic low-altitude wireless networks.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u8de8\u6ce8\u610f\u529b\u878d\u5408\u7684\u591a\u6a21\u6001\u9884\u6d4b\u6ce2\u675f\u8d4b\u5f62\u65b9\u6cd5\uff0c\u5229\u7528\u89c6\u89c9\u548c\u4f20\u611f\u5668\u6570\u636e\uff0c\u5728\u52a8\u6001\u4f4e\u7a7a\u65e0\u7ebf\u7f51\u7edc\u4e2d\u5b9e\u73b0\u51c6\u786e\u6ce2\u675f\u9884\u6d4b", "motivation": "\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u5f02\u6784\u611f\u77e5\u6a21\u6001\u95f4\u7684\u6df1\u5c42\u76f8\u5173\u6027\uff0c\u9650\u5236\u4e86\u5728\u590d\u6742\u4e09\u7ef4\u73af\u5883\u4e2d\u7684\u9002\u5e94\u6027\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u6ce2\u675f\u9884\u6d4b\u65b9\u6cd5", "method": "\u4f7f\u7528CNN\u5b66\u4e60\u89c6\u89c9\u56fe\u50cf\u7684\u591a\u5c3a\u5ea6\u7a7a\u95f4\u7279\u5f81\uff0cTransformer\u7f16\u7801\u5668\u6355\u6349\u4f20\u611f\u5668\u6570\u636e\u7684\u8de8\u7ef4\u5ea6\u4f9d\u8d56\uff0c\u901a\u8fc7\u8de8\u6ce8\u610f\u529b\u878d\u5408\u6a21\u5757\u6574\u5408\u4e24\u79cd\u6a21\u6001\u7684\u4e92\u8865\u4fe1\u606f", "result": "\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8fbe\u523079.7%\u7684Top-1\u51c6\u786e\u7387\u548c99.3%\u7684Top-3\u51c6\u786e\u7387\uff0c\u6bd43D ResNet-Transformer\u57fa\u7ebf\u5728Top-1\u5230Top-5\u6307\u6807\u4e0a\u63d0\u53474.4%-23.2%", "conclusion": "\u591a\u6a21\u6001\u8de8\u6ce8\u610f\u529b\u878d\u5408\u5bf9\u52a8\u6001\u4f4e\u7a7a\u65e0\u7ebf\u7f51\u7edc\u4e2d\u7684\u667a\u80fd\u6ce2\u675f\u9009\u62e9\u662f\u6709\u6548\u7684\uff0c\u80fd\u591f\u751f\u6210\u7edf\u4e00\u4e14\u5177\u6709\u533a\u5206\u6027\u7684\u8868\u793a"}}
{"id": "2512.02783", "categories": ["cs.SD", "cs.NE"], "pdf": "https://arxiv.org/pdf/2512.02783", "abs": "https://arxiv.org/abs/2512.02783", "authors": ["Bj\u00f6rn \u00de\u00f3r J\u00f3nsson", "\u00c7a\u011fr\u0131 Erdem", "Stefano Fasciani", "Kyrre Glette"], "title": "Exploring Definitions of Quality and Diversity in Sonic Measurement Spaces", "comment": null, "summary": "Digital sound synthesis presents the opportunity to explore vast parameter spaces containing millions of configurations. Quality diversity (QD) evolutionary algorithms offer a promising approach to harness this potential, yet their success hinges on appropriate sonic feature representations. Existing QD methods predominantly employ handcrafted descriptors or supervised classifiers, potentially introducing unintended exploration biases and constraining discovery to familiar sonic regions. This work investigates unsupervised dimensionality reduction methods for automatically defining and dynamically reconfiguring sonic behaviour spaces during QD search. We apply Principal Component Analysis (PCA) and autoencoders to project high-dimensional audio features onto structured grids for MAP-Elites, implementing dynamic reconfiguration through model retraining at regular intervals. Comparison across two experimental scenarios shows that automatic approaches achieve significantly greater diversity than handcrafted behaviour spaces while avoiding expert-imposed biases. Dynamic behaviour-space reconfiguration maintains evolutionary pressure and prevents stagnation, with PCA proving most effective among the dimensionality reduction techniques. These results contribute to automated sonic discovery systems capable of exploring vast parameter spaces without manual intervention or supervised training constraints.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u7d22\u4e86\u4f7f\u7528\u65e0\u76d1\u7763\u964d\u7ef4\u65b9\u6cd5\uff08PCA\u548c\u81ea\u7f16\u7801\u5668\uff09\u81ea\u52a8\u5b9a\u4e49\u548c\u52a8\u6001\u91cd\u6784\u58f0\u5b66\u884c\u4e3a\u7a7a\u95f4\uff0c\u4ee5\u63d0\u5347\u8d28\u91cf\u591a\u6837\u6027\u8fdb\u5316\u7b97\u6cd5\u5728\u6570\u5b57\u58f0\u97f3\u5408\u6210\u4e2d\u7684\u63a2\u7d22\u80fd\u529b\u3002", "motivation": "\u6570\u5b57\u58f0\u97f3\u5408\u6210\u5177\u6709\u5de8\u5927\u7684\u53c2\u6570\u7a7a\u95f4\uff0c\u8d28\u91cf\u591a\u6837\u6027\u8fdb\u5316\u7b97\u6cd5\u662f\u63a2\u7d22\u8fd9\u4e00\u6f5c\u529b\u7684\u6709\u524d\u666f\u65b9\u6cd5\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u624b\u5de5\u7279\u5f81\u63cf\u8ff0\u7b26\u6216\u6709\u76d1\u7763\u5206\u7c7b\u5668\uff0c\u53ef\u80fd\u5f15\u5165\u63a2\u7d22\u504f\u89c1\u5e76\u9650\u5236\u53d1\u73b0\u8303\u56f4\u3002", "method": "\u91c7\u7528\u65e0\u76d1\u7763\u964d\u7ef4\u65b9\u6cd5\uff08\u4e3b\u6210\u5206\u5206\u6790\u548c\u81ea\u7f16\u7801\u5668\uff09\u5c06\u9ad8\u7ef4\u97f3\u9891\u7279\u5f81\u6295\u5f71\u5230\u7ed3\u6784\u5316\u7f51\u683c\u4e0a\uff0c\u7528\u4e8eMAP-Elites\u7b97\u6cd5\uff0c\u5e76\u901a\u8fc7\u5b9a\u671f\u91cd\u65b0\u8bad\u7ec3\u6a21\u578b\u5b9e\u73b0\u52a8\u6001\u91cd\u6784\u3002", "result": "\u81ea\u52a8\u65b9\u6cd5\u6bd4\u624b\u5de5\u884c\u4e3a\u7a7a\u95f4\u5b9e\u73b0\u4e86\u663e\u8457\u66f4\u5927\u7684\u591a\u6837\u6027\uff0c\u907f\u514d\u4e86\u4e13\u5bb6\u5f15\u5165\u7684\u504f\u89c1\uff1b\u52a8\u6001\u884c\u4e3a\u7a7a\u95f4\u91cd\u6784\u4fdd\u6301\u4e86\u8fdb\u5316\u538b\u529b\u5e76\u9632\u6b62\u505c\u6ede\uff0c\u5176\u4e2dPCA\u5728\u964d\u7ef4\u6280\u672f\u4e2d\u8868\u73b0\u6700\u6709\u6548\u3002", "conclusion": "\u65e0\u76d1\u7763\u964d\u7ef4\u65b9\u6cd5\u80fd\u591f\u521b\u5efa\u81ea\u52a8\u5316\u7684\u58f0\u97f3\u53d1\u73b0\u7cfb\u7edf\uff0c\u65e0\u9700\u4eba\u5de5\u5e72\u9884\u6216\u6709\u76d1\u7763\u8bad\u7ec3\u7ea6\u675f\u5373\u53ef\u63a2\u7d22\u5de8\u5927\u7684\u53c2\u6570\u7a7a\u95f4\uff0c\u4e3a\u6570\u5b57\u58f0\u97f3\u5408\u6210\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u63a2\u7d22\u6846\u67b6\u3002"}}
{"id": "2512.02573", "categories": ["eess.SP", "cs.IT"], "pdf": "https://arxiv.org/pdf/2512.02573", "abs": "https://arxiv.org/abs/2512.02573", "authors": ["Juan Vidal Alegr\u00eda", "Ashkan Sheikhi", "Ove Edfors"], "title": "Zero-Forcing MU-MIMO Precoding under Power Amplifier Non-Linearities", "comment": "5 pages, 1 figure (2 subfigures). This work has been presented at the Asilomar Conference on Signals, Systems, and Computers. Copyright information may be affected upon publication at the IEEE proceedings", "summary": "In multi-user multiple-input multiple-output (MU-MIMO) systems, the non-linear behavior of the power amplifiers (PAs) may cause degradation of the linear precoding schemes dealing with interference between user equipments (UEs), e.g., the zero-forcing (ZF) precoder. One way to minimize this effect is to use digital-pre-distortion (DPD) modules to linearize the PAs. However, using perfect DPD modules is costly and it may incur significant power consumption. As an alternative, we consider the problem of characterizing non-linearity-aware ZF (NLA-ZF) precoding schemes, hereby defined as linear precoders that achieve perfect interference cancellation in the presence of PA non-linearity by exploiting knowledge of this non-linear response. We provide initial iterative solutions that allow achieving NLA-ZF (up to adjustable tolerance) in a two-UE downlink MU-MIMO scenario where the base station (BS) has an even number of antennas, and each antenna is connected to a PA exhibiting third-order memory-less non-linear behavior. The proposed approach allows for performance gains in scenarios with significant residual interference.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u975e\u7ebf\u6027\u611f\u77e5\u7684\u8feb\u96f6\u9884\u7f16\u7801\u65b9\u6848\uff0c\u7528\u4e8e\u89e3\u51b3\u591a\u7528\u6237MIMO\u7cfb\u7edf\u4e2d\u529f\u7387\u653e\u5927\u5668\u975e\u7ebf\u6027\u5bfc\u81f4\u7684\u5e72\u6270\u6d88\u9664\u6027\u80fd\u4e0b\u964d\u95ee\u9898\u3002", "motivation": "\u5728\u591a\u7528\u6237MIMO\u7cfb\u7edf\u4e2d\uff0c\u529f\u7387\u653e\u5927\u5668\u7684\u975e\u7ebf\u6027\u884c\u4e3a\u4f1a\u964d\u4f4e\u7ebf\u6027\u9884\u7f16\u7801\u65b9\u6848\uff08\u5982\u8feb\u96f6\u9884\u7f16\u7801\uff09\u7684\u6027\u80fd\uff0c\u5bfc\u81f4\u7528\u6237\u95f4\u5e72\u6270\u6d88\u9664\u6548\u679c\u53d8\u5dee\u3002\u867d\u7136\u53ef\u4ee5\u4f7f\u7528\u6570\u5b57\u9884\u5931\u771f\u6a21\u5757\u6765\u7ebf\u6027\u5316\u529f\u7387\u653e\u5927\u5668\uff0c\u4f46\u5b8c\u7f8e\u5b9e\u73b0\u6210\u672c\u9ad8\u4e14\u529f\u8017\u5927\u3002\u56e0\u6b64\u9700\u8981\u5bfb\u627e\u66ff\u4ee3\u65b9\u6848\u6765\u5728\u975e\u7ebf\u6027\u73af\u5883\u4e0b\u5b9e\u73b0\u5b8c\u7f8e\u5e72\u6270\u6d88\u9664\u3002", "method": "\u63d0\u51fa\u975e\u7ebf\u6027\u611f\u77e5\u8feb\u96f6\u9884\u7f16\u7801\u65b9\u6848\uff0c\u901a\u8fc7\u5229\u7528\u529f\u7387\u653e\u5927\u5668\u975e\u7ebf\u6027\u54cd\u5e94\u7684\u77e5\u8bc6\uff0c\u5728\u975e\u7ebf\u6027\u73af\u5883\u4e0b\u5b9e\u73b0\u5b8c\u7f8e\u5e72\u6270\u6d88\u9664\u3002\u9488\u5bf9\u57fa\u7ad9\u5929\u7ebf\u6570\u4e3a\u5076\u6570\u3001\u6bcf\u4e2a\u5929\u7ebf\u8fde\u63a5\u5177\u6709\u4e09\u9636\u65e0\u8bb0\u5fc6\u975e\u7ebf\u6027\u529f\u7387\u653e\u5927\u5668\u7684\u4e24\u7528\u6237\u4e0b\u884cMU-MIMO\u573a\u666f\uff0c\u63d0\u4f9b\u4e86\u521d\u59cb\u8fed\u4ee3\u89e3\u51b3\u65b9\u6848\u3002", "result": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u5728\u5b58\u5728\u663e\u8457\u6b8b\u4f59\u5e72\u6270\u7684\u573a\u666f\u4e2d\u83b7\u5f97\u6027\u80fd\u589e\u76ca\uff0c\u901a\u8fc7\u53ef\u8c03\u8282\u5bb9\u5dee\u5b9e\u73b0\u975e\u7ebf\u6027\u611f\u77e5\u8feb\u96f6\u9884\u7f16\u7801\u3002", "conclusion": "\u975e\u7ebf\u6027\u611f\u77e5\u8feb\u96f6\u9884\u7f16\u7801\u65b9\u6848\u4e3a\u591a\u7528\u6237MIMO\u7cfb\u7edf\u4e2d\u529f\u7387\u653e\u5927\u5668\u975e\u7ebf\u6027\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u907f\u514d\u4e86\u6570\u5b57\u9884\u5931\u771f\u6a21\u5757\u7684\u9ad8\u6210\u672c\u548c\u529f\u8017\u95ee\u9898\uff0c\u5728\u6b8b\u4f59\u5e72\u6270\u663e\u8457\u7684\u60c5\u51b5\u4e0b\u80fd\u591f\u83b7\u5f97\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2512.02628", "categories": ["eess.SP", "cs.IT"], "pdf": "https://arxiv.org/pdf/2512.02628", "abs": "https://arxiv.org/abs/2512.02628", "authors": ["Carolina Nolasco-Ferencikova", "Georg Schwan", "Raphael Rolny", "Alexander Stutz-Tirri", "Christoph Studer"], "title": "Joint Beamforming and Matching for Ultra-Dense Massive Antenna Arrays", "comment": "Presented at the 59th Asilomar Conference on Signals, Systems, and Computers", "summary": "Massive multiple-input multiple-output (MIMO) offers substantial spectral-efficiency gains, but scaling to very large antenna arrays with conventional all-digital and hybrid beamforming architectures quickly results in excessively high costs and power consumption. Low-cost, switch-based architectures have recently emerged as a potential alternative. However, prior studies rely on simplified models that ignore (among others) antenna coupling, radiation patterns, and matching losses, resulting in inaccurate performance predictions. In this paper, we use a physically consistent electromagnetic modeling framework to analyze an ultra-dense patch-antenna array architecture that performs joint beamforming and matching using networks of inexpensive RF switches. Our results demonstrate that simple, switch-based beamforming architectures can approach the antenna-gain of all-digital solutions at significantly lower cost and complexity.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u7535\u78c1\u5efa\u6a21\u5206\u6790\u5f00\u5173\u5f0f\u6ce2\u675f\u8d4b\u5f62\u67b6\u6784\uff0c\u8bc1\u660e\u5176\u80fd\u4ee5\u4f4e\u6210\u672c\u63a5\u8fd1\u5168\u6570\u5b57\u65b9\u6848\u7684\u6027\u80fd", "motivation": "\u4f20\u7edf\u5168\u6570\u5b57\u548c\u6df7\u5408\u6ce2\u675f\u8d4b\u5f62\u67b6\u6784\u5728\u6269\u5c55\u5230\u5927\u89c4\u6a21\u5929\u7ebf\u9635\u5217\u65f6\u6210\u672c\u8fc7\u9ad8\u3001\u529f\u8017\u8fc7\u5927\uff0c\u9700\u8981\u5bfb\u627e\u66f4\u7ecf\u6d4e\u7684\u66ff\u4ee3\u65b9\u6848", "method": "\u91c7\u7528\u7269\u7406\u4e00\u81f4\u7684\u7535\u78c1\u5efa\u6a21\u6846\u67b6\uff0c\u5206\u6790\u4f7f\u7528RF\u5f00\u5173\u7f51\u7edc\u8fdb\u884c\u8054\u5408\u6ce2\u675f\u8d4b\u5f62\u548c\u5339\u914d\u7684\u8d85\u5bc6\u96c6\u8d34\u7247\u5929\u7ebf\u9635\u5217\u67b6\u6784", "result": "\u7b80\u5355\u7684\u5f00\u5173\u5f0f\u6ce2\u675f\u8d4b\u5f62\u67b6\u6784\u80fd\u4ee5\u663e\u8457\u66f4\u4f4e\u7684\u6210\u672c\u548c\u590d\u6742\u5ea6\u63a5\u8fd1\u5168\u6570\u5b57\u89e3\u51b3\u65b9\u6848\u7684\u5929\u7ebf\u589e\u76ca", "conclusion": "\u5f00\u5173\u5f0f\u6ce2\u675f\u8d4b\u5f62\u67b6\u6784\u662f\u89e3\u51b3\u5927\u89c4\u6a21MIMO\u7cfb\u7edf\u6210\u672c\u548c\u529f\u8017\u95ee\u9898\u7684\u6709\u524d\u666f\u7684\u66ff\u4ee3\u65b9\u6848"}}
{"id": "2512.02712", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2512.02712", "abs": "https://arxiv.org/abs/2512.02712", "authors": ["Ibrahim Shahbaz", "Mohammad J. Abdel-Rahman", "Eman Hammad"], "title": "G-PIFNN: A Generalizable Physics-informed Fourier Neural Network Framework for Electrical Circuits", "comment": null, "summary": "Physics-Informed Neural Networks (PINNs) have advanced the data-driven solution of differential equations (DEs) in dynamic physical systems, yet challenges remain in explainability, scalability, and architectural complexity. This paper presents a Generalizable Physics-Informed Fourier Neural Network (G-PIFNN) framework that enhances PINN architectures for efficient and interpretable electrical circuit analysis. The proposed G-PIFNN introduces three key advancements: (1) improved performance and interpretability via a physics activation function (PAF) and a lightweight Physics-Informed Fourier Neural Network (PIFNN) architecture; (2) automated, bond graph (BG) based formulation of physics-informed loss functions for systematic differential equation generation; and (3) integration of intra-circuit and cross-circuit class transfer learning (TL) strategies, enabling unsupervised fine-tuning for rapid adaptation to varying circuit topologies. Numerical simulations demonstrate that G-PIFNN achieves significantly better predictive performance and generalization across diverse circuit classes, while significantly reducing the number of trainable parameters compared to standard PINNs.", "AI": {"tldr": "G-PIFNN\u6846\u67b6\u901a\u8fc7\u7269\u7406\u6fc0\u6d3b\u51fd\u6570\u3001\u81ea\u52a8\u7269\u7406\u635f\u5931\u51fd\u6570\u751f\u6210\u548c\u8fc1\u79fb\u5b66\u4e60\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86PINNs\u5728\u7535\u8def\u5206\u6790\u4e2d\u7684\u6027\u80fd\u3001\u53ef\u89e3\u91ca\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u53ef\u8bad\u7ec3\u53c2\u6570\u6570\u91cf\u3002", "motivation": "\u867d\u7136\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\u5728\u52a8\u6001\u7269\u7406\u7cfb\u7edf\u7684\u5fae\u5206\u65b9\u7a0b\u6c42\u89e3\u65b9\u9762\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u5728\u53ef\u89e3\u91ca\u6027\u3001\u53ef\u6269\u5c55\u6027\u548c\u67b6\u6784\u590d\u6742\u6027\u65b9\u9762\u4ecd\u5b58\u5728\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u7535\u8def\u5206\u6790\u5e94\u7528\u4e2d\u3002", "method": "\u63d0\u51faG-PIFNN\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u5173\u952e\u521b\u65b0\uff1a1) \u7269\u7406\u6fc0\u6d3b\u51fd\u6570\u548c\u8f7b\u91cf\u7ea7PIFNN\u67b6\u6784\uff1b2) \u57fa\u4e8e\u952e\u5408\u56fe\u7684\u81ea\u52a8\u7269\u7406\u635f\u5931\u51fd\u6570\u751f\u6210\uff1b3) \u7535\u8def\u5185\u548c\u8de8\u7535\u8def\u7c7b\u8fc1\u79fb\u5b66\u4e60\u7b56\u7565\u3002", "result": "\u6570\u503c\u6a21\u62df\u8868\u660e\uff0cG-PIFNN\u5728\u591a\u79cd\u7535\u8def\u7c7b\u522b\u4e2d\u5b9e\u73b0\u4e86\u663e\u8457\u66f4\u597d\u7684\u9884\u6d4b\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u540c\u65f6\u76f8\u6bd4\u6807\u51c6PINNs\u5927\u5e45\u51cf\u5c11\u4e86\u53ef\u8bad\u7ec3\u53c2\u6570\u6570\u91cf\u3002", "conclusion": "G-PIFNN\u6846\u67b6\u4e3a\u7535\u8def\u5206\u6790\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u89e3\u91ca\u4e14\u53ef\u6cdb\u5316\u7684\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\u89e3\u51b3\u65b9\u6848\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfPINNs\u5728\u53ef\u89e3\u91ca\u6027\u3001\u53ef\u6269\u5c55\u6027\u548c\u67b6\u6784\u590d\u6742\u6027\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2512.02757", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2512.02757", "abs": "https://arxiv.org/abs/2512.02757", "authors": ["Yunzhe Zhu", "Xuewen Liao", "Zhenzhen Gao", "Yong Zeng"], "title": "Channel Knowledge Map Construction via Physics-Inspired Diffusion Model Without Prior Observations", "comment": null, "summary": "The ability to construct Channel Knowledge Map (CKM) with high precision is essential for environment awareness in 6G wireless systems. However, most existing CKM construction methods formulate the task as an image super-resolution or generation problem, thereby employing models originally developed for computer vision. As a result, the generated CKMs often fail to capture the underlying physical characteristics of wireless propagation. In this paper, we focus on the construction of CKM for large-scale fading scenarios and design three physics-based constraint terms to characterize the spatial distribution patterns of large-scale fading. By integrating these physical constraints with a state-of-the-art diffusion model that possesses superior generative capability, a physics-inspired diffusion model for CKM construction is proposed. Following this motivation, we derive the loss function of the diffusion model augmented with physics-based constraint terms and further design the training and generation framework for the proposed physics-inspired CKM generation diffusion model. Extensive experiments show that our approach outperforms all existing methods in terms of construction accuracy. Moreover, the proposed model provides a unified and effective framework with strong potential for generating diverse, accurate, and physically consistent CKM.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u7ea6\u675f\u7684\u6269\u6563\u6a21\u578b\uff0c\u7528\u4e8e\u6784\u5efa6G\u65e0\u7ebf\u7cfb\u7edf\u4e2d\u66f4\u51c6\u786e\u3001\u7269\u7406\u4e00\u81f4\u7684\u4fe1\u9053\u77e5\u8bc6\u56fe\uff08CKM\uff09\uff0c\u76f8\u6bd4\u73b0\u6709\u8ba1\u7b97\u673a\u89c6\u89c9\u65b9\u6cd5\u80fd\u66f4\u597d\u5730\u6355\u6349\u65e0\u7ebf\u4f20\u64ad\u7684\u7269\u7406\u7279\u6027\u3002", "motivation": "\u73b0\u6709CKM\u6784\u5efa\u65b9\u6cd5\u5927\u591a\u5c06\u4efb\u52a1\u89c6\u4e3a\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u6216\u751f\u6210\u95ee\u9898\uff0c\u91c7\u7528\u8ba1\u7b97\u673a\u89c6\u89c9\u6a21\u578b\uff0c\u5bfc\u81f4\u751f\u6210\u7684CKM\u65e0\u6cd5\u6355\u6349\u65e0\u7ebf\u4f20\u64ad\u7684\u7269\u7406\u7279\u6027\u3002\u672c\u6587\u65e8\u5728\u6784\u5efa\u80fd\u51c6\u786e\u8868\u5f81\u5927\u89c4\u6a21\u8870\u843d\u573a\u666f\u7269\u7406\u7279\u5f81\u7684CKM\u3002", "method": "\u9488\u5bf9\u5927\u89c4\u6a21\u8870\u843d\u573a\u666f\uff0c\u8bbe\u8ba1\u4e86\u4e09\u4e2a\u57fa\u4e8e\u7269\u7406\u7684\u7ea6\u675f\u9879\u6765\u8868\u5f81\u5927\u89c4\u6a21\u8870\u843d\u7684\u7a7a\u95f4\u5206\u5e03\u6a21\u5f0f\u3002\u5c06\u8fd9\u4e9b\u7269\u7406\u7ea6\u675f\u4e0e\u5177\u6709\u5353\u8d8a\u751f\u6210\u80fd\u529b\u7684\u5148\u8fdb\u6269\u6563\u6a21\u578b\u76f8\u7ed3\u5408\uff0c\u63d0\u51fa\u4e86\u7269\u7406\u542f\u53d1\u7684CKM\u6784\u5efa\u6269\u6563\u6a21\u578b\uff0c\u63a8\u5bfc\u4e86\u589e\u5f3a\u7269\u7406\u7ea6\u675f\u7684\u635f\u5931\u51fd\u6570\uff0c\u5e76\u8bbe\u8ba1\u4e86\u8bad\u7ec3\u548c\u751f\u6210\u6846\u67b6\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6784\u5efa\u7cbe\u5ea6\u65b9\u9762\u4f18\u4e8e\u6240\u6709\u73b0\u6709\u65b9\u6cd5\u3002\u6240\u63d0\u51fa\u7684\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u6709\u6548\u7684\u6846\u67b6\uff0c\u5177\u6709\u751f\u6210\u591a\u6837\u5316\u3001\u51c6\u786e\u4e14\u7269\u7406\u4e00\u81f4\u7684CKM\u7684\u5f3a\u5927\u6f5c\u529b\u3002", "conclusion": "\u901a\u8fc7\u5c06\u7269\u7406\u7ea6\u675f\u878d\u5165\u6269\u6563\u6a21\u578b\uff0c\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u751f\u6210\u66f4\u51c6\u786e\u3001\u7269\u7406\u4e00\u81f4\u7684\u4fe1\u9053\u77e5\u8bc6\u56fe\uff0c\u4e3a6G\u65e0\u7ebf\u7cfb\u7edf\u7684\u73af\u5883\u611f\u77e5\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.02765", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2512.02765", "abs": "https://arxiv.org/abs/2512.02765", "authors": ["Alejandro Roig-Herrero", "Luis M. San-Jos\u00e9-Revuelta", "Rafael Navarro-Gonz\u00e1lez", "Rodrigo de Luis-Garc\u00eda", "Vicente Molina"], "title": "Effects of disease duration and antipsychotics on brain age in schizophrenia", "comment": "20 pages; 4 figures; 4 tables", "summary": "Accelerated brain aging has been consistently reported in patients with schizophrenia. Over the past decade, these findings have been replicated using the Brain Age paradigm, which applies machine learning techniques to estimate brain age from neuroimaging data. This approach yields a single index, the Brain Age Gap, defined as the difference between predicted and chronological age. Nevertheless, both the progressive nature of this phenomenon and the potential role of antipsychotic medication remain unclear. To investigate its progression, we compared the Brain Age Gap between individuals experiencing a first episode of psychosis and healthy controls using ANCOVA, adjusting for age, sex, body mass index, and estimated total intracranial volume. To enhance the robustness of our findings, we employed two distinct models: a transformer-inspired model based on harmonized volumetric brain features extracted with FastSurfer, and a previously trained deep learning model. To assess the potential effect of medication, we further compared bipolar patients who received antipsychotic treatment with those who did not. Mann-Whitney U test consistently showed that medicated bipolar patients did not exhibit a significantly larger Brain Age Gap. Both models converge on the conclusion that accelerated brain aging is unlikely to be explained by antipsychotic medication alone. Longitudinal studies are therefore required to clarify the temporal dynamics of brain aging in schizophrenia.", "AI": {"tldr": "\u7cbe\u795e\u5206\u88c2\u75c7\u60a3\u8005\u5b58\u5728\u52a0\u901f\u8111\u8001\u5316\u73b0\u8c61\uff0c\u4f46\u6297\u7cbe\u795e\u75c5\u836f\u7269\u53ef\u80fd\u4e0d\u662f\u4e3b\u8981\u539f\u56e0\uff0c\u9700\u8981\u7eb5\u5411\u7814\u7a76\u8fdb\u4e00\u6b65\u9a8c\u8bc1", "motivation": "\u7814\u7a76\u7cbe\u795e\u5206\u88c2\u75c7\u60a3\u8005\u52a0\u901f\u8111\u8001\u5316\u7684\u8fdb\u5c55\u6027\u7279\u5f81\uff0c\u5e76\u63a2\u7a76\u6297\u7cbe\u795e\u75c5\u836f\u7269\u5728\u8fd9\u4e00\u73b0\u8c61\u4e2d\u7684\u4f5c\u7528", "method": "\u4f7f\u7528\u8111\u5e74\u9f84\u8303\u5f0f\uff0c\u6bd4\u8f83\u9996\u53d1\u7cbe\u795e\u75c5\u60a3\u8005\u4e0e\u5065\u5eb7\u5bf9\u7167\u7684\u8111\u5e74\u9f84\u5dee\u8ddd\uff1b\u91c7\u7528\u4e24\u79cd\u6a21\u578b\uff1a\u57fa\u4e8eFastSurfer\u63d0\u53d6\u7684\u8111\u4f53\u79ef\u7279\u5f81\u7684Transformer\u6a21\u578b\u548c\u9884\u8bad\u7ec3\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff1b\u901a\u8fc7Mann-Whitney U\u68c0\u9a8c\u6bd4\u8f83\u63a5\u53d7\u4e0e\u672a\u63a5\u53d7\u6297\u7cbe\u795e\u75c5\u836f\u7269\u6cbb\u7597\u7684\u53cc\u76f8\u60c5\u611f\u969c\u788d\u60a3\u8005", "result": "\u4e24\u79cd\u6a21\u578b\u5747\u663e\u793a\u63a5\u53d7\u6297\u7cbe\u795e\u75c5\u836f\u7269\u6cbb\u7597\u7684\u53cc\u76f8\u60c5\u611f\u969c\u788d\u60a3\u8005\u5e76\u672a\u8868\u73b0\u51fa\u663e\u8457\u66f4\u5927\u7684\u8111\u5e74\u9f84\u5dee\u8ddd\uff0c\u8868\u660e\u52a0\u901f\u8111\u8001\u5316\u4e0d\u592a\u53ef\u80fd\u4ec5\u7531\u6297\u7cbe\u795e\u75c5\u836f\u7269\u89e3\u91ca", "conclusion": "\u52a0\u901f\u8111\u8001\u5316\u73b0\u8c61\u5728\u7cbe\u795e\u5206\u88c2\u75c7\u4e2d\u786e\u5b9e\u5b58\u5728\uff0c\u4f46\u6297\u7cbe\u795e\u75c5\u836f\u7269\u53ef\u80fd\u4e0d\u662f\u4e3b\u8981\u539f\u56e0\uff0c\u9700\u8981\u7eb5\u5411\u7814\u7a76\u6765\u9610\u660e\u8111\u8001\u5316\u7684\u65f6\u95f4\u52a8\u6001"}}
{"id": "2512.02768", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2512.02768", "abs": "https://arxiv.org/abs/2512.02768", "authors": ["Hefei Gao", "Tianyao Huang", "Letian Guo", "Jie He", "Yonina C. Eldar"], "title": "Diffusion-Prior Split Gibbs Sampling for Synthetic Aperture Radar Imaging under Incomplete Measurements", "comment": null, "summary": "Synthetic aperture radar (SAR) imaging plays a critical role in all-weather, day-and-night remote sensing, yet reconstruction is often challenged by noise, undersampling, and complex scattering scenarios. Conventional methods, including matched filtering and sparsity-based compressed sensing, are limited in capturing intricate scene structures and frequently suffer from artifacts, elevated sidelobes, and loss of fine details. Recent diffusion models have demonstrated superior capability in representing high-order priors; however, existing diffusion-based SAR methods still yield degraded reconstructions due to oversimplified likelihood approximations in guided sampling. In this work, we propose a diffusion-driven split Gibbs sampling framework for SAR reconstruction, rigorously integrating measurement fidelity with learned diffusion priors. By alternately performing likelihood- and prior-driven updates via proximal sampling, this method ensures progressive convergence toward the true posterior while fully leveraging the expressive power of diffusion priors. Extensive experiments on simulated and Sentinel-1A datasets demonstrate substantial performance improvements: over 7 dB average PSNR gain in simulations, along with significant sidelobe suppression (MPLSR +2.96 dB, MISLR +11.5 dB) with respect to the best baseline result. On real-world Sentinel-1A data, the method achieves an average PSNR gain of 1.6 dB while effectively reducing artifacts and preserving scene details, including ridges, edges, and fine textures. These results underscore the potential of the adapted framework as a robust and generalizable solution for high-fidelity SAR imaging across diverse sensing scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u548c\u5206\u88c2\u5409\u5e03\u65af\u91c7\u6837\u7684SAR\u56fe\u50cf\u91cd\u5efa\u6846\u67b6\uff0c\u901a\u8fc7\u4ea4\u66ff\u8fdb\u884c\u4f3c\u7136\u9a71\u52a8\u548c\u5148\u9a8c\u9a71\u52a8\u7684\u66f4\u65b0\uff0c\u663e\u8457\u63d0\u5347\u4e86SAR\u56fe\u50cf\u91cd\u5efa\u8d28\u91cf\u3002", "motivation": "\u4f20\u7edfSAR\u91cd\u5efa\u65b9\u6cd5\uff08\u5982\u5339\u914d\u6ee4\u6ce2\u548c\u538b\u7f29\u611f\u77e5\uff09\u96be\u4ee5\u6355\u6349\u590d\u6742\u573a\u666f\u7ed3\u6784\uff0c\u5b58\u5728\u4f2a\u5f71\u3001\u9ad8\u65c1\u74e3\u548c\u7ec6\u8282\u4e22\u5931\u95ee\u9898\u3002\u73b0\u6709\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u65b9\u6cd5\u7531\u4e8e\u91c7\u6837\u8fc7\u7a0b\u4e2d\u7684\u4f3c\u7136\u8fd1\u4f3c\u8fc7\u4e8e\u7b80\u5316\uff0c\u5bfc\u81f4\u91cd\u5efa\u8d28\u91cf\u4e0b\u964d\u3002", "method": "\u63d0\u51fa\u6269\u6563\u9a71\u52a8\u7684\u5206\u88c2\u5409\u5e03\u65af\u91c7\u6837\u6846\u67b6\uff0c\u901a\u8fc7\u4ea4\u66ff\u6267\u884c\u4f3c\u7136\u9a71\u52a8\u66f4\u65b0\uff08\u57fa\u4e8e\u6d4b\u91cf\u4fdd\u771f\u5ea6\uff09\u548c\u5148\u9a8c\u9a71\u52a8\u66f4\u65b0\uff08\u57fa\u4e8e\u5b66\u4e60\u5230\u7684\u6269\u6563\u5148\u9a8c\uff09\uff0c\u4f7f\u7528\u8fd1\u7aef\u91c7\u6837\u786e\u4fdd\u9010\u6b65\u6536\u655b\u5230\u771f\u5b9e\u540e\u9a8c\u5206\u5e03\u3002", "result": "\u5728\u4eff\u771f\u6570\u636e\u4e0a\u83b7\u5f97\u8d85\u8fc77dB\u7684\u5e73\u5747PSNR\u63d0\u5347\uff0c\u65c1\u74e3\u6291\u5236\u663e\u8457\uff08MPLSR +2.96 dB\uff0cMISLR +11.5 dB\uff09\u3002\u5728\u771f\u5b9eSentinel-1A\u6570\u636e\u4e0a\u83b7\u5f971.6dB\u7684\u5e73\u5747PSNR\u63d0\u5347\uff0c\u6709\u6548\u51cf\u5c11\u4f2a\u5f71\u5e76\u4fdd\u7559\u5c71\u810a\u3001\u8fb9\u7f18\u548c\u7eb9\u7406\u7b49\u7ec6\u8282\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u4e25\u8c28\u6574\u5408\u6d4b\u91cf\u4fdd\u771f\u5ea6\u548c\u5b66\u4e60\u5230\u7684\u6269\u6563\u5148\u9a8c\uff0c\u4e3a\u9ad8\u4fdd\u771fSAR\u6210\u50cf\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9c81\u68d2\u4e14\u53ef\u6cdb\u5316\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u5404\u79cd\u4f20\u611f\u573a\u666f\u4e2d\u5c55\u73b0\u51fa\u5f3a\u5927\u6f5c\u529b\u3002"}}

{"id": "2512.03319", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2512.03319", "abs": "https://arxiv.org/abs/2512.03319", "authors": ["Ataher Sams", "Yu-Cheng Hsiao", "Muhammad Talha", "Besma Smida", "Ashutosh Sabharwal"], "title": "Optimizing ISAC MIMO Systems with Reconfigurable Pixel Antennas", "comment": "6 pages, 6 figures, Accepted at The 2025 Asilomar Conference on Signals, Systems, and Computers", "summary": "The integration of sensing and communication demands architectures that can flexibly exploit spatial and electromagnetic (EM) degrees of freedom (DoF). This paper proposes an Integrated Sensing and Communication (ISAC) MIMO framework that uses Reconfigurable Pixel Antenna (RPixA), which introduces additional EM-domain DoF that are electronically controlled through binary antenna coder switch networks. We introduce a beamforming architecture combining this EM and digital precoding to jointly optimize Sensing and Communication. Based on full-wave simulation of pixel antenna, we formulate a non-convex joint optimization problem to maximize sensing rate under user-specific constraints on communication rate. We utilize an Alternating Optimization framework incorporating genetic algorithm for port states of Pixel antennas, and semi-definite relaxation (SDR) for digital beamforming. Numerical results demonstrate that the proposed EM-aware design achieves considerably higher sensing rate compared to conventional arrays and enables considerable antenna reduction for equivalent ISAC performance. These findings highlight the potential of reconfigurable pixel antennas to realize efficient and scalable EM-aware ISAC systems for future 6G networks."}
{"id": "2512.03563", "categories": ["cs.SD", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.03563", "abs": "https://arxiv.org/abs/2512.03563", "authors": ["Chengyu Tang", "Sanjeev Baskiyar"], "title": "State Space Models for Bioacoustics: A comparative Evaluation with Transformers", "comment": null, "summary": "In this study, we evaluate the efficacy of the Mamba model in the field of bioacoustics. We first pretrain a Mamba-based audio large language model (LLM) on a large corpus of audio data using self-supervised learning. We fine-tune and evaluate BioMamba on the BEANS benchmark, a collection of diverse bioacoustic tasks including classification and detection, and compare its performance and efficiency with multiple baseline models, including AVES, a state-of-the-art Transformer-based model. The results show that BioMamba achieves comparable performance with AVES while consumption significantly less VRAM, demonstrating its potential in this domain."}
{"id": "2512.03458", "categories": ["eess.SP", "cs.LG", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2512.03458", "abs": "https://arxiv.org/abs/2512.03458", "authors": ["Maryam Maghsoudi", "Mohsen Rezaeizadeh", "Shihab Shamma"], "title": "A Convolutional Framework for Mapping Imagined Auditory MEG into Listened Brain Responses", "comment": null, "summary": "Decoding imagined speech engages complex neural processes that are difficult to interpret due to uncertainty in timing and the limited availability of imagined-response datasets. In this study, we present a Magnetoencephalography (MEG) dataset collected from trained musicians as they imagined and listened to musical and poetic stimuli. We show that both imagined and perceived brain responses contain consistent, condition-specific information. Using a sliding-window ridge regression model, we first mapped imagined responses to listened responses at the single-subject level, but found limited generalization across subjects. At the group level, we developed an encoder-decoder convolutional neural network with a subject-specific calibration layer that produced stable and generalizable mappings. The CNN consistently outperformed the null model, yielding significantly higher correlations between predicted and true listened responses for nearly all held-out subjects. Our findings demonstrate that imagined neural activity can be transformed into perception-like responses, providing a foundation for future brain-computer interface applications involving imagined speech and music."}
{"id": "2512.03637", "categories": ["cs.SD", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.03637", "abs": "https://arxiv.org/abs/2512.03637", "authors": ["Kohei Yamamoto", "Kosuke Okusa"], "title": "AaPE: Aliasing-aware Patch Embedding for Self-Supervised Audio Representation Learning", "comment": "11 pages, 4 figures", "summary": "Transformer-based audio SSL (self-supervised learning) models often treat spectrograms as images, applying convolutional patchification with heavy temporal downsampling. This lowers the effective Nyquist frequency and introduces aliasing, while naïve low-pass filtering removes task-relevant high-frequency cues. In this study, we present Aliasing-aware Patch Embedding (AaPE), a drop-in patch stem that mitigates aliasing while preserving high-frequency information. AaPE augments standard patch tokens with features produced by a band-limited complex sinusoidal kernel using a two-sided exponential window that dynamically targets alias-prone bands. Frequency and decay parameters of the kernel are estimated from the input, enabling parallel, adaptive subband analysis whose outputs are fused with the standard patch tokens. AaPE integrates seamlessly into the masked teacher-student self-supervised learning. In addition, we combine a multi-mask strategy with a contrastive objective to enforce consistency across diverse mask patterns, stabilizing training. Pre-training on AudioSet followed by fine-tuning evaluation across diverse downstream benchmarks, which spanned categories, such as environmental sounds and other common audio domains. This approach yields state-of-the-art performance on a subset of tasks and competitive results across the remainder. Complementary linear probing evaluation mirrors this pattern, yielding clear gains on several benchmarks and strong performance elsewhere. The collective analysis of these results indicates that AaPE serves to mitigate the effects of aliasing without discarding of informative high-frequency content."}
{"id": "2512.03502", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2512.03502", "abs": "https://arxiv.org/abs/2512.03502", "authors": ["Songtao Xue", "Jingjing Zhao", "Kaiquan Cai", "Xidong Mu", "Zhenyu Xiao", "Yuanwei Liu"], "title": "Resource Allocation for Pinching-Antenna Systems (PASS)-enabled NOMA Communications", "comment": null, "summary": "Pinching-antenna systems (PASS) have emerged as a promising technology due to their ability to dynamically reconfigure wireless propagation environments. A novel PASS-based multi-user non-orthogonal multiple access (NOMA) framework is proposed by exploiting the waveguide-division (WD) transmission characteristic. Specifically, each NOMA user cluster is served by one dedicated waveguide, and the corresponding pinching beamforming is exploited to enhance the intra-cluster performance while mitigating the inter-cluster interference. Based on this framework, a sum-rate maximization problem is formulated for jointly optimizing power allocation, pinching beamforming, and user scheduling. To solve this problem, a two-step algorithm is developed, which decomposes the original problem into two subproblems. For the joint power allocation and pinching beamforming design, a penalty dual decomposition (PDD) algorithm is proposed to obtain the locally optimal solutions. Specifically, the coupling constraints are alleviated through augmented Lagrangian relaxation, and the resulting augmented Lagrangian (AL) problem is decomposed into four subproblems, which are solved by the block coordinate descent (BCD) method. For the user scheduling, a low-complexity matching algorithm is developed to solve the user-to-waveguide assignment problem. Simulation results demonstrate that 1) the proposed PASS-based NOMA framework under the WD transmission structure achieves significant sum-rate gain over conventional fixed-position antenna systems and orthogonal multiple access (OMA) scheme; and 2) the proposed matching-based user scheduling algorithm achieves near-optimal user-waveguide association with low computational complexity."}
{"id": "2512.03458", "categories": ["eess.SP", "cs.LG", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2512.03458", "abs": "https://arxiv.org/abs/2512.03458", "authors": ["Maryam Maghsoudi", "Mohsen Rezaeizadeh", "Shihab Shamma"], "title": "A Convolutional Framework for Mapping Imagined Auditory MEG into Listened Brain Responses", "comment": null, "summary": "Decoding imagined speech engages complex neural processes that are difficult to interpret due to uncertainty in timing and the limited availability of imagined-response datasets. In this study, we present a Magnetoencephalography (MEG) dataset collected from trained musicians as they imagined and listened to musical and poetic stimuli. We show that both imagined and perceived brain responses contain consistent, condition-specific information. Using a sliding-window ridge regression model, we first mapped imagined responses to listened responses at the single-subject level, but found limited generalization across subjects. At the group level, we developed an encoder-decoder convolutional neural network with a subject-specific calibration layer that produced stable and generalizable mappings. The CNN consistently outperformed the null model, yielding significantly higher correlations between predicted and true listened responses for nearly all held-out subjects. Our findings demonstrate that imagined neural activity can be transformed into perception-like responses, providing a foundation for future brain-computer interface applications involving imagined speech and music."}
{"id": "2512.03506", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2512.03506", "abs": "https://arxiv.org/abs/2512.03506", "authors": ["Yameng Liu", "Yuxiang Zhang", "Jianhua Zhang", "Yuanpeng Pei", "Changsheng Zhao", "Shilin Luo", "Lei Tian", "Yingyang Li", "Wei Hong", "Jianming Wu", "Guangyi Liu", "Yan Li", "Tao Jiang", "Chuangxin Jiang", "Junchen Liu", "Yongqiang Fei", "Woo-Suk Ko", "Jing Xu", "Bin Liang", "Takahiro Tomie"], "title": "A Comprehensive Survey of 3GPP Release 19 ISAC Channel Modeling: From Empirical Features to Unified Methodology and Standardized Simulator", "comment": null, "summary": "Integrated Sensing and Communication (ISAC) has been identified as a key 6G application by ITU and 3GPP. Channel measurement and modeling is a prerequisite for ISAC system design and has attracted widespread attention from both academia and industry. 3GPP Release 19 initiated the ISAC channel study item in December 2023 and finalized its modeling specification in May 2025 after extensive technical discussions. However, a comprehensive survey that provides a systematic overview,from empirical channel features to modeling methodologies and standardized simulators,remains unavailable. In this paper, the key requirements and challenges in ISAC channel research are first analyzed, followed by a structured overview of the standardization workflow throughout the 3GPP Release 19 process. Then, critical aspects of ISAC channels, including physical objects, target channels, and background channels, are examined in depth, together with additional features such as spatial consistency, environment objects, Doppler characteristics, and shared clusters, supported by measurement-based analysis. To establish a unified ISAC channel modeling framework, an Extended Geometry-based Stochastic Model (E-GBSM) is proposed, incorporating all the aforementioned ISAC channel characteristics. Finally, a standardized simulator is developed based on E-GBSM, and a two-phase calibration procedure aligned with 3GPP Release 19 is conducted to validate both the model and the simulator, demonstrating close agreement with industrial reference results. Overall, this paper provides a systematic survey of 3GPP Release 19 ISAC channel standardization and offers insights into best practices for new feature characterization, unified modeling methodology, and standardized simulator implementation, which can effectively supporting ISAC technology evaluation and future 6G standardization."}
{"id": "2512.03301", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2512.03301", "abs": "https://arxiv.org/abs/2512.03301", "authors": ["Mohan Shi", "Natarajan Balaji Shankar", "Kaiyuan Zhang", "Zilai Wang", "Abeer Alwan"], "title": "Comparing Unsupervised and Supervised Semantic Speech Tokens: A Case Study of Child ASR", "comment": "ASRU-AI4CSL", "summary": "Discrete speech tokens have gained attention for their storage efficiency and integration with Large Language Models (LLMs). They are commonly categorized into acoustic and semantic tokens, with the latter being more advantageous for Automatic Speech Recognition (ASR). Traditionally, unsupervised K-means clustering has been used to extract semantic speech tokens from Speech Foundation Models (SFMs). Recently, supervised methods, such as finite scalar quantization (FSQ) trained with ASR loss, have emerged for speech generation. Both approaches leverage pre-trained SFMs, benefiting low-resource tasks such as child ASR.\n  This paper systematically compares supervised and unsupervised semantic speech tokens for child ASR. Results show that supervised methods not only outperform unsupervised ones but even unexpectedly surpass continuous representations, and they perform well even in ultra-low bitrate settings. These findings highlight the advantages of supervised semantic tokens and offer insights for improving discrete speech tokenization."}
{"id": "2512.03703", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2512.03703", "abs": "https://arxiv.org/abs/2512.03703", "authors": ["Jichen Zhang", "Junhui Rao", "Tianqu Kang", "Zhaoyang Ming", "Yijun Chen", "Alikhan Umirbayev", "Chi-Yuk Chiu", "Ross Murch"], "title": "Scalable Pixel-based Reconfigurable Beamforming Networks for Designing Fluid Antenna Systems", "comment": "18 pages, 27 figures, submitted to IEEE TMTT", "summary": "A novel scalable pixel-based reconfigurable beamforming network (PRBFN) that can be used to form a Fluid Antenna System (FAS), referred to as a PRBFN-FAS, is introduced. The concept of FAS has emerged as an attractive new technology for use in sixth-generation (6G) wireless systems, but most implementations of FAS rely on mechanically reconfigurable antennas which are hindered by inertia, and are therefore too slow to be useful. Using the insight that changing an antenna's physical position is equivalent to switching the excitation current vector of a multi-port antenna, a novel beamformer is proposed with a scalable methodology for incorporating into FAS to form a PRBFN-FAS. Key novelties in creating the PRBFN include selecting the required current vectors and concatenating beamforming networks together to form the desired PRBFN. Two PRBFN-FAS design examples are demonstrated with FAS equivalent physical movements of 0.5 and 1.5 wavelengths. Measurements demonstrate that the PRBFN-FAS provides good matching and Bessel correlation across the desired bandwidth, satisfying FAS requirements. System-level experiments confirm the viability of PRBFN-FAS in practical communication scenarios."}
{"id": "2512.03486", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2512.03486", "abs": "https://arxiv.org/abs/2512.03486", "authors": ["Nan Xu", "Zhaolong Huang", "Xiao Zeng"], "title": "A Universal Harmonic Discriminator for High-quality GAN-based Vocoder", "comment": "Accepted by ASRU2025", "summary": "With the emergence of GAN-based vocoders, the discriminator, as a crucial component, has been developed recently. In our work, we focus on improving the time-frequency based discriminator. Particularly, Short-Time Fourier Transform (STFT) representation is usually used as input of time-frequency based discriminator. However, the STFT spectrogram has the same frequency resolution at different frequency bins, which results in an inferior performance, especially for singing voices. Motivated by this, we propose a universal harmonic discriminator for dynamic frequency resolution modeling and harmonic tracking. Specifically, we design a harmonic filter with learnable triangular band-pass filter banks, where each frequency bin has a flexible bandwidth. Additionally, we add a half-harmonic to capture fine-grained harmonic relationships at low-frequency band. Experiments on speech and singing datasets validate the effectiveness of the proposed discriminator on both subjective and objective metrics."}
{"id": "2512.03802", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2512.03802", "abs": "https://arxiv.org/abs/2512.03802", "authors": ["Yuan Liu", "Wen-Xuan Long", "M. R. Bhavani Shankar", "Marco Moretti", "Rui Chen", "Björn Ottersten"], "title": "Doppler Robust Vortex Wavefront Design for Integrated Sensing and Communication", "comment": "This manuscript has been submitted and is under review", "summary": "Integrated sensing and communication (ISAC) is a promising paradigm for future wireless systems due to spectrum reuse, hardware sharing, and joint waveform design. In dynamic scenes, Doppler shifts degrade both sensing and communication, which is particularly critical for beam-sensitive orbital angular momentum (OAM) wavefronts. To address this, we propose a Doppler-robust ISAC framework, which first senses and then communicates. Specifically, in the sensing phase, multiple vortex modes are simultaneously transmitted via code-division mode-multiplexing (CDMM). To solve Doppler-induced inter-mode interference, we propose a velocity-consistency matching (VCM)-expectation maximization (EM) algorithm that jointly decodes the sensing matrix and estimates range, azimuth, elevation, and velocity for multiple moving targets. In the communication phase, the joint transmitter (Tx) beamforming and receiver (Rx) beam steering are configured from the estimated channel state information (CSI). We further quantify the sensing-communication allocation trade-off by evaluating how pilot length affects estimation accuracy, beam alignment, and spectral efficiency (SE). Simulation results show that the proposed VCM-EM and ISAC designs achieve higher sensing accuracy and communication SE than baseline schemes in dynamic scenarios."}
{"id": "2512.03458", "categories": ["eess.SP", "cs.LG", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2512.03458", "abs": "https://arxiv.org/abs/2512.03458", "authors": ["Maryam Maghsoudi", "Mohsen Rezaeizadeh", "Shihab Shamma"], "title": "A Convolutional Framework for Mapping Imagined Auditory MEG into Listened Brain Responses", "comment": null, "summary": "Decoding imagined speech engages complex neural processes that are difficult to interpret due to uncertainty in timing and the limited availability of imagined-response datasets. In this study, we present a Magnetoencephalography (MEG) dataset collected from trained musicians as they imagined and listened to musical and poetic stimuli. We show that both imagined and perceived brain responses contain consistent, condition-specific information. Using a sliding-window ridge regression model, we first mapped imagined responses to listened responses at the single-subject level, but found limited generalization across subjects. At the group level, we developed an encoder-decoder convolutional neural network with a subject-specific calibration layer that produced stable and generalizable mappings. The CNN consistently outperformed the null model, yielding significantly higher correlations between predicted and true listened responses for nearly all held-out subjects. Our findings demonstrate that imagined neural activity can be transformed into perception-like responses, providing a foundation for future brain-computer interface applications involving imagined speech and music."}

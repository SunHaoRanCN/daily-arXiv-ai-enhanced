{"id": "2512.04100", "categories": ["eess.SP", "cs.AI", "cs.CE"], "pdf": "https://arxiv.org/pdf/2512.04100", "abs": "https://arxiv.org/abs/2512.04100", "authors": ["Mukaram Shahid", "Kunal Das", "Hadia Ushaq", "Hongwei Zhang", "Jiming Song", "Daji Qiao", "Sarath Babu", "Yong Guan", "Zhengyuan Zhu", "Arsalan Ahmad"], "title": "ReVeal-MT: A Physics-Informed Neural Network for Multi-Transmitter Radio Environment Mapping", "comment": null, "summary": "Accurately mapping the radio environment (e.g., identifying wireless signal strength at specific frequency bands and geographic locations) is crucial for efficient spectrum sharing, enabling Secondary Users~(SUs) to access underutilized spectrum bands while protecting Primary Users~(PUs). While existing models have made progress, they often degrade in performance when multiple transmitters coexist, due to the compounded effects of shadowing, interference from adjacent transmitters. To address this challenge, we extend our prior work on Physics-Informed Neural Networks~(PINNs) for single-transmitter mapping to derive a new multi-transmitter Partial Differential Equation~(PDE) formulation of the Received Signal Strength Indicator~(RSSI). We then propose \\emph{ReVeal-MT} (Re-constructor and Visualizer of Spectrum Landscape for Multiple Transmitters), a novel PINN which integrates the multi-source PDE residual into a neural network loss function, enabling accurate spectrum landscape reconstruction from sparse RF sensor measurements. ReVeal-MT is validated using real-world measurements from the ARA wireless living lab across rural and suburban environments, and benchmarked against 3GPP and ITU-R channel models and a baseline PINN model for a single transmitter use-case. Results show that ReVeal-MT achieves substantial accuracy gains in multi-transmitter scenarios, e.g., achieving an RMSE of only 2.66\\,dB with as few as 45 samples over a 370-square-kilometer region, while maintaining low computational complexity. These findings demonstrate that ReVeal-MT significantly advances radio environment mapping under realistic multi-transmitter conditions, with strong potential for enabling fine-grained spectrum management and precise coexistence between PUs and SUs."}
{"id": "2512.04182", "categories": ["eess.SP", "cs.AR"], "pdf": "https://arxiv.org/pdf/2512.04182", "abs": "https://arxiv.org/abs/2512.04182", "authors": ["Ali Rasteh", "Andrew Hennessee", "Ishaan Shivhare", "Siddharth Garg", "Sundeep Rangan", "Brandon Reagen"], "title": "A Spatial Array for Spectrally Agile Wireless Processing", "comment": "Accepted and presented in 2025 Asilomar Conference on Signals, Systems, and Computers", "summary": "Massive MIMO is a cornerstone of next-generation wireless communication, offering significant gains in capacity, reliability, and energy efficiency. However, to meet emerging demands such as high-frequency operation, wide bandwidths, co-existence, integrated sensing, and resilience to dynamic interference, future systems must exhibit both scalability and spectral agility. These requirements place increasing pressure on the underlying processing hardware to be both efficient and reconfigurable. This paper proposes a custom-designed spatial array architecture that serves as a reconfigurable, general-purpose core optimized for a class of wireless kernels that commonly arise in diverse communications and sensing tasks. The proposed spatial array is evaluated against specialized cores for each kernel using High-Level Synthesis (HLS). Both the reconfigurable and specialized designs are synthesized in a 32 nm process to assess latency, throughput, area, and power in realistic processes. The results identify conditions under which general-purpose systolic architectures can approach the efficiency of specialized cores, thereby paving the way toward more scalable and agile systems."}
{"id": "2512.04293", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2512.04293", "abs": "https://arxiv.org/abs/2512.04293", "authors": ["Jia Guo", "Yuanwei Liu", "Arumugam Nallanathan"], "title": "Learning Beamforming for Pinching Antenna System-Enabled ISAC in Low-Altitude Wireless Networks", "comment": "13 pages, 6 figures", "summary": "This work investigates the joint learning of pinching antenna (PA) positions and transmit beamforming for PA-aided integrated sensing and communication (ISAC) in the low-altitude wireless networks. By freely deploying antenna positions along waveguides, the pinching antenna system effectively mitigates the impact of path loss and thus enhances the capacities of sensing and communicating unmanned aerial vehicles (UAVs) that fly over a large range. We first model the problem of maximizing the sensing performance of multiple targets while satisfying the communication performance requirements of multiple users, where both the targets and users are UAVs. For mitigating in-waveguide attenuation and improving sensing performance, the segmented waveguide-enabled pinching antenna (SWAN) system is adopted. Furthermore, an alternative optimization (AO) algorithm for SWAN-based ISAC (SWISAC-AO) is developed, where the optimal structure of the transmit beamforming solution is derived. A graph neural network (GNN), termed SWISAC-GNN, is then proposed to jointly learn PA positions and transmit beamforming, with its alternative update procedure inspired by the SWISAC-AO algorithm. Numerical results show that the GNN achieves sensing performance comparable to or better than the AO algorithm while better satisfying communication requirements. Moreover, the SWISAC-GNN is with much lower implementation complexity, enabling real-time deployment."}
{"id": "2512.04317", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2512.04317", "abs": "https://arxiv.org/abs/2512.04317", "authors": ["Nikhil Deveshwar", "Abhejit Rajagopal", "Peder E. Z. Larson"], "title": "A Mixed Precision FFT with applications in MRI", "comment": "Submitted to ICASSP 2026", "summary": "A mixed precision Fast Fourier transform (FFT) implementation is presented. The procedure uses per-block microscaling (MX), a global power-of-two prescale, and prequantized low bit twiddles. We evaluate forward and round-trip FFT fidelity on two public MRI datasets and compare the effect of various low precision formats, image sizes, and MX block sizes on image quality. Results show that mantissa precision is the primary limiter under MX scaling while ablations suggest weak dependence on image size but a clear block-size trade-off with larger block sizes resulting in better numerical performance."}
{"id": "2512.04792", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2512.04792", "abs": "https://arxiv.org/abs/2512.04792", "authors": ["Thomas Biberger", "Stephan D. Ewert"], "title": "Towards predicting binaural audio quality in listeners with normal and impaired hearing", "comment": "accepted for publication in Forum Acusticum", "summary": "Eurich et al. (2024) recently introduced the computationally efficient monaural and binaural audio quality model (eMoBi-Q). This model integrates both monaural and binaural auditory features and has been validated across six audio datasets encompassing quality ratings for music and speech, processed via algorithms commonly employed in modern hearing devices (e.g., acoustic transparency, feedback cancellation, and binaural beamforming) or presented via loudspeakers. In the current study, we expand eMoBi-Q to account for perceptual effects of sensorineural hearing loss (HL) on audio quality. For this, the model was extended by a nonlinear auditory filterbank. Given that altered loudness perception is a prevalent issue among listeners with hearing impairment, our goal is to incorporate loudness as a sub-dimension for predicting audio quality in both normal-hearing and hearing-impaired populations. While predicting loudness itself is important in the context of loudness-based hearing aid fitting, loudness as audio quality sub-measure may be helpful for the selection of reliable auditory features in hearing impaired listeners. The parameters of the filterbank and subsequent processing stages were informed by the physiologically-based (binaural) loudness model proposed by Pieper et al. (2018). This study presents and discusses the initial implementation of the extended binaural quality model."}
{"id": "2512.04551", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2512.04551", "abs": "https://arxiv.org/abs/2512.04551", "authors": ["Cong Wang", "Yizhong Geng", "Yuhua Wen", "Qifei Li", "Yingming Gao", "Ruimin Wang", "Chunfeng Wang", "Hao Li", "Ya Li", "Wei Chen"], "title": "Multi-Loss Learning for Speech Emotion Recognition with Energy-Adaptive Mixup and Frame-Level Attention", "comment": "Submitted to ICASSP 2026. Copyright 2026 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works", "summary": "Speech emotion recognition (SER) is an important technology in human-computer interaction. However, achieving high performance is challenging due to emotional complexity and scarce annotated data. To tackle these challenges, we propose a multi-loss learning (MLL) framework integrating an energy-adaptive mixup (EAM) method and a frame-level attention module (FLAM). The EAM method leverages SNR-based augmentation to generate diverse speech samples capturing subtle emotional variations. FLAM enhances frame-level feature extraction for multi-frame emotional cues. Our MLL strategy combines Kullback-Leibler divergence, focal, center, and supervised contrastive loss to optimize learning, address class imbalance, and improve feature separability. We evaluate our method on four widely used SER datasets: IEMOCAP, MSP-IMPROV, RAVDESS, and SAVEE. The results demonstrate our method achieves state-of-the-art performance, suggesting its effectiveness and robustness."}
{"id": "2512.04365", "categories": ["eess.SP", "cs.AR", "cs.ET"], "pdf": "https://arxiv.org/pdf/2512.04365", "abs": "https://arxiv.org/abs/2512.04365", "authors": ["Pushen Zuo", "Zhong Sun"], "title": "RRAM-Based Analog Matrix Computing for Massive MIMO Signal Processing: A Review", "comment": null, "summary": "Resistive random-access memory (RRAM) provides an excellent platform for analog matrix computing (AMC), enabling both matrix-vector multiplication (MVM) and the solution of matrix equations through open-loop and closed-loop circuit architectures. While RRAM-based AMC has been widely explored for accelerating neural networks, its application to signal processing in massive multiple-input multiple-output (MIMO) wireless communication is rapidly emerging as a promising direction. In this Review, we summarize recent advances in applying AMC to massive MIMO, including DFT/IDFT computation for OFDM modulation and demodulation using MVM circuits; MIMO detection and precoding using MVM-based iterative algorithms; and rapid one-step solutions enabled by matrix inversion (INV) and generalized inverse (GINV) circuits. We also highlight additional opportunities, such as AMC-based compressed-sensing recovery for channel estimation and eigenvalue circuits for leakage-based precoding. Finally, we outline key challenges, including RRAM device reliability, analog circuit precision, array scalability, and data conversion bottlenecks, and discuss the opportunities for overcoming these barriers. With continued progress in device-circuit-algorithm co-design, RRAM-based AMC holds strong promise for delivering high-efficiency, high-reliability solutions to (ultra)massive MIMO signal processing in the 6G era."}
{"id": "2512.04945", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2512.04945", "abs": "https://arxiv.org/abs/2512.04945", "authors": ["Ziling Huang"], "title": "TripleC Learning and Lightweight Speech Enhancement for Multi-Condition Target Speech Extraction", "comment": "Submitted to ICASSP2026", "summary": "In our recent work, we proposed Lightweight Speech Enhancement Guided Target Speech Extraction (LGTSE) and demonstrated its effectiveness in multi-speaker-plus-noise scenarios. However, real-world applications often involve more diverse and complex conditions, such as one-speaker-plus-noise or two-speaker-without-noise. To address this challenge, we extend LGTSE with a Cross-Condition Consistency learning strategy, termed TripleC Learning. This strategy is first validated under multi-speaker-plus-noise condition and then evaluated for its generalization across diverse scenarios. Moreover, building upon the lightweight front-end denoiser in LGTSE, which can flexibly process both noisy and clean mixtures and shows strong generalization to unseen conditions, we integrate TripleC learning with a proposed parallel universal training scheme that organizes batches containing multiple scenarios for the same target speaker. By enforcing consistent extraction across different conditions, easier cases can assist harder ones, thereby fully exploiting diverse training data and fostering a robust universal model. Experimental results on the Libri2Mix three-condition tasks demonstrate that the proposed LGTSE with TripleC learning achieves superior performance over condition-specific models, highlighting its strong potential for universal deployment in real-world speech applications."}
{"id": "2512.04552", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2512.04552", "abs": "https://arxiv.org/abs/2512.04552", "authors": ["Cong Wang", "Changfeng Gao", "Yang Xiang", "Zhihao Du", "Keyu An", "Han Zhao", "Qian Chen", "Xiangang Li", "Yingming Gao", "Ya Li"], "title": "RRPO: Robust Reward Policy Optimization for LLM-based Emotional TTS", "comment": "Submitted to ICASSP 2026. Copyright 2026 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works", "summary": "Differentiable reinforcement learning (RL) frameworks like DiffRO offer a powerful approach for controllable text-to-speech (TTS), but are vulnerable to reward hacking, particularly for nuanced tasks like emotion control. The policy model can exploit a vanilla Reward Model (RM) by generating acoustic artifacts to achieve spurious rewards, but at the cost of degrading perceptual quality. To address this, we propose Robust Reward Policy Optimization (RRPO), a novel framework that employs a hybrid regularization scheme. This scheme develops a robust RM whose reward signal is more reliably aligned with human perception, compelling the policy to abandon detrimental shortcuts and instead learn the complex features of genuine emotions. Our ablation study confirms the enhanced robustness of our RM, as evidenced by its strong cross-lingual generalization. The subjective evaluation demonstrates that this robust RM effectively mitigates reward hacking, leading to significant improvements in both emotional expressiveness and naturalness over all baselines. Demo page: https://lrwinr.github.io/RRPO-CosyVoice."}
{"id": "2512.04405", "categories": ["eess.SP", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.04405", "abs": "https://arxiv.org/abs/2512.04405", "authors": ["Chenyuan Feng", "Anbang Zhang", "Geyong Min", "Yongming Huang", "Tony Q. S. Quek", "Xiaohu You"], "title": "Towards 6G Native-AI Edge Networks: A Semantic-Aware and Agentic Intelligence Paradigm", "comment": "submitted to Digital Communications and Networks", "summary": "The evolution toward sixth-generation wireless systems positions intelligence as a native network capability, fundamentally transforming the design of radio access networks (RANs). Within this vision, Semantic-native communication and agentic intelligence are expected to play central roles. SemCom departs from bit-level fidelity and instead emphasizes task-oriented meaning exchange, enabling compact SC and introducing new performance measures such as semantic fidelity and task success rate. Agentic intelligence endows distributed RAN entities with goal-driven autonomy, reasoning, planning, and multi-agent collaboration, increasingly supported by foundation models and knowledge graphs. In this work, we first introduce the conceptual foundations of SemCom and agentic networking, and discuss why existing AI-driven O-RAN solutions remain largely bit-centric and task-siloed. We then present a unified taxonomy that organizes recent research along three axes: i) semantic abstraction level (symbol/feature/intent/knowledge), ii) agent autonomy and coordination granularity (single-, multi-, and hierarchical-agent), and iii) RAN control placement across PHY/MAC, near-real-time RIC, and non-real-time RIC. Based on this taxonomy, we systematically introduce enabling technologies including task-oriented semantic encoders/decoders, multi-agent reinforcement learning, foundation-model-assisted RAN agents, and knowledge-graph-based reasoning for cross-layer awareness. Representative 6G use cases, such as immersive XR, vehicular V2X, and industrial digital twins, are analyzed to illustrate the semantic-agentic convergence in practice. Finally, we identify open challenges in semantic representation standardization, scalable trustworthy agent coordination, O-RAN interoperability, and energy-efficient AI deployment, and outline research directions toward operational semantic-agentic AI-RAN."}
{"id": "2512.04964", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2512.04964", "abs": "https://arxiv.org/abs/2512.04964", "authors": ["Bi-Cheng Yan", "Hsin-Wei Wang", "Fu-An Chao", "Tien-Hong Lo", "Yung-Chang Hsu", "Berlin Chen"], "title": "HiPPO: Exploring A Novel Hierarchical Pronunciation Assessment Approach for Spoken Languages", "comment": "Accepted and to appear in AACL-IJCNLP2025", "summary": "Automatic pronunciation assessment (APA) seeks to quantify a second language (L2) learner's pronunciation proficiency in a target language by offering timely and fine-grained diagnostic feedback. Most existing efforts on APA have predominantly concentrated on highly constrained reading-aloud tasks (where learners are prompted to read a reference text aloud); however, assessing pronunciation quality in unscripted speech (or free-speaking scenarios) remains relatively underexplored. In light of this, we first propose HiPPO, a hierarchical pronunciation assessment model tailored for spoken languages, which evaluates an L2 learner's oral proficiency at multiple linguistic levels based solely on the speech uttered by the learner. To improve the overall accuracy of assessment, a contrastive ordinal regularizer and a curriculum learning strategy are introduced for model training. The former aims to generate score-discriminative features by exploiting the ordinal nature of regression targets, while the latter gradually ramps up the training complexity to facilitate the assessment task that takes unscripted speech as input. Experiments conducted on the Speechocean762 benchmark dataset validates the feasibility and superiority of our method in relation to several cutting-edge baselines."}
{"id": "2512.04616", "categories": ["cs.SD", "physics.med-ph"], "pdf": "https://arxiv.org/pdf/2512.04616", "abs": "https://arxiv.org/abs/2512.04616", "authors": ["Chen Xu", "Lena Schell-Majoor", "Birger Kollmeier"], "title": "Standard audiogram classification from loudness scaling data using unsupervised, supervised, and explainable machine learning techniques", "comment": null, "summary": "To address the calibration and procedural challenges inherent in remote audiogram assessment for rehabilitative audiology, this study investigated whether calibration-independent adaptive categorical loudness scaling (ACALOS) data can be used to approximate individual audiograms by classifying listeners into standard Bisgaard audiogram types using machine learning. Three classes of machine learning approaches - unsupervised, supervised, and explainable - were evaluated. Principal component analysis (PCA) was performed to extract the first two principal components, which together explained more than 50 percent of the variance. Seven supervised multi-class classifiers were trained and compared, alongside unsupervised and explainable methods. Model development and evaluation used a large auditory reference database containing ACALOS data (N = 847). The PCA factor map showed substantial overlap between listeners, indicating that cleanly separating participants into six Bisgaard classes based solely on their loudness patterns is challenging. Nevertheless, the models demonstrated reasonable classification performance, with logistic regression achieving the highest accuracy among supervised approaches. These findings demonstrate that machine learning models can predict standard Bisgaard audiogram types, within certain limits, from calibration-independent loudness perception data, supporting potential applications in remote or resource-limited settings without requiring a traditional audiogram."}
{"id": "2512.04418", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2512.04418", "abs": "https://arxiv.org/abs/2512.04418", "authors": ["Marwan Jalaleddine", "Mohamad Ali Jarkas", "Jiajie Li", "Warren J. Gross"], "title": "Enabling Fast Polar SC Decoding with IR-HARQ", "comment": null, "summary": "To extend the applications of polar codes within next-generation wireless communication systems, it is essential to incorporate support for Incremental Redundancy (IR) Hybrid Automatic Repeat Request (HARQ) schemes. For very high-throughput applications, Successive Cancellation (SC) decoding is particularly appealing for polar codes owing to its high area efficiency. In this paper, we propose modifications to SC decoders that employ special nodes to accelerate decoding. Our modifications enable the use of polar IR-HARQ with SC decoding for high throughput applications. Compared to the unmodified SC IR-HARQ scheme, our proposed approach allows us to achieve a 72% reduction in node traversals with a polar code of length 2048. Simulation results confirm that the proposed special node modifications do not cause any degradation in FER performance."}
{"id": "2512.04551", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2512.04551", "abs": "https://arxiv.org/abs/2512.04551", "authors": ["Cong Wang", "Yizhong Geng", "Yuhua Wen", "Qifei Li", "Yingming Gao", "Ruimin Wang", "Chunfeng Wang", "Hao Li", "Ya Li", "Wei Chen"], "title": "Multi-Loss Learning for Speech Emotion Recognition with Energy-Adaptive Mixup and Frame-Level Attention", "comment": "Submitted to ICASSP 2026. Copyright 2026 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works", "summary": "Speech emotion recognition (SER) is an important technology in human-computer interaction. However, achieving high performance is challenging due to emotional complexity and scarce annotated data. To tackle these challenges, we propose a multi-loss learning (MLL) framework integrating an energy-adaptive mixup (EAM) method and a frame-level attention module (FLAM). The EAM method leverages SNR-based augmentation to generate diverse speech samples capturing subtle emotional variations. FLAM enhances frame-level feature extraction for multi-frame emotional cues. Our MLL strategy combines Kullback-Leibler divergence, focal, center, and supervised contrastive loss to optimize learning, address class imbalance, and improve feature separability. We evaluate our method on four widely used SER datasets: IEMOCAP, MSP-IMPROV, RAVDESS, and SAVEE. The results demonstrate our method achieves state-of-the-art performance, suggesting its effectiveness and robustness."}
{"id": "2512.04711", "categories": ["cs.SD", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.04711", "abs": "https://arxiv.org/abs/2512.04711", "authors": ["Yun Tian", "Zhijin Qin", "Guocheng Lv", "Ye Jin", "Kaibin Huang", "Zhu Han"], "title": "Large Speech Model Enabled Semantic Communication", "comment": "15 pages, 9 figures", "summary": "Existing speech semantic communication systems mainly based on Joint Source-Channel Coding (JSCC) architectures have demonstrated impressive performance, but their effectiveness remains limited by model structures specifically designed for particular tasks and datasets. Recent advances indicate that generative large models pre-trained on massive datasets, can achieve outstanding performance arexhibit exceptional performance across diverse downstream tasks with minimal fine-tuning. To exploit the rich semantic knowledge embedded in large models and enable adaptive transmission over lossy channels, we propose a Large Speech Model enabled Semantic Communication (LargeSC) system. Simultaneously achieving adaptive compression and robust transmission over lossy channels remains challenging, requiring trade-offs among compression efficiency, speech quality, and latency. In this work, we employ the Mimi as a speech codec, converting speech into discrete tokens compatible with existing network architectures. We propose an adaptive controller module that enables adaptive transmission and in-band Unequal Error Protection (UEP), dynamically adjusting to both speech content and packet loss probability under bandwidth constraints. Additionally, we employ Low-Rank Adaptation (LoRA) to finetune the Moshi foundation model for generative recovery of lost speech tokens. Simulation results show that the proposed system supports bandwidths ranging from 550 bps to 2.06 kbps, outperforms conventional baselines in speech quality under high packet loss rates and achieves an end-to-end latency of approximately 460 ms, thereby demonstrating its potential for real-time deployment."}
{"id": "2512.04595", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2512.04595", "abs": "https://arxiv.org/abs/2512.04595", "authors": ["Mattia Fabiani", "Giulia Torcolacci", "Davide Dardari"], "title": "Nonlinear EM-based Signal Processing", "comment": null, "summary": "The use of high-frequency bands, combined with antenna arrays containing an extremely large number of elements (XL-MIMO), is pushing current technology to its limits in terms of hardware complexity, latency, and power consumption. A promising approach to achieving scalable and sustainable solutions is to shift part of the signal processing directly into the electromagnetic (EM) domain. In this paper, we investigate novel architectures that harness the interaction of reconfigurable passive linear and nonlinear (NL) scattering elements positioned in the reactive near field of signal sources. The objective is to enable multifunctional linear and NL EM signal processing to occur directly \"over-the-air.\" Numerical results highlight the potential to significantly reduce both system complexity and the number of RF chains, while still achieving key performance metrics in applications such as direction-of-arrival and position estimation, without the need for additional analog or digital processing."}
{"id": "2512.04552", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2512.04552", "abs": "https://arxiv.org/abs/2512.04552", "authors": ["Cong Wang", "Changfeng Gao", "Yang Xiang", "Zhihao Du", "Keyu An", "Han Zhao", "Qian Chen", "Xiangang Li", "Yingming Gao", "Ya Li"], "title": "RRPO: Robust Reward Policy Optimization for LLM-based Emotional TTS", "comment": "Submitted to ICASSP 2026. Copyright 2026 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works", "summary": "Differentiable reinforcement learning (RL) frameworks like DiffRO offer a powerful approach for controllable text-to-speech (TTS), but are vulnerable to reward hacking, particularly for nuanced tasks like emotion control. The policy model can exploit a vanilla Reward Model (RM) by generating acoustic artifacts to achieve spurious rewards, but at the cost of degrading perceptual quality. To address this, we propose Robust Reward Policy Optimization (RRPO), a novel framework that employs a hybrid regularization scheme. This scheme develops a robust RM whose reward signal is more reliably aligned with human perception, compelling the policy to abandon detrimental shortcuts and instead learn the complex features of genuine emotions. Our ablation study confirms the enhanced robustness of our RM, as evidenced by its strong cross-lingual generalization. The subjective evaluation demonstrates that this robust RM effectively mitigates reward hacking, leading to significant improvements in both emotional expressiveness and naturalness over all baselines. Demo page: https://lrwinr.github.io/RRPO-CosyVoice."}
{"id": "2512.04720", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2512.04720", "abs": "https://arxiv.org/abs/2512.04720", "authors": ["Xiaopeng Wang", "Chunyu Qiang", "Ruibo Fu", "Zhengqi Wen", "Xuefei Liu", "Yukun Liu", "Yuzhe Liang", "Kang Yin", "Yuankun Xie", "Heng Xie", "Chenxing Li", "Chen Zhang", "Changsheng Li"], "title": "M3-TTS: Multi-modal DiT Alignment & Mel-latent for Zero-shot High-fidelity Speech Synthesis", "comment": "Submitted to ICASSP 2026", "summary": "Non-autoregressive (NAR) text-to-speech synthesis relies on length alignment between text sequences and audio representations, constraining naturalness and expressiveness. Existing methods depend on duration modeling or pseudo-alignment strategies that severely limit naturalness and computational efficiency. We propose M3-TTS, a concise and efficient NAR TTS paradigm based on multi-modal diffusion transformer (MM-DiT) architecture. M3-TTS employs joint diffusion transformer layers for cross-modal alignment, achieving stable monotonic alignment between variable-length text-speech sequences without pseudo-alignment requirements. Single diffusion transformer layers further enhance acoustic detail modeling. The framework integrates a mel-vae codec that provides 3* training acceleration. Experimental results on Seed-TTS and AISHELL-3 benchmarks demonstrate that M3-TTS achieves state-of-the-art NAR performance with the lowest word error rates (1.36\\% English, 1.31\\% Chinese) while maintaining competitive naturalness scores. Code and demos will be available at https://wwwwxp.github.io/M3-TTS."}
{"id": "2512.04719", "categories": ["eess.SP", "cs.IT"], "pdf": "https://arxiv.org/pdf/2512.04719", "abs": "https://arxiv.org/abs/2512.04719", "authors": ["Yanqing Xu", "Yang Lu", "Zhiguo Ding", "Tsung-Hui Chang"], "title": "Pinching-Antenna System Design under Random LoS and NLoS Channels", "comment": "13 pages, 8 figures, 2 tables", "summary": "Pinching antennas, realized through position-adjustable radiating elements along dielectric waveguides, have emerged as a promising flexible-antenna technology thanks to their ability to dynamically reshape large-scale channel conditions. However, most existing studies focus on idealized LoS-dominated environments, overlooking the stochastic nature of realistic wireless propagation. This paper investigates a more practical multiuser pinching-antenna system under a composite probabilistic channel model that captures distance-dependent LoS blockage and NLoS scattering. To account for both efficiency and reliability aspects of communication, two complementary design metrics are considered: an average signal-to-noise ratio (SNR) metric characterizing long-term throughput and fairness, and an outage-constrained metric ensuring a prescribed reliability level. Based on these metrics, we formulate two optimization problems: the first maximizes the max-min average SNR across users, while the second maximizes a guaranteed SNR threshold under per-user outage constraints. Although both problems are inherently nonconvex, we exploit their underlying monotonic structures and develop low-complexity, bisection-based algorithms that achieve globally optimal solutions using only simple scalar evaluations. Extensive simulations validate the effectiveness of the proposed methods and demonstrate that pinching-antenna systems significantly outperform conventional fixed-antenna designs even under random LoS and NLoS channels."}
{"id": "2512.04779", "categories": ["cs.SD", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.04779", "abs": "https://arxiv.org/abs/2512.04779", "authors": ["Junjie Zheng", "Chunbo Hao", "Guobin Ma", "Xiaoyu Zhang", "Gongyu Chen", "Chaofan Ding", "Zihao Chen", "Lei Xie"], "title": "YingMusic-Singer: Zero-shot Singing Voice Synthesis and Editing with Annotation-free Melody Guidance", "comment": "13 pages, 3 figures", "summary": "Singing Voice Synthesis (SVS) remains constrained in practical deployment due to its strong dependence on accurate phoneme-level alignment and manually annotated melody contours, requirements that are resource-intensive and hinder scalability. To overcome these limitations, we propose a melody-driven SVS framework capable of synthesizing arbitrary lyrics following any reference melody, without relying on phoneme-level alignment. Our method builds on a Diffusion Transformer (DiT) architecture, enhanced with a dedicated melody extraction module that derives melody representations directly from reference audio. To ensure robust melody encoding, we employ a teacher model to guide the optimization of the melody extractor, alongside an implicit alignment mechanism that enforces similarity distribution constraints for improved melodic stability and coherence. Additionally, we refine duration modeling using weakly annotated song data and introduce a Flow-GRPO reinforcement learning strategy with a multi-objective reward function to jointly enhance pronunciation clarity and melodic fidelity. Experiments show that our model achieves superior performance over existing approaches in both objective measures and subjective listening tests, especially in zero-shot and lyric adaptation settings, while maintaining high audio quality without manual annotation. This work offers a practical and scalable solution for advancing data-efficient singing voice synthesis. To support reproducibility, we release our inference code and model checkpoints."}
{"id": "2512.04723", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2512.04723", "abs": "https://arxiv.org/abs/2512.04723", "authors": ["Gang Liu", "Yanling Hao", "Yixuan Zou"], "title": "CIG-MAE: Cross-Modal Information-Guided Masked Autoencoder for Self-Supervised WiFi Sensing", "comment": "11 pages, 7 figures", "summary": "Human Action Recognition using WiFi Channel State Information (CSI) has emerged as an attractive alternative to vision-based methods due to its ubiquity, device-agnostic nature, and inherent privacy-preserving capabilities. However, the high cost of manual annotation and the limited scale of publicly available CSI datasets restrict the performance of supervised approaches. Self-supervised learning (SSL) offers a promising avenue, but existing contrastive paradigms rely on data augmentations that conflict with the physical semantics of radio signals and require large-batch training, making them poorly suited for CSI. To overcome these challenges, we introduce CIG-MAE -- a Cross-modal Information-Guided Masked Autoencoder -- that reconstructs both the amplitude and phase of CSI using a symmetric dual-stream architecture with a high masking ratio. Specifically, we propose an Adaptive Information-Guided Masking strategy that dynamically allocates attention to time-frequency regions with high information density to improve learning efficiency, and incorporate a Barlow Twins regularizer to align cross-modal representations without negative samples. Experiments on three public datasets show that CIG-MAE consistently outperforms SOTA SSL methods and even surpasses a fully supervised baseline, demonstrating superior data efficiency, robustness, and representation generalization."}
{"id": "2512.04793", "categories": ["cs.SD", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.04793", "abs": "https://arxiv.org/abs/2512.04793", "authors": ["Gongyu Chen", "Xiaoyu Zhang", "Zhenqiang Weng", "Junjie Zheng", "Da Shen", "Chaofan Ding", "Wei-Qiang Zhang", "Zihao Chen"], "title": "YingMusic-SVC: Real-World Robust Zero-Shot Singing Voice Conversion with Flow-GRPO and Singing-Specific Inductive Biases", "comment": "17 pages, 5 figures", "summary": "Singing voice conversion (SVC) aims to render the target singer's timbre while preserving melody and lyrics. However, existing zero-shot SVC systems remain fragile in real songs due to harmony interference, F0 errors, and the lack of inductive biases for singing. We propose YingMusic-SVC, a robust zero-shot framework that unifies continuous pre-training, robust supervised fine-tuning, and Flow-GRPO reinforcement learning. Our model introduces a singing-trained RVC timbre shifter for timbre-content disentanglement, an F0-aware timbre adaptor for dynamic vocal expression, and an energy-balanced rectified flow matching loss to enhance high-frequency fidelity. Experiments on a graded multi-track benchmark show that YingMusic-SVC achieves consistent improvements over strong open-source baselines in timbre similarity, intelligibility, and perceptual naturalness, especially under accompanied and harmony-contaminated conditions, demonstrating its effectiveness for real-world SVC deployment."}
{"id": "2512.04756", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2512.04756", "abs": "https://arxiv.org/abs/2512.04756", "authors": ["Mattia Piana", "Stefano Tomasin"], "title": "Secret Key Generation on Aerial Rician Fading Channels Against a Curious Receiver", "comment": null, "summary": "Secret key generation at the physical layer is expected to be a fundamental enabler for next-generation networks. We consider a network where the user equipment is a drone and propose a novel secret key generation solution when the eavesdropper is another node belonging to the network (curious device). We exploit drone mobility over realistic Rician fading channels. In our protocol, after a prior training phase, drone Alice chooses a trajectory of positions in space and transmits a message to Bob, on the ground, from each position. From the received messages, Bob estimates the channel gain from which a secret key is extracted. The choice of the positions is made to maximize a lower bound on the secret key capacity. Numerical simulations are used to prove the effectiveness of the proposed approach."}
{"id": "2512.04814", "categories": ["cs.SD", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.04814", "abs": "https://arxiv.org/abs/2512.04814", "authors": ["Christopher Simic", "Korbinian Riedhammer", "Tobias Bocklet"], "title": "Shared Multi-modal Embedding Space for Face-Voice Association", "comment": "Ranked 1st in Fame 2026 Challenge, ICASSP", "summary": "The FAME 2026 challenge comprises two demanding tasks: training face-voice associations combined with a multilingual setting that includes testing on languages on which the model was not trained. Our approach consists of separate uni-modal processing pipelines with general face and voice feature extraction, complemented by additional age-gender feature extraction to support prediction. The resulting single-modal features are projected into a shared embedding space and trained with an Adaptive Angular Margin (AAM) loss. Our approach achieved first place in the FAME 2026 challenge, with an average Equal-Error Rate (EER) of 23.99%."}
{"id": "2512.04802", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2512.04802", "abs": "https://arxiv.org/abs/2512.04802", "authors": ["Luyang Sun", "Zhiqing Wei", "Haotian Liu", "Kan Yu", "Zhendong Li", "Zhiyong Feng"], "title": "Movable Antenna Assisted Flexible Beamforming for Integrated Sensing and Communication in Vehicular Networks", "comment": null, "summary": "Integrated sensing and communication (ISAC) has been recognized as a key technology in sixth-generation wireless networks, and the additional spatial degrees of freedom obtained by movable antenna (MA) technology can significantly improve the performance of ISAC systems. This paper considers an ISAC-assisted vehicle-to-infrastructure (V2I) network, where extended kalman filter-based prediction is combined with real-time optimization to jointly optimize transmit antenna positions and beamforming and power allocation vectors in dynamic environments. We propose two algorithms: a preprocessing-schur complement-projected gradient ascent algorithm for scenarios without sensing quality of service (QoS) constraints, which explores the potential range of sensing performance to provide reference and warm-starting for subsequent constrained optimization; and a heuristic reflective projected dynamic particle swarm optimization algorithm for sensing QoS-constrained scenarios, which achieves substantial performance gains under non-convex constraints with a small number of iterations. Simulation results demonstrate that these approaches enhance both the communication sum-rate and the lower of the Cramér-Rao lower bound of motion parameter estimation, validating the effectiveness of MA-assisted beamforming in dynamic V2I ISAC networks."}
{"id": "2512.04827", "categories": ["cs.SD", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.04827", "abs": "https://arxiv.org/abs/2512.04827", "authors": ["Wenzhang Du"], "title": "Contract-Driven QoE Auditing for Speech and Singing Services: From MOS Regression to Service Graphs", "comment": "11 pages, 3 figures", "summary": "Subjective mean opinion scores (MOS) remain the de-facto target for non-intrusive speech and singing quality assessment. However, MOS is a scalar that collapses heterogeneous user expectations, ignores service-level objectives, and is difficult to compare across deployment graphs. We propose a contract-driven QoE auditing framework: each service graph G is evaluated under a set of human-interpretable experience contracts C, yielding a contract-level satisfaction vector Q(G, C). We show that (i) classical MOS regression is a special case with a degenerate contract set, (ii) contract-driven quality is more stable than MOS under graph view transformations (e.g., pooling by system vs. by system type), and (iii) the effective sample complexity of learning contracts is governed by contract semantics rather than merely the dimensionality of C. We instantiate the framework on URGENT2024 MOS (6.9k speech utterances with raw rating vectors) and SingMOS v1 (7,981 singing clips; 80 systems). On URGENT, we train a contract-aware neural auditor on self-supervised WavLM embeddings; on SingMOS, we perform contract-driven graph auditing using released rating vectors and metadata without decoding audio. Empirically, our auditor matches strong MOS predictors in MOS accuracy while providing calibrated contract probabilities; on SingMOS, Q(G, C) exhibits substantially smaller cross-view drift than raw MOS and graph-only baselines; on URGENT, difficulty curves reveal that mis-specified \"simple\" contracts can be harder to learn than richer but better aligned contract sets."}
{"id": "2512.04872", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2512.04872", "abs": "https://arxiv.org/abs/2512.04872", "authors": ["Carlos Rafael Nogueira da Silva", "Maria Cecilia Luna Alvarado", "Fernando Darío Almeida García", "Michel Daoud Yacoub"], "title": "Cute but Cunning: Effective Closed-Form Alternatives to the Exact Lognormal Statistics", "comment": null, "summary": "The Lognormal distribution is a fundamental statistical model widely used in different fields of science, including biology, finance, economics, engineering, etc. In wireless communications, it is the primary statistic for large-scale fading modeling. However, its known analytical intractability presents persistent channel characterization and performance analysis challenges. This paper introduces two effective and mathematically tractable surrogate models for the Lognormal distribution, constructed from the product of Nakagami-$m$ and Inverse Nakagami-$m$ (I-Nakagami-$m$) variates. These models yield asymptotically exact closed-form expressions for key performance metrics -- including the characteristic function, bit error rate, and Shannon's capacity -- and enable analytically tractable expressions for the probability density function and cumulative distribution function of the composite $α$-$μ$-Lognormal fading model. To facilitate implementation, a moment-matching framework is developed to map the Lognormal parameters to the surrogate model parameters. In addition, a random mixture approach is proposed to enhance convergence by exploiting the complementary approximation properties of the Nakagami-$m$ and I-Nakagami-$m$ distributions. The methodology is further extended to heterogeneous cascaded fading channels comprising arbitrary combinations of $α$-$μ$, $κ$-$μ$, and $η$-$μ$ variates, for which moment-based mappings to the equivalent Lognormal distributions are derived. Numerical results confirm the accuracy and efficiency of the proposed approach, positioning it as a practical and reliable alternative to exact Lognormal statistics."}
{"id": "2512.04847", "categories": ["cs.SD", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.04847", "abs": "https://arxiv.org/abs/2512.04847", "authors": ["Tsai-Ning Wang", "Lin-Lin Chen", "Neil Zeghidour", "Aaqib Saeed"], "title": "Language Models as Semantic Teachers: Post-Training Alignment for Medical Audio Understanding", "comment": null, "summary": "Pre-trained audio models excel at detecting acoustic patterns in auscultation sounds but often fail to grasp their clinical significance, limiting their use and performance in diagnostic tasks. To bridge this gap, we introduce AcuLa (Audio-Clinical Understanding via Language Alignment), a lightweight post-training framework that instills semantic understanding into any audio encoder by aligning it with a medical language model, which acts as a \"semantic teacher.\" To enable alignment at scale, we construct a large-scale dataset by leveraging off-the-shelf large language models to translate the rich, structured metadata accompanying existing audio recordings into coherent clinical reports. Our alignment strategy combines a representation-level contrastive objective with a self-supervised modeling, ensuring that the model learns clinical semantics while preserving fine-grained temporal cues. AcuLa achieves state-of-the-art results across 18 diverse cardio-respiratory tasks from 10 different datasets, improving the mean AUROC on classification benchmarks from 0.68 to 0.79 and, on the most challenging COVID-19 cough detection task, boosting the AUROC from 0.55 to 0.89. Our work demonstrates that this audio-language alignment transforms purely acoustic models into clinically-aware diagnostic tools, establishing a novel paradigm for enhancing physiological understanding in audio-based health monitoring."}
{"id": "2512.04881", "categories": ["eess.SP", "cs.IT"], "pdf": "https://arxiv.org/pdf/2512.04881", "abs": "https://arxiv.org/abs/2512.04881", "authors": ["Xiao Cai", "Hei Victor Cheng", "Daniel E. Lucani"], "title": "Beampattern Synthesis for Discrete Phase RIS in Communication and Sensing Systems", "comment": null, "summary": "Extensive research on Reconfigurable Intelligent Surfaces (RIS) has primarily focused on optimizing reflective coefficients for passive beamforming in specific target directions. This optimization typically assumes prior knowledge of the target direction, which is unavailable before the target is detected. To enhance direction estimation, it is critical to develop array pattern synthesis techniques that yield a wider beam by maximizing the received power over the entire target area. Although this challenge has been addressed with active antennas, RIS systems pose a unique challenge due to their inherent phase constraints, which can be continuous or discrete.\n  This work addresses this challenge through a novel array pattern synthesis method tailored for discrete phase constraints in RIS. We introduce a penalty method that pushes these constraints to the boundary of the convex hull. Then, the Minorization-Maximization (MM) method is utilized to reformulate the problem into a convex one. Our numerical results show that our algorithm can generate a wide beam pattern comparable to that achievable with per-power constraints, with both the amplitudes and phases being adjustable. We compare our method with a traditional beam sweeping technique, showing a) several orders of magnitude reduction of the MSE of Angle of Arrival (AOA) at low to medium Signal-to-Noise Ratio (SNR)s; and b) $8$~dB SNR reduction to achieve a high probability of detection."}
{"id": "2512.04899", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2512.04899", "abs": "https://arxiv.org/abs/2512.04899", "authors": ["Yunpeng Qu", "Yazhou Sun", "Bingyu Hui", "Jintao Wang", "Jian Wang"], "title": "Channel-Aware Multi-Domain Feature Extraction for Automatic Modulation Recognition in MIMO Systems", "comment": "5 pages, 3 figures", "summary": "Automatic modulation recognition (AMR) is a key technology in non-cooperative communication systems, aiming to identify the modulation scheme from signals without prior information. Deep learning (DL)-based methods have gained wide attention due to their excellent performance, but research mainly focuses on single-input single-output (SISO) systems, with limited exploration for multiple-input multiple-output (MIMO) systems. The confounding effects of multi-antenna channels can interfere with the statistical properties of MIMO signals, making identification particularly challenging. To overcome these limitations, we propose a Channel-Aware Multi-Domain feature extraction (CAMD) framework for AMR in MIMO systems. Our CAMD framework reconstructs the transmitted signal through an efficient channel compensation module and achieves a more robust representation capability against channel interference by extracting and integrating multi-domain features, including intra-antenna temporal correlations and inter-antenna channel correlations. We have verified our method on the widely-used dataset, MIMOSig-Ref, with complex mobile channel environments. Extensive experiments confirm the performance advantages of CAMD over previous state-of-the-art methods."}
{"id": "2512.04914", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2512.04914", "abs": "https://arxiv.org/abs/2512.04914", "authors": ["Marta Płonka", "Rafał Klimas", "Dimitar Stanev", "Lorenza Angelini", "Natan Napiórkowski", "Gabriela González Chan", "Lisa Bunn", "Paul S Glazier", "Richard Hosking", "Jenny Freeman", "Jeremy Hobart", "Mattia Zanon", "Jonathan Marsden", "Licinio Craveiro", "Mike D Rinderknecht"], "title": "Analytical and Cross-Sectional Clinical Validity of a Smartphone-Based U-Turn Test in Multiple Sclerosis", "comment": null, "summary": "The observational GaitLab study (ISRCTN15993728) enrolled adult people with multiple sclerosis (PwMS) with Expanded Disability Status Scale (EDSS) <=6.5. PwMS performed the U-Turn Test (UTT), a smartphone-based assessment of dynamic balance, in a gait laboratory (supervised setting) using 6 smartphones at different body locations and daily during a 2-week remote period (unsupervised setting) using 1 smartphone. In the supervised setting, the accuracy of detecting turns with smartphones was compared against turns detected with a motion capture system (mocap) using F1 scores. Agreement between turn speed measured with smartphones and mocap was assessed by intraclass correlation coefficient (ICC[3,1]) and bias. In the unsupervised setting, test-retest reliability was assessed by ICC(2,1), and correlations with clinical and patient-reported measures by Spearman rank correlation. Ninety-six PwMS were included. In the supervised setting, turns were detected with high accuracy (F1 scores >95% across smartphone wear locations). Smartphone-derived turn speed was comparable across the supervised (1.44 rad/s) and unsupervised settings (1.47 rad/s), and with mocap-derived turn speed (1.47 rad/s). ICC(3,1) revealed high agreement between smartphone- and mocap-derived turn speed (ICC[3,1]: 0.87-0.92 across smartphone wear locations). Bias was minimal (-0.04 to 0.11 rad/s). In the unsupervised setting, test-retest reliability (ICC[2,1]) was >0.90 when aggregating >=2 tests. The UTT correlated with Timed 25-Foot Walk gait speed, EDSS, Ambulation score, 12-item Multiple Sclerosis Walking Scale, and Activities-specific Balance Confidence scale (r=-0.79 to -0.61). The UTT measures turn speed accurately and reproducibly irrespective of smartphone wear location and settings. These findings affirm its potential as a valuable tool in multiple sclerosis trials."}
{"id": "2512.04915", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2512.04915", "abs": "https://arxiv.org/abs/2512.04915", "authors": ["Xiuheng Wang", "Ricardo Borsoi", "Cédric Richard", "Ali H. Sayed"], "title": "Distributed Riemannian Optimization in Geodesically Non-convex Environments", "comment": null, "summary": "This paper studies the problem of distributed Riemannian optimization over a network of agents whose cost functions are geodesically smooth but possibly geodesically non-convex. Extending a well-known distributed optimization strategy called diffusion adaptation to Riemannian manifolds, we show that the resulting algorithm, the Riemannian diffusion adaptation, provably exhibits several desirable behaviors when minimizing a sum of geodesically smooth non-convex functions over manifolds of bounded curvature. More specifically, we establish that the algorithm can approximately achieve network agreement in the sense that Fréchet variance of the iterates among the agents is small. Moreover, the algorithm is guaranteed to converge to a first-order stationary point for general geodesically non-convex cost functions. When the global cost function additionally satisfies the Riemannian Polyak-Lojasiewicz (PL) condition, we also show that it converges linearly under a constant step size up to a steady-state error. Finally, we apply this algorithm to a decentralized robust principal component analysis (PCA) problem formulated on the Grassmann manifold and illustrate its convergence and performance through numerical simulations."}
{"id": "2512.04924", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2512.04924", "abs": "https://arxiv.org/abs/2512.04924", "authors": ["Weijian Zhang", "Prateek Chennuri", "Hashan K. Weerasooriya", "Bole Ma", "Stanley H. Chan"], "title": "Markov-Renewal Single-Photon LiDAR Simulator", "comment": null, "summary": "Single-photon LiDAR (SP-LiDAR) simulators face a dilemma: fast but inaccurate Poisson models or accurate but prohibitively slow sequential models. This paper breaks that compromise. We present a simulator that achieves both fidelity and speed by focusing on the critical, yet overlooked, component of simulation: the photon count statistics. Our key contribution is a Markov-renewal process (MRP) formulation that, for the first time, analytically predicts the mean and variance of registered photon counts under dead time. To make this MRP model computationally tractable, we introduce a spectral truncation rule that efficiently computes the complex covariance statistics. By proving the shift-invariance of the process, we extend this per-pixel model to full histogram cube generation via a precomputed lookup table. Our method generates 3D cubes indistinguishable from the sequential gold-standard, yet is orders of magnitude faster. This finally enables large-scale, physically-faithful data generation for learning-based SP-LiDAR reconstruction."}
{"id": "2512.04979", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2512.04979", "abs": "https://arxiv.org/abs/2512.04979", "authors": ["Kaidi Wang", "Zhiguo Ding", "Lajos Hanzo"], "title": "Generalized Pinching-Antenna Systems: A Leaky-Coaxial-Cable Perspective", "comment": null, "summary": "The evolution toward the sixth-generation (6G) wireless networks has flexible reconfigurable antenna architectures capable of adapting their radiation characteristics to the surrounding environment. At the center-stage, while waveguide based pinching antennas have been shown to beneficially ameliorate wireless propagation environments, their applications have remained confined to high-frequency scenarios. As a remedy, we propose a downlink generalized pinching-antenna system that adapts this compelling concept to low-frequency operation through a leaky-coaxial-cable (LCX) implementation. By endowing LCX structures with controllable radiation slots, the system inherits the key capabilities of waveguide based pinching antennas. Explicitly, these include reconfigurable line-of-sight (LoS) links, reduced path loss, and flexible deployment, while supporting a practical implementation of the pinching-antenna concept at low frequencies. A twin-stage propagation model is developed for characterizing both the guided transmission and wireless radiation encountered over LoS and non-line-of-sight (NLoS) paths. Analytical results reveal strong local gain, complemented by rapid distance-dependent decay. Hence, we conceive a matching joint optimization framework, which maximizes throughput by harnessing game-theoretic association and convex power allocation. Simulation results demonstrate substantial performance gains over conventional fixed-antenna benchmarks."}
{"id": "2512.05028", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2512.05028", "abs": "https://arxiv.org/abs/2512.05028", "authors": ["Siva Aditya Gooty", "Hessam Mahdavifar"], "title": "Efficient Decoders for Sensing Subspace Code", "comment": "This paper was accepted for presentation at the 59th Annual Asilomar Conference on Signals, Systems, and Computers", "summary": "Sparse antenna array sensing of source/target via direction of arrival (DoA) estimation motivates design of the sensing framework in joint communication and sensing (JCAS) systems for sixth generation (6G) communication systems. Recently, it is established by Mahdavifar, Rajamäki, and Pal that array geometry of sparse arrays has fundamental connections with the design of subspace codes in coding theory. This was then utilized to design efficient \\textit{sensing subspace codes} that estimate the DoA with good resolution. Specifically, the Bose-Chowla sensing subspace code provides near optimal code design for unique DoA estimation with tight theoretical upper bound on the error performance. However, the currently known decoder for these codes, to estimate the DoA, is a traditional \\textit{Maximum-a-Posterior (MAP) decoder} with complexity that is cubic with the number of antennas. In this work, we propose novel efficient decoding algorithms for sensing subspace codes, that reduce the complexity down to quadratic while providing new knobs to tune in order to tradeoff complexity with error performance. The decoders are further evaluated for their performance via Monte Carlo simulations for a range of SNRs demonstrating promising performance that smoothly approaches the MAP performance as the complexity grows from quadratic to cubic in the number of antennas."}

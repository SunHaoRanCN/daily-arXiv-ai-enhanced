{"id": "2512.06099", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2512.06099", "abs": "https://arxiv.org/abs/2512.06099", "authors": ["Khondakar Ashik Shahriar"], "title": "Why Nonlinear Models Matter: Unified Analysis of Cognitive Load, Stress, and Exercise Using Wearable Physiological Signals", "comment": "28 pages, 8 tables, 13 figures", "summary": "Wearable physiological signals exhibit strong nonlinear and subject-dependent behavior, challenging traditional linear models. This study provides a unified evaluation of cognitive load, stress, and physical exercise recognition using three public Empatica~E4 datasets. Across all conditions, nonlinear machine learning models consistently outperformed linear baselines, achieving 0.89--0.98 accuracy and 0.96--0.99 ROC--AUC, while linear models remained below 0.70--0.73 AUC. Although Leave-One-Subject-Out validation revealed substantial inter-individual variability, nonlinear models maintained moderate cross-person generalization. Ablation and statistical analyses confirmed the necessity of multimodal fusion, particularly EDA, temperature, and ACC, while SHAP interpretability validated these findings by uncovering physiologically meaningful feature contributions across tasks. Overall, the results demonstrate that physiological state recognition is fundamentally nonlinear and establish a unified benchmark to guide the development of more robust wearable health-monitoring systems."}
{"id": "2512.06234", "categories": ["eess.SP", "cs.IT"], "pdf": "https://arxiv.org/pdf/2512.06234", "abs": "https://arxiv.org/abs/2512.06234", "authors": ["Canan Cebeci", "Oveys Delafrooz Noroozi", "Upamanyu Madhow"], "title": "Beamspace Dimensionality Reduction for Massive MU-MIMO: Geometric Insights and Information-Theoretic Limits", "comment": "13 pages", "summary": "Beamspace dimensionality reduction, a classical tool in array processing, has been shown in recent work to significantly reduce computational complexity and training overhead for adaptive reception in massive multiuser (MU) MIMO. For sparse multipath propagation and uniformly spaced antenna arrays, beamspace transformation, or application of a spatial FFT, concentrates the energy of each user into a small number of spatial frequency bins. Empirical evaluations demonstrate the efficacy of linear Minimum Mean Squared Error (LMMSE) detection performed in parallel using a beamspace window of small, fixed size for each user, even as the number of antennas and users scale up, while being robust to moderate variations in the relative powers of the users. In this paper, we develop a fundamental geometric understanding of this ``unreasonable effectiveness'' in a regime in which zero-forcing solutions do not exist. For simplified channel models, we show that, when we enforce a suitable separation in spatial frequency between users, the interference power falling into a desired user's beamspace window of size $W$ concentrates into a number of dominant eigenmodes smaller than $W$, with the desired user having relatively small projection onto these modes. Thus, linear suppression of dominant interference modes can be accomplished with small noise enhancement. We show that similar observations apply for MIMO-OFDM over wideband multipath channels synthesized from measured 28 GHz data. We propose, and evaluate via information-theoretic benchmarks, per-subcarrier reduced dimension beamspace LMMSE in this setting."}
{"id": "2512.06494", "categories": ["eess.SP", "cs.IT"], "pdf": "https://arxiv.org/pdf/2512.06494", "abs": "https://arxiv.org/abs/2512.06494", "authors": ["Sandesh Rao Mattu", "Nishant Mehrotra", "Robert Calderbank"], "title": "Non-Equiprobable Signaling for Wireless Channels Subject to Mobility and Delay Spread", "comment": "5 pages, 6 figures. Submitted to IEEE Wireless Communications Letters", "summary": "This letter describes how to improve performance of OFDM systems by combining non-equiprobable signaling with low density parity check (LDPC) coding. We partition a standard QAM constellation into annular subconstellations of equal size, and we implement non-equiprobable signaling through a shaping code which selects subconstellations with large average energy less frequently than subconstellations with small average energy. In equiprobable signaling, the LDPC code selects a signal point from the inner subconstellation. In non-equiprobable signaling this inner signal point has a representative in each subconstellation and the shaping code selects the representative for transmission. It is possible to use standard QAM constellations to achieve any desired fractional bit rate with this method of shaping the energy distribution of the transmitted signal. We describe how to combine coding and shaping by integrating shaping into the calculation of log-likelihood ratios (LLRs) necessary for decoding LDPC codes. We present simulation results for non-equiprobable transmission at $1.5$ bits/symbol on a representative Veh-A channel showing gains of $4$ dB at a bit error rate (BER) of $10^{-3}$. As the transmission rate increases, the gains from non-equiprobable signaling diminish, but we show through simulation that they are still significant for $16$-QAM."}
{"id": "2512.06532", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2512.06532", "abs": "https://arxiv.org/abs/2512.06532", "authors": ["Amirmohammad Haddad", "Oveys Delafrooz Noroozi", "Canan Cebeci", "Mark J. W. Rodwell", "Upamanyu Madhow"], "title": "Scaling Wideband Hybrid Beamforming for sub-THz Communication", "comment": null, "summary": "We investigate the capacity attainable for a multiuser MIMO uplink as we scale both array size and bandwidth for regimes in which all-digital arrays incur excessive hardware complexity and power consumption. We consider a tiled hybrid beamforming architecture in which each tile, or subarray, is a phased array performing analog (or RF) beamforming, followed by DSP on the tile outputs. For parameters compatible with sub-THz fixed access links, we discuss hardware and power consumption considerations for choosing tile size and the number of tiles. Noting that the problem of optimal multiuser MIMO in our wideband regime is open even for the simplest possible channel models, we compare the spectral efficiencies attainable by a number of reasonable strategies for tile-level RF beamforming, assuming flexibility in the digital signal processing (DSP) of the tile outputs. We consider a number of beam broadening approaches for addressing the ``beam squint'' incurred by RF beamforming in our wideband regime, along with strategies for sharing tiles among users. Information-theoretic benchmarks are computed for an idealized MIMO-OFDM system, with linear per-subcarrier multiuser detection compared against an unconstrained complexity receiver."}
{"id": "2512.05994", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2512.05994", "abs": "https://arxiv.org/abs/2512.05994", "authors": ["Rohan Sharma", "Dancheng Liu", "Jingchen Sun", "Shijie Zhou", "Jiayu Qin", "Jinjun Xiong", "Changyou Chen"], "title": "KidSpeak: A General Multi-purpose LLM for Kids' Speech Recognition and Screening", "comment": null, "summary": "With the rapid advancement of conversational and diffusion-based AI, there is a growing adoption of AI in educational services, ranging from grading and assessment tools to personalized learning systems that provide targeted support for students. However, this adaptability has yet to fully extend to the domain of children's speech, where existing models often fail due to their reliance on datasets designed for clear, articulate adult speech. Children, particularly those in early developmental stages or with speech and language pathologies, present unique challenges that current AI models and datasets are ill-equipped to handle. To address this, we introduce KidSpeak, a multi-task speech-enhanced Foundation Model capable of both generative and discriminative tasks specifically tailored to children's speech patterns. Our framework employs a two-stage training process that incorporates phonetic knowledge into the speech encoder, achieving an average accuracy of 87% across four separate tasks. Furthermore, recognizing the limitations of scalable human annotation and existing speech alignment tools, we propose the Flexible and Automatic Speech Aligner (FASA) and leverage the method to construct high quality datasets for training and evaluation. This novel alignment tool significantly improves the quality of aligned children's speech from noisy data, enhancing data quality by 13.6x compared to human annotations, as demonstrated on the CHILDES dataset. To the best of our knowledge, KidSpeak and FASA represent the first comprehensive solution designed for speech and language therapy in children, offering both a multi-purpose speech LLM and a robust alignment tool."}
{"id": "2512.06022", "categories": ["cs.SD", "cs.MM"], "pdf": "https://arxiv.org/pdf/2512.06022", "abs": "https://arxiv.org/abs/2512.06022", "authors": ["Fu Li", "Weichao Zhao", "You Li", "Zhichao Zhou", "Dongliang He"], "title": "DreamFoley: Scalable VLMs for High-Fidelity Video-to-Audio Generation", "comment": "10 pages; Bytedance", "summary": "Recent advances in video generation have achieved remarkable improvements in visual content fidelity. However, the absence of synchronized audio severely undermines immersive experience and restricts practical applications of these technologies. To address this challenge, several pioneering works have explored diffusion transformer architectures for generating plausible video-synchronized audio, including Kling-foley, HunyuanVideo-foley and Thinksound. Distinct from existing works, we introduce an autoregressive audio generation architecture (DreamFoley) that harnesses the capabilities of large vision-language models (VLMs) to jointly model sequential interactions among video, audio, and text modalities. Our approach features a dual-visual encoder module that effectively captures both audio-aligned and text-aligned visual features. Additionally, we employ a Residual Vector Quantization audio tokenizer with a delay-pattern generation scheme to balance the trade-off between training efficiency and audio quality. Moreover, we introduce the classifier-free guidance strategy into VLMs to bootstrap generated audio quality. Furthermore, we establish an efficient data production pipeline to scale audio-video-text triple collection. Finally, extensive experiments are conducted to validate the effectiveness of our model, achieving promising performance across popular benchmarks. We hope that the findings in this study provide a strong foundation for future video-to-audio generation research. We also release the previously missing audio-visual textual descriptions from the public benchmark, aiming to facilitate subsequent researchers in conducting more convenient and effective evaluations and comparisons."}
{"id": "2512.06536", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2512.06536", "abs": "https://arxiv.org/abs/2512.06536", "authors": ["Oveys Delafrooz Noroozi", "Jiyoon Han", "Wei Tang", "Zhengya Zhang", "Upamanyu Madhow"], "title": "Scaling Wideband Massive MIMO Radar via Tiled Beamspace Processing", "comment": null, "summary": "We present a coordinated tiled architecture for scalable wideband digital beamforming in massive MIMO radar systems. As aperture size increases, conventional full-array MVDR processing becomes prohibitive due to the cubic complexity of covariance estimation and inversion. Building on the principle of energy concentration in beamspace, we introduce a tiled windowed-beamspace MVDR framework that distributes spatial FFT processing across subarrays while performing joint beamforming in a compact global beamspace domain. Each tile applies a 2D spatial DFT followed by an angle-of-arrival dependent beamspace window, producing a reduced-dimensional representation that preserves the dominant spatial structure of the received signal. The windowed outputs from the tiles are concatenated and processed by a centralized MVDR beamformer, enabling coherent full-aperture processing with drastically reduced dimensionality. Our numerical results demonstrate that the proposed architecture achieves detection and interference suppression performance comparable to full-dimensional processing, while substantially lowering computational cost, memory usage, and training requirements. The framework also reveals tradeoffs among tile size, window size, and beamspace resolution that govern overall system scalability."}
{"id": "2512.06304", "categories": ["eess.AS", "cs.AI", "cs.CR", "cs.SD"], "pdf": "https://arxiv.org/pdf/2512.06304", "abs": "https://arxiv.org/abs/2512.06304", "authors": ["Xining Song", "Zhihua Wei", "Rui Wang", "Haixiao Hu", "Yanxiang Chen", "Meng Han"], "title": "Degrading Voice: A Comprehensive Overview of Robust Voice Conversion Through Input Manipulation", "comment": null, "summary": "Identity, accent, style, and emotions are essential components of human speech. Voice conversion (VC) techniques process the speech signals of two input speakers and other modalities of auxiliary information such as prompts and emotion tags. It changes para-linguistic features from one to another, while maintaining linguistic contents. Recently, VC models have made rapid advancements in both generation quality and personalization capabilities. These developments have attracted considerable attention for diverse applications, including privacy preservation, voice-print reproduction for the deceased, and dysarthric speech recovery. However, these models only learn non-robust features due to the clean training data. Subsequently, it results in unsatisfactory performances when dealing with degraded input speech in real-world scenarios, including additional noise, reverberation, adversarial attacks, or even minor perturbation. Hence, it demands robust deployments, especially in real-world settings. Although latest researches attempt to find potential attacks and countermeasures for VC systems, there remains a significant gap in the comprehensive understanding of how robust the VC model is under input manipulation. here also raises many questions: For instance, to what extent do different forms of input degradation attacks alter the expected output of VC models? Is there potential for optimizing these attack and defense strategies? To answer these questions, we classify existing attack and defense methods from the perspective of input manipulation and evaluate the impact of degraded input speech across four dimensions, including intelligibility, naturalness, timbre similarity, and subjective perception. Finally, we outline open issues and future directions."}
{"id": "2512.06040", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2512.06040", "abs": "https://arxiv.org/abs/2512.06040", "authors": ["Alireza Mohammadi", "Keshav Sood", "Dhananjay Thiruvady", "Asef Nazari"], "title": "Physics-Guided Deepfake Detection for Voice Authentication Systems", "comment": null, "summary": "Voice authentication systems deployed at the network edge face dual threats: a) sophisticated deepfake synthesis attacks and b) control-plane poisoning in distributed federated learning protocols. We present a framework coupling physics-guided deepfake detection with uncertainty-aware in edge learning. The framework fuses interpretable physics features modeling vocal tract dynamics with representations coming from a self-supervised learning module. The representations are then processed via a Multi-Modal Ensemble Architecture, followed by a Bayesian ensemble providing uncertainty estimates. Incorporating physics-based characteristics evaluations and uncertainty estimates of audio samples allows our proposed framework to remain robust to both advanced deepfake attacks and sophisticated control-plane poisoning, addressing the complete threat model for networked voice authentication."}
{"id": "2512.06617", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2512.06617", "abs": "https://arxiv.org/abs/2512.06617", "authors": ["De Bi", "Chengbai Xu", "Lingfeng Chen", "Panhe Hu"], "title": "Teaching large language models to see in radar: aspect-distributed prototypes for few-shot HRRP ATR", "comment": "PAPER UNDER REVIEW", "summary": "High-resolution range profiles (HRRPs) play a critical role in automatic target recognition (ATR) due to their richinformationregarding target scattering centers (SCs), which encapsulate the geometric and electromagnetic characteristics of thetarget.Under few-shot circumstances, traditional learning-based methods often suffer from overfitting and struggle togeneralizeeffectively. The recently proposed HRRPLLM, which leverages the in-context learning (ICL) capabilities of largelanguagemodels (LLMs) for one-shot HRRP ATR, is limited in few-shot scenarios. This limitation arises because it primarilyutilizesthe distribution of SCs for recognition while neglecting the variance of the samples caused by aspect sensitivity. Thispaperproposes a straightforward yet effective Aspect-Distributed Prototype (ADP) strategy for LLM-based ATRunder few-shotconditions to enhance aspect robustness. Experiments conducted on both simulated and measured aircraft electromagneticdatasets demonstrate that the proposed method significantly outperforms current benchmarks."}
{"id": "2512.07226", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2512.07226", "abs": "https://arxiv.org/abs/2512.07226", "authors": ["Runwu Shi", "Chang Li", "Jiang Wang", "Rui Zhang", "Nabeela Khan", "Benjamin Yen", "Takeshi Ashizawa", "Kazuhiro Nakadai"], "title": "Unsupervised Single-Channel Audio Separation with Diffusion Source Priors", "comment": "15 pages, 31 figures, accepted by The 40th Annual AAAI Conference on Artificial Intelligence (AAAI 2026)", "summary": "Single-channel audio separation aims to separate individual sources from a single-channel mixture. Most existing methods rely on supervised learning with synthetically generated paired data. However, obtaining high-quality paired data in real-world scenarios is often difficult. This data scarcity can degrade model performance under unseen conditions and limit generalization ability. To this end, in this work, we approach this problem from an unsupervised perspective, framing it as a probabilistic inverse problem. Our method requires only diffusion priors trained on individual sources. Separation is then achieved by iteratively guiding an initial state toward the solution through reconstruction guidance. Importantly, we introduce an advanced inverse problem solver specifically designed for separation, which mitigates gradient conflicts caused by interference between the diffusion prior and reconstruction guidance during inverse denoising. This design ensures high-quality and balanced separation performance across individual sources. Additionally, we find that initializing the denoising process with an augmented mixture instead of pure Gaussian noise provides an informative starting point that significantly improves the final performance. To further enhance audio prior modeling, we design a novel time-frequency attention-based network architecture that demonstrates strong audio modeling capability. Collectively, these improvements lead to significant performance gains, as validated across speech-sound event, sound event, and speech separation tasks."}
{"id": "2512.06041", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2512.06041", "abs": "https://arxiv.org/abs/2512.06041", "authors": ["Candy Olivia Mawalim", "Haotian Zhang", "Shogo Okada"], "title": "Technical Report of Nomi Team in the Environmental Sound Deepfake Detection Challenge 2026", "comment": null, "summary": "This paper presents our work for the ICASSP 2026 Environmental Sound Deepfake Detection (ESDD) Challenge. The challenge is based on the large-scale EnvSDD dataset that consists of various synthetic environmental sounds. We focus on addressing the complexities of unseen generators and low-resource black-box scenarios by proposing an audio-text cross-attention model. Experiments with individual and combined text-audio models demonstrate competitive EER improvements over the challenge baseline (BEATs+AASIST model)."}
{"id": "2512.06693", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2512.06693", "abs": "https://arxiv.org/abs/2512.06693", "authors": ["Xu Gan", "Xidong Mu", "Yuanwei Liu", "Marco Di Renzo", "Josep Miquel Jornet", "Nuria Gonz√°lez Prelcic", "Arman Shojaeifard", "Tie Jun Cui"], "title": "Multi-Functional Programmable Metasurfaces for 6G and Beyond", "comment": null, "summary": "The sixth-generation and beyond (B6G) networks are envisioned to support advanced applications that demand high-speed communication, high-precision sensing, and high-performance computing. To underpin this multi-functional evolution, energy- and cost-efficient programmable metasurfaces (PMs) have emerged as a promising technology for dynamically manipulating electromagnetic waves. This paper provides a comprehensive survey of representative multi-functional PM paradigms, with a specific focus on achieving \\emph{full-space communication coverage}, \\emph{ubiquitous sensing}, as well as \\emph{intelligent signal processing and computing}. i) For simultaneously transmitting and reflecting surfaces (STARS)-enabled full-space communications, we elaborate on their operational protocols and pivotal applications in supporting efficient communications, physical layer security, unmanned aerial vehicle networks, and wireless power transfer. ii) For PM-underpinned ubiquitous sensing, we formulate the signal models for the PM-assisted architecture and systematically characterize its advantages in near-field and cooperative sensing, while transitioning to the PM-enabled transceiver architecture and demonstrating its superior performance in multi-band operations. iii) For advanced signal processing and computing, we explore the novel paradigm of stacked intelligent metasurfaces (SIMs), investigating their implementation in wave-domain analog processing and over-the-air mathematical computing. Finally, we identify key research challenges and envision future directions for multi-functional PMs towards B6G."}
{"id": "2512.07570", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2512.07570", "abs": "https://arxiv.org/abs/2512.07570", "authors": ["Jens Ahrens"], "title": "Introduction to Ambisonics, Part 1: The Part With No Math", "comment": null, "summary": "The present document is Part 1 of a 2-part introduction to ambisonics and aims at readers who would like to work practically with ambisonics. We leave out deep technical details in this part and focus on helping the reader to develop an intuitive understanding of the underlying concept. We explain what ambisonic signals are, how they can be obtained, what manipulations can be applied to them, and how they can be reproduced to a listener. We provide a variety of audio examples that illustrate the matter. Part 2 of this introduction into ambisonics is provided in a separate document and aims at readers who would like to understand the mathematical details."}
{"id": "2512.06259", "categories": ["cs.SD", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.06259", "abs": "https://arxiv.org/abs/2512.06259", "authors": ["Yash Choudhary", "Preeti Rao", "Pushpak Bhattacharyya"], "title": "Who Will Top the Charts? Multimodal Music Popularity Prediction via Adaptive Fusion of Modality Experts and Temporal Engagement Modeling", "comment": "8 pages", "summary": "Predicting a song's commercial success prior to its release remains an open and critical research challenge for the music industry. Early prediction of music popularity informs strategic decisions, creative planning, and marketing. Existing methods suffer from four limitations:(i) temporal dynamics in audio and lyrics are averaged away; (ii) lyrics are represented as a bag of words, disregarding compositional structure and affective semantics; (iii) artist- and song-level historical performance is ignored; and (iv) multimodal fusion approaches rely on simple feature concatenation, resulting in poorly aligned shared representations. To address these limitations, we introduce GAMENet, an end-to-end multimodal deep learning architecture for music popularity prediction. GAMENet integrates modality-specific experts for audio, lyrics, and social metadata through an adaptive gating mechanism. We use audio features from Music4AllOnion processed via OnionEnsembleAENet, a network of autoencoders designed for robust feature extraction; lyric embeddings derived through a large language model pipeline; and newly introduced Career Trajectory Dynamics (CTD) features that capture multi-year artist career momentum and song-level trajectory statistics. Using the Music4All dataset (113k tracks), previously explored in MIR tasks but not popularity prediction, GAMENet achieves a 12% improvement in R^2 over direct multimodal feature concatenation. Spotify audio descriptors alone yield an R^2 of 0.13. Integrating aggregate CTD features increases this to 0.69, with an additional 7% gain from temporal CTD features. We further validate robustness using the SpotGenTrack Popularity Dataset (100k tracks), achieving a 16% improvement over the previous baseline. Extensive ablations confirm the model's effectiveness and the distinct contribution of each modality."}
{"id": "2512.06706", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2512.06706", "abs": "https://arxiv.org/abs/2512.06706", "authors": ["Yuan Xu", "Wenhui Xiong"], "title": "Message Passing Based Demodulation of the Time-Encoded Digital Modulation Signal", "comment": null, "summary": "This letter proposes a Message Passing (MP) based algorithm for demodulating the time-encoded digital modulation signal. The proposed algorithm processes the spikes generated by the Time-Encoding Machine (TEM) directly on a per-spike basis, which enables \"on-the-fly\" demodulatio with low latency. The computational complexity of our proposed method scales linearly with the number of demodulated symbols, while the computational complexity of the pseudo-inverse-based method scales cubically with the same number of demodulated symbols."}
{"id": "2512.06040", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2512.06040", "abs": "https://arxiv.org/abs/2512.06040", "authors": ["Alireza Mohammadi", "Keshav Sood", "Dhananjay Thiruvady", "Asef Nazari"], "title": "Physics-Guided Deepfake Detection for Voice Authentication Systems", "comment": null, "summary": "Voice authentication systems deployed at the network edge face dual threats: a) sophisticated deepfake synthesis attacks and b) control-plane poisoning in distributed federated learning protocols. We present a framework coupling physics-guided deepfake detection with uncertainty-aware in edge learning. The framework fuses interpretable physics features modeling vocal tract dynamics with representations coming from a self-supervised learning module. The representations are then processed via a Multi-Modal Ensemble Architecture, followed by a Bayesian ensemble providing uncertainty estimates. Incorporating physics-based characteristics evaluations and uncertainty estimates of audio samples allows our proposed framework to remain robust to both advanced deepfake attacks and sophisticated control-plane poisoning, addressing the complete threat model for networked voice authentication."}
{"id": "2512.06380", "categories": ["cs.SD", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06380", "abs": "https://arxiv.org/abs/2512.06380", "authors": ["Xiao Zhan", "Guangzhi Sun", "Jose Such", "Phil Woodland"], "title": "Protecting Bystander Privacy via Selective Hearing in LALMs", "comment": "Dataset: https://huggingface.co/datasets/BrianatCambridge/SelectiveHearingBench", "summary": "Large audio language models (LALMs) are increasingly deployed in real-world settings where they inevitably capture speech from unintended nearby bystanders, raising privacy risks that existing benchmarks and defences largely overlook. We introduce SH-Bench, the first benchmark designed to evaluate selective hearing: a model's ability to attend to an intended main speaker while refusing to process or reveal information about incidental bystander speech. SH-Bench contains 3,968 multi-speaker audio mixtures spanning both real-world and synthetic scenarios, paired with 77k multiple-choice questions that probe models under general and selective operating modes. We propose Selective Efficacy (SE), a unified metric capturing both multi-speaker comprehension and bystander-privacy protection. Our evaluation of state-of-the-art open-source and proprietary LALMs reveals substantial privacy leakage, with strong audio understanding failing to translate into selective protection of bystander privacy. To mitigate this gap, we introduce Bystander Privacy Fine-Tuning (BPFT), a training pipeline that teaches models to refuse bystander-related queries without degrading main-speaker comprehension. BPFT yields substantial gains which improve SE by up to 15.9% over Gemini 2.5 Pro, demonstrating that selective hearing is learnable but far from achieved in current LALMs. SH-Bench and BPFT provide the first systematic framework for measuring and improving bystander privacy in audio foundation models."}
{"id": "2512.06799", "categories": ["eess.SP", "physics.app-ph"], "pdf": "https://arxiv.org/pdf/2512.06799", "abs": "https://arxiv.org/abs/2512.06799", "authors": ["Philipp del Hougne"], "title": "Effective Electromagnetic Degrees of Freedom in Backscatter MIMO Systems", "comment": "10 pages including 4 figures", "summary": "While the definition of the effective electromagnetic degrees of freedom (EEMDOFs) of a static linear multiple-input multiple-output (MIMO) system is well established, the counterpart for a backscatter MIMO (BS-MIMO) system is so far missing. A BS-MIMO system encodes the input information into the loads of backscatter elements. Due to mutual coupling, the mapping from load configuration to observed fields is fundamentally non-linear, which complicates the analysis of BS-EEMDOFs. We introduce a definition of BS-EEMDOFs based on the Jacobian of the observed fields with respect to the load configuration. We derive a closed-form expression from multiport network theory which demonstrates that the number of BS-EEMDOFs is fundamentally a distributed variable, whose distribution depends on the mutual coupling between the backscatter elements and the coherent illumination. The modes associated with BS-EEMDOFs lie in the column space of the end-to-end channel matrix from backscatter array ports to receiver ports, but the number of BS-EEMDOFs is generally different from the number of benchmark EEMDOFs associated with the same array being coherently fed rather than tunably terminated. The dependence on the coherent illumination yields optimized coherent illumination as a control knob for the number of BS-EEMDOFs. We present numerical and experimental results for the evaluation and optimization of the number of BS-EEMDOFs in different radio environments with reconfigurable intelligent surfaces."}
{"id": "2512.06041", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2512.06041", "abs": "https://arxiv.org/abs/2512.06041", "authors": ["Candy Olivia Mawalim", "Haotian Zhang", "Shogo Okada"], "title": "Technical Report of Nomi Team in the Environmental Sound Deepfake Detection Challenge 2026", "comment": null, "summary": "This paper presents our work for the ICASSP 2026 Environmental Sound Deepfake Detection (ESDD) Challenge. The challenge is based on the large-scale EnvSDD dataset that consists of various synthetic environmental sounds. We focus on addressing the complexities of unseen generators and low-resource black-box scenarios by proposing an audio-text cross-attention model. Experiments with individual and combined text-audio models demonstrate competitive EER improvements over the challenge baseline (BEATs+AASIST model)."}
{"id": "2512.06757", "categories": ["cs.SD", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06757", "abs": "https://arxiv.org/abs/2512.06757", "authors": ["Zhihua Fang", "Shumei Tao", "Junxu Wang", "Liang He"], "title": "XM-ALIGN: Unified Cross-Modal Embedding Alignment for Face-Voice Association", "comment": "FAME 2026 Technical Report", "summary": "This paper introduces our solution, XM-ALIGN (Unified Cross-Modal Embedding Alignment Framework), proposed for the FAME challenge at ICASSP 2026. Our framework combines explicit and implicit alignment mechanisms, significantly improving cross-modal verification performance in both \"heard\" and \"unheard\" languages. By extracting feature embeddings from both face and voice encoders and jointly optimizing them using a shared classifier, we employ mean squared error (MSE) as the embedding alignment loss to ensure tight alignment between modalities. Additionally, data augmentation strategies are applied during model training to enhance generalization. Experimental results show that our approach demonstrates superior performance on the MAV-Celeb dataset. The code will be released at https://github.com/PunkMale/XM-ALIGN."}
{"id": "2512.06909", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2512.06909", "abs": "https://arxiv.org/abs/2512.06909", "authors": ["Qiankai Shen", "Yuanhao Cui", "Jie Yang", "Xiaojun Jing", "Zhiyong Feng", "Shi Jin"], "title": "Bruxism Recognition via Wireless Signal", "comment": null, "summary": "Bruxism is an oromandibular movement disorder involving teeth grinding and clenching, which severely impairs sleep quality and dental health. However, its diagnosis remains challenging, as existing methods often cause discomfort or compromise user privacy. To address these limitations, we establish a contactless bruxism recognition system based on millimeter-wave radar. First, we analyzed the potential impact of the movement patterns of teeth grinding on radar echo signals. Based on this analysis, 11 features were extracted. Subsequently, using these features, we performed classification with a Random Forest model on the dataset constructed via millimeter-wave radar. Experimental results demonstrate that the proposed method achieves an accuracy of 96.1% on the test set, with precision, recall, and F1-score all remaining at a relatively high level. This study validates the effectiveness of millimeter-wave radar for SB recognition, providing a non-invasive and privacy-friendly alternative to existing recognition techniques. Future research will focus on enhancing the robustness of the method across diverse populations and environments, as well as striving to mitigate the interference of other facial micro-movements on teeth grinding recognition."}
{"id": "2512.07168", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2512.07168", "abs": "https://arxiv.org/abs/2512.07168", "authors": ["Georgios Ioannides", "Christos Constantinou", "Aman Chadha", "Aaron Elkins", "Linsey Pang", "Ravid Shwartz-Ziv", "Yann LeCun"], "title": "JEPA as a Neural Tokenizer: Learning Robust Speech Representations with Density Adaptive Attention", "comment": "UniReps: Unifying Representations in Neural Models (NeurIPS 2025 Workshop)", "summary": "We introduce a two-stage self-supervised framework that combines the Joint-Embedding Predictive Architecture (JEPA) with a Density Adaptive Attention Mechanism (DAAM) for learning robust speech representations. Stage~1 uses JEPA with DAAM to learn semantic audio features via masked prediction in latent space, fully decoupled from waveform reconstruction. Stage~2 leverages these representations for efficient tokenization using Finite Scalar Quantization (FSQ) and a mixed-radix packing scheme, followed by high-fidelity waveform reconstruction with a HiFi-GAN decoder. By integrating Gaussian mixture-based density-adaptive gating into the JEPA encoder, the model performs adaptive temporal feature selection and discovers hierarchical speech structure at a low frame rate of 2.5~Hz. The resulting tokens (47.5 tokens/sec) provide a reversible, highly compressed, and language-model-friendly representation that is competitive with, and often more efficient than, existing neural audio codecs."}
{"id": "2512.06890", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2512.06890", "abs": "https://arxiv.org/abs/2512.06890", "authors": ["Roger K. Moore"], "title": "What Needs to be Known in Order to Perform a Meaningful Scientific Comparison Between Animal Communications and Human Spoken Language", "comment": "5 pages, 1 figure, Proc. Vocal Interactivity in-and-between Humans, Animals and Robots (VIHAR-24), Kos, Greece, 6 Sept. 2024", "summary": "Human spoken language has long been the subject of scientific investigation, particularly with regard to the mechanisms underpinning speech production. Likewise, the study of animal communications has a substantial literature, with many studies focusing on vocalisation. More recently, there has been growing interest in comparing animal communications and human speech. However, it is proposed here that such a comparison necessitates the appraisal of a minimum set of critical phenomena: i) the number of degrees-of-freedom of the vocal apparatus, ii) the ability to control those degrees-of-freedom independently, iii) the properties of the acoustic environment in which communication takes place, iv) the perceptual salience of the generated sounds, v) the degree to which sounds are contrastive, vi) the presence/absence of compositionality, and vii) the information rate(s) of the resulting communications."}
{"id": "2512.07053", "categories": ["eess.SP", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.07053", "abs": "https://arxiv.org/abs/2512.07053", "authors": ["Hyunwoo Lee", "Ian P. Roberts", "Jinkyo Jeong", "Daesik Hong"], "title": "Random Access for LEO Satellite Communication Systems via Deep Learning", "comment": "12 pages, 8 figures, 4 tables", "summary": "Integrating contention-based random access procedures into low Earth orbit (LEO) satellite communication (SatCom) systems poses new challenges, including long propagation delays, large Doppler shifts, and a large number of simultaneous access attempts. These factors degrade the efficiency and responsiveness of conventional random access schemes, particularly in scenarios such as satellite-based internet of things and direct-to-device services. In this paper, we propose a deep learning-based random access framework designed for LEO SatCom systems. The framework incorporates an early preamble collision classifier that uses multi-antenna correlation features and a lightweight 1D convolutional neural network to estimate the number of collided users at the earliest stage. Based on this estimate, we introduce an opportunistic transmission scheme that balances access probability and resource efficiency to improve success rates and reduce delay. Simulation results under 3GPP-compliant LEO settings confirm that the proposed framework achieves higher access success probability, lower delay, better physical uplink shared channel utilization, and reduced computational complexity compared to existing schemes."}
{"id": "2512.06999", "categories": ["cs.SD", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06999", "abs": "https://arxiv.org/abs/2512.06999", "authors": ["Zihao Wang", "Ruibin Yuan", "Ziqi Geng", "Hengjia Li", "Xingwei Qu", "Xinyi Li", "Songye Chen", "Haoying Fu", "Roger B. Dannenberg", "Kejun Zhang"], "title": "Singing Timbre Popularity Assessment Based on Multimodal Large Foundation Model", "comment": "Accepted to ACMMM 2025 oral", "summary": "Automated singing assessment is crucial for education and entertainment. However, existing systems face two fundamental limitations: reliance on reference tracks, which stifles creative expression, and the simplification of complex performances into non-diagnostic scores based solely on pitch and rhythm. We advocate for a shift from discriminative to descriptive evaluation, creating a complete ecosystem for reference-free, multi-dimensional assessment. First, we introduce Sing-MD, a large-scale dataset annotated by experts across four dimensions: breath control, timbre quality, emotional expression, and vocal technique. Our analysis reveals significant annotation inconsistencies among experts, challenging the validity of traditional accuracy-based metrics. Second, addressing the memory limitations of Multimodal Large Language Models (MLLMs) in analyzing full-length songs, we propose VocalVerse. This efficient hybrid architecture leverages a lightweight acoustic encoder to model global performance features and long-term dependencies. Third, to address automated metric shortcomings, we establish the H-TPR (Human-in-the-loop Tiered Perceptual Ranking) benchmark, which evaluates a model's ability to generate perceptually valid rankings rather than predicting noisy ground-truth scores."}
{"id": "2512.07054", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2512.07054", "abs": "https://arxiv.org/abs/2512.07054", "authors": ["Yiyan Ma", "Bo Ai", "Jingli Li", "Weijie Yuan", "Boxiang He", "Weiyang Feng", "Zhengyu Zhang", "Qingqing Cheng", "Zhangdui Zhong"], "title": "Integrated Sensing, Communication, Computing, and Control Meets UAV Swarms in 6G", "comment": null, "summary": "To develop the low-altitude economy, the establishment of the low-altitude wireless network (LAWN) is the first priority. As the number of unmanned aerial vehicles (UAVs) increases, how to support the reliable flying and effective functioning of UAV swarms is challenging. Recently, the integrated sensing, communication, computing, and control (ISCCC) strategy was designed, which could act as effective physical {\\it reflex arc} links in the intelligent LAWN system. Thus, in this article, we outline the challenges and opportunities when ISCCC meets UAV swarm in LAWN in 6G. First, we propose a three-layer ISCCC structure for the UAV swarm, which is categorized according to the UAV swarm's requirements, i.e., flying, self-organizing, and functioning. Second, for different scenarios, we study the basic problem, promising technologies, and challenges to design ISCCC for UAV swarms. Third, through a case study that minimizes the flying trajectory error of the UAV swarm based on the ISCCC principle, we show that ISCCC is promising to simultaneously improve the reliability and efficiency of LAWN via jointly designing four components. Finally, we discuss the promising directions for the ISCCC-based UAV swarm in LAWN."}
{"id": "2512.07005", "categories": ["cs.SD", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.07005", "abs": "https://arxiv.org/abs/2512.07005", "authors": ["Zihao Wang", "Ruibin Yuan", "Ziqi Geng", "Hengjia Li", "Xingwei Qu", "Xinyi Li", "Songye Chen", "Haoying Fu", "Roger B. Dannenberg", "Kejun Zhang"], "title": "Multi-Accent Mandarin Dry-Vocal Singing Dataset: Benchmark for Singing Accent Recognition", "comment": "Accepted by ACMMM 2025", "summary": "Singing accent research is underexplored compared to speech accent studies, primarily due to the scarcity of suitable datasets. Existing singing datasets often suffer from detail loss, frequently resulting from the vocal-instrumental separation process. Additionally, they often lack regional accent annotations. To address this, we introduce the Multi-Accent Mandarin Dry-Vocal Singing Dataset (MADVSD). MADVSD comprises over 670 hours of dry vocal recordings from 4,206 native Mandarin speakers across nine distinct Chinese regions. In addition to each participant recording audio of three popular songs in their native accent, they also recorded phonetic exercises covering all Mandarin vowels and a full octave range. We validated MADVSD through benchmark experiments in singing accent recognition, demonstrating its utility for evaluating state-of-the-art speech models in singing contexts. Furthermore, we explored dialectal influences on singing accent and analyzed the role of vowels in accentual variations, leveraging MADVSD's unique phonetic exercises."}
{"id": "2512.07097", "categories": ["eess.SP", "cs.SE"], "pdf": "https://arxiv.org/pdf/2512.07097", "abs": "https://arxiv.org/abs/2512.07097", "authors": ["David Wang", "Jiale Zhang", "Pei Zhang"], "title": "TagLabel: RFID Based Orientation and Material Sensing for Automated Package Inspection", "comment": "10 pages, 17 figures, 5 tables", "summary": "Modern logistics systems face increasing difficulty in identifying counterfeit products, fraudulent returns, and hazardous items concealed within packages, yet current package screening methods remain too slow, expensive, and impractical for widespread use. This paper presents TagLabel, an RFID based system that determines both the orientation and contents of packages using low cost passive UHF tags. By analyzing how materials change RSSI and phase, the system identifies the contents of a package without opening it. Using orientation inferred from phase differences, tag occlusion, and antenna gain patterns, the system selects the tag with the greatest occlusion for accurate material sensing. We evaluate two and three tag configurations, and show that both can deliver high orientation and material sensing performance through the use of machine learning classifiers, even in realistic RF environments. When combined into a unified pipeline, TagLabel achieves more than 80 percent accuracy across all package orientations. Because it requires only standard RFID hardware and offers fast scanning times, this approach provides a practical way to enhance package inspection and improve automation in logistics operations."}
{"id": "2512.07168", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2512.07168", "abs": "https://arxiv.org/abs/2512.07168", "authors": ["Georgios Ioannides", "Christos Constantinou", "Aman Chadha", "Aaron Elkins", "Linsey Pang", "Ravid Shwartz-Ziv", "Yann LeCun"], "title": "JEPA as a Neural Tokenizer: Learning Robust Speech Representations with Density Adaptive Attention", "comment": "UniReps: Unifying Representations in Neural Models (NeurIPS 2025 Workshop)", "summary": "We introduce a two-stage self-supervised framework that combines the Joint-Embedding Predictive Architecture (JEPA) with a Density Adaptive Attention Mechanism (DAAM) for learning robust speech representations. Stage~1 uses JEPA with DAAM to learn semantic audio features via masked prediction in latent space, fully decoupled from waveform reconstruction. Stage~2 leverages these representations for efficient tokenization using Finite Scalar Quantization (FSQ) and a mixed-radix packing scheme, followed by high-fidelity waveform reconstruction with a HiFi-GAN decoder. By integrating Gaussian mixture-based density-adaptive gating into the JEPA encoder, the model performs adaptive temporal feature selection and discovers hierarchical speech structure at a low frame rate of 2.5~Hz. The resulting tokens (47.5 tokens/sec) provide a reversible, highly compressed, and language-model-friendly representation that is competitive with, and often more efficient than, existing neural audio codecs."}
{"id": "2512.07167", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2512.07167", "abs": "https://arxiv.org/abs/2512.07167", "authors": ["Samyadip Sarkar", "Arunashish Datta", "Sihun Kim", "Amir Mokhtarpour", "Shweta Katakdhond", "Shovan Maity", "Shreyas Sen"], "title": "Near Field Electric (NFE): Energy-efficient, High-speed Communication at Decimeter-range", "comment": "10 pages, 14 Figures", "summary": "Near-field technologies enable contactless payments, building access, automotive keyless entry, and supply chain tracking. Existing approaches face fundamental trade-offs: magnetic-based methods (NFC/NFMI) achieve low power but are limited to sub-megabit rates, while millimeter-wave techniques provide gigabits/sec connectivity at higher power consumption and only centimeter-scale ranges. We demonstrate that near-field electric (NFE) communication breaks this trade-off via capacitive coupling enabled by confined electric fields. NFE simultaneously achieves ultra-low power ($<$1 mW per transceiver), high-speed data throughput ($>$3 Mbps), and configurable decimeter-range (5-30 cm) capabilities previously considered mutually exclusive. Systematic measurements across multiple orientations and configurations show NFE can support decimeter communication coverage. The power consumption of 0.4 mW at the transmitter (Tx) and 0.6 mW at the receiver (Rx), when combined is up to $\\sim$24$\\times$ lower than NFC and $\\sim$3$\\times$ lower than NFMI while achieving significantly higher data rates, and a couple of orders of magnitude lower power than mm-wave based technique. Testing with symmetrical electrodes across eight orientations validated consistent performance and robustness for practical deployments. Extended-range experiments achieved stable 2 Mbps throughput at 3.5 meters using conductive media, demonstrating NFE's unique ability to leverage environmental conductors. Optimized device design can facilitate achieving an extended range for Body-assisted NFE up to 1 m. Results establish NFE as foundational for next-generation wireless applications where security, low power, and throughput converge, enabling dense IoT deployments, secure payment systems, and high-speed device-to-device communication previously limited by the power-performance trade-off."}
{"id": "2512.07352", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2512.07352", "abs": "https://arxiv.org/abs/2512.07352", "authors": ["Xueping Zhang", "Zhenshan Zhang", "Yechen Wang", "Linxi Li", "Liwei Jin", "Ming Li"], "title": "MultiAPI Spoof: A Multi-API Dataset and Local-Attention Network for Speech Anti-spoofing Detection", "comment": null, "summary": "Existing speech anti-spoofing benchmarks rely on a narrow set of public models, creating a substantial gap from real-world scenarios in which commercial systems employ diverse, often proprietary APIs. To address this issue, we introduce MultiAPI Spoof, a multi-API audio anti-spoofing dataset comprising about 230 hours of synthetic speech generated by 30 distinct APIs, including commercial services, open-source models, and online platforms. Based on this dataset, we define the API tracing task, enabling fine-grained attribution of spoofed audio to its generation source. We further propose Nes2Net-LA, a local-attention enhanced variant of Nes2Net that improves local context modeling and fine-grained spoofing feature extraction. Experiments show that Nes2Net-LA achieves state-of-the-art performance and offers superior robustness, particularly under diverse and unseen spoofing conditions. Code \\footnote{https://github.com/XuepingZhang/MultiAPI-Spoof} and dataset \\footnote{https://xuepingzhang.github.io/MultiAPI-Spoof-Dataset/} have released."}
{"id": "2512.07267", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.07267", "abs": "https://arxiv.org/abs/2512.07267", "authors": ["Samuel Rey", "Gonzalo Mateos"], "title": "Non-negative DAG Learning from Time-Series Data", "comment": null, "summary": "This work aims to learn the directed acyclic graph (DAG) that captures the instantaneous dependencies underlying a multivariate time series. The observed data follow a linear structural vector autoregressive model (SVARM) with both instantaneous and time-lagged dependencies, where the instantaneous structure is modeled by a DAG to reflect potential causal relationships. While recent continuous relaxation approaches impose acyclicity through smooth constraint functions involving powers of the adjacency matrix, they lead to non-convex optimization problems that are challenging to solve. In contrast, we assume that the underlying DAG has only non-negative edge weights, and leverage this additional structure to impose acyclicity via a convex constraint. This enables us to cast the problem of non-negative DAG recovery from multivariate time-series data as a convex optimization problem in abstract form, which we solve using the method of multipliers. Crucially, the convex formulation guarantees global optimality of the solution. Finally, we assess the performance of the proposed method on synthetic time-series data, where it outperforms existing alternatives."}
{"id": "2512.07627", "categories": ["cs.SD", "cs.AI", "cs.SC"], "pdf": "https://arxiv.org/pdf/2512.07627", "abs": "https://arxiv.org/abs/2512.07627", "authors": ["Maximos Kaliakatsos-Papakostas", "Konstantinos Soiledis", "Theodoros Tsamis", "Dimos Makris", "Vassilis Katsouros", "Emilios Cambouropoulos"], "title": "Incorporating Structure and Chord Constraints in Symbolic Transformer-based Melodic Harmonization", "comment": "Proceedings of the 6th Conference on AI Music Creativity (AIMC 2025), Brussels, Belgium, September 10th-12th", "summary": "Transformer architectures offer significant advantages regarding the generation of symbolic music; their capabilities for incorporating user preferences toward what they generate is being studied under many aspects. This paper studies the inclusion of predefined chord constraints in melodic harmonization, i.e., where a desired chord at a specific location is provided along with the melody as inputs and the autoregressive transformer model needs to incorporate the chord in the harmonization that it generates. The peculiarities of involving such constraints is discussed and an algorithm is proposed for tackling this task. This algorithm is called B* and it combines aspects of beam search and A* along with backtracking to force pretrained transformers to satisfy the chord constraints, at the correct onset position within the correct bar. The algorithm is brute-force and has exponential complexity in the worst case; however, this paper is a first attempt to highlight the difficulties of the problem and proposes an algorithm that offers many possibilities for improvements since it accommodates the involvement of heuristics."}
{"id": "2512.07279", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.07279", "abs": "https://arxiv.org/abs/2512.07279", "authors": ["Shreyas Jayant Grampurohit", "Satish Mulleti", "Ajit Rajwade"], "title": "Verifiable Deep Quantitative Group Testing", "comment": "11 pages, 2 figures, 3 tables", "summary": "We present a neural network-based framework for solving the quantitative group testing (QGT) problem that achieves both high decoding accuracy and structural verifiability. In QGT, the objective is to identify a small subset of defective items among $N$ candidates using only $M \\ll N$ pooled tests, each reporting the number of defectives in the tested subset. We train a multi-layer perceptron to map noisy measurement vectors to binary defect indicators, achieving accurate and robust recovery even under sparse, bounded perturbations. Beyond accuracy, we show that the trained network implicitly learns the underlying pooling structure that links items to tests, allowing this structure to be recovered directly from the network's Jacobian. This indicates that the model does not merely memorize training patterns but internalizes the true combinatorial relationships governing QGT. Our findings reveal that standard feedforward architectures can learn verifiable inverse mappings in structured combinatorial recovery problems."}
{"id": "2512.05994", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2512.05994", "abs": "https://arxiv.org/abs/2512.05994", "authors": ["Rohan Sharma", "Dancheng Liu", "Jingchen Sun", "Shijie Zhou", "Jiayu Qin", "Jinjun Xiong", "Changyou Chen"], "title": "KidSpeak: A General Multi-purpose LLM for Kids' Speech Recognition and Screening", "comment": null, "summary": "With the rapid advancement of conversational and diffusion-based AI, there is a growing adoption of AI in educational services, ranging from grading and assessment tools to personalized learning systems that provide targeted support for students. However, this adaptability has yet to fully extend to the domain of children's speech, where existing models often fail due to their reliance on datasets designed for clear, articulate adult speech. Children, particularly those in early developmental stages or with speech and language pathologies, present unique challenges that current AI models and datasets are ill-equipped to handle. To address this, we introduce KidSpeak, a multi-task speech-enhanced Foundation Model capable of both generative and discriminative tasks specifically tailored to children's speech patterns. Our framework employs a two-stage training process that incorporates phonetic knowledge into the speech encoder, achieving an average accuracy of 87% across four separate tasks. Furthermore, recognizing the limitations of scalable human annotation and existing speech alignment tools, we propose the Flexible and Automatic Speech Aligner (FASA) and leverage the method to construct high quality datasets for training and evaluation. This novel alignment tool significantly improves the quality of aligned children's speech from noisy data, enhancing data quality by 13.6x compared to human annotations, as demonstrated on the CHILDES dataset. To the best of our knowledge, KidSpeak and FASA represent the first comprehensive solution designed for speech and language therapy in children, offering both a multi-purpose speech LLM and a robust alignment tool."}
{"id": "2512.07330", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2512.07330", "abs": "https://arxiv.org/abs/2512.07330", "authors": ["Xuancheng Zhu", "Zhiwen Zhou", "Zhenjun Dong", "Yong Zeng"], "title": "Cost-Effective XL-MIMO Communication with Cylinder Directly-Connected Antenna Array", "comment": null, "summary": "Extremely large-scale multi-input multi-output (XL-MIMO) is a promising technology for the sixth generation (6G) wireless networks, thanks to its superior spatial resolution and beamforming gains. In order to realize XL-MIMO costeffectively, an innovative ray antenna array (RAA) architecture with directly-connected uniform linear array (ULA) was recently proposed, which achieves flexible beamforming without relying on traditional analog phase shifters or digital beamforming. However, RAA suffers from the signal blockage issue since its ray-configured ULAs are placed in the same plane. To address this issue, this paper proposes a novel antenna array architecture termed cylinder directly-connected antenna array (DCAA), which is achieved via multiple simple uniform circular array (sUCA) with carefully designed orientations in a layered three-dimensional structure. The so-called sUCA partitions the uniform circular array (UCA) into two sub-arrays where each sub-array has all antenna elements directly connected to achieve a desired beam direction corresponding to the sub-array's physical orientation, thus achieving full spatial coverage. Compared with the conventional ULA architecture with hybrid analog/digital beamforming (HBF), the proposed cylinder DCAA can achieve uniform spatial resolution, enhanced communication rate and lower hardware costs. Simulation results are provided to validate the promised gains of cylinder DCAA, demonstrating its great potential for high-frequency systems such as millimeter wave (mmWave) and Terahertz (THz) systems."}
{"id": "2512.06304", "categories": ["eess.AS", "cs.AI", "cs.CR", "cs.SD"], "pdf": "https://arxiv.org/pdf/2512.06304", "abs": "https://arxiv.org/abs/2512.06304", "authors": ["Xining Song", "Zhihua Wei", "Rui Wang", "Haixiao Hu", "Yanxiang Chen", "Meng Han"], "title": "Degrading Voice: A Comprehensive Overview of Robust Voice Conversion Through Input Manipulation", "comment": null, "summary": "Identity, accent, style, and emotions are essential components of human speech. Voice conversion (VC) techniques process the speech signals of two input speakers and other modalities of auxiliary information such as prompts and emotion tags. It changes para-linguistic features from one to another, while maintaining linguistic contents. Recently, VC models have made rapid advancements in both generation quality and personalization capabilities. These developments have attracted considerable attention for diverse applications, including privacy preservation, voice-print reproduction for the deceased, and dysarthric speech recovery. However, these models only learn non-robust features due to the clean training data. Subsequently, it results in unsatisfactory performances when dealing with degraded input speech in real-world scenarios, including additional noise, reverberation, adversarial attacks, or even minor perturbation. Hence, it demands robust deployments, especially in real-world settings. Although latest researches attempt to find potential attacks and countermeasures for VC systems, there remains a significant gap in the comprehensive understanding of how robust the VC model is under input manipulation. here also raises many questions: For instance, to what extent do different forms of input degradation attacks alter the expected output of VC models? Is there potential for optimizing these attack and defense strategies? To answer these questions, we classify existing attack and defense methods from the perspective of input manipulation and evaluate the impact of degraded input speech across four dimensions, including intelligibility, naturalness, timbre similarity, and subjective perception. Finally, we outline open issues and future directions."}
{"id": "2512.07361", "categories": ["eess.SP", "cs.NE"], "pdf": "https://arxiv.org/pdf/2512.07361", "abs": "https://arxiv.org/abs/2512.07361", "authors": ["Giuseppe Leo", "Paolo Gibertini", "Irem Ilter", "Erika Covi", "Ole Richter", "Elisabetta Chicca"], "title": "An Asynchronous Mixed-Signal Resonate-and-Fire Neuron", "comment": null, "summary": "Analog computing at the edge is an emerging strategy to limit data storage and transmission requirements, as well as energy consumption, and its practical implementation is in its initial stages of development. Translating properties of biological neurons into hardware offers a pathway towards low-power, real-time edge processing. Specifically, resonator neurons offer selectivity to specific frequencies as a potential solution for temporal signal processing. Here, we show a fabricated Complementary Metal-Oxide-Semiconductor (CMOS) mixed-signal Resonate-and-Fire (R&F) neuron circuit implementation that emulates the behavior of these neural cells responsible for controlling oscillations within the central nervous system. We integrate the design with asynchronous handshake capabilities, perform comprehensive variability analyses, and characterize its frequency detection functionality. Our results demonstrate the feasibility of large-scale integration within neuromorphic systems, thereby advancing the exploitation of bio-inspired circuits for efficient edge temporal signal processing."}
{"id": "2512.07226", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2512.07226", "abs": "https://arxiv.org/abs/2512.07226", "authors": ["Runwu Shi", "Chang Li", "Jiang Wang", "Rui Zhang", "Nabeela Khan", "Benjamin Yen", "Takeshi Ashizawa", "Kazuhiro Nakadai"], "title": "Unsupervised Single-Channel Audio Separation with Diffusion Source Priors", "comment": "15 pages, 31 figures, accepted by The 40th Annual AAAI Conference on Artificial Intelligence (AAAI 2026)", "summary": "Single-channel audio separation aims to separate individual sources from a single-channel mixture. Most existing methods rely on supervised learning with synthetically generated paired data. However, obtaining high-quality paired data in real-world scenarios is often difficult. This data scarcity can degrade model performance under unseen conditions and limit generalization ability. To this end, in this work, we approach this problem from an unsupervised perspective, framing it as a probabilistic inverse problem. Our method requires only diffusion priors trained on individual sources. Separation is then achieved by iteratively guiding an initial state toward the solution through reconstruction guidance. Importantly, we introduce an advanced inverse problem solver specifically designed for separation, which mitigates gradient conflicts caused by interference between the diffusion prior and reconstruction guidance during inverse denoising. This design ensures high-quality and balanced separation performance across individual sources. Additionally, we find that initializing the denoising process with an augmented mixture instead of pure Gaussian noise provides an informative starting point that significantly improves the final performance. To further enhance audio prior modeling, we design a novel time-frequency attention-based network architecture that demonstrates strong audio modeling capability. Collectively, these improvements lead to significant performance gains, as validated across speech-sound event, sound event, and speech separation tasks."}
{"id": "2512.07411", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2512.07411", "abs": "https://arxiv.org/abs/2512.07411", "authors": ["Zawar Hussain", "Faran Awais Butt", "Ali Hussein Muqaibel", "Saleh Ahmed Alawsh", "Ijaz Haider Naqvi"], "title": "Impact of RIS Orientation on Throughput in UAV-Assisted Wireless Systems", "comment": null, "summary": "This paper investigates the impact of Reconfigurable Intelligent Surface (RIS) orientation on the throughput performance of Unmanned Aerial Vehicle (UAV)-assisted wireless communication systems. Specifically, we study how physical rotation of the RIS, through controlled azimuth and elevation adjustments, influences the effective channel and data rate. A UAV-mounted RIS enables directional alignment to serve ground users in scenarios where the direct Base Station (BS)-to-user path is blocked. Using the SimRIS channel simulator, we analyze the system under various rotation angles and present performance heatmaps that highlight optimal RIS orientations. The study shows that RIS alignment has a substantial effect on achievable rates, thereby motivating orientation-aware optimization in practical deployments."}
{"id": "2512.07512", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2512.07512", "abs": "https://arxiv.org/abs/2512.07512", "authors": ["Zawar Hussain", "Arslan Majal", "Aamir Hussain Chughtai", "Talha Nadeem"], "title": "Dictionary-Based Contrastive Learning for GNSS Jamming Detection", "comment": null, "summary": "Global Navigation Satellite System (GNSS) signals are fundamental in applications across navigation, transportation, and industrial networks. However, their extremely low received power makes them highly vulnerable to radio-frequency interference (RFI) and intentional jamming. Modern data-driven methods offer powerful representational power for such applications, however real-time and reliable jamming detection on resource-limited embedded receivers remains a key challenge due to the high computational and memory demands of the conventional learning paradigm. To address these challenges, this work presents a dictionary-based contrastive learning (DBCL) framework for GNSS jamming detection that integrates transfer learning, contrastive representation learning, and model compression techniques. The framework combines tuned contrastive and dictionary-based loss functions to enhance feature separability under low-data conditions and applies structured pruning and knowledge distillation to reduce model complexity while maintaining high accuracy. Extensive evaluation across varying data regimes demonstrate that the proposed algorithm consistently outperforms modern CNN, MobileViT, and ResNet-18 architectures. The framework achieves a substantial reduction in memory footprint and inference latency, confirming its suitability for real-time, low-power GNSS interference detection on embedded platforms."}
{"id": "2512.07649", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2512.07649", "abs": "https://arxiv.org/abs/2512.07649", "authors": ["Hao Jiang", "Chongjun Ouyang", "Zhaolin Wang", "Yuanwei Liu", "Arumugam Nallanathan", "Zhiguo Ding", "Robert Schober"], "title": "Segmented Waveguide-Enabled Pinching-Antenna Systems (SWANs) for ISAC", "comment": "submitted to IEEE journal for possible publication", "summary": "A segmented waveguide-enabled pinching-antenna system (SWAN)-assisted integrated sensing and communications (ISAC) framework is proposed. Unlike conventional pinching antenna systems (PASS), which use a single long waveguide, SWAN divides the waveguide into multiple short segments, each with a dedicated feed point. Thanks to the segmented structure, SWAN enhances sensing performance by significantly simplifying the reception model and reducing the in-waveguide propagation loss. To balance performance and complexity, three segment controlling protocols are proposed for the transceivers, namely i) \\emph{segment selection} to select a single segment for signal transceiving, ii) \\emph{segment aggregation} to aggregate signals from all segments using a single RF chain, and iii) \\emph{segment multiplexing} to jointly process the signals from all segments using individual RF chains. The theoretical sensing performance limit is first analyzed for different protocols, unveiling how the sensing performance gain of SWAN scales with the number of segments. Based on this performance limit, the Pareto fronts of sensing and communication performance are characterized for the simple one-user one-target case, which is then extended to the general multi-user single-target case based on time-division multiple access (TDMA). Numerical results are presented to verify the correctness of the derivations and the effectiveness of the proposed algorithms, which jointly confirm the advantages of SWAN-assisted ISAC."}

{"id": "2512.17932", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2512.17932", "abs": "https://arxiv.org/abs/2512.17932", "authors": ["Yang Xiao"], "title": "Continual Learning for Acoustic Event Classification", "comment": "Master project report", "summary": "Continuously learning new classes without catastrophic forgetting is a challenging problem for on-device acoustic event classification given the restrictions on computation resources (e.g., model size, running memory). To alleviate such an issue, we propose two novel diversity-aware incremental learning method for Spoken Keyword Spotting and Environmental Sound Classification. Our method selects the historical data for the training by measuring the per-sample classification uncertainty. For the Spoken Keyword Spotting application, the proposed RK approach introduces a diversity-aware sampler to select a diverse set from historical and incoming keywords by calculating classification uncertainty. As a result, the RK approach can incrementally learn new tasks without forgetting prior knowledge. Besides, the RK approach also proposes data augmentation and knowledge distillation loss function for efficient memory management on the edge device. For the Environmental Sound Classification application, we measure the uncertainty by observing how the classification probability of data fluctuates against the parallel perturbations added to the classifier embedding. In this way, the computation cost can be significantly reduced compared with adding perturbation to the raw data. Experimental results show that the proposed RK approach achieves 4.2% absolute improvement in terms of average accuracy over the best baseline on Google Speech Command dataset with less required memory. Experimental results on the DCASE 2019 Task 1 and ESC-50 dataset show that our proposed method outperforms baseline continual learning methods on classification accuracy and computational efficiency, indicating our method can efficiently and incrementally learn new classes without the catastrophic forgetting problem for on-device environmental sound classification"}
{"id": "2512.17937", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2512.17937", "abs": "https://arxiv.org/abs/2512.17937", "authors": ["Ram C. M. C. Shekar", "Iván López-Espejo"], "title": "LIWhiz: A Non-Intrusive Lyric Intelligibility Prediction System for the Cadenza Challenge", "comment": null, "summary": "We present LIWhiz, a non-intrusive lyric intelligibility prediction system submitted to the ICASSP 2026 Cadenza Challenge. LIWhiz leverages Whisper for robust feature extraction and a trainable back-end for score prediction. Tested on the Cadenza Lyric Intelligibility Prediction (CLIP) evaluation set, LIWhiz achieves a 22.4% relative root mean squared error reduction over the STOI-based baseline, yielding a substantial improvement in normalized cross-correlation."}
{"id": "2512.18099", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2512.18099", "abs": "https://arxiv.org/abs/2512.18099", "authors": ["Bowen Shi", "Andros Tjandra", "John Hoffman", "Helin Wang", "Yi-Chiao Wu", "Luya Gao", "Julius Richter", "Matt Le", "Apoorv Vyas", "Sanyuan Chen", "Christoph Feichtenhofer", "Piotr Dollár", "Wei-Ning Hsu", "Ann Lee"], "title": "SAM Audio: Segment Anything in Audio", "comment": null, "summary": "General audio source separation is a key capability for multimodal AI systems that can perceive and reason about sound. Despite substantial progress in recent years, existing separation models are either domain-specific, designed for fixed categories such as speech or music, or limited in controllability, supporting only a single prompting modality such as text. In this work, we present SAM Audio, a foundation model for general audio separation that unifies text, visual, and temporal span prompting within a single framework. Built on a diffusion transformer architecture, SAM Audio is trained with flow matching on large-scale audio data spanning speech, music, and general sounds, and can flexibly separate target sources described by language, visual masks, or temporal spans. The model achieves state-of-the-art performance across a diverse suite of benchmarks, including general sound, speech, music, and musical instrument separation in both in-the-wild and professionally produced audios, substantially outperforming prior general-purpose and specialized systems. Furthermore, we introduce a new real-world separation benchmark with human-labeled multimodal prompts and a reference-free evaluation model that correlates strongly with human judgment."}
{"id": "2512.18263", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.18263", "abs": "https://arxiv.org/abs/2512.18263", "authors": ["Haolong Zheng", "Yekaterina Yegorova", "Mark Hasegawa-Johnson"], "title": "TICL+: A Case Study On Speech In-Context Learning for Children's Speech Recognition", "comment": "Published at IEEE ASRU 2025 Satellite Workshop-AI for Children's Speech and Language", "summary": "Children's speech recognition remains challenging due to substantial acoustic and linguistic variability, limited labeled data, and significant differences from adult speech. Speech foundation models can address these challenges through Speech In-Context Learning (SICL), allowing adaptation to new domains without fine-tuning. However, the effectiveness of SICL depends on how in-context examples are selected. We extend an existing retrieval-based method, Text-Embedding KNN for SICL (TICL), introducing an acoustic reranking step to create TICL+. This extension prioritizes examples that are both semantically and acoustically aligned with the test input. Experiments on four children's speech corpora show that TICL+ achieves up to a 53.3% relative word error rate reduction over zero-shot performance and 37.6% over baseline TICL, highlighting the value of combining semantic and acoustic information for robust, scalable ASR in children's speech."}
{"id": "2512.17919", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2512.17919", "abs": "https://arxiv.org/abs/2512.17919", "authors": ["Marie-Dominique van Damme", "Yann Méneroux", "Ana-Maria Olteanu-Raimond"], "title": "An extensive analysis and calibration of the Modular Aggregation Algorithm across three categories of for GNSS trajectories data sources", "comment": null, "summary": "This technical report aims to complement the conference paper (https://doi.org/10.1145/3678717.3691325) by providing additional experiments or further details that could not be included in the paper."}
{"id": "2512.17935", "categories": ["cs.SD", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2512.17935", "abs": "https://arxiv.org/abs/2512.17935", "authors": ["Mason Youngblood"], "title": "chatter: a Python library for applying information theory and AI/ML models to animal communication", "comment": null, "summary": "The study of animal communication often involves categorizing units into types (e.g. syllables in songbirds, or notes in humpback whales). While this approach is useful in many cases, it necessarily flattens the complexity and nuance present in real communication systems. chatter is a new Python library for analyzing animal communication in continuous latent space using information theory and modern machine learning techniques. It is taxonomically agnostic, and has been tested with the vocalizations of birds, bats, whales, and primates. By leveraging a variety of different architectures, including variational autoencoders and vision transformers, chatter represents vocal sequences as trajectories in high-dimensional latent space, bypassing the need for manual or automatic categorization of units. The library provides an end-to-end workflow -- from preprocessing and segmentation to model training and feature extraction -- that enables researchers to quantify the complexity, predictability, similarity, and novelty of vocal sequences."}
{"id": "2512.18286", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2512.18286", "abs": "https://arxiv.org/abs/2512.18286", "authors": ["Shuai Wang", "Yanmin Qian", "Kai Yu"], "title": "What Does the Speaker Embedding Encode?", "comment": "This paper was accepted by Interspeech 2017. However, no public version is currently available, as the original link provided by ISCA is no longer accessible. The version uploaded herein has undergone automatic English polishing using GPT (Expanded for better calarity)", "summary": "Developing a good speaker embedding has received tremendous interest in the speech community, with representations such as i-vector and d-vector demonstrating remarkable performance across various tasks. Despite their widespread adoption, a fundamental question remains largely unexplored: what properties are actually encoded in these embeddings? To address this gap, we conduct a comprehensive analysis of three prominent speaker embedding methods: i-vector, d-vector, and RNN/LSTM-based sequence-vector (s-vector). Through carefully designed classification tasks, we systematically investigate their encoding capabilities across multiple dimensions, including speaker identity, gender, speaking rate, text content, word order, and channel information. Our analysis reveals distinct strengths and limitations of each embedding type: i-vector excels at speaker discrimination but encodes limited sequential information; s-vector captures text content and word order effectively but struggles with speaker identity; d-vector shows balanced performance but loses sequential information through averaging. Based on these insights, we propose a novel multi-task learning framework that integrates i-vector and s-vector, resulting in a new speaker embedding (i-s-vector) that combines their complementary advantages. Experimental results on RSR2015 demonstrate that the proposed i-s-vector achieves more than 50% EER reduction compared to the i-vector baseline on content mismatch trials, validating the effectiveness of our approach."}
{"id": "2512.17928", "categories": ["eess.SP", "cs.AI", "cs.IT", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.17928", "abs": "https://arxiv.org/abs/2512.17928", "authors": ["Dongdong Yang", "Bin Li", "Jiguang He", "Yicheng Yan", "Xiaoyu Zhang", "Chongwen Huang"], "title": "Efficient Beamforming Optimization for STAR-RIS-Assisted Communications: A Gradient-Based Meta Learning Approach", "comment": null, "summary": "Simultaneously transmitting and reflecting reconfigurable intelligent surface (STAR-RIS) has emerged as a promising technology to realize full-space coverage and boost spectral efficiency in next-generation wireless networks. Yet, the joint design of the base station precoding matrix as well as the STAR-RIS transmission and reflection coefficient matrices leads to a high-dimensional, strongly nonconvex, and NP-hard optimization problem. Conventional alternating optimization (AO) schemes typically involve repeated large-scale matrix inversion operations, resulting in high computational complexity and poor scalability, while existing deep learning approaches often rely on expensive pre-training and large network models. In this paper, we develop a gradient-based meta learning (GML) framework that directly feeds optimization gradients into lightweight neural networks, thereby removing the need for pre-training and enabling fast adaptation. Specifically, we design dedicated GML-based schemes for both independent-phase and coupled-phase STAR-RIS models, effectively handling their respective amplitude and phase constraints while achieving weighted sum-rate performance very close to that of AO-based benchmarks. Extensive simulations demonstrate that, for both phase models, the proposed methods substantially reduce computational overhead, with complexity growing nearly linearly when the number of BS antennas and STAR-RIS elements grows, and yielding up to 10 times runtime speedup over AO, which confirms the scalability and practicality of the proposed GML method for large-scale STAR-RIS-assisted communications."}
{"id": "2512.17946", "categories": ["cs.SD", "cs.AI", "cs.MM"], "pdf": "https://arxiv.org/pdf/2512.17946", "abs": "https://arxiv.org/abs/2512.17946", "authors": ["Haiying Xia", "Zhongyi Huang", "Yumei Tan", "Shuxiang Song"], "title": "Let the Model Learn to Feel: Mode-Guided Tonality Injection for Symbolic Music Emotion Recognition", "comment": "Accepted by AAAI 2026", "summary": "Music emotion recognition is a key task in symbolic music understanding (SMER). Recent approaches have shown promising results by fine-tuning large-scale pre-trained models (e.g., MIDIBERT, a benchmark in symbolic music understanding) to map musical semantics to emotional labels. While these models effectively capture distributional musical semantics, they often overlook tonal structures, particularly musical modes, which play a critical role in emotional perception according to music psychology. In this paper, we investigate the representational capacity of MIDIBERT and identify its limitations in capturing mode-emotion associations. To address this issue, we propose a Mode-Guided Enhancement (MoGE) strategy that incorporates psychological insights on mode into the model. Specifically, we first conduct a mode augmentation analysis, which reveals that MIDIBERT fails to effectively encode emotion-mode correlations. We then identify the least emotion-relevant layer within MIDIBERT and introduce a Mode-guided Feature-wise linear modulation injection (MoFi) framework to inject explicit mode features, thereby enhancing the model's capability in emotional representation and inference. Extensive experiments on the EMOPIA and VGMIDI datasets demonstrate that our mode injection strategy significantly improves SMER performance, achieving accuracies of 75.2% and 59.1%, respectively. These results validate the effectiveness of mode-guided modeling in symbolic music emotion recognition."}
{"id": "2512.18371", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2512.18371", "abs": "https://arxiv.org/abs/2512.18371", "authors": ["Te Ma", "Nanjie Li", "Hao Huang", "Zhijian Ou"], "title": "Phoneme-based speech recognition driven by large language models and sampling marginalization", "comment": "Published at NCMMSC 2025, in Chinese language", "summary": "Recently, the Large Language Model-based Phoneme-to-Grapheme (LLM-P2G) method has shown excellent performance in speech recognition tasks and has become a feasible direction to replace the traditional WFST decoding method. This framework takes into account both recognition accuracy and system scalability through two-stage modeling of phoneme prediction and text generation. However, the existing LLM-P2G adopts the Top-K Marginalized (TKM) training strategy, and its candidate phoneme sequences rely on beam search generation, which has problems such as insufficient path diversity, low training efficiency, and high resource overhead. To this end, this paper proposes a sampling marginalized training strategy (Sampling-K Marginalized, SKM), which replaces beam search with random sampling to generate candidate paths, improving marginalized modeling and training efficiency. Experiments were conducted on Polish and German datasets, and the results showed that SKM further improved the model learning convergence speed and recognition performance while maintaining the complexity of the model. Comparative experiments with a speech recognition method that uses a projector combined with a large language model (SpeechLLM) also show that the SKM-driven LLM-P2G has more advantages in recognition accuracy and structural simplicity. The study verified the practical value and application potential of this method in cross-language speech recognition systems."}
{"id": "2512.18071", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2512.18071", "abs": "https://arxiv.org/abs/2512.18071", "authors": ["Meysam Ghanbari", "Mohammad Taghi Dabiri", "Mazen Hasna", "Tanvir Alam", "Khalid Qaraqe"], "title": "Deep Learning Surrogate for Fast CIR Prediction in Reactive Molecular Diffusion Advection Channels", "comment": "Conference paper, proposes a deep-learning surrogate for fast prediction of channel impulse responses in reactive molecular diffusion advection channels", "summary": "Accurate channel impulse response (CIR) modeling in molecular communication (MC) often requires solving coupled reactive diffusion-advection equations, which is computationally expensive for large parameter sweeps or design loops. We develop a deep-learning surrogate for a three-dimensional duct MC channel with reactive diffusion-advection transport and reversible ligand-receptor binding on a finite ring receiver. Using a physics-based partial differential equation (PDE)-ordinary differential equation (ODE) model, we generate a large CIR dataset across broad transport, reaction, and geometric ranges and train a neural network that maps these parameters directly to the CIR. On an independent test set, the surrogate closely matches reference CIRs both qualitatively and quantitatively: the empirical cumulative distribution function (CDF) of the normalized root mean square error (NRMSE) shows that 90% of test channels are predicted with error below 0.15, with only weak dependence on individual parameters. The surrogate therefore offers an accurate and computationally efficient replacement for repeated PDE-based CIR evaluations in MC system analysis and design."}
{"id": "2512.18162", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2512.18162", "abs": "https://arxiv.org/abs/2512.18162", "authors": ["Steven Hu", "Sophia H. Kim", "Helena H. Kim", "Hugo Mackay", "Eric J. Heller"], "title": "Influence of string register locations on vibratos among violoncellists", "comment": null, "summary": "This study analyzes how vibrato changes with finger position along the cello string. Examining 94 excerpts, we found moving the finger toward the bridge strongly increases acoustic vibrato depth ($ρ=0.6902$, $p=1.408\\cdot 10^{-14}$). However, the performer's physical finger amplitude simultaneously decreases ($ρ=-0.6391$, $p=4.172\\cdot 10^{-12}$). This shows players reduce finger motion in higher positions, but not enough to counteract the greater pitch deviation there, revealing both the presence and limits of compensatory vibrato behavior."}
{"id": "2512.18572", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2512.18572", "abs": "https://arxiv.org/abs/2512.18572", "authors": ["Riki Shimizu", "Xilin Jiang", "Nima Mesgarani"], "title": "MeanFlow-TSE: One-Step Generative Target Speaker Extraction with Mean Flow", "comment": "6 pages, 2 figures, 2 tables", "summary": "Target speaker extraction (TSE) aims to isolate a desired speaker's voice from a multi-speaker mixture using auxiliary information such as a reference utterance. Although recent advances in diffusion and flow-matching models have improved TSE performance, these methods typically require multi-step sampling, which limits their practicality in low-latency settings. In this work, we propose MeanFlow-TSE, a one-step generative TSE framework trained with mean-flow objectives, enabling fast and high-quality generation without iterative refinement. Building on the AD-FlowTSE paradigm, our method defines a flow between the background and target source that is governed by the mixing ratio (MR). Experiments on the Libri2Mix corpus show that our approach outperforms existing diffusion- and flow-matching-based TSE models in separation quality and perceptual metrics while requiring only a single inference step. These results demonstrate that mean-flow-guided one-step generation offers an effective and efficient alternative for real-time target speaker extraction. Code is available at https://github.com/rikishimizu/MeanFlow-TSE."}
{"id": "2512.18075", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2512.18075", "abs": "https://arxiv.org/abs/2512.18075", "authors": ["Mingjun Sun", "Chongjun Ouyang", "Shaochuan Wu", "Yuanwei Liu"], "title": "Robust Beamforming for Pinching-Antenna Systems", "comment": null, "summary": "Pinching-antenna system (PASS) mitigates large-scale path loss by enabling flexible placement of pinching antennas (PAs) along the dielectric waveguide. However, most existing studies assume perfect channel state information (CSI), overlooking the impact of channel uncertainty. This paper addresses this gap by proposing a robust beamforming framework for both lossy and lossless waveguides. For baseband beamforming, the lossy case yields an second-order cone programming-based solution, while the lossless case admits a closed-form solution via maximum ratio transmission. The PAs' positions in both cases are optimized through the Gauss-Seidel-based method. Numerical results validate the effectiveness of the proposed algorithm and demonstrate that PASS exhibits superior robustness against channel uncertainty compared with conventional fixed-antenna systems. Notably, its worst-case achievable rate can even exceed the fixed-antenna baseline under perfect CSI."}
{"id": "2512.18210", "categories": ["cs.SD", "eess.SP"], "pdf": "https://arxiv.org/pdf/2512.18210", "abs": "https://arxiv.org/abs/2512.18210", "authors": ["Wen Huang", "Yuchen Mao", "Yanmin Qian"], "title": "A Data-Centric Approach to Generalizable Speech Deepfake Detection", "comment": null, "summary": "Achieving robust generalization in speech deepfake detection (SDD) remains a primary challenge, as models often fail to detect unseen forgery methods. While research has focused on model-centric and algorithm-centric solutions, the impact of data composition is often underexplored. This paper proposes a data-centric approach, analyzing the SDD data landscape from two practical perspectives: constructing a single dataset and aggregating multiple datasets. To address the first perspective, we conduct a large-scale empirical study to characterize the data scaling laws for SDD, quantifying the impact of source and generator diversity. To address the second, we propose the Diversity-Optimized Sampling Strategy (DOSS), a principled framework for mixing heterogeneous data with two implementations: DOSS-Select (pruning) and DOSS-Weight (re-weighting). Our experiments show that DOSS-Select outperforms the naive aggregation baseline while using only 3% of the total available data. Furthermore, our final model, trained on a 12k-hour curated data pool using the optimal DOSS-Weight strategy, achieves state-of-the-art performance, outperforming large-scale baselines with greater data and model efficiency on both public benchmarks and a new challenge set of various commercial APIs."}
{"id": "2512.18967", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2512.18967", "abs": "https://arxiv.org/abs/2512.18967", "authors": ["Jian You", "Xiangfeng Li", "Erwan Zerhouni"], "title": "Enhancing Fully Formatted End-to-End Speech Recognition with Knowledge Distillation via Multi-Codebook Vector Quantization", "comment": "Accepted to ASRU 2025", "summary": "Conventional automatic speech recognition (ASR) models typically produce outputs as normalized texts lacking punctuation and capitalization, necessitating post-processing models to enhance readability. This approach, however, introduces additional complexity and latency due to the cascaded system design. In response to this challenge, there is a growing trend to develop end-to-end (E2E) ASR models capable of directly predicting punctuation and capitalization, though this area remains underexplored. In this paper, we propose an enhanced fully formatted E2E ASR model that leverages knowledge distillation (KD) through multi-codebook vector quantization (MVQ). Experimental results demonstrate that our model significantly outperforms previous works in word error rate (WER) both with and without punctuation and capitalization, and in punctuation error rate (PER). Evaluations on the LibriSpeech-PC test-clean and test-other subsets show that our model achieves state-of-the-art results."}
{"id": "2512.18087", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2512.18087", "abs": "https://arxiv.org/abs/2512.18087", "authors": ["Meysam Ghanbari", "Mohammad Taghi Dabiri", "Rula Ammuri", "Mazen Hasna", "Khalid Qaraqe"], "title": "AI Assisted Next Gen Outdoor Optical Networks: Camera Sensing for Monitoring and User Localization", "comment": null, "summary": "We consider outdoor optical access points (OAPs), which, enabled by recent advances in metasurface technology, have attracted growing interest. While OAPs promise high data rates and strong physical-layer security, practical deployments still expose vulnerabilities and misuse patterns that necessitate a dedicated monitoring layer - the focus of this work. We therefore propose a user positioning and monitoring system that infers locations from spatial intensity measurements on a photodetector (PD) array. Specifically, our hybrid approach couples an optics-informed forward model and sparse, model-based inversion with a lightweight data-driven calibration stage, yielding high accuracy at low computational cost. This design preserves the interpretability and stability of model-based reconstruction while leveraging learning to absorb residual nonidealities and device-specific distortions. Under identical hardware and training conditions (both with 5 x 10^5 samples), the hybrid method attains consistently lower mean-squared error than a generic deep-learning baseline while using substantially less training time and compute. Accuracy improves with array resolution and saturates around 60 x 60-80 x 80, indicating a favorable accuracy-complexity trade-off for real-time deployment. The resulting position estimates can be cross-checked with real-time network logs to enable continuous monitoring, anomaly detection (e.g., potential eavesdropping), and access control in outdoor optical access networks."}
{"id": "2512.18232", "categories": ["cs.SD", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.18232", "abs": "https://arxiv.org/abs/2512.18232", "authors": ["Stephen Ni-Hahn", "Rico Zhu", "Jerry Yin", "Yue Jiang", "Cynthia Rudin", "Simon Mak"], "title": "AutoSchA: Automatic Hierarchical Music Representations via Multi-Relational Node Isolation", "comment": null, "summary": "Hierarchical representations provide powerful and principled approaches for analyzing many musical genres. Such representations have been broadly studied in music theory, for instance via Schenkerian analysis (SchA). Hierarchical music analyses, however, are highly cost-intensive; the analysis of a single piece of music requires a great deal of time and effort from trained experts. The representation of hierarchical analyses in a computer-readable format is a further challenge. Given recent developments in hierarchical deep learning and increasing quantities of computer-readable data, there is great promise in extending such work for an automatic hierarchical representation framework. This paper thus introduces a novel approach, AutoSchA, which extends recent developments in graph neural networks (GNNs) for hierarchical music analysis. AutoSchA features three key contributions: 1) a new graph learning framework for hierarchical music representation, 2) a new graph pooling mechanism based on node isolation that directly optimizes learned pooling assignments, and 3) a state-of-the-art architecture that integrates such developments for automatic hierarchical music analysis. We show, in a suite of experiments, that AutoSchA performs comparably to human experts when analyzing Baroque fugue subjects."}
{"id": "2512.17935", "categories": ["cs.SD", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2512.17935", "abs": "https://arxiv.org/abs/2512.17935", "authors": ["Mason Youngblood"], "title": "chatter: a Python library for applying information theory and AI/ML models to animal communication", "comment": null, "summary": "The study of animal communication often involves categorizing units into types (e.g. syllables in songbirds, or notes in humpback whales). While this approach is useful in many cases, it necessarily flattens the complexity and nuance present in real communication systems. chatter is a new Python library for analyzing animal communication in continuous latent space using information theory and modern machine learning techniques. It is taxonomically agnostic, and has been tested with the vocalizations of birds, bats, whales, and primates. By leveraging a variety of different architectures, including variational autoencoders and vision transformers, chatter represents vocal sequences as trajectories in high-dimensional latent space, bypassing the need for manual or automatic categorization of units. The library provides an end-to-end workflow -- from preprocessing and segmentation to model training and feature extraction -- that enables researchers to quantify the complexity, predictability, similarity, and novelty of vocal sequences."}
{"id": "2512.18097", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2512.18097", "abs": "https://arxiv.org/abs/2512.18097", "authors": ["Mohammad Taghi Dabiri", "Meysam Ghanbari", "Rula Ammuri", "Saif Al-Kuwari", "Mazen Hasna", "Khalid Qaraqe"], "title": "CV Quantum Communications with Angular Rejection Filtering: Modeling and Security Analysis", "comment": null, "summary": "Continuous-variable quantum key distribution (CVQKD) over free-space optical links is a promising approach for secure communication, but its performance is limited by turbulence, pointing errors, and angular leakage that can be exploited by an eavesdropper. To mitigate this, we consider an angular rejection filter that defines a safe-zone at the receiver and blocks signals from outside the desired cone. A system and channel model is developed including turbulence, misalignment, and safe-zone effects, and information theoretic metrics are derived to evaluate security. Simulation results show that the safe zone significantly reduces information leakage and that careful tuning of beam waist, angular threshold, and aperture size is essential for maximizing the secret key rate. Larger apertures improve performance but increase receiver size, while longer links require sub 100 urad alignment accuracy. These results highlight safe-zone enforcement and parameter optimization as effective strategies for practical and secure CV-QKD."}
{"id": "2512.18298", "categories": ["cs.SD", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.18298", "abs": "https://arxiv.org/abs/2512.18298", "authors": ["Sudip Chakrabarty", "Pappu Bishwas", "Rajdeep Chatterjee"], "title": "Explainable Transformer-CNN Fusion for Noise-Robust Speech Emotion Recognition", "comment": null, "summary": "Speech Emotion Recognition (SER) systems often degrade in performance when exposed to the unpredictable acoustic interference found in real-world environments. Additionally, the opacity of deep learning models hinders their adoption in trust-sensitive applications. To bridge this gap, we propose a Hybrid Transformer-CNN framework that unifies the contextual modeling of Wav2Vec 2.0 with the spectral stability of 1D-Convolutional Neural Networks. Our dual-stream architecture processes raw waveforms to capture long-range temporal dependencies while simultaneously extracting noise-resistant spectral features (MFCC, ZCR, RMSE) via a custom Attentive Temporal Pooling mechanism. We conducted extensive validation across four diverse benchmark datasets: RAVDESS, TESS, SAVEE, and CREMA-D. To rigorously test robustness, we subjected the model to non-stationary acoustic interference using real-world noise profiles from the SAS-KIIT dataset. The proposed framework demonstrates superior generalization and state-of-the-art accuracy across all datasets, significantly outperforming single-branch baselines under realistic environmental interference. Furthermore, we address the ``black-box\" problem by integrating SHAP and Score-CAM into the evaluation pipeline. These tools provide granular visual explanations, revealing how the model strategically shifts attention between temporal and spectral cues to maintain reliability in the presence of complex environmental noise."}
{"id": "2512.19090", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2512.19090", "abs": "https://arxiv.org/abs/2512.19090", "authors": ["Fan Yu", "Tao Wang", "You Wu", "Lin Zhu", "Wei Deng", "Weisheng Han", "Wenchao Wang", "Lin Hu", "Xiangyu Liang", "Xiaodong He", "Yankun Huang", "Yu Gu", "Yuan Liu", "Yuxuan Wang", "Zhangyu Xiao", "Ziteng Wang", "Boya Dong", "Feng Dang", "Jinming Chen", "Jingdong Li", "Jun Wang", "Yechen Jin", "Yuan Zhang", "Zhengyan Sheng", "Xin Wang"], "title": "JoyVoice: Long-Context Conditioning for Anthropomorphic Multi-Speaker Conversational Synthesis", "comment": null, "summary": "Large speech generation models are evolving from single-speaker, short sentence synthesis to multi-speaker, long conversation geneartion. Current long-form speech generation models are predominately constrained to dyadic, turn-based interactions. To address this, we introduce JoyVoice, a novel anthropomorphic foundation model designed for flexible, boundary-free synthesis of up to eight speakers. Unlike conventional cascaded systems, JoyVoice employs a unified E2E-Transformer-DiT architecture that utilizes autoregressive hidden representations directly for diffusion inputs, enabling holistic end-to-end optimization. We further propose a MM-Tokenizer operating at a low bitrate of 12.5 Hz, which integrates multitask semantic and MMSE losses to effectively model both semantic and acoustic information. Additionally, the model incorporates robust text front-end processing via large-scale data perturbation. Experiments show that JoyVoice achieves state-of-the-art results in multilingual generation (Chinese, English, Japanese, Korean) and zero-shot voice cloning. JoyVoice achieves top-tier results on both the Seed-TTS-Eval Benchmark and multi-speaker long-form conversational voice cloning tasks, demonstrating superior audio quality and generalization. It achieves significant improvements in prosodic continuity for long-form speech, rhythm richness in multi-speaker conversations, paralinguistic naturalness, besides superior intelligibility. We encourage readers to listen to the demo at https://jea-speech.github.io/JoyVoice"}
{"id": "2512.18326", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2512.18326", "abs": "https://arxiv.org/abs/2512.18326", "authors": ["Meidong Xia", "Min Fan", "Wei Xu", "Haiming Wang", "Xiaohu You"], "title": "Two-Stage Signal Reconstruction for Amplitude-Phase-Time Block Modulation-based Communications", "comment": null, "summary": "Operating power amplifiers (PAs) at lower input back-off (IBO) levels is an effective way to improve PA efficiency, but often introduces severe nonlinear distortion that degrades transmission performance. Amplitude-phase-time block modulation (APTBM) has recently emerged as an effective solution to this problem. By leveraging the intrinsic amplitude and phase constraints of each APTBM block, PA-induced nonlinear distortion can be mitigated through constraint-guided signal reconstruction. However, existing reconstruction methods apply these constraints only heuristically and statistically, limiting the achievable IBO reduction and PA efficiency improvement. This paper addresses this limitation by decomposing the nonlinear distortion into dominant and residual components, and accordingly develops a novel two-stage signal reconstruction algorithm consisting of coarse and fine reconstruction stages. The coarse reconstruction stage eliminates the dominant distortion by jointly exploiting the APTBM block structure and PA nonlinear characteristics. The fine reconstruction stage minimizes the residual distortion by formulating a nonconvex optimization problem that explicitly enforces the APTBM constraints. To handle this problem efficiently, a low-complexity iterative variable substitution method is introduced, which relaxes the problem into a sequence of trust-region subproblems, each solvable in closed form. The proposed algorithm is validated through comprehensive numerical simulations and testbed experiments. Results show that it achieves up to 4 dB IBO reduction in simulations and up to 2 dB IBO reduction in experiments while maintaining transmission performance, corresponding to PA efficiency improvements of 59.1\\% and 33.9\\%, respectively, over existing methods."}
{"id": "2512.18699", "categories": ["cs.SD", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.18699", "abs": "https://arxiv.org/abs/2512.18699", "authors": ["Pengchao Feng", "Yao Xiao", "Ziyang Ma", "Zhikang Niu", "Shuai Fan", "Yao Li", "Sheng Wang", "Xie Chen"], "title": "Task Vector in TTS: Toward Emotionally Expressive Dialectal Speech Synthesis", "comment": null, "summary": "Recent advances in text-to-speech (TTS) have yielded remarkable improvements in naturalness and intelligibility. Building on these achievements, research has increasingly shifted toward enhancing the expressiveness of generated speech, such as dialectal and emotional TTS. However, cross-style synthesis combining both dialect and emotion remains challenging and largely unexplored, mainly due to the scarcity of dialectal data with emotional labels. To address this, we propose Hierarchical Expressive Vector (HE-Vector), a two-stage method for Emotional Dialectal TTS. In the first stage, we construct different task vectors to model dialectal and emotional styles independently, and then enhance single-style synthesis by adjusting their weights, a method we refer to as Expressive Vector (E-Vector). For the second stage, we hierarchically integrate these vectors to achieve controllable emotionally expressive dialect synthesis without requiring jointly labeled data, corresponding to Hierarchical Expressive Vector (HE-Vector). Experimental results demonstrate that HE-Vectors achieve superior performance in dialect synthesis, and promising results in synthesizing emotionally expressive dialectal speech in a zero-shot setting."}
{"id": "2512.18346", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2512.18346", "abs": "https://arxiv.org/abs/2512.18346", "authors": ["Vishesh Bhardwaj", "Aman Yadav", "Srikireddy Dhanunjay Reddy", "Tharun Kumar Reddy Bollu"], "title": "Cognitive Inference based Feature Pyramid Network for Sentimental Analysis using EEG Signals", "comment": null, "summary": "Sentiment analysis using Electroencephalography (EEG) sensor signals provides a deeper behavioral understanding of a person's emotional state, offering insights into real-time mood fluctuations. This approach takes advantage of brain electrical activity, making it a promising tool for various applications, including mental health monitoring, affective computing, and personalised user experiences. An encoder-based model for EEG-to-sentiment analysis, utilizing the ZUCO 2.0 dataset and incorporating a Feature Pyramid Network (FPN), is proposed to enhance this process. FPNs are adapted here for EEG sensor data, enabling multiscale feature extraction to capture local and global sentiment-related patterns. The raw EEG sensor data from the ZUCO 2.0 dataset is pre-processed and passed through the FPN, which extracts hierarchical features. In addition, extracted features are passed to a Gated Recurrent Unit (GRU) to model temporal dependencies, thereby enhancing the accuracy of sentiment classification. The ZUCO 2.0 dataset is utilized for its clear and detailed representation in 128 channels, offering rich spatial and temporal resolution. The experimental metric results show that the proposed architecture achieves a 6.88\\% performance gain compared to the existing methods. Furthermore, the proposed framework demonstrated its efficacy on the validation datasets DEAP and SEED."}
{"id": "2512.18706", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2512.18706", "abs": "https://arxiv.org/abs/2512.18706", "authors": ["Zhanxun Liu", "Yifan Duan", "Mengmeng Wang", "Pengchao Feng", "Haotian Zhang", "Xiaoyu Xing", "Yijia Shan", "Haina Zhu", "Yuhang Dai", "Chaochao Lu", "Xipeng Qiu", "Lei Xie", "Lan Wang", "Nan Yan", "Zilong Zheng", "Ziyang Ma", "Kai Yu", "Xie Chen"], "title": "X-Talk: On the Underestimated Potential of Modular Speech-to-Speech Dialogue System", "comment": "14 pages", "summary": "We present X-Talk, an open-source framework that champions a decoupled, modular design for LLM-driven speech-to-speech (S2S) systems. While the dominant trend favors end-to-end (E2E) modeling to optimize information flow, these \"omni-models\" often struggle to balance the competing objectives of complex speech tasks within a single network. X-Talk challenges this paradigm by demonstrating that a systematically optimized cascaded pipeline can achieve sub-second latency without sacrificing modular flexibility. Our framework seamlessly integrates specialized front-end components (e.g., VAD, speech enhancement) and diverse understanding models (e.g., ASR, emotion, and environmental sound analysis) with LLM capabilities like retrieval-augmented generation (RAG) and tool use. By revitalizing the cascaded approach, X-Talk highlights the underestimated potential of modular S2S systems and provides a robust foundation for future research and applications."}
{"id": "2512.18426", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2512.18426", "abs": "https://arxiv.org/abs/2512.18426", "authors": ["Xinrui Li", "R. Michael Buehrer", "Steven W. Ellingson"], "title": "RIS-Aided Spatial Nulling: Algorithms, Analysis, and Nulling Limits", "comment": null, "summary": "Reconfigurable Intelligent Surfaces (RIS) have recently gained attention as a means to dynamically shape the wireless propagation environment through programmable reflection control. Among the numerous applications, an important emerging use case is employing RIS as an auxiliary mechanism for spatial interference nulling, particularly in large ground-based reflector antennas where sidelobe interference can significantly degrade the system performance. With the growing density of satellites and terrestrial emitters, algorithms with faster convergence speed and better performance are needed. This work investigates RIS-equipped reflector antennas as a representative example of RIS-assisted spatial nulling and develop algorithms for sidelobe cancellation at specific directions and frequencies under various constraints. For the continuous-phase case, we adapt the gradient projection (GP) and alternating projection (AP) algorithms for scalability and propose a closed-form near-optimal solution that achieves satisfactory nulling performance with significantly reduced complexity. For the discrete-phase case, we reformulate the problem using a penalty method and solve it via majorization-minimization, outperforming the heuristic methods from our earlier work. Further, we analyze the electric field characteristics across multiple interference directions and frequencies to quantify the nulling capability of the RIS-aided reflectors, and identify a simple criterion for the existence of unimodular weights enabling perfect nulls. Simulation results demonstrate the effectiveness of the proposed methods and confirm the theoretical nulling limits."}
{"id": "2512.18791", "categories": ["cs.SD", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2512.18791", "abs": "https://arxiv.org/abs/2512.18791", "authors": ["Yichuan Zhang", "Chengxin Li", "Yujie Gu"], "title": "Smark: A Watermark for Text-to-Speech Diffusion Models via Discrete Wavelet Transform", "comment": null, "summary": "Text-to-Speech (TTS) diffusion models generate high-quality speech, which raises challenges for the model intellectual property protection and speech tracing for legal use. Audio watermarking is a promising solution. However, due to the structural differences among various TTS diffusion models, existing watermarking methods are often designed for a specific model and degrade audio quality, which limits their practical applicability. To address this dilemma, this paper proposes a universal watermarking scheme for TTS diffusion models, termed Smark. This is achieved by designing a lightweight watermark embedding framework that operates in the common reverse diffusion paradigm shared by all TTS diffusion models. To mitigate the impact on audio quality, Smark utilizes the discrete wavelet transform (DWT) to embed watermarks into the relatively stable low-frequency regions of the audio, which ensures seamless watermark-audio integration and is resistant to removal during the reverse diffusion process. Extensive experiments are conducted to evaluate the audio quality and watermark performance in various simulated real-world attack scenarios. The experimental results show that Smark achieves superior performance in both audio quality and watermark extraction accuracy."}
{"id": "2512.18427", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2512.18427", "abs": "https://arxiv.org/abs/2512.18427", "authors": ["Xinrui Li", "R. Michael Buehrer"], "title": "On the Limits of Coherent Time-Domain Cancellation of Radio Frequency Interference", "comment": null, "summary": "In many sensing (viz., radio astronomy) and radar applications, the received signal of interest (SOI) exhibits a significantly wider bandwidth or weaker power than the interference signal, rendering it indistinguishable from the background noise. Such scenarios arise frequently in applications such as passive radar, cognitive radio, low-probability-of-intercept (LPI) radar, and planetary radar for radio astronomy, where canceling the radio frequency interference (RFI) is critical for uncovering the SOI. In this work, we examine the Demodulation-Remodulation (Demod-Remod) based interference cancellation framework for the RFI. This approach demodulates the unknown interference, creates a noise-free interference replica, and coherently subtracts it from the received signal. To evaluate the performance limits, we employ the performance metric termed \\textit{interference rejection ratio} (IRR), which quantifies the interference canceled. We derive the analytical expressions of IRR as a function of the optimal estimation variances of the signal parameters. Simulation results confirm the accuracy of the analytical expression for both single-carrier and multi-carrier interference signals and demonstrate that the method can substantially suppress the interference at a sufficient interference-to-noise ratio (INR), enabling enhanced detection and extraction of the SOI. We further extend the analysis to the scenario where the SOI is above the noise floor, and confirm the validity of the theoretical IRR expression in this scenario. Lastly, we compare the Demod-Remod technique to other time-domain cancellation methods. The result of the comparison identifies the conditions under which each method is preferred, offering practical guidelines for interference mitigation under different scenarios."}
{"id": "2512.18797", "categories": ["cs.SD", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.18797", "abs": "https://arxiv.org/abs/2512.18797", "authors": ["Lisan Al Amin", "Vandana P. Janeja"], "title": "Reliable Audio Deepfake Detection in Variable Conditions via Quantum-Kernel SVMs", "comment": "This paper is accepted in ICDM 2025-MLC workshop", "summary": "Detecting synthetic speech is challenging when labeled data are scarce and recording conditions vary. Existing end-to-end deep models often overfit or fail to generalize, and while kernel methods can remain competitive, their performance heavily depends on the chosen kernel. Here, we show that using a quantum kernel in audio deepfake detection reduces falsepositive rates without increasing model size. Quantum feature maps embed data into high-dimensional Hilbert spaces, enabling the use of expressive similarity measures and compact classifiers. Building on this motivation, we compare quantum-kernel SVMs (QSVMs) with classical SVMs using identical mel-spectrogram preprocessing and stratified 5-fold cross-validation across four corpora (ASVspoof 2019 LA, ASVspoof 5 (2024), ADD23, and an In-the-Wild set). QSVMs achieve consistently lower equalerror rates (EER): 0.183 vs. 0.299 on ASVspoof 5 (2024), 0.081 vs. 0.188 on ADD23, 0.346 vs. 0.399 on ASVspoof 2019, and 0.355 vs. 0.413 In-the-Wild. At the EER operating point (where FPR equals FNR), these correspond to absolute false-positiverate reductions of 0.116 (38.8%), 0.107 (56.9%), 0.053 (13.3%), and 0.058 (14.0%), respectively. We also report how consistent the results are across cross-validation folds and margin-based measures of class separation, using identical settings for both models. The only modification is the kernel; the features and SVM remain unchanged, no additional trainable parameters are introduced, and the quantum kernel is computed on a conventional computer."}
{"id": "2512.18641", "categories": ["eess.SP", "physics.ins-det"], "pdf": "https://arxiv.org/pdf/2512.18641", "abs": "https://arxiv.org/abs/2512.18641", "authors": ["Ziad Hatab", "Michael Gadringer", "Wolfgang Bösch"], "title": "The Choice of Line Lengths in Multiline Thru-Reflect-Line Calibration", "comment": "https://github.com/ZiadHatab/line-length-multiline-trl-calibration", "summary": "This paper presents an analysis and rigorous procedure for determining the optimal lengths of line standards in multiline thru-reflect-line (TRL) calibration of vector network analyzers (VNAs). The solution is obtained through nonlinear constrained optimization of the eigenvalue problem in multiline TRL calibration. Additionally, we propose a simplified approach for near-optimal length selection based on predefined sparse rulers. Alongside the length calculation, we discuss the required number of lines to meet bandwidth requirements. The sensitivity of the proposed procedure is evaluated numerically via Monte Carlo simulations, demonstrating that the derived lengths have lower uncertainty than those from existing industry standards. Practical examples are provided for various applications, including lossy and dispersive lines, as well as banded solutions for waveguides."}
{"id": "2512.18902", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2512.18902", "abs": "https://arxiv.org/abs/2512.18902", "authors": ["Saurabh Bhardwaj", "Smriti Srivastava", "Abhishek Bhandari", "Krit Gupta", "Hitesh Bahl", "J. R. P. Gupta"], "title": "Speaker Recognition -- Wavelet Packet Based Multiresolution Feature Extraction Approach", "comment": "This paper was originally written in Summer 2013 and previously made available on Figshare. The present submission is uploaded for archival and citation purposes", "summary": "This paper proposes a novel Wavelet Packet based feature extraction approach for the task of text independent speaker recognition. The features are extracted by using the combination of Mel Frequency Cepstral Coefficient (MFCC) and Wavelet Packet Transform (WPT).Hybrid Features technique uses the advantage of human ear simulation offered by MFCC combining it with multi-resolution property and noise robustness of WPT. To check the validity of the proposed approach for the text independent speaker identification and verification we have used the Gaussian Mixture Model (GMM) and Hidden Markov Model (HMM) respectively as the classifiers. The proposed paradigm is tested on voxforge speech corpus and CSTR US KED Timit database. The paradigm is also evaluated after adding standard noise signal at different level of SNRs for evaluating the noise robustness. Experimental results show that better results are achieved for the tasks of both speaker identification as well as speaker verification."}
{"id": "2512.18711", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2512.18711", "abs": "https://arxiv.org/abs/2512.18711", "authors": ["Yue Zhang", "Yaru Fu", "Pei Liu", "Yalin Liu", "Kevin Hung"], "title": "Multi-Waveguide Pinching Antenna Placement Optimization for Rate Maximization", "comment": null, "summary": "Pinching antenna systems (PASS) have emerged as a technology that enables the large-scale movement of antenna elements, offering significant potential for performance gains in next-generation wireless networks. This paper investigates the problem of maximizing the average per-user data rate by optimizing the antenna placement of a multi-waveguide PASS, subject to a stringent physical minimum spacing constraint. To address this complex challenge, which involves a coupled fractional objective and a non-convex constraint, we employ the fractional programming (FP) framework to transform the non-convex rate maximization problem into a more tractable one, and devise a projected gradient ascent (PGA)-based algorithm to iteratively solve the transformed problem. Simulation results demonstrate that our proposed scheme significantly outperforms various geometric placement baselines, achieving superior per-user data rates by actively mitigating multi-user interference."}
{"id": "2512.19090", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2512.19090", "abs": "https://arxiv.org/abs/2512.19090", "authors": ["Fan Yu", "Tao Wang", "You Wu", "Lin Zhu", "Wei Deng", "Weisheng Han", "Wenchao Wang", "Lin Hu", "Xiangyu Liang", "Xiaodong He", "Yankun Huang", "Yu Gu", "Yuan Liu", "Yuxuan Wang", "Zhangyu Xiao", "Ziteng Wang", "Boya Dong", "Feng Dang", "Jinming Chen", "Jingdong Li", "Jun Wang", "Yechen Jin", "Yuan Zhang", "Zhengyan Sheng", "Xin Wang"], "title": "JoyVoice: Long-Context Conditioning for Anthropomorphic Multi-Speaker Conversational Synthesis", "comment": null, "summary": "Large speech generation models are evolving from single-speaker, short sentence synthesis to multi-speaker, long conversation geneartion. Current long-form speech generation models are predominately constrained to dyadic, turn-based interactions. To address this, we introduce JoyVoice, a novel anthropomorphic foundation model designed for flexible, boundary-free synthesis of up to eight speakers. Unlike conventional cascaded systems, JoyVoice employs a unified E2E-Transformer-DiT architecture that utilizes autoregressive hidden representations directly for diffusion inputs, enabling holistic end-to-end optimization. We further propose a MM-Tokenizer operating at a low bitrate of 12.5 Hz, which integrates multitask semantic and MMSE losses to effectively model both semantic and acoustic information. Additionally, the model incorporates robust text front-end processing via large-scale data perturbation. Experiments show that JoyVoice achieves state-of-the-art results in multilingual generation (Chinese, English, Japanese, Korean) and zero-shot voice cloning. JoyVoice achieves top-tier results on both the Seed-TTS-Eval Benchmark and multi-speaker long-form conversational voice cloning tasks, demonstrating superior audio quality and generalization. It achieves significant improvements in prosodic continuity for long-form speech, rhythm richness in multi-speaker conversations, paralinguistic naturalness, besides superior intelligibility. We encourage readers to listen to the demo at https://jea-speech.github.io/JoyVoice"}
{"id": "2512.18715", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2512.18715", "abs": "https://arxiv.org/abs/2512.18715", "authors": ["Kaiyi Chi", "Yinghui He", "Qianqian Yang", "Yuanchao Shu", "Zhiqin Wang", "Jun Luo", "Jiming Chen"], "title": "DeepGuard: Defending Deep Joint Source-Channel Coding Against Eavesdropping at Physical-Layer", "comment": "16 pages, 34 figures", "summary": "Deep joint source-channel coding (DeepJSCC) has emerged as a promising paradigm for efficient and robust information transmission. However, its intrinsic characteristics also pose new security challenges, notably an increased vulnerability to eavesdropping attacks. Existing studies on defending against eavesdropping attacks in DeepJSCC, while demonstrating certain effectiveness, often incur considerable computational overhead or introduce performance trade-offs that may adversely affect legitimate users. In this paper, we present DeepGuard, to the best of our knowledge, the first physical-layer defense framework for DeepJSCC against eavesdropping attacks, validated through over-the-air experiments using software-defined radios (SDRs). Considering that existing eavesdropping attacks against DeepJSCC are limited to simulation under ideal channels, we take a step further by identifying and implementing four representative types of attacks under various configurations in orthogonal frequency-division multiplexing systems. These attacks are evaluated over-the-air under diverse scenarios, allowing us to comprehensively characterize the real-world threat landscape. To mitigate these threats, DeepGuard introduces a novel preamble perturbation mechanism that modifies the preamble shared only between legitimate transceivers. To realize it, we first conduct a theoretical analysis of the perturbation's impact on the signals intercepted by the eavesdropper. Building upon this, we develop an end-to-end perturbation optimization algorithm that significantly degrades eavesdropping performance while preserving reliable communication for legitimate users. We prototype DeepGuard using SDRs and conduct extensive over-the-air experiments in practical scenarios. Extensive experiments demonstrate that DeepGuard effectively mitigates eavesdropping threats."}
{"id": "2512.19374", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2512.19374", "abs": "https://arxiv.org/abs/2512.19374", "authors": ["Wenyu Luo", "Jinhui Chen"], "title": "DeepGESI: A Non-Intrusive Objective Evaluation Model for Predicting Speech Intelligibility in Hearing-Impaired Listeners", "comment": null, "summary": "Speech intelligibility assessment is essential for many speech-related applications. However, most objective intelligibility metrics are intrusive, as they require clean reference speech in addition to the degraded or processed signal for evaluation. Furthermore, existing metrics such as STOI are primarily designed for normal hearing listeners, and their predictive accuracy for hearing impaired speech intelligibility remains limited. On the other hand, the GESI (Gammachirp Envelope Similarity Index) can be used to estimate intelligibility for hearing-impaired listeners, but it is also intrusive, as it depends on reference signals. This requirement limits its applicability in real-world scenarios.\n  To overcome this limitation, this study proposes DeepGESI, a non-intrusive deep learning-based model capable of accurately and efficiently predicting the speech intelligibility of hearing-impaired listeners without requiring any clean reference speech. Experimental results demonstrate that, under the test conditions of the 2nd Clarity Prediction Challenge(CPC2) dataset, the GESI scores predicted by DeepGESI exhibit a strong correlation with the actual GESI scores. In addition, the proposed model achieves a substantially faster prediction speed compared to conventional methods."}
{"id": "2512.18773", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2512.18773", "abs": "https://arxiv.org/abs/2512.18773", "authors": ["Xue Xian Zheng", "Xing Liu", "Tareq Y. Al-Naffouri"], "title": "Decentralized GNSS at Global Scale via Graph-Aware Diffusion Adaptation", "comment": null, "summary": "Network-based Global Navigation Satellite Systems (GNSS) underpin critical infrastructure and autonomous systems, yet typically rely on centralized processing hubs that limit scalability, resilience, and latency. Here we report a global-scale, decentralized GNSS architecture spanning hundreds of ground stations. By modeling the receiver network as a time-varying graph, we employ a deep linear neural network approach to learn topology-aware mixing schedules that optimize information exchange. This enables a gradient tracking diffusion strategy wherein stations execute local inference and exchange succinct messages to achieve two concurrent objectives: centimeter-level self-localization and network-wide consensus on satellite correction products. The consensus products are broadcast to user receivers as corrections, supporting precise point positioning (PPP) and precise point positioning-real-time kinematic (PPP-RTK). Numerical results demonstrate that our method matches the accuracy of centralized baselines while significantly outperforming existing decentralized methods in convergence speed and communication overhead. By reframing decentralized GNSS as a networked signal processing problem, our results pave the way for integrating decentralized optimization, consensus-based inference, and graph-aware learning as effective tools in operational satellite navigation."}
{"id": "2512.19687", "categories": ["cs.SD", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.19687", "abs": "https://arxiv.org/abs/2512.19687", "authors": ["Apoorv Vyas", "Heng-Jui Chang", "Cheng-Fu Yang", "Po-Yao Huang", "Luya Gao", "Julius Richter", "Sanyuan Chen", "Matt Le", "Piotr Dollár", "Christoph Feichtenhofer", "Ann Lee", "Wei-Ning Hsu"], "title": "Pushing the Frontier of Audiovisual Perception with Large-Scale Multimodal Correspondence Learning", "comment": null, "summary": "We introduce Perception Encoder Audiovisual, PE-AV, a new family of encoders for audio and video understanding trained with scaled contrastive learning. Built on PE, PE-AV makes several key contributions to extend representations to audio, and natively support joint embeddings across audio-video, audio-text, and video-text modalities. PE-AV's unified cross-modal embeddings enable novel tasks such as speech retrieval, and set a new state of the art across standard audio and video benchmarks. We unlock this by building a strong audiovisual data engine that synthesizes high-quality captions for O(100M) audio-video pairs, enabling large-scale supervision consistent across modalities. Our audio data includes speech, music, and general sound effects-avoiding single-domain limitations common in prior work. We exploit ten pairwise contrastive objectives, showing that scaling cross-modality and caption-type pairs strengthens alignment and improves zero-shot performance. We further develop PE-A-Frame by fine-tuning PE-AV with frame-level contrastive objectives, enabling fine-grained audio-frame-to-text alignment for tasks such as sound event detection."}
{"id": "2512.18780", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2512.18780", "abs": "https://arxiv.org/abs/2512.18780", "authors": ["Yifeng Zhang", "Xiao Liang"], "title": "Domain Adaptation in Structural Health Monitoring of Civil Infrastructure: A Systematic Review", "comment": null, "summary": "This study provides a comprehensive review of domain adaptation (DA) techniques in vibration-based structural health monitoring (SHM). As data-driven models increasingly support the assessment of civil structures, the persistent challenge of transferring knowledge across varying geometries, materials, and environmental conditions remains a major obstacle. DA offers a systematic approach to mitigate these discrepancies by aligning feature distributions between simulated, laboratory, and field domains while preserving the sensitivity of damage-related information. Drawing on more than sixty representative studies, this paper analyzes the evolution of DA methods for SHM, including statistical alignment, adversarial and subdomain learning, physics-informed adaptation, and generative modeling for simulation-to-real transfer. The review summarizes their contributions and limitations across bridge and building applications, revealing that while DA has improved generalization significantly, key challenges persist: managing domain discrepancy, addressing data scarcity, enhancing model interpretability, and enabling adaptability to multiple sources and time-varying conditions. Future research directions emphasize integrating physical constraints into learning objectives, developing physics-consistent generative frameworks to enhance data realism, establishing interpretable and certifiable DA systems for engineering practice, and advancing multi-source and lifelong adaptation for scalable monitoring. Overall, this review consolidates the methodological foundation of DA for SHM, identifies existing barriers to generalization and trust, and outlines the technological trajectory toward transparent, physics-aware, and adaptive monitoring systems that support the long-term resilience of civil infrastructure."}
{"id": "2512.17932", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2512.17932", "abs": "https://arxiv.org/abs/2512.17932", "authors": ["Yang Xiao"], "title": "Continual Learning for Acoustic Event Classification", "comment": "Master project report", "summary": "Continuously learning new classes without catastrophic forgetting is a challenging problem for on-device acoustic event classification given the restrictions on computation resources (e.g., model size, running memory). To alleviate such an issue, we propose two novel diversity-aware incremental learning method for Spoken Keyword Spotting and Environmental Sound Classification. Our method selects the historical data for the training by measuring the per-sample classification uncertainty. For the Spoken Keyword Spotting application, the proposed RK approach introduces a diversity-aware sampler to select a diverse set from historical and incoming keywords by calculating classification uncertainty. As a result, the RK approach can incrementally learn new tasks without forgetting prior knowledge. Besides, the RK approach also proposes data augmentation and knowledge distillation loss function for efficient memory management on the edge device. For the Environmental Sound Classification application, we measure the uncertainty by observing how the classification probability of data fluctuates against the parallel perturbations added to the classifier embedding. In this way, the computation cost can be significantly reduced compared with adding perturbation to the raw data. Experimental results show that the proposed RK approach achieves 4.2% absolute improvement in terms of average accuracy over the best baseline on Google Speech Command dataset with less required memory. Experimental results on the DCASE 2019 Task 1 and ESC-50 dataset show that our proposed method outperforms baseline continual learning methods on classification accuracy and computational efficiency, indicating our method can efficiently and incrementally learn new classes without the catastrophic forgetting problem for on-device environmental sound classification"}
{"id": "2512.18788", "categories": ["eess.SP", "cs.IT", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.18788", "abs": "https://arxiv.org/abs/2512.18788", "authors": ["George C. Alexandropoulos", "Kostantinos D. Katsanos", "George Stamatelis", "Ioannis Gavras"], "title": "RIS-Enabled Smart Wireless Environments: Fundamentals and Distributed Optimization", "comment": "48 pages; 12 figures; book chapter", "summary": "This chapter overviews the concept of Smart Wireless Environments (SWEs) motivated by the emerging technology of Reconfigurable Intelligent Surfaces (RISs). The operating principles and state-of-the-art hardware architectures of programmable metasurfaces are first introduced. Subsequently, key performance objectives and use cases of RIS-enabled SWEs, including spectral and energy efficiency, physical-layer security, integrated sensing and communications, as well as the emerging paradigm of over-the-air computing, are discussed. Focusing on the recent trend of Beyond-Diagonal (BD) RISs, two distributed designs of respective SWEs are presented. The first deals with a multi-user Multiple-Input Single-Output (MISO) system operating within the area of influence of a SWE comprising multiple BD-RISs. A hybrid distributed and fusion machine learning framework based on multi-branch attention-based convolutional Neural Networks (NNs), NN parameter sharing, and neuroevolutionary training is presented, which enables online mapping of channel realizations to the BD-RIS configurations as well as the multi-user transmit precoder. Performance evaluation results showcase that the distributedly optimized RIS-enabled SWE achieves near-optimal sum-rate performance with low online computational complexity. The second design focuses on the wideband interference MISO broadcast channel, where each base station exclusively controls one BD-RIS to serve its assigned group of users. A cooperative optimization framework that jointly designs the base station transmit precoders as well as the tunable capacitances and switch matrices of all metasurfaces is presented. Numerical results demonstrating the superior sum-rate performance of the designed RIS-enabled SWE for multi-cell MISO networks over benchmark schemes, considering non-cooperative configuration and conventional diagonal metasurfaces, are presented."}
{"id": "2512.17937", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2512.17937", "abs": "https://arxiv.org/abs/2512.17937", "authors": ["Ram C. M. C. Shekar", "Iván López-Espejo"], "title": "LIWhiz: A Non-Intrusive Lyric Intelligibility Prediction System for the Cadenza Challenge", "comment": null, "summary": "We present LIWhiz, a non-intrusive lyric intelligibility prediction system submitted to the ICASSP 2026 Cadenza Challenge. LIWhiz leverages Whisper for robust feature extraction and a trainable back-end for score prediction. Tested on the Cadenza Lyric Intelligibility Prediction (CLIP) evaluation set, LIWhiz achieves a 22.4% relative root mean squared error reduction over the STOI-based baseline, yielding a substantial improvement in normalized cross-correlation."}
{"id": "2512.18854", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2512.18854", "abs": "https://arxiv.org/abs/2512.18854", "authors": ["Xiarui Su", "Xihui Teng", "Yiyang Yu", "Yiming Yang", "Atif Shamim"], "title": "A 100-GHz CMOS-Compatible RIS-on-Chip Based on Phase-Delay Lines for 6G Applications", "comment": null, "summary": "On-chip reconfigurable intelligent surfaces (RIS) are expected to play a vital role in future 6G communication systems. This work proposed a CMOS-compatible on-chip RIS capable of achieving beam steering for the first time. The proposed unit cell design is a combination of a slot, a phase-delay line with VO2, and a ground. Under the two states of the VO2, the unit cell has a 180 deg phase difference at the center frequency, while maintaining reflection magnitudes better than -1.2 dB. Moreover, a 60by60 RIS array based on the present novel unit is designed, demonstrating the beam-steering capability. Finally, to validate the design concept, a prototype is fabricated, and the detailed fabrication process is presented. The measurement result demonstrates a 27.1 dB enhancement between ON and OFF states. The proposed RIS has the advantages of low loss, CMOS-compatibility, providing a foundation for future 6G applications."}
{"id": "2512.18371", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2512.18371", "abs": "https://arxiv.org/abs/2512.18371", "authors": ["Te Ma", "Nanjie Li", "Hao Huang", "Zhijian Ou"], "title": "Phoneme-based speech recognition driven by large language models and sampling marginalization", "comment": "Published at NCMMSC 2025, in Chinese language", "summary": "Recently, the Large Language Model-based Phoneme-to-Grapheme (LLM-P2G) method has shown excellent performance in speech recognition tasks and has become a feasible direction to replace the traditional WFST decoding method. This framework takes into account both recognition accuracy and system scalability through two-stage modeling of phoneme prediction and text generation. However, the existing LLM-P2G adopts the Top-K Marginalized (TKM) training strategy, and its candidate phoneme sequences rely on beam search generation, which has problems such as insufficient path diversity, low training efficiency, and high resource overhead. To this end, this paper proposes a sampling marginalized training strategy (Sampling-K Marginalized, SKM), which replaces beam search with random sampling to generate candidate paths, improving marginalized modeling and training efficiency. Experiments were conducted on Polish and German datasets, and the results showed that SKM further improved the model learning convergence speed and recognition performance while maintaining the complexity of the model. Comparative experiments with a speech recognition method that uses a projector combined with a large language model (SpeechLLM) also show that the SKM-driven LLM-P2G has more advantages in recognition accuracy and structural simplicity. The study verified the practical value and application potential of this method in cross-language speech recognition systems."}
{"id": "2512.18890", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2512.18890", "abs": "https://arxiv.org/abs/2512.18890", "authors": ["Yuchen Zhang", "Eva Lagunas", "Xue Xian Zheng", "Symeon Chatzinotas", "Tareq Y. Al-Naffouri"], "title": "Decentralized Cooperative Beamforming for Networked LEO Satellites with Statistical CSI", "comment": "This paper has been submitted to IEEE for peer review", "summary": "Inter-satellite-link-enabled low-Earth-orbit (LEO) satellite constellations are evolving toward networked architectures that support constellation-level cooperation, enabling multiple satellites to jointly serve user terminals through cooperative beamforming. While such cooperation can substantially enhance link budgets and achievable rates, its practical realization is challenged by the scalability limitations of centralized beamforming designs and the stringent computational and signaling constraints of large LEO constellations. This paper develops a fully decentralized cooperative beamforming framework for networked LEO satellite downlinks. Using an ergodic-rate-based formulation, we first derive a centralized weighted minimum mean squared error (WMMSE) solution as a performance benchmark. Building on this formulation, we propose a topology-agnostic decentralized beamforming algorithm by localizing the benchmark and exchanging a set of globally coupled variables whose dimensions are independent of the antenna number and enforcing consensus over arbitrary connected inter-satellite networks. The resulting algorithm admits fully parallel execution across satellites. To further enhance scalability, we eliminate the consensus-related auxiliary variables in closed form and derive a low-complexity per-satellite update rule that is optimal to local iteration and admits a quasi-closed-form solution via scalar line search. Simulation results show that the proposed decentralized schemes closely approach centralized performance under practical inter-satellite topologies, while significantly reducing computational complexity and signaling overhead, enabling scalable cooperative beamforming for large LEO constellations."}
{"id": "2512.19442", "categories": ["eess.SP", "cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2512.19442", "abs": "https://arxiv.org/abs/2512.19442", "authors": ["Simon Welker", "Bunlong Lay", "Maris Hillemann", "Tal Peer", "Timo Gerkmann"], "title": "Real-Time Streamable Generative Speech Restoration with Flow Matching", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Diffusion-based generative models have greatly impacted the speech processing field in recent years, exhibiting high speech naturalness and spawning a new research direction. Their application in real-time communication is, however, still lagging behind due to their computation-heavy nature involving multiple calls of large DNNs.\n  Here, we present Stream.FM, a frame-causal flow-based generative model with an algorithmic latency of 32 milliseconds (ms) and a total latency of 48 ms, paving the way for generative speech processing in real-time communication. We propose a buffered streaming inference scheme and an optimized DNN architecture, show how learned few-step numerical solvers can boost output quality at a fixed compute budget, explore model weight compression to find favorable points along a compute/quality tradeoff, and contribute a model variant with 24 ms total latency for the speech enhancement task.\n  Our work looks beyond theoretical latencies, showing that high-quality streaming generative speech processing can be realized on consumer GPUs available today. Stream.FM can solve a variety of speech processing tasks in a streaming fashion: speech enhancement, dereverberation, codec post-filtering, bandwidth extension, STFT phase retrieval, and Mel vocoding. As we verify through comprehensive evaluations and a MUSHRA listening test, Stream.FM establishes a state-of-the-art for generative streaming speech restoration, exhibits only a reasonable reduction in quality compared to a non-streaming variant, and outperforms our recent work (Diffusion Buffer) on generative streaming speech enhancement while operating at a lower latency."}
{"id": "2512.18970", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2512.18970", "abs": "https://arxiv.org/abs/2512.18970", "authors": ["Tuo Wu", "Xiazhi Lai", "Maged Elkashlan", "Naofal Al-Dhahir", "Matthew C. Valenti", "Fumiyuki Adachi"], "title": "FAS-RIS for V2X: Unlocking Realistic Performance Analysis with Finite Elements", "comment": "Accepted by IEEE TVT", "summary": "The synergy of fluid antenna systems (FAS) and reconfigurable intelligent surfaces (RIS) is poised to unlock robust Vehicle-to-Everything (V2X) communications. However, a critical gap persists between theoretical predictions and real-world performance. Existing analyses predominantly rely on the Central Limit Theorem (CLT), an assumption valid only for a large number of RIS elements, which fails to represent practical, finite-sized deployments constrained by cost and urban infrastructure. This paper bridges this gap by presenting a novel framework that unlocks a realistic performance analysis for FAS-RIS systems with finite elements. Leveraging a Gamma distribution approximation, we derive a new, tractable closed-form expression for the outage probability. Numerical results validate our approach, demonstrating that it offers a significantly more accurate performance characterization than conventional CLT-based methods, particularly in the practical regime of small-scale RIS. This work provides a crucial foundation for the design and deployment of reliable FAS-RIS-aided vehicular networks."}
{"id": "2512.18981", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2512.18981", "abs": "https://arxiv.org/abs/2512.18981", "authors": ["Jiaji Ren", "Ye Tian", "Baiyang Liu", "Tuo Wu", "Wei Liu", "Kai-Kit Wong", "Kin-Fai Tong", "Kwai-Man Luk"], "title": "An Fluid Antenna Array-Enabled DOA Estimation Method: End-Fire Effect Suppression", "comment": null, "summary": "Direction of Arrival (DOA) estimation serves as a critical sensing technology poised to play a vital role in future intelligent and ubiquitous communication systems. Despite the development of numerous mature super-resolution algorithms, the inherent end-fire effect problem in fixed antenna arrays remains inadequately addressed. This work proposed a novel array architecture composed of fluid antennas. By exploiting the spatial reconfigurability of their positions to equivalently modulate the array steering vector and integrating it with the classical MUSIC algorithm, this approach achieved high-precision DOA estimation. Simulation results demonstrated that the proposed method delivers outstanding estimation performance even in highly challenging end-fire regions."}
{"id": "2512.18982", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2512.18982", "abs": "https://arxiv.org/abs/2512.18982", "authors": ["Tuo Wu", "Kai-Kit Wong", "Jie Tang", "Junteng Yao", "Baiyang Liu", "Kin-Fai Tong", "Chan-Byoung Chae", "Matthew C. Valenti", "Kwai-Man Luk"], "title": "Reimagining Wireless Connectivity: The FAS-RIS Synergy for 6G Smart Cities", "comment": null, "summary": "Fluid antenna system (FAS) represents the concept of treating antenna as a reconfigurable physical-layer resource to broaden system design and network optimization and inspire next-generation reconfigurable antennas. FAS can unleash new degree of freedom (DoF) via antenna reconfigurations for novel spatial diversity. Reconfigurable intelligent surfaces (RISs) on the other hand can reshape wireless propagation environments but often face limitations from double path-loss and minimal signal processing capability when operating independently. This article envisions a transformative FAS-RIS integrated architecture for future smart city networks, uniting the adaptability of FAS with the environmental control of RIS. The proposed framework has five key applications: FAS-enabled base stations (BSs) for large-scale beamforming, FAS-equipped user devices with finest spatial diversity, and three novel RIS paradigms -- fluid RIS (FRIS) with reconfigurable elements, FAS-embedded RIS as active relays, and enormous FAS (E-FAS) exploiting surface waves on facades to re-establish line-of-sight (LoS) communication. A two-timescale control mechanism coordinates network-level beamforming with rapid, device-level adaptation. Applications spanning from simultaneous wireless information and power transfer (SWIPT) to integrated sensing and communications (ISAC), with challenges in co-design, channel modeling, and optimization, are discussed. This article concludes with simulation results demonstrating the robustness and effectiveness of the FAS-RIS system."}
{"id": "2512.19010", "categories": ["eess.SP", "cs.RO"], "pdf": "https://arxiv.org/pdf/2512.19010", "abs": "https://arxiv.org/abs/2512.19010", "authors": ["Devi Yuliarti", "Ravi Prakash", "Hiu Ching Cheung", "Amy Strong", "Patrick J. Codd", "Shan Lin"], "title": "PalpAid: Multimodal Pneumatic Tactile Sensor for Tissue Palpation", "comment": null, "summary": "The tactile properties of tissue, such as elasticity and stiffness, often play an important role in surgical oncology when identifying tumors and pathological tissue boundaries. Though extremely valuable, robot-assisted surgery comes at the cost of reduced sensory information to the surgeon; typically, only vision is available. Sensors proposed to overcome this sensory desert are often bulky, complex, and incompatible with the surgical workflow. We present PalpAid, a multimodal pneumatic tactile sensor equipped with a microphone and pressure sensor, converting contact force into an internal pressure differential. The pressure sensor acts as an event detector, while the auditory signature captured by the microphone assists in tissue delineation. We show the design, fabrication, and assembly of sensory units with characterization tests to show robustness to use, inflation-deflation cycles, and integration with a robotic system. Finally, we show the sensor's ability to classify 3D-printed hard objects with varying infills and soft ex vivo tissues. Overall, PalpAid aims to fill the sensory gap intelligently and allow improved clinical decision-making."}
{"id": "2512.19013", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2512.19013", "abs": "https://arxiv.org/abs/2512.19013", "authors": ["Seongkyu Jung", "Namyoon Lee", "Jeonghun Park"], "title": "The MIMO-ME-MS Channel: Analysis and Algorithm for Secure MIMO Integrated Sensing and Communications", "comment": "16 pages, 5 figures. Submitted to an IEEE journal", "summary": "This paper studies precoder design for secure MIMO integrated sensing and communications (ISAC) by introducing the MIMO-ME-MS channel, where a multi-antenna transmitter serves a legitimate multi-antenna receiver in the presence of a multi-antenna eavesdropper while simultaneously enabling sensing via a multi-antenna sensing receiver. Using sensing mutual information as the sensing metric, we formulate a nonconvex weighted objective that jointly captures secure communication (via secrecy rate) and sensing performance. A high-SNR analysis based on subspace decomposition characterizes the maximum achievable weighted degrees of freedom and reveals that a quasi-optimal precoder must span a \"useful subspace,\" highlighting why straightforward extensions of classical wiretap/ISAC precoders can be suboptimal in this tripartite setting. Motivated by these insights, we develop a practical two-stage iterative algorithm that alternates between sequential basis construction and power allocation via a difference-of-convex program. Numerical results show that the proposed approach captures the desirable precoding structure predicted by the analysis and yields substantial gains in the MIMO-ME-MS channel."}
{"id": "2512.19054", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2512.19054", "abs": "https://arxiv.org/abs/2512.19054", "authors": ["Chengyong Jiang", "Jiajia Guo", "Yuqing Hua", "Chao-Kai Wen", "Shi Jin"], "title": "AI-Driven Subcarrier-Level CQI Feedback", "comment": null, "summary": "The Channel Quality Indicator (CQI) is a fundamental component of channel state information (CSI) that enables adaptive modulation and coding by selecting the optimal modulation and coding scheme to meet a target block error rate. While AI-enabled CSI feedback has achieved significant advances, especially in precoding matrix index feedback, AI-based CQI feedback remains underexplored. Conventional subband-based CQI approaches, due to coarse granularity, often fail to capture fine frequency-selective variations and thus lead to suboptimal resource allocation. In this paper, we propose an AI-driven subcarrier-level CQI feedback framework tailored for 6G and NextG systems. First, we introduce CQInet, an autoencoder-based scheme that compresses per-subcarrier CQI at the user equipment and reconstructs it at the base station, significantly reducing feedback overhead without compromising CQI accuracy. Simulation results show that CQInet increases the effective data rate by 7.6% relative to traditional subband CQI under equivalent feedback overhead. Building on this, we develop SR-CQInet, which leverages super-resolution to infer fine-grained subcarrier CQI from sparsely reported CSI reference signals (CSI-RS). SR-CQInet reduces CSI-RS overhead to 3.5% of CQInet's requirements while maintaining comparable throughput. These results demonstrate that AI-driven subcarrier-level CQI feedback can substantially enhance spectral efficiency and reliability in future wireless networks."}
{"id": "2512.19089", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2512.19089", "abs": "https://arxiv.org/abs/2512.19089", "authors": ["Marie Jose Perez Peralta", "Daniela Flores Casillas", "Benjamin Wilson", "Cristian Aviles Medina", "Yira Itzae Rendon Hernandez", "Vladimir Orrante Bracho"], "title": "Wireless sEMG-IMU Wearable for Real-Time Squat Kinematics and Muscle Activation", "comment": "6 pages, 9 figures. Technical report / preprint (wearable sEMG + IMU system for squat analysis)", "summary": "This work presents the design and implementation of a wireless, wearable system that combines surface electromyography (sEMG) and inertial measurement units (IMUs) to analyze a single lower-limb functional task: the free bodyweight squat in a healthy adult. The system records bipolar EMG from one agonist and one antagonist muscle of the dominant leg (vastus lateralis and semitendinosus) while simultaneously estimating knee joint angle, angular velocity, and angular acceleration using two MPU6050 IMUs. A custom dual-channel EMG front end with differential instrumentation preamplification, analog filtering (5-500 Hz band-pass and 60 Hz notch), high final gain, and rectified-integrated output was implemented on a compact 10 cm x 12 cm PCB. Data are digitized by an ESP32 microcontroller and transmitted wirelessly via ESP-NOW to a second ESP32 connected to a PC. A Python-based graphical user interface (GUI) displays EMG and kinematic signals in real time, manages subject metadata, and exports a summary of each session to Excel. The complete system is battery-powered to reduce electrical risk during human use. The resulting prototype demonstrates the feasibility of low-cost, portable EMG-IMU instrumentation for integrated analysis of muscle activation and squat kinematics and provides a platform for future biomechanical applications in sports performance and rehabilitation."}
{"id": "2512.19109", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2512.19109", "abs": "https://arxiv.org/abs/2512.19109", "authors": ["Sai Zhao", "Fanjin Kong", "Dong Tang", "Tuo Wu", "Shunxing Yang", "Kai-Kit Wong", "Kin-Fai Tong", "Kwai-Man Luk"], "title": "Intelligent Sky Mirrors: SAC-Driven MF-RIS Optimization for Secure NOMA in Low-Altitude Economy", "comment": null, "summary": "Low-altitude economy (LAE) has become a key driving force for smart cities and economic growth. To address spectral efficiency and communication security challenges in LAE, this paper investigates secure energy efficiency (SEE) maximization using intelligent sky mirrors, UAV-mounted multifunctional reconfigurable intelligent surfaces (MF-RIS) assisting nonorthogonal multiple access (NOMA) systems. These aerial mirrors intelligently amplify legitimate signals while simultaneously generating jamming against eavesdroppers. We formulate a joint optimization problem encompassing UAV trajectory, base station power allocation, RIS phase shifts, amplification factors, and scheduling matrices. Given the fractional SEE objective and dynamic UAV scenarios, we propose a two-layer optimization scheme: SAC-driven first layer for trajectory and power management, and channel alignment-based second layer for phase optimization. Simulations demonstrate that our proposed scheme significantly outperforms benchmark approaches."}
{"id": "2512.19127", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2512.19127", "abs": "https://arxiv.org/abs/2512.19127", "authors": ["Yuhao Chen", "Boxiang He", "Junshan Luo", "Shilian Wang", "Lei Yao", "Jing Lei"], "title": "Specific Multi-emitter Identification: Theoretical Limits and Low-complexity Design", "comment": null, "summary": "Specific emitter identification (SEI) distinguishes emitters by utilizing hardware-induced signal imperfections. However, conventional SEI techniques are primarily designed for single-emitter scenarios. This poses a fundamental limitation in distributed wireless networks, where simultaneous transmissions from multiple emitters result in overlapping signals that conventional single-emitter identification methods cannot effectively handle. To overcome this limitation, we present a specific multi-emitter identification (SMEI) framework via multi-label learning, treating identification as a problem of directly decoding emitter states from overlapping signals. Theoretically, we establish performance bounds using Fano's inequality. Methodologically, the multi-label formulation reduces output dimensionality from exponential to linear scale, thereby substantially decreasing computational complexity. Additionally, we propose an improved SMEI (I-SMEI), which incorporates multi-head attention to effectively capture features in correlated signal combinations. Experimental results demonstrate that SMEI achieves high identification accuracy with a linear computational complexity. Furthermore, the proposed I-SMEI scheme significantly improves identification accuracy across various overlapping scenarios compared to the proposed SMEI and other advanced methods."}
{"id": "2512.19166", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2512.19166", "abs": "https://arxiv.org/abs/2512.19166", "authors": ["Luca Reggiani", "Arnaldo Spalvieri"], "title": "Energy Optimization for Time-of-Arrival Based Tracking", "comment": null, "summary": "The paper analyzes energy allocation in a scenario where the position of a moving target is tracked by exploiting the Time-of-Arrivals of bandwidth-constrained signals received by or transmitted from a fixed number of anchors located at known positions. The signal of each anchor is generated by transmitting a sequence of known symbols, allowing for amplitude and duration (number of symbols) to be different from anchor to anchor. The problem is the minimization of the sum of the energies of the transmitted signals imposing a constraint on the performance of the tracking procedure. Specifically, the constraint is the Posterior Cramer-Rao Bound, below the mean square error achieved by any unbiased estimator. The main improvement over the previous literature is the derivation of a formula that, at each step of the tracking, allows to calculate in closed form the first-order variation of the Posterior Cramer-Rao Bound as a function of the variation of the total energy. To concretely show the application of our approach, we present also two numerical algorithms that implement the constrained optimization in the case of signals of fixed amplitude and variable duration transmitted from the anchors in a time division multiplexing scheme."}
{"id": "2512.19220", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2512.19220", "abs": "https://arxiv.org/abs/2512.19220", "authors": ["Bob Aubouin-Pairault", "Mazen Alamir", "Benjamin Meyer", "Rémi Wolf", "Kaouther Moussa"], "title": "How is remifentanil dosed without dedicated indicator?", "comment": null, "summary": "This study investigates the paradigm of intraoperative analgesic dosage using a data-driven approach based on retrospective clinical data. Remifentanil, an analgesic widely used during anesthesia, presents a dosing challenge due to the absence of an universally accepted indicator of analgesia. To examine how changes in patient state correlate with adjustments in remifentanil target concentration triggered by the practitioner, we analyzed data from two sources: VitalDB (Seoul, Korea) and PREDIMED (Grenoble, France). Results show that only features derived from arterial pressure are consistently associated with changes in remifentanil targets. This finding is robust across both datasets despite variations in specific thresholds. In particular, increases in remifentanil targets are associated with high or rising arterial pressure over short periods (1--2 minutes), whereas decreases are linked to low, stable, or declining arterial pressure over longer periods (5--7 minutes). By capturing anesthesiologists' dosing strategies we provide a foundation for the future development of closed-loop control algorithms. Beyond the specific example of remifentanil's change prediction, the proposed feature generation and associated sparse fitting approach can be applied to other domain where human decision can be viewed as sensors interpretation."}
{"id": "2512.19263", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2512.19263", "abs": "https://arxiv.org/abs/2512.19263", "authors": ["Zonghan Wang", "Zahra Mobini", "Hien Quoc Ngo", "Hyundong Shin", "Michail Matthaiou"], "title": "Anti-Malicious ISAC: How to Jointly Monitor and Disrupt Your Foes?", "comment": null, "summary": "Integrated sensing and communication (ISAC) systems are key enablers of future networks but raise significant security concerns. In this realm, the emergence of malicious ISAC systems has amplified the need for authorized parties to legitimately monitor suspicious communication links and protect legitimate targets from potential detection or exploitation by malicious foes. In this paper, we propose a new wireless proactive monitoring paradigm, where a legitimate monitor intercepts a suspicious communication link while performing cognitive jamming to enhance the monitoring success probability (MSP) and simultaneously safeguard the target. To this end, we derive closed-form expressions of the signal-to-interference-plus-noise-ratio (SINR) at the user (UE), sensing access points (S-APs), and an approximating expression of the SINR at the proactive monitor. Moreover, we propose an optimization technique under which the legitimate monitor minimizes the success detection probability (SDP) of the legitimate target, by optimizing the jamming power allocation over both communication and sensing channels subject to total power constraints and monitoring performance requirement. To enhance the monitor's longevity and reduce the risk of detection by malicious ISAC systems, we further propose an adaptive power allocation scheme aimed at minimizing the total transmit power at the monitor while meeting a pre-selected sensing SINR threshold and ensuring successful monitoring. Our numerical results show that the proposed algorithm significantly compromises the sensing and communication performance of malicious ISAC."}
{"id": "2512.19442", "categories": ["eess.SP", "cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2512.19442", "abs": "https://arxiv.org/abs/2512.19442", "authors": ["Simon Welker", "Bunlong Lay", "Maris Hillemann", "Tal Peer", "Timo Gerkmann"], "title": "Real-Time Streamable Generative Speech Restoration with Flow Matching", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Diffusion-based generative models have greatly impacted the speech processing field in recent years, exhibiting high speech naturalness and spawning a new research direction. Their application in real-time communication is, however, still lagging behind due to their computation-heavy nature involving multiple calls of large DNNs.\n  Here, we present Stream.FM, a frame-causal flow-based generative model with an algorithmic latency of 32 milliseconds (ms) and a total latency of 48 ms, paving the way for generative speech processing in real-time communication. We propose a buffered streaming inference scheme and an optimized DNN architecture, show how learned few-step numerical solvers can boost output quality at a fixed compute budget, explore model weight compression to find favorable points along a compute/quality tradeoff, and contribute a model variant with 24 ms total latency for the speech enhancement task.\n  Our work looks beyond theoretical latencies, showing that high-quality streaming generative speech processing can be realized on consumer GPUs available today. Stream.FM can solve a variety of speech processing tasks in a streaming fashion: speech enhancement, dereverberation, codec post-filtering, bandwidth extension, STFT phase retrieval, and Mel vocoding. As we verify through comprehensive evaluations and a MUSHRA listening test, Stream.FM establishes a state-of-the-art for generative streaming speech restoration, exhibits only a reasonable reduction in quality compared to a non-streaming variant, and outperforms our recent work (Diffusion Buffer) on generative streaming speech enhancement while operating at a lower latency."}
{"id": "2512.19639", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2512.19639", "abs": "https://arxiv.org/abs/2512.19639", "authors": ["Alejandro Ramírez-Arroyo", "O. S. Peñaherrera-Pulla", "Preben Mogensen"], "title": "Towards Reliable Connectivity: Measurement-Driven Assessment of Starlink and OneWeb Non-Terrestrial and 5G Terrestrial Networks", "comment": "15 pages, 12 figures", "summary": "The emergence of commercial satellite communications networks, such as Starlink and OneWeb, has significantly transformed the communications landscape over the last years. As a complement to terrestrial cellular networks, non-terrestrial systems enable coverage extension and reliability enhancement beyond the limits of conventional infrastructure. Currently, the high reliance on terrestrial networks exposes communications to vulnerabilities in the event of terrestrial infrastructure failures, e.g., due to natural disasters. Therefore, this work proposes the joint evaluation of Key Performance Indicators (KPIs) for two non-terrestrial satellite networks (Starlink and OneWeb) and two terrestrial cellular networks to assess the current performance of these technologies across three different environments: (i) urban, (ii) suburban, and (iii) forest scenarios. Additionally, multi-connectivity techniques are explored to determine the benefits in connectivity when two technologies are used simultaneously. For instance, the outage probability of Starlink and OneWeb in urban areas is reduced from approximately 12-21\\% to 2\\% when both solutions are employed together. Finally, the joint analysis of KPIs in both terrestrial and non-terrestrial networks demonstrates that their integration enhances coverage, improves performance, and increases reliability, highlighting the benefits of combining satellite and terrestrial systems in the analyzed environments."}
{"id": "2512.18210", "categories": ["cs.SD", "eess.SP"], "pdf": "https://arxiv.org/pdf/2512.18210", "abs": "https://arxiv.org/abs/2512.18210", "authors": ["Wen Huang", "Yuchen Mao", "Yanmin Qian"], "title": "A Data-Centric Approach to Generalizable Speech Deepfake Detection", "comment": null, "summary": "Achieving robust generalization in speech deepfake detection (SDD) remains a primary challenge, as models often fail to detect unseen forgery methods. While research has focused on model-centric and algorithm-centric solutions, the impact of data composition is often underexplored. This paper proposes a data-centric approach, analyzing the SDD data landscape from two practical perspectives: constructing a single dataset and aggregating multiple datasets. To address the first perspective, we conduct a large-scale empirical study to characterize the data scaling laws for SDD, quantifying the impact of source and generator diversity. To address the second, we propose the Diversity-Optimized Sampling Strategy (DOSS), a principled framework for mixing heterogeneous data with two implementations: DOSS-Select (pruning) and DOSS-Weight (re-weighting). Our experiments show that DOSS-Select outperforms the naive aggregation baseline while using only 3% of the total available data. Furthermore, our final model, trained on a 12k-hour curated data pool using the optimal DOSS-Weight strategy, achieves state-of-the-art performance, outperforming large-scale baselines with greater data and model efficiency on both public benchmarks and a new challenge set of various commercial APIs."}

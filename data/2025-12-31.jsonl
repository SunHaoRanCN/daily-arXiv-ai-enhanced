{"id": "2512.22143", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.22143", "abs": "https://arxiv.org/abs/2512.22143", "authors": ["Gaofeng Dong", "Kang Yang", "Mani Srivastava"], "title": "UniFi: Combining Irregularly Sampled CSI from Diverse Communication Packets and Frequency Bands for Wi-Fi Sensing", "comment": "14 pages, 10 figures", "summary": "Existing Wi-Fi sensing systems rely on injecting high-rate probing packets to extract channel state information (CSI), leading to communication degradation and poor deployability. Although Integrated Sensing and Communication (ISAC) is a promising direction, existing solutions still rely on auxiliary packet injection because they exploit only CSI from data frames. We present UniFi, the first Wi-Fi-based ISAC framework that fully eliminates intrusive packet injection by directly exploiting irregularly sampled CSI from diverse communication packets across multiple frequency bands. UniFi integrates a CSI sanitization pipeline to harmonize heterogeneous packets and remove burst-induced redundancy, together with a time-aware attention model that learns directly from non-uniform CSI sequences without resampling. We further introduce CommCSI-HAR, the first dataset with irregularly sampled CSI from real-world dual-band communication traffic. Extensive evaluations on this dataset and four public benchmarks show that UniFi achieves state-of-the-art accuracy with a compact model size, while fully preserving communication throughput."}
{"id": "2512.22146", "categories": ["eess.SP", "cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2512.22146", "abs": "https://arxiv.org/abs/2512.22146", "authors": ["Hanbeot Park", "Yunjeong Cho", "Hunhee Kim"], "title": "EEG-to-Voice Decoding of Spoken and Imagined speech Using Non-Invasive EEG", "comment": "20 pages, 7 figures, 4 tables", "summary": "Restoring speech communication from neural signals is a central goal of brain-computer interface research, yet EEG-based speech reconstruction remains challenging due to limited spatial resolution, susceptibility to noise, and the absence of temporally aligned acoustic targets in imagined speech. In this study, we propose an EEG-to-Voice paradigm that directly reconstructs speech from non-invasive EEG signals without dynamic time warping (DTW) or explicit temporal alignment. The proposed pipeline generates mel-spectrograms from EEG in an open-loop manner using a subject-specific generator, followed by pretrained vocoder and automatic speech recognition (ASR) modules to synthesize speech waveforms and decode text. Separate generators were trained for spoken speech and imagined speech, and transfer learning-based domain adaptation was applied by pretraining on spoken speech and adapting to imagined speech. A minimal language model-based correction module was optionally applied to correct limited ASR errors while preserving semantic structure. The framework was evaluated under 2 s and 4 s speech conditions using acoustic-level metrics (PCC, RMSE, MCD) and linguistic-level metrics (CER, WER). Stable acoustic reconstruction and comparable linguistic accuracy were observed for both spoken speech and imagined speech. While acoustic similarity decreased for longer utterances, text-level decoding performance was largely preserved, and word-position analysis revealed a mild increase in decoding errors toward later parts of sentences. The language model-based correction consistently reduced CER and WER without introducing semantic distortion. These results demonstrate the feasibility of direct, open-loop EEG-to-Voice reconstruction for spoken speech and imagined speech without explicit temporal alignment."}
{"id": "2512.22151", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.22151", "abs": "https://arxiv.org/abs/2512.22151", "authors": ["Emna Bouzid", "Noura Baccar", "Kamran Iqbal", "Yassine Chaouch", "Fares Ben Youssef", "Amine Regayeg", "Sarra Toumi", "Houda Nsir", "Amina Mseddi", "Leila Costelle"], "title": "Machine Learning-Based Basil Yield Prediction in IoT-Enabled Indoor Vertical Hydroponic Farms", "comment": "38 pages, 11 figures, 7 tables", "summary": "As agriculture faces increasing pressure from water scarcity, especially in regions like Tunisia, innovative, resource-efficient solutions are urgently needed. This work explores the integration of indoor vertical hydroponics with Machine Learning (ML) techniques to optimize basil yield while saving water. This research develops a prediction system that uses different ML models and assesses their performance. The models were systematically trained and tested using data collected from IoT sensors of various environmental parameters like CO2, light. The experimental setup features 21 basil crops and uses Raspberry Pi and Arduino. 10k data points were collected and used to train and evaluate three ML models: Linear Regression (LR), Long Short-Term Memory (LSTM), and Deep Neural Networks (DNN). The comparative analysis of the performance of each model revealed that, while LSTM showed high predictive capability and accuracy of 99%, its execution time was 10 times longer than LR and its RAM usage was about 3 times higher than DNN's when simulated on a standard CPU environment. Conversely, the DNN model had an accuracy rate of 98%. This proves an efficient balance between computational speed and prediction quality, which makes this model well-suited for real-life deployment. Moreover, LR excelled in fast processing of basic prediction with an execution time of 11 seconds. This makes the LR model more suitable for low-complexity or resource-limited applications. These performance trade-offs highlight the potential of DNN-based solutions for building responsive, high-accuracy decision-support systems tailored to agricultural environments, making it suitable for future edge-device deployment."}
{"id": "2512.22172", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.22172", "abs": "https://arxiv.org/abs/2512.22172", "authors": ["Md Shahriar Sajid", "Abhijit Kumar Ghosh", "Fariha Nusrat"], "title": "PaperNet: Efficient Temporal Convolutions and Channel Residual Attention for EEG Epilepsy Detection", "comment": "15 pages, 4 figures, International Conference on Intelligent Data Analysis and Applications (IDAA 2025)", "summary": "Electroencephalography (EEG) signals contain rich temporal-spectral structure but are difficult to model due to noise, subject variability, and multi-scale dynamics. Lightweight deep learning models have shown promise, yet many either rely solely on local convolutions or require heavy recurrent modules. This paper presents PaperNet, a compact hybrid architecture that combines temporal convolutions, a channel-wise residual attention module, and a lightweight bidirectional recurrent block which is used for short-window classification. Using the publicly available BEED: Bangalore EEG Epilepsy Dataset, we evaluate PaperNet under a clearly defined subject-independent training protocol and compare it against established and widely used lightweight baselines. The model achieves a macro-F1 of 0.96 on the held-out test set with approximately 0.6M parameters, while maintaining balanced performance across all four classes. An ablation study demonstrates the contribution of temporal convolutions, residual attention, and recurrent aggregation. Channel-wise attention weights further offer insights into electrode relevance. Computational profiling shows that PaperNet remains efficient enough for practical deployment on resource-constrained systems through out the whole process. These results indicate that carefully combining temporal filtering, channel reweighting, and recurrent context modeling can yield strong EEG classification performance without excessive computational cost."}
{"id": "2512.22564", "categories": ["eess.AS", "cs.AI", "cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2512.22564", "abs": "https://arxiv.org/abs/2512.22564", "authors": ["Atakan Işık", "Selin Vulga Işık", "Ahmet Feridun Işık", "Mahşuk Taylan"], "title": "Geometry-Aware Optimization for Respiratory Sound Classification: Enhancing Sensitivity with SAM-Optimized Audio Spectrogram Transformers", "comment": "10 pages, 3 figures,2 tables", "summary": "Respiratory sound classification is hindered by the limited size, high noise levels, and severe class imbalance of benchmark datasets like ICBHI 2017. While Transformer-based models offer powerful feature extraction capabilities, they are prone to overfitting and often converge to sharp minima in the loss landscape when trained on such constrained medical data. To address this, we introduce a framework that enhances the Audio Spectrogram Transformer (AST) using Sharpness-Aware Minimization (SAM). Instead of merely minimizing the training loss, our approach optimizes the geometry of the loss surface, guiding the model toward flatter minima that generalize better to unseen patients. We also implement a weighted sampling strategy to handle class imbalance effectively. Our method achieves a state-of-the-art score of 68.10% on the ICBHI 2017 dataset, outperforming existing CNN and hybrid baselines. More importantly, it reaches a sensitivity of 68.31%, a crucial improvement for reliable clinical screening. Further analysis using t-SNE and attention maps confirms that the model learns robust, discriminative features rather than memorizing background noise."}
{"id": "2512.22148", "categories": ["cs.SD", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.22148", "abs": "https://arxiv.org/abs/2512.22148", "authors": ["Jin Sob Kim", "Hyun Joon Park", "Wooseok Shin", "Sung Won Han"], "title": "Rethinking Leveraging Pre-Trained Multi-Layer Representations for Speaker Verification", "comment": "Accepted to Interspeech 2025", "summary": "Recent speaker verification studies have achieved notable success by leveraging layer-wise output from pre-trained Transformer models. However, few have explored the advancements in aggregating these multi-level features beyond the static weighted average. We present Layer Attentive Pooling (LAP), a novel strategy for aggregating inter-layer representations from pre-trained speech models for speaker verification. LAP assesses the significance of each layer from multiple perspectives time-dynamically, and employs max pooling instead of averaging. Additionally, we propose a lightweight backend speaker model comprising LAP and Attentive Statistical Temporal Pooling (ASTP) to extract speaker embeddings from pre-trained model output. Experiments on the VoxCeleb benchmark reveal that our compact architecture achieves state-of-the-art performance while greatly reducing the training time. We further analyzed LAP design and its dynamic weighting mechanism for capturing speaker characteristics."}
{"id": "2512.22393", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2512.22393", "abs": "https://arxiv.org/abs/2512.22393", "authors": ["Alexander Venus", "Erik Leitinger", "Klaus Witrisal"], "title": "Simultaneous Source Separation, Synchronization, Localization and Mapping for 6G Systems", "comment": "8 pages, 6 figures", "summary": "Multipath-based simultaneous localization and mapping (MP-SLAM) is a promising approach for future 6G networks to jointly estimate the positions of transmitters and receivers together with the propagation environment. In cooperative MP-SLAM, information collected by multiple mobile terminals (MTs) is fused to enhance accuracy and robustness. Existing methods, however, typically assume perfectly synchronized base stations (BSs) and orthogonal transmission sequences, rendering inter-BS interference at the MTs negligible. In this work, we relax these assumptions and address simultaneous source separation, synchronization, and mapping. A relevant example arises in modern 5G systems, where BSs employ muting patterns to mitigate interference, yet localization performance still degrades. We propose a novel BS-dependent data association and synchronization bias model, integrated into a joint Bayesian framework and inferred via the sum-product algorithm on a factor graph. The impact of joint synchronization and source separation is analyzed under various system configurations. Compared with state-of-the-art cooperative MP-SLAM assuming orthogonal and synchronized BSs, our statistical analysis shows no significant performance degradation."}
{"id": "2512.22915", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2512.22915", "abs": "https://arxiv.org/abs/2512.22915", "authors": ["Ken Kurata", "Gen Sato", "Izumi Tsunokuni", "Yusuke Ikeda"], "title": "Spatial Interpolation of Room Impulse Responses based on Deeper Physics-Informed Neural Networks with Residual Connections", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "The room impulse response (RIR) characterizes sound propagation in a room from a loudspeaker to a microphone under the linear time-invariant assumption. Estimating RIRs from a limited number of measurement points is crucial for sound propagation analysis and visualization. Physics-informed neural networks (PINNs) have recently been introduced for accurate RIR estimation by embedding governing physical laws into deep learning models; however, the role of network depth has not been systematically investigated. In this study, we developed a deeper PINN architecture with residual connections and analyzed how network depth affects estimation performance. We further compared activation functions, including tanh and sinusoidal activations. Our results indicate that the residual PINN with sinusoidal activations achieves the highest accuracy for both interpolation and extrapolation of RIRs. Moreover, the proposed architecture enables stable training as the depth increases and yields notable improvements in estimating reflection components. These results provide practical guidelines for designing deep and stable PINNs for acoustic-inverse problems."}
{"id": "2512.22156", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2512.22156", "abs": "https://arxiv.org/abs/2512.22156", "authors": ["Jin Sob Kim", "Hyun Joon Park", "Wooseok Shin", "Sung Won Han"], "title": "A Robust framework for sound event localization and detection on real recordings", "comment": "Technical Report submitted to DCASE 2022 Challenge Task 3 (Winner of the Judge's Award)", "summary": "This technical report describes the systems submitted to the DCASE2022 challenge task 3: sound event localization and detection (SELD). The task aims to detect occurrences of sound events and specify their class, furthermore estimate their position. Our system utilizes a ResNet-based model under a proposed robust framework for SELD. To guarantee the generalized performance on the real-world sound scenes, we design the total framework with augmentation techniques, a pipeline of mixing datasets from real-world sound scenes and emulations, and test time augmentation. Augmentation techniques and exploitation of external sound sources enable training diverse samples and keeping the opportunity to train the real-world context enough by maintaining the number of the real recording samples in the batch. In addition, we design a test time augmentation and a clustering-based model ensemble method to aggregate confident predictions. Experimental results show that the model under a proposed framework outperforms the baseline methods and achieves competitive performance in real-world sound recordings."}
{"id": "2512.22479", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2512.22479", "abs": "https://arxiv.org/abs/2512.22479", "authors": ["Hong-Bae Jeon"], "title": "FARIS: Fluid-Active-RIS", "comment": "12 pages, 8 figures", "summary": "In this paper, we introduce a fluid-active reconfigurable intelligent surface (FARIS) that combines fluid-based port repositioning with per-element active amplification to enhance the performance of 6G network. To characterize the performance, we formulate an ergodic-rate maximization problem that jointly optimizes both the active amplification-reflection vector and the discrete selection of fluid active elements under practical hardware constraints. The problem is addressed via an alternating optimization (AO) framework, which progressively improves the rate. Complexity and convergence analyses that follow furnish deeper insight into the algorithmic operation and performance enhancement. Numerical results confirm that the proposed FARIS with AO framework consistently outperforms conventional FRIS/ARIS, delivering higher rates across diverse environments, often even when using fewer active elements or a smaller physical aperture."}
{"id": "2512.23278", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2512.23278", "abs": "https://arxiv.org/abs/2512.23278", "authors": ["Zengwei Yao", "Wei Kang", "Han Zhu", "Liyong Guo", "Lingxuan Ye", "Fangjun Kuang", "Weiji Zhuang", "Zhaoqing Li", "Zhifeng Han", "Long Lin", "Daniel Povey"], "title": "Flow2GAN: Hybrid Flow Matching and GAN with Multi-Resolution Network for Few-step High-Fidelity Audio Generation", "comment": null, "summary": "Existing dominant methods for audio generation include Generative Adversarial Networks (GANs) and diffusion-based methods like Flow Matching. GANs suffer from slow convergence and potential mode collapse during training, while diffusion methods require multi-step inference that introduces considerable computational overhead. In this work, we introduce Flow2GAN, a two-stage framework that combines Flow Matching training for learning generative capabilities with GAN fine-tuning for efficient few-step inference. Specifically, given audio's unique properties, we first improve Flow Matching for audio modeling through: 1) reformulating the objective as endpoint estimation, avoiding velocity estimation difficulties when involving empty regions; 2) applying spectral energy-based loss scaling to emphasize perceptually salient quieter regions. Building on these Flow Matching adaptations, we demonstrate that a further stage of lightweight GAN fine-tuning enables us to obtain one-step generator that produces high-quality audio. In addition, we develop a multi-branch network architecture that processes Fourier coefficients at different time-frequency resolutions, which improves the modeling capabilities compared to prior single-resolution designs. Experimental results indicate that our Flow2GAN delivers high-fidelity audio generation from Mel-spectrograms or discrete audio tokens, achieving better quality-efficiency trade-offs than existing state-of-the-art GAN-based and Flow Matching-based methods. Online demo samples are available at https://flow2gan.github.io, and the source code is released at https://github.com/k2-fsa/Flow2GAN."}
{"id": "2512.22165", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2512.22165", "abs": "https://arxiv.org/abs/2512.22165", "authors": ["Xuanfan Ni", "Fei Yang", "Fengping Tian", "Qingjuan Li", "Chenyang Lyu", "Yichao Du", "Longyue Wang", "Weihua Luo", "Kaifu Zhang"], "title": "Marco-ASR: A Principled and Metric-Driven Framework for Fine-Tuning Large-Scale ASR Models for Domain Adaptation", "comment": "Technical Report", "summary": "Automatic Speech Recognition (ASR) models have achieved remarkable accuracy in general settings, yet their performance often degrades in domain-specific applications due to data mismatch and linguistic variability. This challenge is amplified for modern Large Language Model (LLM)-based ASR systems, whose massive scale and complex training dynamics make effective fine-tuning non-trivial. To address this gap, this paper proposes a principled and metric-driven fine-tuning framework for adapting both traditional and LLM-based ASR models to specialized domains. The framework emphasizes learning rate optimization based on performance metrics, combined with domain-specific data transformation and augmentation. We empirically evaluate our framework on state-of-the-art models, including Whisper, Whisper-Turbo, and Qwen2-Audio, across multi-domain, multilingual, and multi-length datasets. Our results not only validate the proposed framework but also establish practical protocols for improving domain-specific ASR performance while preventing overfitting."}
{"id": "2512.22513", "categories": ["eess.SP", "eess.IV"], "pdf": "https://arxiv.org/pdf/2512.22513", "abs": "https://arxiv.org/abs/2512.22513", "authors": ["Jipeng Gan", "Le Liang", "Hua Zhang", "Chongtao Guo", "Shi Jin"], "title": "CoDS: Collaborative Perception via Digital Semantic Communication", "comment": null, "summary": "Semantic communication has been introduced into collaborative perception systems for autonomous driving, offering a promising approach to enhancing data transmission efficiency and robustness. Despite its potential, existing semantic communication approaches predominantly rely on analog transmission models, rendering these systems fundamentally incompatible with the digital architecture of modern vehicle-to-everything (V2X) networks and posing a significant barrier to real-world deployment. To bridge this critical gap, we propose CoDS, a novel collaborative perception framework based on digital semantic communication, designed to realize semantic-level transmission efficiency within practical digital communication systems. Specifically, we develop a semantic compression codec that extracts and compresses task-oriented semantic features while preserving downstream perception accuracy. Building on this, we propose a novel semantic analog-to-digital converter that converts these continuous semantic features into a discrete bitstream, ensuring integration with existing digital communication pipelines. Furthermore, we develop an uncertainty-aware network (UAN) that assesses the reliability of each received feature and discards those corrupted by decoding failures, thereby mitigating the cliff effect of conventional channel coding schemes under low signal-to-noise ratio (SNR) conditions. Extensive experiments demonstrate that CoDS significantly outperforms existing semantic communication and traditional digital communication schemes, achieving state-of-the-art perception performance while ensuring compatibility with practical digital V2X systems."}
{"id": "2512.23322", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2512.23322", "abs": "https://arxiv.org/abs/2512.23322", "authors": ["Dhruv Nigam"], "title": "Single Channel Blind Dereverberation of Speech Signals", "comment": null, "summary": "Dereverberation of recorded speech signals is one of the most pertinent problems in speech processing. In the present work, the objective is to understand and implement dereverberation techniques that aim at enhancing the magnitude spectrogram of reverberant speech signals to remove the reverberant effects introduced. An approach to estimate a clean speech spectrogram from the reverberant speech spectrogram is proposed. This is achieved through non-negative matrix factor deconvolution(NMFD). Further, this approach is extended using the NMF representation for speech magnitude spectrograms. To exploit temporal dependencies, a convolutive NMF-based representation and a frame-stacked model are incorporated into the NMFD framework for speech. A novel approach for dereverberation by applying NMFD to the activation matrix of the reverberated magnitude spectrogram is also proposed. Finally, a comparative analysis of the performance of the listed techniques, using sentence recordings from the TIMIT database and recorded room impulse responses from the Reverb 2014 challenge, is presented based on two key objective measures - PESQ and Cepstral Distortion.\\\\ Although we were qualitatively able to verify the claims made in literature regarding these techniques, exact results could not be matched. The novel approach, as it is suggested, provides improvement in quantitative metrics, but is not consistent"}
{"id": "2512.22166", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2512.22166", "abs": "https://arxiv.org/abs/2512.22166", "authors": ["HaeChun Chung"], "title": "AudioGAN: A Compact and Efficient Framework for Real-Time High-Fidelity Text-to-Audio Generation", "comment": "10 pages, 6 figures, Accepted to AES AIMLA 2025", "summary": "Text-to-audio (TTA) generation can significantly benefit the media industry by reducing production costs and enhancing work efficiency. However, most current TTA models (primarily diffusion-based) suffer from slow inference speeds and high computational costs. In this paper, we introduce AudioGAN, the first successful Generative Adversarial Networks (GANs)-based TTA framework that generates audio in a single pass, thereby reducing model complexity and inference time. To overcome the inherent difficulties in training GANs, we integrate multiple ,contrastive losses and propose innovative components Single-Double-Triple (SDT) Attention and Time-Frequency Cross-Attention (TF-CA). Extensive experiments on the AudioCaps dataset demonstrate that AudioGAN achieves state-of-the-art performance while using 90% fewer parameters and running 20 times faster, synthesizing audio in under one second. These results establish AudioGAN as a practical and powerful solution for real-time TTA."}
{"id": "2512.22527", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2512.22527", "abs": "https://arxiv.org/abs/2512.22527", "authors": ["Hongwei Xu", "Weichao Zheng", "Zai Yang"], "title": "Compressive Toeplitz Covariance Estimation From Few-Bit Quantized Measurements With Applications to DOA Estimation", "comment": null, "summary": "This paper addresses the problem of estimating the Hermitian Toeplitz covariance matrix under practical hardware constraints of sparse observations and coarse quantization. Within the triangular-dithered quantization framework, we propose an estimator called Toeplitz-projected sample covariance matrix (Q-TSCM) to compensate for the quantization-induced bias, together with its finite-bit counterpart termed the $2k$-bit Toeplitz-projected sample covariance matrix ($2k$-TSCM), obtained by truncating the pre-quantization observations. Under the complex Gaussian assumption, we derive non-asymptotic error bounds of the estimators that reveal a quadratic dependence on the quantization level and capture the effect of sparse sampling patterns through the so-called coverage coefficient. To further improve performance, we propose the quantized sparse and parametric approach (Q-SPA) based on a covariance-fitting criterion, which enforces additionally positive semidefiniteness at the cost of solving a semidefinite program. Numerical experiments are presented that corroborate our theoretical findings and demonstrate the effectiveness of the proposed estimators in the application to direction-of-arrival estimation."}
{"id": "2512.22166", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2512.22166", "abs": "https://arxiv.org/abs/2512.22166", "authors": ["HaeChun Chung"], "title": "AudioGAN: A Compact and Efficient Framework for Real-Time High-Fidelity Text-to-Audio Generation", "comment": "10 pages, 6 figures, Accepted to AES AIMLA 2025", "summary": "Text-to-audio (TTA) generation can significantly benefit the media industry by reducing production costs and enhancing work efficiency. However, most current TTA models (primarily diffusion-based) suffer from slow inference speeds and high computational costs. In this paper, we introduce AudioGAN, the first successful Generative Adversarial Networks (GANs)-based TTA framework that generates audio in a single pass, thereby reducing model complexity and inference time. To overcome the inherent difficulties in training GANs, we integrate multiple ,contrastive losses and propose innovative components Single-Double-Triple (SDT) Attention and Time-Frequency Cross-Attention (TF-CA). Extensive experiments on the AudioCaps dataset demonstrate that AudioGAN achieves state-of-the-art performance while using 90% fewer parameters and running 20 times faster, synthesizing audio in under one second. These results establish AudioGAN as a practical and powerful solution for real-time TTA."}
{"id": "2512.22621", "categories": ["cs.SD", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.22621", "abs": "https://arxiv.org/abs/2512.22621", "authors": ["Pierre Mackenzie"], "title": "Chord Recognition with Deep Learning", "comment": null, "summary": "Progress in automatic chord recognition has been slow since the advent of deep learning in the field. To understand why, I conduct experiments on existing methods and test hypotheses enabled by recent developments in generative models. Findings show that chord classifiers perform poorly on rare chords and that pitch augmentation boosts accuracy. Features extracted from generative models do not help and synthetic data presents an exciting avenue for future work. I conclude by improving the interpretability of model outputs with beat detection, reporting some of the best results in the field and providing qualitative analysis. Much work remains to solve automatic chord recognition, but I hope this thesis will chart a path for others to try."}
{"id": "2512.22578", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2512.22578", "abs": "https://arxiv.org/abs/2512.22578", "authors": ["Syed Luqman Shah", "Nurul Huda Mahmood"], "title": "A Novel Geometry-Aware GPR-Based Energy-Efficient and Low-Overhead Channel Estimation Scheme", "comment": "Submitted for possible publication in IEEE", "summary": "In this work, we model the wireless channel as a complex-valued Gaussian process (GP) over the transmit and receive antenna arrays. The channel covariance is characterized using an antenna-geometry-based spectral mixture covariance function (GB-SMCF), which captures the spatial structure of the antenna arrays. To address the problem of accurate channel state information (CSI) estimation from very few noisy observations, we develop a Gaussian process regression (GPR)-based channel estimation framework that employs the GB-SMCF as a prior covariance model with online hyperparameter optimization. In the proposed scheme, the full channel is learned by transmitting pilots from only a small subset of transmit antennas while receiving them at all receive antennas, resulting in noisy partial CSI at the receiver. These limited observations are then processed by the GPR framework, which updates the GB-SMCF hyperparameters online from incoming measurements and reconstructs the full CSI in real time. Simulation results demonstrate that the proposed GB-SMCF-based estimator outperforms baseline methods while reducing pilot overhead and training energy by up to 50$\\%$ compared to conventional schemes."}
{"id": "2512.23435", "categories": ["cs.SD", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.23435", "abs": "https://arxiv.org/abs/2512.23435", "authors": ["Saifelden M. Ismail"], "title": "Mobile-Efficient Speech Emotion Recognition Using DistilHuBERT: A Cross-Corpus Validation Study", "comment": "5 pages, 2 tables, 1 figure. Submitted to IEEE conference", "summary": "Speech Emotion Recognition (SER) has significant potential for mobile applications, yet deployment remains constrained by the computational demands of state-of-the-art transformer architectures. This paper presents a mobile-efficient SER system based on DistilHuBERT, a distilled and 8-bit quantized transformer that achieves 92% parameter reduction compared to full-scale Wav2Vec 2.0 models while maintaining competitive accuracy. We conduct a rigorous 5-fold Leave-One-Session-Out (LOSO) cross-validation on the IEMOCAP dataset to ensure speaker independence, augmented with cross-corpus training on CREMA-D to enhance generalization. Cross-corpus training with CREMA-D yields a 1.2% improvement in Weighted Accuracy, a 1.4% gain in Macro F1-score, and a 32% reduction in cross-fold variance, with the Neutral class showing the most substantial benefit at 5.4% F1-score improvement. Our approach achieves an Unweighted Accuracy of 61.4% with a quantized model footprint of only 23 MB, representing approximately 91% of full-scale baseline performance. Cross-corpus evaluation on RAVDESS reveals that the theatrical nature of acted emotions causes predictions to cluster by arousal level rather than valence: happiness is systematically confused with anger due to acoustic saturation in high-energy expressions. Despite this theatricality effect reducing overall RAVDESS accuracy to 43.29%, the model maintains robust arousal detection with 97% recall for anger and 64% for sadness. These findings establish a Pareto-optimal tradeoff between model size and accuracy, enabling practical affect recognition on resource-constrained mobile devices."}
{"id": "2512.22582", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2512.22582", "abs": "https://arxiv.org/abs/2512.22582", "authors": ["Xinyang Li", "Hian Zing Voon", "Vlad C. Andrei", "Alexander Sessler", "Nunzio Sciammetta", "Ullrich J. Mönich", "Dominic A. Schupke", "Holger Boche"], "title": "Real-Time Multi-Target Detection and Tracking with mmWave 5G NR Waveforms on RFSoC", "comment": null, "summary": "We demonstrate a real-time implementation of multi-target detection and tracking using 5G New Radio (NR) physical downlink shared channel (PDSCH) waveform with 400 MHz bandwidth at 28 GHz carrier frequency. The hardware platform is built on a radio frequency system-on-chip (RFSoC) 4x2 board connected with a pair of Sivers EVK02001 mmWave beamformers for transmission and reception. The entire sensing transceiver processing and fast beam control are realized purely in the programmable logic (PL) part of the RFSoC, enabling low-latency and fully hardware-accelerated operation. The continuously acquired sensing data constitute 3D range-angle (RA) tensors, which are processed on a host PC using adaptive background subtraction, cell-averaging constant false alarm rate (CA-CFAR) detection with density-based spatial clustering of applications with noise (DBSCAN) clustering, and extended Kalman filtering (EKF), to detect and track targets in the environment. Our software-defined radio (SDR) testbed integrates heterogeneous computing resources, including CPUs, GPUs, and FPGAs, thereby providing design flexibility for a wide range of tasks."}
{"id": "2512.22146", "categories": ["eess.SP", "cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2512.22146", "abs": "https://arxiv.org/abs/2512.22146", "authors": ["Hanbeot Park", "Yunjeong Cho", "Hunhee Kim"], "title": "EEG-to-Voice Decoding of Spoken and Imagined speech Using Non-Invasive EEG", "comment": "20 pages, 7 figures, 4 tables", "summary": "Restoring speech communication from neural signals is a central goal of brain-computer interface research, yet EEG-based speech reconstruction remains challenging due to limited spatial resolution, susceptibility to noise, and the absence of temporally aligned acoustic targets in imagined speech. In this study, we propose an EEG-to-Voice paradigm that directly reconstructs speech from non-invasive EEG signals without dynamic time warping (DTW) or explicit temporal alignment. The proposed pipeline generates mel-spectrograms from EEG in an open-loop manner using a subject-specific generator, followed by pretrained vocoder and automatic speech recognition (ASR) modules to synthesize speech waveforms and decode text. Separate generators were trained for spoken speech and imagined speech, and transfer learning-based domain adaptation was applied by pretraining on spoken speech and adapting to imagined speech. A minimal language model-based correction module was optionally applied to correct limited ASR errors while preserving semantic structure. The framework was evaluated under 2 s and 4 s speech conditions using acoustic-level metrics (PCC, RMSE, MCD) and linguistic-level metrics (CER, WER). Stable acoustic reconstruction and comparable linguistic accuracy were observed for both spoken speech and imagined speech. While acoustic similarity decreased for longer utterances, text-level decoding performance was largely preserved, and word-position analysis revealed a mild increase in decoding errors toward later parts of sentences. The language model-based correction consistently reduced CER and WER without introducing semantic distortion. These results demonstrate the feasibility of direct, open-loop EEG-to-Voice reconstruction for spoken speech and imagined speech without explicit temporal alignment."}
{"id": "2512.22676", "categories": ["eess.SP", "cs.AR", "cs.DC", "math.NA"], "pdf": "https://arxiv.org/pdf/2512.22676", "abs": "https://arxiv.org/abs/2512.22676", "authors": ["Sergey Salishev"], "title": "Synthesis of signal processing algorithms with constraints on minimal parallelism and memory space", "comment": "English translation of PhD thesis (Candidate of Physical and Mathematical Sciences), defended at Saint Petersburg State University (2017). 191 pages", "summary": "This thesis develops signal-processing algorithms and implementation schemes under constraints of minimal parallelism and memory space, with the goal of improving energy efficiency of low-power computing hardware. We propose (i) a power/energy consumption model for clocked CMOS logic that supports selecting optimal parallelism, (ii) integer-friendly approximation methods for elementary functions that reduce lookup-table size via constrained piecewise-polynomial (quasi-spline) constructions with accuracy guarantees, (iii) provably conflict-free data placement and execution order for mixed-radix streaming FFT on multi-bank and single-port memories, including a self-sorting FFT variant, and (iv) a parallelism/memory analysis of the fast Schur algorithm for superfast Toeplitz system solving, motivated by echo-cancellation workloads. The results provide constructive theorems, schedules, and design trade-offs enabling efficient specialized accelerators."}
{"id": "2512.22564", "categories": ["eess.AS", "cs.AI", "cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2512.22564", "abs": "https://arxiv.org/abs/2512.22564", "authors": ["Atakan Işık", "Selin Vulga Işık", "Ahmet Feridun Işık", "Mahşuk Taylan"], "title": "Geometry-Aware Optimization for Respiratory Sound Classification: Enhancing Sensitivity with SAM-Optimized Audio Spectrogram Transformers", "comment": "10 pages, 3 figures,2 tables", "summary": "Respiratory sound classification is hindered by the limited size, high noise levels, and severe class imbalance of benchmark datasets like ICBHI 2017. While Transformer-based models offer powerful feature extraction capabilities, they are prone to overfitting and often converge to sharp minima in the loss landscape when trained on such constrained medical data. To address this, we introduce a framework that enhances the Audio Spectrogram Transformer (AST) using Sharpness-Aware Minimization (SAM). Instead of merely minimizing the training loss, our approach optimizes the geometry of the loss surface, guiding the model toward flatter minima that generalize better to unseen patients. We also implement a weighted sampling strategy to handle class imbalance effectively. Our method achieves a state-of-the-art score of 68.10% on the ICBHI 2017 dataset, outperforming existing CNN and hybrid baselines. More importantly, it reaches a sensitivity of 68.31%, a crucial improvement for reliable clinical screening. Further analysis using t-SNE and attention maps confirms that the model learns robust, discriminative features rather than memorizing background noise."}
{"id": "2512.22686", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2512.22686", "abs": "https://arxiv.org/abs/2512.22686", "authors": ["Kumar Sai Bondada", "Daniel J. Jakubisin", "R. Michael Buehrer"], "title": "Multistatic Radar Performance in the Presence of Distributed Wireless Synchronization", "comment": null, "summary": "This paper proposes a multistatic radar (MSR) system utilizing a distributed wireless synchronization protocol. The wireless synchronization protocol uses a two-tone waveform exchange for frequency synchronization and a bi-directional waveform exchange for time synchronization, independent of GPS. A Bayesian Cramer-Rao lower bound (BCRLB) framework is developed to quantify the impact of synchronization offsets on joint delay and Doppler estimation, and consequently, on target localization and velocity estimation accuracy. Simulation results derived from the analytical expressions establish the extent to which the residual synchronization offsets degrade the MSR's performance. The performance of the synchronization links primarily depends on the synchronization-link channel and transmit parameters; optimizing these parameters enables the MSR configuration to surpass the monostatic performance and approach the ideal case. Furthermore, the simulated synchronization-link parameters suggest that practical implementation is feasible."}
{"id": "2512.22693", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2512.22693", "abs": "https://arxiv.org/abs/2512.22693", "authors": ["Daiqi Zhang", "Bizhu Wang", "Wenqi Zhang", "Chen Sun", "Xiaodong Xu"], "title": "Instance Communication System for Intelligent Connected Vehicles: Bridging the Gap from Semantic to Instance-Level Transmission", "comment": "5 pages, 3 figures", "summary": "Intelligent Connected Vehicles (ICVs) rely on high-speed data transmission for efficient and safety-critical services. However, the scarcity of wireless resources limits the capabilities of ICVs. Semantic Communication (SemCom) systems can alleviate this issue by extracting and transmitting task-relevant information, termed semantic information, instead of the entire raw data. Despite this, we reveal that residual redundancy persists within SemCom systems, where not all instances under the same semantic category are equally critical for downstream tasks. To tackle this issue, we introduce Instance Communication (InsCom), which elevates communication from the semantic level to the instance level for ICVs. Specifically, InsCom uses a scene graph generation model to identify all image instances and analyze their inter-relationships, thus distinguishing between semantically identical instances. Additionally, it applies user-configurable, task-critical criteria based on subject semantics and relation-object pairs to filter recognized instances. Consequently, by transmitting only task-critical instances, InsCom significantly reduces data redundancy, substantially enhancing transmission efficiency within limited wireless resources. Evaluations across various datasets and wireless channel conditions show that InsCom achieves a data volume reduction of over 7.82 times and a quality improvement ranging from 1.75 to 14.03 dB compared to the state-of-the-art SemCom systems."}
{"id": "2512.22825", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2512.22825", "abs": "https://arxiv.org/abs/2512.22825", "authors": ["Ke Wang", "Chan-Tong Lam", "Benjamin K. Ng", "Yue Liu"], "title": "On the Impact of Phase Errors in Phase-Dependent Amplitudes of Near-Field RISs", "comment": "Accepted for publication in IEEE Transactions on Vehicular Technology, 2025, doi: 10.1109/TVT.2025.3647594", "summary": "This paper investigates mutual coupling between phase-dependent amplitudes (PDAs) and designed phase shifts within pixels of near-field (NF) reconfigurable intelligent surfaces (RISs) in the presence of phase errors (PEs). In contrast to existing research that treats phase shifts with errors (PSEs) and the PDAs separately, we introduce a remaining power (RP) metric to quantify the proportion of power preserved in the signals reflected by the RIS, and we prove its asymptotic convergence to theoretical values by leveraging extended Glivenko-Cantelli theorem. Then, the RP of signals passing through RIS pixels is jointly examined under combined phase and amplitude uncertainties. In addition, we propose four pixel reflection models to capture practical conditions, and we derive approximate polynomial upper bounds for the RP with error terms by applying Taylor expansion. Furthermore, based on Friis transmission formula and projected aperture, we propose a general NF channel model that incorporates the coupling between the PSEs and the PDAs. By using Cauchy-Bunyakovsky-Schwarz inequality and Riemann sums, we derive a closed-form upper bound on spectral efficiency, and the bound becomes tighter as the pixel area decreases. We reveal that as the RIS phase shifts approach the ends of their range, the RP under independent and identically distributed PEs is smaller than that under fully correlated PEs, whereas this relationship reverses when the phase shifts are near the middle of their range. Neglecting the PEs in the PDAs leads to an overestimation of the RIS performance gain, explaining the discrepancies between theoretical and measured results."}
{"id": "2512.22840", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2512.22840", "abs": "https://arxiv.org/abs/2512.22840", "authors": ["Haoyu Wang", "Zhi Sun", "Shuangfeng Han", "Xiaoyun Wang", "Zhaocheng Wang"], "title": "Generalizable Learning for Massive MIMO CSI Feedback in Unseen Environments", "comment": null, "summary": "Deep learning is promising to enhance the accuracy and reduce the overhead of channel state information (CSI) feedback, which can boost the capacity of frequency division duplex (FDD) massive multiple-input multiple-output (MIMO) systems. Nevertheless, the generalizability of current deep learning-based CSI feedback algorithms cannot be guaranteed in unseen environments, which induces a high deployment cost. In this paper, the generalizability of deep learning-based CSI feedback is promoted with physics interpretation. Firstly, the distribution shift of the cluster-based channel is modeled, which comprises the multi-cluster structure and single-cluster response. Secondly, the physics-based distribution alignment is proposed to effectively address the distribution shift of the cluster-based channel, which comprises multi-cluster decoupling and fine-grained alignment. Thirdly, the efficiency and robustness of physics-based distribution alignment are enhanced. Explicitly, an efficient multi-cluster decoupling algorithm is proposed based on the Eckart-Young-Mirsky (EYM) theorem to support real-time CSI feedback. Meanwhile, a hybrid criterion to estimate the number of decoupled clusters is designed, which enhances the robustness against channel estimation error. Fourthly, environment-generalizable neural network for CSI feedback (EG-CsiNet) is proposed as a novel learning framework with physics-based distribution alignment. Based on extensive simulations and sim-to-real experiments in various conditions, the proposed EG-CsiNet can robustly reduce the generalization error by more than 3 dB compared to the state-of-the-arts."}
{"id": "2512.22926", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2512.22926", "abs": "https://arxiv.org/abs/2512.22926", "authors": ["Dongli Cai", "Xihe Chen", "Yaosheng Chen", "Hong Xian", "Baoxian Yu", "Han Zhang"], "title": "Confidence analysis-based hybrid heartbeat detection for ballistocardiogram using template matching and deep learning", "comment": null, "summary": "Heartbeat interval can be detected from ballistocardiogram (BCG) signals in a non-contact manner. Conventional methods achieved heartbeat detection from different perspectives, where template matching (TM) and deep learning (DL) were based on the similarity of neighboring heartbeat episodes and robust spatio-temporal characteristics, respectively, and thus, performed varied from case to case. Inspired by the above facts, we propose confidence analysis-based hybrid heartbeat detection using both TM and DL, and further explore the advantages of both methods in various scenarios. To be specific, the confidence of the heartbeat detection results was evaluated by the consistency of signal morphology and the variability of the detected heartbeat intervals, which could be formulated by the averaged correlation between each heartbeat episode and the detected template and the normalized standard deviation among detected heartbeat intervals, respectively, where the results with higher confidence were remained. In order to validate the effectiveness of the proposed hybrid method, we conducted experiments using practical clinical BCG dataset with 34 subjects including 924,235 heartbeats. Numerical results showed that the proposed hybrid method achieved an average absolute interval error of 20.73 ms, yielding a reduction of 29.28 ms and 10.13 ms compared to solo TM and DL methods, respectively. Besides, case study showed the robustness of heartbeat detection of TM and DL to individual differences and signal quality, respectively, and in turn, validated that the hybrid method could benefit from the complementary advantages of both methods, which demonstrated the superiority of the proposed hybrid method in practical BCG monitoring scenarios."}
{"id": "2512.23000", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2512.23000", "abs": "https://arxiv.org/abs/2512.23000", "authors": ["Mohammed Salah", "Eman Ouda", "Stefano Sfarra", "Davor Svetinovic", "Yusra Abdulrahman"], "title": "Masked Sequence Autoencoding for Enhanced Defect Visualization in Active Infrared Thermography", "comment": null, "summary": "Active infrared thermography (AIRT) became a crucial tool in aerospace non-destructive testing (NDT), enabling the detection of hidden defects and anomalies in materials by capturing thermal responses over time. In AIRT, autoencoders are widely used to enhance defect detection by reducing the dimensionality of thermal data and improving the signal-to-noise ratio. However, traditional AIRT autoencoders often struggle to disentangle subtle defect features from dominant background responses, leading to suboptimal defect analysis under varying material and inspection conditions. To overcome this challenge, this work proposes a Masked CNN-Attention Autoencoder (AIRT-Masked-CAAE) that integrates convolutional feature extraction with attention mechanisms to capture both local thermal patterns and global contextual dependencies. The AIRT-Masked-CAAE introduces a masked sequence autoencoding strategy, where the network learns to infer missing thermal responses from surrounding contextual cues, while suppressing background redundancy. In addition, the proposed masked sequence autoencoding approach enables training on only a subset of the thermal sequence, while providing generalizable latent representations and reducing training time by a factor of 30. The AIRT-Masked-CAAE framework was evaluated using specimens made of PVC, CFRP, and PLA. The results demonstrate that the AIRT-Masked-CAAE surpasses state-of-the-art AIRT autoencoders in terms of contrast, signal-to-noise ratio (SNR), and metrics based on neural networks."}
{"id": "2512.23045", "categories": ["eess.SP", "cs.IT"], "pdf": "https://arxiv.org/pdf/2512.23045", "abs": "https://arxiv.org/abs/2512.23045", "authors": ["Vaibhav Kumar", "Anastasios Papazafeiropoulos", "Pandelis Kourtessis", "John Senior", "Marwa Chafii", "Dimitra I. Kaklamani", "Iakovos S. Venieris"], "title": "Flexible Intelligent Metasurface for Downlink Communications under Statistical CSI", "comment": "5 pages, 4 figures, accepted in IEEE WCL", "summary": "Flexible intelligent metasurface (FIM) is a recently developed, groundbreaking hardware technology with promising potential for 6G wireless systems. Unlike conventional rigid antenna array (RAA)-based transmitters, FIM-assisted transmitters can dynamically alter their physical surface through morphing, offering new degrees of freedom to enhance system performance. In this letter, we depart from prior works that rely on instantaneous channel state information (CSI) and instead address the problem of average sum spectral efficiency maximization under statistical CSI in a FIM-assisted downlink multiuser multiple-input single-output setting. To this end, we first derive the spatial correlation matrix for the FIM-aided transmitter and then propose an iterative FIM optimization algorithm based on the gradient projection method. Simulation results show that with statistical CSI, the FIM-aided system provides a significant performance gain over its RAA-based counterpart in scenarios with strong spatial channel correlation, whereas the gain diminishes when the channels are weakly correlated."}
{"id": "2512.23152", "categories": ["eess.SP", "math.PR"], "pdf": "https://arxiv.org/pdf/2512.23152", "abs": "https://arxiv.org/abs/2512.23152", "authors": ["Jackson Kulik", "Braden Hastings", "Keith A. LeGrand"], "title": "Unscented and Higher-Order Linear Covariance Fidelity Checks and Measures of Non-Gaussianity", "comment": null, "summary": "Linear covariance (LinCov) techniques have gained widespread traction in the modeling of uncertainty, including in the preliminary study of spacecraft navigation performance. While LinCov methods offer improved computational efficiency compared to Monte Carlo based uncertainty analysis, they inherently rely on linearization approximations. Understanding the fidelity of these approximations and identifying when they are deficient is critically important for spacecraft navigation and mission planning, especially when dealing with highly nonlinear systems and large state uncertainties. This work presents a number of computational techniques for assessing linear covariance performance. These new LinCov fidelity measures are formulated using higher-order statistics, constrained optimization, and the unscented transform."}
{"id": "2512.23246", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2512.23246", "abs": "https://arxiv.org/abs/2512.23246", "authors": ["Ziwei Wan", "Zhen Gao", "Fabien Heliot", "Qu Luo", "Pei Xiao", "Haiyang Zhang", "Christos Masouros", "Yonina C. Eldar", "Sheng Chen"], "title": "Ultra-Massive MIMO with Orthogonal Chirp Division Multiplexing for Near-Field Sensing and Communication Integration", "comment": null, "summary": "This paper integrates the emerging ultra-massive multiple-input multiple-output (UM-MIMO) technique with orthogonal chirp division multiplexing (OCDM) waveform to tackle the challenging near-field integrated sensing and communication (ISAC) problem. Specifically, we conceive a comprehensive ISAC architecture, where an UM-MIMO base station adopts OCDM waveform for communications and a co-located sensing receiver adopts the frequency-modulated continuous wave (FMCW) detection principle to simplify the associated hardware. For sensing tasks, several OCDM subcarriers, namely, dedicated sensing subcarriers (DSSs), are each transmitted through a dedicated sensing antenna (DSA) within the transmit antenna array. By judiciously designing the DSS selection scheme and optimizing receiver parameters, the FMCW-based sensing receiver can decouple the echo signals from different DSAs with significantly reduced hardware complexity. This setup enables the estimation of ranges and velocities of near-field targets in an antenna-pairwise manner. Moreover, by leveraging the spatial diversity of UM-MIMO, we introduce the concept of virtual bistatic sensing (VIBS), which incorporates the estimates from multiple antenna pairs to achieve high-accuracy target positioning and three-dimensional velocity measurement. The VIBS paradigm is immune to hostile channel environments characterized by spatial non-stationarity and uncorrelated multipath environment. Furthermore, the channel estimation of UM-MIMO OCDM systems enhanced by the sensing results is investigated. Simulation results demonstrate that the proposed ISAC scheme enhances sensing accuracy, and also benefits communication performance."}
{"id": "2512.23381", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2512.23381", "abs": "https://arxiv.org/abs/2512.23381", "authors": ["Lorenz Bielefeld", "Paul Zheng", "Oner Hanay", "Yao Zhu", "Yulin Hu", "Anke Schmeink"], "title": "On Signal Peak Power Constraint of Over-the-Air Federated Learning", "comment": "Submitted to IEEE", "summary": "Federated learning (FL) has been considered a promising privacy preserving distributed edge learning framework. Over-the-air computation (AirComp) technique leveraging analog transmission enables the aggregation of local updates directly over-the-air by exploiting the superposition properties of wireless multiple-access channel, thereby drastically reducing the communication bottleneck issues of FL compared with digital transmission schemes. This work points out that existing AirComp-FL overlooks a key practical constraint, the instantaneous peak-power constraints imposed by the non-linearity of radiofrequency power amplifiers. We present and analyze the effect of the classic method to deal with this issue, amplitude clipping combined with filtering. We investigate the effect of instantaneous peak-power constraints in AirComp-FL for both single-carrier and multi-carrier orthogonal frequency-division multiplexing (OFDM) systems. We highlight the specificity of AirComp-FL: the samples depend on the gradient value distribution, leading to a higher peak-to-average power ratio (PAPR) than that observed for uniformly distributed signals. Simulation results demonstrate that, in practical settings, the instantaneous transmit power regularly exceeds the power-amplifier limit; however, by applying clipping and filtering, the FL performance can be degraded. The degradation becomes pronounced especially in multi-carrier OFDM systems due to the in-band distortions caused by clipping and filtering."}

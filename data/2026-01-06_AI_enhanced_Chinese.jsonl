{"id": "2601.00819", "categories": ["eess.SP", "eess.SY"], "pdf": "https://arxiv.org/pdf/2601.00819", "abs": "https://arxiv.org/abs/2601.00819", "authors": ["Houtianfu Wang", "Ozgur Akan"], "title": "AFDM for LEO Inter-Satellite Links: Path-Level CSI Prediction and CRLB-Guided Pre-Equalization", "comment": null, "summary": "Low-Earth-orbit (LEO) inter-satellite links must cope with strongly doubly selective channels and aged channel state information (CSI). In this paper, the term ``sensing'' refers to the receiver-side identifiability of a small set of dominant delay--Doppler path parameters, quantified via CRLB-type proxies, rather than a full-fledged target-sensing pipeline. Affine frequency division multiplexing (AFDM) provides a sparse delay--Doppler (DD) representation well suited to such channels, yet most existing AFDM designs assume ideal CSI, operate on grid-based channel coefficients, and optimize only communication performance. This paper proposes a two-stage AFDM-based ISAC framework for mobile LEO ISLs that explicitly operates under predicted CSI. In Stage~I, we model the channel by a small number of dominant specular paths and perform sequence prediction directly on their complex gains, delays, and Dopplers, from which we reconstruct the AFDM DD-domain kernel used as the sole instantaneous CSI at the transmitter. In Stage~II, we design a sensing-aware AFDM pre-equalizer by augmenting the classical minimum mean-square error (MMSE) solution with a term obtained from Cram\u00e9r--Rao-type sensitivity measures evaluated under the predicted channel model, leading to a first-order surrogate of a CRLB-regularized pre-equalizer with a single tuning parameter that controls the communication--sensing tradeoff. Simulation results for representative LEO ISL trajectories show that the proposed path-level predictor improves effective-kernel reconstruction over AFDM-unaware baselines, and that, under predicted CSI, the sensing-aware pre-equalizer significantly improves sensing-oriented metrics over outdated-CSI baselines while keeping symbol error rates close to a communication-oriented MMSE design with only modest additional complexity.", "AI": {"tldr": "\u63d0\u51fa\u7528\u4e8eLEO\u661f\u95f4\u94fe\u8def\u7684AFDM\u53cc\u9636\u6bb5ISAC\u6846\u67b6\uff0c\u5305\u542b\u8def\u5f84\u7ea7\u4fe1\u9053\u9884\u6d4b\u548c\u611f\u77e5\u589e\u5f3a\u9884\u5747\u8861\u5668\u8bbe\u8ba1\uff0c\u5728\u9884\u6d4bCSI\u4e0b\u5e73\u8861\u901a\u4fe1\u4e0e\u611f\u77e5\u6027\u80fd", "motivation": "LEO\u661f\u95f4\u94fe\u8def\u9762\u4e34\u5f3a\u53cc\u9009\u62e9\u6027\u4fe1\u9053\u548c\u8fc7\u65f6CSI\u95ee\u9898\uff0c\u73b0\u6709AFDM\u8bbe\u8ba1\u5047\u8bbe\u7406\u60f3CSI\u4e14\u4ec5\u4f18\u5316\u901a\u4fe1\u6027\u80fd\uff0c\u9700\u8981\u80fd\u5904\u7406\u9884\u6d4bCSI\u5e76\u5e73\u8861\u901a\u4fe1-\u611f\u77e5\u7684\u89e3\u51b3\u65b9\u6848", "method": "\u4e24\u9636\u6bb5\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u5bf9\u5c11\u91cf\u4e3b\u5bfc\u955c\u9762\u8def\u5f84\u8fdb\u884c\u5e8f\u5217\u9884\u6d4b\uff0c\u91cd\u5efaAFDM DD\u57df\u6838\u4f5c\u4e3a\u77ac\u65f6CSI\uff1b\u7b2c\u4e8c\u9636\u6bb5\u8bbe\u8ba1\u611f\u77e5\u589e\u5f3aAFDM\u9884\u5747\u8861\u5668\uff0c\u5728\u7ecf\u5178MMSE\u57fa\u7840\u4e0a\u52a0\u5165CRLB\u578b\u654f\u611f\u5ea6\u5ea6\u91cf\u9879", "result": "\u8def\u5f84\u7ea7\u9884\u6d4b\u5668\u5728AFDM DD\u57df\u6838\u91cd\u5efa\u4e0a\u4f18\u4e8e\u57fa\u7ebf\uff0c\u611f\u77e5\u589e\u5f3a\u9884\u5747\u8861\u5668\u5728\u9884\u6d4bCSI\u4e0b\u663e\u8457\u63d0\u5347\u611f\u77e5\u6307\u6807\uff0c\u540c\u65f6\u4fdd\u6301\u63a5\u8fd1\u901a\u4fe1\u5bfc\u5411MMSE\u8bbe\u8ba1\u7684\u7b26\u53f7\u9519\u8bef\u7387", "conclusion": "\u63d0\u51fa\u7684AFDM\u53cc\u9636\u6bb5ISAC\u6846\u67b6\u80fd\u6709\u6548\u5904\u7406LEO\u661f\u95f4\u94fe\u8def\u7684\u9884\u6d4bCSI\u95ee\u9898\uff0c\u901a\u8fc7\u8def\u5f84\u7ea7\u9884\u6d4b\u548c\u611f\u77e5\u589e\u5f3a\u9884\u5747\u8861\u5b9e\u73b0\u901a\u4fe1\u4e0e\u611f\u77e5\u7684\u826f\u597d\u6743\u8861"}}
{"id": "2601.00820", "categories": ["eess.SP", "eess.SY"], "pdf": "https://arxiv.org/pdf/2601.00820", "abs": "https://arxiv.org/abs/2601.00820", "authors": ["Houtianfu Wang", "Haofan Dong", "Hanlin Cai", "Ozgur B. Akan"], "title": "Environment-to-Link ISAC with Space-Weather Sensing for Ka-Band LEO Downlinks", "comment": null, "summary": "Ka-band low-Earth-orbit (LEO) downlinks can suffer second-scale reliability collapses during flare-driven ionospheric disturbances, where fixed fade margins and reactive adaptive coding and modulation (ACM) are either overly conservative or too slow. This paper presents a GNSS-free, link-internal predictive controller that senses the same downlink via a geometry-free dual-carrier phase observable at 10~Hz: a high-pass filter and template-based onset detector, followed by a four-state nearly-constant-velocity Kalman filter, estimate $\u0394$VTEC and its rate, and a short look-ahead (60~s) yields an endpoint outage probability used as a risk gate to trigger one-step discrete MCS down-switch and pilot-time update with hysteresis. Evaluation uses physics-informed log replay driven by real GOES X-ray flare morphologies under a disjoint-day frozen-calibration protocol, with uncertainty reported via paired moving-block bootstrap. Across stressed 60~s windows, the controller reduces peak BLER by 25--30\\% and increases goodput by 0.10--0.15~bps/Hz versus no-adaptation baselines under a unified link-level abstraction. The loop runs in $\\mathcal{O}(1)$ per 0.1~s epoch (about 0.042~ms measured), making on-board implementation feasible, and scope and deployment considerations for dispersion-dominated events are discussed.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u53cc\u8f7d\u6ce2\u76f8\u4f4d\u89c2\u6d4b\u7684GNSS-free\u9884\u6d4b\u63a7\u5236\u5668\uff0c\u7528\u4e8eKa\u6ce2\u6bb5LEO\u4e0b\u884c\u94fe\u8def\u5728\u7535\u79bb\u5c42\u6270\u52a8\u671f\u95f4\u7684\u53ef\u9760\u901a\u4fe1\uff0c\u901a\u8fc7\u63d0\u524d\u68c0\u6d4b\u548c\u9884\u6d4bVTEC\u53d8\u5316\u6765\u89e6\u53d1\u81ea\u9002\u5e94\u8c03\u5236\u7f16\u7801\u5207\u6362\uff0c\u63d0\u5347\u94fe\u8def\u53ef\u9760\u6027\u3002", "motivation": "Ka\u6ce2\u6bb5\u4f4e\u5730\u7403\u8f68\u9053\u4e0b\u884c\u94fe\u8def\u5728\u592a\u9633\u8000\u6591\u9a71\u52a8\u7684\u7535\u79bb\u5c42\u6270\u52a8\u671f\u95f4\u4f1a\u51fa\u73b0\u79d2\u7ea7\u53ef\u9760\u6027\u5d29\u6e83\uff0c\u4f20\u7edf\u7684\u56fa\u5b9a\u8870\u51cf\u88d5\u5ea6\u548c\u53cd\u5e94\u5f0f\u81ea\u9002\u5e94\u7f16\u7801\u8c03\u5236\u8981\u4e48\u8fc7\u4e8e\u4fdd\u5b88\u8981\u4e48\u54cd\u5e94\u592a\u6162\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u9884\u6d4b\u6027\u63a7\u5236\u65b9\u6848\u3002", "method": "\u91c7\u7528GNSS-free\u3001\u94fe\u8def\u5185\u90e8\u7684\u9884\u6d4b\u63a7\u5236\u5668\uff0c\u901a\u8fc710Hz\u7684\u53cc\u8f7d\u6ce2\u76f8\u4f4d\u89c2\u6d4b\u611f\u77e5\u4e0b\u884c\u94fe\u8def\uff0c\u4f7f\u7528\u9ad8\u901a\u6ee4\u6ce2\u548c\u6a21\u677f\u5339\u914d\u7684\u8d77\u59cb\u68c0\u6d4b\u5668\uff0c\u7ed3\u5408\u56db\u72b6\u6001\u8fd1\u6052\u5b9a\u901f\u5ea6\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\u4f30\u8ba1\u0394VTEC\u53ca\u5176\u53d8\u5316\u7387\uff0c\u901a\u8fc760\u79d2\u524d\u77bb\u9884\u6d4b\u8ba1\u7b97\u7aef\u70b9\u4e2d\u65ad\u6982\u7387\uff0c\u4f5c\u4e3a\u98ce\u9669\u95e8\u9650\u89e6\u53d1\u79bb\u6563\u8c03\u5236\u7f16\u7801\u65b9\u6848\u964d\u7ea7\u5207\u6362\u548c\u5bfc\u9891\u65f6\u95f4\u66f4\u65b0\u3002", "result": "\u572860\u79d2\u538b\u529b\u7a97\u53e3\u5185\uff0c\u63a7\u5236\u5668\u76f8\u6bd4\u65e0\u81ea\u9002\u5e94\u57fa\u7ebf\u5c06\u5cf0\u503cBLER\u964d\u4f4e\u4e8625-30%\uff0c\u541e\u5410\u91cf\u63d0\u5347\u4e860.10-0.15 bps/Hz\uff1b\u63a7\u5236\u5668\u6bcf0.1\u79d2\u5468\u671f\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u4e3aO(1)\uff0c\u5b9e\u6d4b\u7ea60.042\u6beb\u79d2\uff0c\u9002\u5408\u661f\u4e0a\u5b9e\u73b0\u3002", "conclusion": "\u8be5\u9884\u6d4b\u63a7\u5236\u5668\u80fd\u6709\u6548\u5e94\u5bf9\u7535\u79bb\u5c42\u6270\u52a8\uff0c\u63d0\u9ad8Ka\u6ce2\u6bb5LEO\u4e0b\u884c\u94fe\u8def\u7684\u53ef\u9760\u6027\uff0c\u8ba1\u7b97\u6548\u7387\u9ad8\u9002\u5408\u661f\u8f7d\u5b9e\u73b0\uff0c\u5e76\u8ba8\u8bba\u4e86\u8272\u6563\u4e3b\u5bfc\u4e8b\u4ef6\u7684\u8303\u56f4\u548c\u90e8\u7f72\u8003\u8651\u3002"}}
{"id": "2601.00999", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.00999", "abs": "https://arxiv.org/abs/2601.00999", "authors": ["Marcin Kolakowski", "Vitomir Djaja-Josko"], "title": "Dynamic Accuracy Estimation in a Wi-Fi-based Positioning System", "comment": "Originally presented at 2025 33rd Telecommunications Forum (TELFOR), Belgrade, Serbia", "summary": "The paper presents a concept of a dynamic accuracy estimation method, in which the localization errors are derived based on the measurement results used by the positioning algorithm. The concept was verified experimentally in a Wi\\nobreakdash-Fi based indoor positioning system, where several regression methods were tested (linear regression, random forest, k-nearest neighbors, and neural networks). The highest positioning error estimation accuracy was achieved for random forest regression, with a mean absolute error of 0.72 m.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u7cbe\u5ea6\u4f30\u8ba1\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b9a\u4f4d\u7b97\u6cd5\u4f7f\u7528\u7684\u6d4b\u91cf\u7ed3\u679c\u6765\u63a8\u5bfc\u5b9a\u4f4d\u8bef\u5dee\uff0c\u5728Wi-Fi\u5ba4\u5185\u5b9a\u4f4d\u7cfb\u7edf\u4e2d\u9a8c\u8bc1\uff0c\u968f\u673a\u68ee\u6797\u56de\u5f52\u83b7\u5f97\u4e86\u6700\u4f73\u8bef\u5dee\u4f30\u8ba1\u7cbe\u5ea6\uff08\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee0.72\u7c73\uff09\u3002", "motivation": "\u5ba4\u5185\u5b9a\u4f4d\u7cfb\u7edf\u901a\u5e38\u65e0\u6cd5\u5b9e\u65f6\u8bc4\u4f30\u5b9a\u4f4d\u7cbe\u5ea6\uff0c\u7528\u6237\u96be\u4ee5\u4e86\u89e3\u5f53\u524d\u4f4d\u7f6e\u4f30\u8ba1\u7684\u53ef\u9760\u6027\u3002\u73b0\u6709\u65b9\u6cd5\u5f80\u5f80\u9700\u8981\u989d\u5916\u786c\u4ef6\u6216\u590d\u6742\u8ba1\u7b97\uff0c\u65e0\u6cd5\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u52a8\u6001\u4f30\u8ba1\u5b9a\u4f4d\u8bef\u5dee\u3002", "method": "\u63d0\u51fa\u52a8\u6001\u7cbe\u5ea6\u4f30\u8ba1\u6982\u5ff5\uff0c\u57fa\u4e8e\u5b9a\u4f4d\u7b97\u6cd5\u5b9e\u9645\u4f7f\u7528\u7684\u6d4b\u91cf\u7ed3\u679c\u6765\u63a8\u5bfc\u5b9a\u4f4d\u8bef\u5dee\u3002\u5728Wi-Fi\u5ba4\u5185\u5b9a\u4f4d\u7cfb\u7edf\u4e2d\uff0c\u6d4b\u8bd5\u4e86\u591a\u79cd\u56de\u5f52\u65b9\u6cd5\uff1a\u7ebf\u6027\u56de\u5f52\u3001\u968f\u673a\u68ee\u6797\u3001k\u8fd1\u90bb\u548c\u795e\u7ecf\u7f51\u7edc\uff0c\u7528\u4e8e\u8bef\u5dee\u4f30\u8ba1\u5efa\u6a21\u3002", "result": "\u968f\u673a\u68ee\u6797\u56de\u5f52\u5728\u5b9a\u4f4d\u8bef\u5dee\u4f30\u8ba1\u65b9\u9762\u8868\u73b0\u6700\u4f73\uff0c\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\u4e3a0.72\u7c73\u3002\u5176\u4ed6\u65b9\u6cd5\u5982\u7ebf\u6027\u56de\u5f52\u3001k\u8fd1\u90bb\u548c\u795e\u7ecf\u7f51\u7edc\u4e5f\u8fdb\u884c\u4e86\u6d4b\u8bd5\uff0c\u4f46\u968f\u673a\u68ee\u6797\u83b7\u5f97\u4e86\u6700\u9ad8\u7684\u8bef\u5dee\u4f30\u8ba1\u7cbe\u5ea6\u3002", "conclusion": "\u52a8\u6001\u7cbe\u5ea6\u4f30\u8ba1\u65b9\u6cd5\u53ef\u884c\u4e14\u6709\u6548\uff0c\u968f\u673a\u68ee\u6797\u56de\u5f52\u662f\u6700\u9002\u5408\u7684\u8bef\u5dee\u4f30\u8ba1\u6280\u672f\u3002\u8be5\u65b9\u6cd5\u80fd\u591f\u4e3a\u5ba4\u5185\u5b9a\u4f4d\u7cfb\u7edf\u63d0\u4f9b\u5b9e\u65f6\u7684\u7cbe\u5ea6\u8bc4\u4f30\uff0c\u589e\u5f3a\u7528\u6237\u5bf9\u5b9a\u4f4d\u7ed3\u679c\u7684\u4fe1\u4efb\u5ea6\u3002"}}
{"id": "2601.00827", "categories": ["eess.AS", "cs.AI", "cs.MM"], "pdf": "https://arxiv.org/pdf/2601.00827", "abs": "https://arxiv.org/abs/2601.00827", "authors": ["Mariam Saeed", "Manar Amr", "Farida Adel", "Nada Hassan", "Nour Walid", "Eman Mohamed", "Mohamed Hussein", "Marwan Torki"], "title": "Speak the Art: A Direct Speech to Image Generation Framework", "comment": null, "summary": "Direct speech-to-image generation has recently shown promising results. However, compared to text-to-image generation, there is still a large gap to enclose. Current approaches use two stages to tackle this task: speech encoding network and image generative adversarial network (GAN). The speech encoding networks in these approaches produce embeddings that do not capture sufficient linguistic information to semantically represent the input speech. GANs suffer from issues such as non-convergence, mode collapse, and diminished gradient, which result in unstable model parameters, limited sample diversity, and ineffective generator learning, respectively. To address these weaknesses, we introduce a framework called \\textbf{Speak the Art (STA)} which consists of a speech encoding network and a VQ-Diffusion network conditioned on speech embeddings. To improve speech embeddings, the speech encoding network is supervised by a large pre-trained image-text model during training. Replacing GANs with diffusion leads to more stable training and the generation of diverse images. Additionally, we investigate the feasibility of extending our framework to be multilingual. As a proof of concept, we trained our framework with two languages: English and Arabic. Finally, we show that our results surpass state-of-the-art models by a large margin.", "AI": {"tldr": "STA\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u8bed\u97f3\u7f16\u7801\u7f51\u7edc\u548cVQ-Diffusion\u7f51\u7edc\uff0c\u5b9e\u73b0\u4e86\u76f4\u63a5\u4ece\u8bed\u97f3\u751f\u6210\u9ad8\u8d28\u91cf\u56fe\u50cf\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u4e2d\u8bed\u97f3\u5d4c\u5165\u4fe1\u606f\u4e0d\u8db3\u548cGAN\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u7684\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u8bed\u97f3\u5230\u56fe\u50cf\u751f\u6210\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1\uff09\u8bed\u97f3\u7f16\u7801\u7f51\u7edc\u751f\u6210\u7684\u5d4c\u5165\u65e0\u6cd5\u5145\u5206\u6355\u6349\u8bed\u97f3\u7684\u8bed\u4e49\u4fe1\u606f\uff1b2\uff09\u4f7f\u7528GAN\u5b58\u5728\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u3001\u6a21\u5f0f\u5d29\u6e83\u548c\u68af\u5ea6\u6d88\u5931\u7b49\u95ee\u9898\uff0c\u5bfc\u81f4\u6837\u672c\u591a\u6837\u6027\u6709\u9650\u548c\u751f\u6210\u5668\u5b66\u4e60\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u63d0\u51faSTA\u6846\u67b6\uff0c\u5305\u542b\u8bed\u97f3\u7f16\u7801\u7f51\u7edc\u548c\u57fa\u4e8e\u8bed\u97f3\u5d4c\u5165\u7684VQ-Diffusion\u7f51\u7edc\u3002\u8bed\u97f3\u7f16\u7801\u7f51\u7edc\u5728\u8bad\u7ec3\u65f6\u901a\u8fc7\u5927\u578b\u9884\u8bad\u7ec3\u56fe\u50cf-\u6587\u672c\u6a21\u578b\u8fdb\u884c\u76d1\u7763\uff0c\u4ee5\u63d0\u5347\u8bed\u97f3\u5d4c\u5165\u8d28\u91cf\u3002\u7528\u6269\u6563\u6a21\u578b\u66ff\u4ee3GAN\uff0c\u5b9e\u73b0\u66f4\u7a33\u5b9a\u7684\u8bad\u7ec3\u548c\u66f4\u591a\u6837\u5316\u7684\u56fe\u50cf\u751f\u6210\u3002\u6846\u67b6\u8fd8\u652f\u6301\u591a\u8bed\u8a00\u6269\u5c55\uff0c\u5df2\u7528\u82f1\u8bed\u548c\u963f\u62c9\u4f2f\u8bed\u9a8c\u8bc1\u3002", "result": "STA\u6846\u67b6\u5728\u8bed\u97f3\u5230\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u4e0a\u5927\u5e45\u8d85\u8d8a\u4e86\u73b0\u6709\u6700\u5148\u8fdb\u6a21\u578b\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002\u591a\u8bed\u8a00\u6269\u5c55\u4e5f\u5c55\u793a\u4e86\u6846\u67b6\u7684\u901a\u7528\u6027\u3002", "conclusion": "STA\u6846\u67b6\u901a\u8fc7\u6539\u8fdb\u8bed\u97f3\u5d4c\u5165\u8d28\u91cf\u548c\u91c7\u7528\u6269\u6563\u6a21\u578b\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u8bed\u97f3\u5230\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u5173\u952e\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u7a33\u5b9a\u3001\u591a\u6837\u548c\u9ad8\u8d28\u91cf\u7684\u56fe\u50cf\u751f\u6210\uff0c\u5e76\u5177\u5907\u591a\u8bed\u8a00\u6269\u5c55\u6f5c\u529b\u3002"}}
{"id": "2601.01033", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.01033", "abs": "https://arxiv.org/abs/2601.01033", "authors": ["Abidemi Orimogunje", "Hyunwoo Park", "Igbafe Orikumhi", "Sunwoo Kim", "Dejan Vukobratovic"], "title": "System-Level Comparison of Multimodal and In-Band mmWave Sensing for Beam Prediction in 6G ISAC", "comment": null, "summary": "Integrated sensing and communication (ISAC) can reduce beam-training overhead in mmWave vehicle-to-infrastructure (V2I) links by enabling in-band sensing-based beam prediction, while exteroceptive sensors can further enhance the prediction accuracy. This work develop a system-level framework that evaluates camera, LiDAR, radar, GPS, and in-band mmWave power, both individually and in multimodal fusion using the DeepSense-6G Scenario-33 dataset. A latency-aware neural network composed of lightweight convolutional (CNN) and multilayer-perceptron (MLP) encoders predict a 64-beam index. We assess performance using Top-k accuracy alongside spectral-efficiency (SE) gap, signal-to-noise-ratio (SNR) gap, rate loss, and end-to-end latency. Results show that the mmWave power vector is a strong standalone predictor, and fusing exteroceptive sensors with it preserves high performance: mmWave alone and mmWave+LiDAR/GPS/Radar achieve 98% Top-5 accuracy, while mmWave+camera achieves 94% Top-5 accuracy. The proposed framework establishes calibrated baselines for 6G ISAC-assisted beam prediction in V2I systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7cfb\u7edf\u7ea7\u6846\u67b6\uff0c\u8bc4\u4f30\u591a\u79cd\u4f20\u611f\u5668\uff08\u76f8\u673a\u3001LiDAR\u3001\u96f7\u8fbe\u3001GPS\u548c\u6beb\u7c73\u6ce2\u529f\u7387\uff09\u5728\u8f66\u5bf9\u57fa\u7840\u8bbe\u65bd\u901a\u4fe1\u4e2d\u7684\u6ce2\u675f\u9884\u6d4b\u6027\u80fd\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u795e\u7ecf\u7f51\u7edc\u5b9e\u73b0\u4f4e\u5ef6\u8fdf\u9884\u6d4b\uff0c\u4e3a6G ISAC\u8f85\u52a9\u7684V2I\u7cfb\u7edf\u5efa\u7acb\u4e86\u6821\u51c6\u57fa\u51c6\u3002", "motivation": "\u6beb\u7c73\u6ce2\u8f66\u5bf9\u57fa\u7840\u8bbe\u65bd\u901a\u4fe1\u4e2d\u6ce2\u675f\u8bad\u7ec3\u5f00\u9500\u5927\uff0c\u96c6\u6210\u611f\u77e5\u4e0e\u901a\u4fe1\uff08ISAC\uff09\u53ef\u4ee5\u901a\u8fc7\u5e26\u5185\u611f\u77e5\u51cf\u5c11\u5f00\u9500\uff0c\u800c\u5916\u90e8\u4f20\u611f\u5668\u53ef\u4ee5\u8fdb\u4e00\u6b65\u63d0\u9ad8\u9884\u6d4b\u7cbe\u5ea6\u3002\u9700\u8981\u7cfb\u7edf\u8bc4\u4f30\u4e0d\u540c\u4f20\u611f\u5668\u53ca\u5176\u878d\u5408\u5728\u6ce2\u675f\u9884\u6d4b\u4e2d\u7684\u6027\u80fd\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u7cfb\u7edf\u7ea7\u6846\u67b6\uff0c\u4f7f\u7528DeepSense-6G Scenario-33\u6570\u636e\u96c6\u8bc4\u4f30\u76f8\u673a\u3001LiDAR\u3001\u96f7\u8fbe\u3001GPS\u548c\u6beb\u7c73\u6ce2\u529f\u7387\u7684\u5355\u72ec\u53ca\u591a\u6a21\u6001\u878d\u5408\u6027\u80fd\u3002\u91c7\u7528\u8f7b\u91cf\u7ea7\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u548c\u591a\u5c42\u611f\u77e5\u673a\u7f16\u7801\u5668\u7ec4\u6210\u7684\u5ef6\u8fdf\u611f\u77e5\u795e\u7ecf\u7f51\u7edc\uff0c\u9884\u6d4b64\u4e2a\u6ce2\u675f\u7d22\u5f15\u3002", "result": "\u6beb\u7c73\u6ce2\u529f\u7387\u5411\u91cf\u662f\u5f3a\u5927\u7684\u72ec\u7acb\u9884\u6d4b\u5668\uff0c\u4e0e\u5916\u90e8\u4f20\u611f\u5668\u878d\u5408\u4fdd\u6301\u9ad8\u6027\u80fd\uff1a\u6beb\u7c73\u6ce2\u5355\u72ec\u53ca\u6beb\u7c73\u6ce2+LiDAR/GPS/\u96f7\u8fbe\u8fbe\u523098%\u7684Top-5\u51c6\u786e\u7387\uff0c\u6beb\u7c73\u6ce2+\u76f8\u673a\u8fbe\u523094%\u7684Top-5\u51c6\u786e\u7387\u3002\u901a\u8fc7\u9891\u8c31\u6548\u7387\u5dee\u8ddd\u3001\u4fe1\u566a\u6bd4\u5dee\u8ddd\u3001\u901f\u7387\u635f\u5931\u548c\u7aef\u5230\u7aef\u5ef6\u8fdf\u7b49\u6307\u6807\u5168\u9762\u8bc4\u4f30\u6027\u80fd\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a6G ISAC\u8f85\u52a9\u7684V2I\u7cfb\u7edf\u6ce2\u675f\u9884\u6d4b\u5efa\u7acb\u4e86\u6821\u51c6\u57fa\u51c6\uff0c\u8bc1\u660e\u4e86\u6beb\u7c73\u6ce2\u529f\u7387\u4f5c\u4e3a\u6838\u5fc3\u9884\u6d4b\u5668\u7684\u6709\u6548\u6027\uff0c\u4ee5\u53ca\u591a\u4f20\u611f\u5668\u878d\u5408\u5728\u4fdd\u6301\u9ad8\u6027\u80fd\u7684\u540c\u65f6\u51cf\u5c11\u6ce2\u675f\u8bad\u7ec3\u5f00\u9500\u7684\u6f5c\u529b\u3002"}}
{"id": "2601.00935", "categories": ["eess.AS", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.00935", "abs": "https://arxiv.org/abs/2601.00935", "authors": ["Yue Heng Yeo", "Yuchen Hu", "Shreyas Gopal", "Yizhou Peng", "Hexin Liu", "Eng Siong Chng"], "title": "Improving Code-Switching Speech Recognition with TTS Data Augmentation", "comment": "This paper was accepted by APSIPA 2025", "summary": "Automatic speech recognition (ASR) for conversational code-switching speech remains challenging due to the scarcity of realistic, high-quality labeled speech data. This paper explores multilingual text-to-speech (TTS) models as an effective data augmentation technique to address this shortage. Specifically, we fine-tune the multilingual CosyVoice2 TTS model on the SEAME dataset to generate synthetic conversational Chinese-English code-switching speech, significantly increasing the quantity and speaker diversity of available training data. Our experiments demonstrate that augmenting real speech with synthetic speech reduces the mixed error rate (MER) from 12.1 percent to 10.1 percent on DevMan and from 17.8 percent to 16.0 percent on DevSGE, indicating consistent performance gains. These results confirm that multilingual TTS is an effective and practical tool for enhancing ASR robustness in low-resource conversational code-switching scenarios.", "AI": {"tldr": "\u4f7f\u7528\u591a\u8bed\u8a00TTS\u6a21\u578b\u751f\u6210\u4e2d\u82f1\u6587\u8bed\u7801\u8f6c\u6362\u5408\u6210\u8bed\u97f3\uff0c\u6709\u6548\u589e\u5f3a\u4f4e\u8d44\u6e90\u5bf9\u8bddASR\u7cfb\u7edf\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u663e\u8457\u964d\u4f4e\u9519\u8bef\u7387", "motivation": "\u5bf9\u8bdd\u5f0f\u8bed\u7801\u8f6c\u6362\u8bed\u97f3\u8bc6\u522b\u9762\u4e34\u9ad8\u8d28\u91cf\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u7684\u6311\u6218\uff0c\u9700\u8981\u6709\u6548\u7684\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u6765\u63d0\u5347ASR\u7cfb\u7edf\u6027\u80fd", "method": "\u5728SEAME\u6570\u636e\u96c6\u4e0a\u5fae\u8c03\u591a\u8bed\u8a00CosyVoice2 TTS\u6a21\u578b\uff0c\u751f\u6210\u5408\u6210\u7684\u4e2d\u82f1\u6587\u8bed\u7801\u8f6c\u6362\u5bf9\u8bdd\u8bed\u97f3\uff0c\u589e\u52a0\u8bad\u7ec3\u6570\u636e\u91cf\u548c\u8bf4\u8bdd\u4eba\u591a\u6837\u6027", "result": "\u4f7f\u7528\u5408\u6210\u8bed\u97f3\u589e\u5f3a\u771f\u5b9e\u8bed\u97f3\u8bad\u7ec3\u540e\uff0c\u6df7\u5408\u9519\u8bef\u7387\u5728DevMan\u4e0a\u4ece12.1%\u964d\u81f310.1%\uff0c\u5728DevSGE\u4e0a\u4ece17.8%\u964d\u81f316.0%\uff0c\u6027\u80fd\u4e00\u81f4\u63d0\u5347", "conclusion": "\u591a\u8bed\u8a00TTS\u662f\u589e\u5f3a\u4f4e\u8d44\u6e90\u5bf9\u8bdd\u8bed\u7801\u8f6c\u6362\u573a\u666f\u4e0bASR\u9c81\u68d2\u6027\u7684\u6709\u6548\u5b9e\u7528\u5de5\u5177"}}
{"id": "2601.00890", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.00890", "abs": "https://arxiv.org/abs/2601.00890", "authors": ["Zheshu Song", "Lu Wang", "Wei Deng", "Zhuo Yang", "Yong Wu", "Bin Xia"], "title": "Index-ASR Technical Report", "comment": "Index-ASR technical report", "summary": "Automatic speech recognition (ASR) has witnessed remarkable progress in recent years, largely driven by the emergence of LLM-based ASR paradigm. Despite their strong performance on a variety of open-source benchmarks, existing LLM-based ASR systems still suffer from two critical limitations. First, they are prone to hallucination errors, often generating excessively long and repetitive outputs that are not well grounded in the acoustic input. Second, they provide limited support for flexible and fine-grained contextual customization. To address these challenges, we propose Index-ASR, a large-scale LLM-based ASR system designed to simultaneously enhance robustness and support customizable hotword recognition. The core idea of Index-ASR lies in the integration of LLM and large-scale training data enriched with background noise and contextual information. Experimental results show that our Index-ASR achieves strong performance on both open-source benchmarks and in-house test sets, highlighting its robustness and practicality for real-world ASR applications.", "AI": {"tldr": "Index-ASR\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\u7cfb\u7edf\uff0c\u901a\u8fc7\u96c6\u6210LLM\u548c\u5927\u89c4\u6a21\u8bad\u7ec3\u6570\u636e\u6765\u89e3\u51b3\u73b0\u6709LLM-ASR\u7cfb\u7edf\u7684\u5e7b\u89c9\u9519\u8bef\u548c\u4e0a\u4e0b\u6587\u5b9a\u5236\u652f\u6301\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709LLM-ASR\u7cfb\u7edf\u5b58\u5728\u4e24\u4e2a\u5173\u952e\u9650\u5236\uff1a1\uff09\u5bb9\u6613\u4ea7\u751f\u5e7b\u89c9\u9519\u8bef\uff0c\u751f\u6210\u4e0e\u58f0\u5b66\u8f93\u5165\u4e0d\u7b26\u7684\u8fc7\u957f\u91cd\u590d\u8f93\u51fa\uff1b2\uff09\u5bf9\u7075\u6d3b\u7ec6\u7c92\u5ea6\u7684\u4e0a\u4e0b\u6587\u5b9a\u5236\u652f\u6301\u6709\u9650\u3002\u9700\u8981\u540c\u65f6\u589e\u5f3a\u9c81\u68d2\u6027\u548c\u652f\u6301\u53ef\u5b9a\u5236\u7684\u70ed\u8bcd\u8bc6\u522b\u3002", "method": "\u63d0\u51faIndex-ASR\u7cfb\u7edf\uff0c\u6838\u5fc3\u601d\u60f3\u662f\u96c6\u6210LLM\u548c\u5927\u89c4\u6a21\u8bad\u7ec3\u6570\u636e\uff0c\u8fd9\u4e9b\u6570\u636e\u5bcc\u542b\u80cc\u666f\u566a\u58f0\u548c\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u4ee5\u540c\u65f6\u589e\u5f3a\u9c81\u68d2\u6027\u548c\u652f\u6301\u53ef\u5b9a\u5236\u7684\u70ed\u8bcd\u8bc6\u522b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cIndex-ASR\u5728\u5f00\u6e90\u57fa\u51c6\u6d4b\u8bd5\u548c\u5185\u90e8\u6d4b\u8bd5\u96c6\u4e0a\u90fd\u8868\u73b0\u51fa\u8272\uff0c\u7a81\u663e\u4e86\u5176\u5728\u73b0\u5b9e\u4e16\u754cASR\u5e94\u7528\u4e2d\u7684\u9c81\u68d2\u6027\u548c\u5b9e\u7528\u6027\u3002", "conclusion": "Index-ASR\u901a\u8fc7\u96c6\u6210LLM\u548c\u5bcc\u542b\u566a\u58f0\u4e0e\u4e0a\u4e0b\u6587\u4fe1\u606f\u7684\u5927\u89c4\u6a21\u8bad\u7ec3\u6570\u636e\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709LLM-ASR\u7cfb\u7edf\u7684\u5e7b\u89c9\u95ee\u9898\u548c\u4e0a\u4e0b\u6587\u5b9a\u5236\u9650\u5236\uff0c\u4e3a\u5b9e\u9645ASR\u5e94\u7528\u63d0\u4f9b\u4e86\u9c81\u68d2\u4e14\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.01152", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.01152", "abs": "https://arxiv.org/abs/2601.01152", "authors": ["Haojin Li", "Kaiqian Qu", "Chen Sun", "Anbang Zhang", "Xiaoxue Wang", "Wenqi Zhang", "Haijun Zhang"], "title": "Towards a Theoretical Framework for Robust Node Deployment in Cooperative ISAC Networks", "comment": null, "summary": "This paper investigates node deployment strategies for robust multi-node cooperative localization in integrated sensing and communication (ISAC) networks.We first analyze how steering vector correlation across different positions affects localization performance and introduce a novel distance-weighted correlation metric to characterize this effect. Building upon this insight, we propose a deployment optimization framework that minimizes the maximum weighted steering vector correlation by optimizing simultaneously node positions and array orientations, thereby enhancing worst-case network robustness. Then, a genetic algorithm (GA) is developed to solve this min-max optimization, yielding optimized node positions and array orientations. Extensive simulations using both multiple signal classification (MUSIC) and neural-network (NN)-based localization validate the effectiveness of the proposed methods, demonstrating significant improvements in robust localization performance.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86ISAC\u7f51\u7edc\u4e2d\u591a\u8282\u70b9\u534f\u540c\u5b9a\u4f4d\u7684\u8282\u70b9\u90e8\u7f72\u7b56\u7565\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u52a0\u6743\u5bfc\u5411\u77e2\u91cf\u76f8\u5173\u6027\u6765\u4f18\u5316\u8282\u70b9\u4f4d\u7f6e\u548c\u9635\u5217\u65b9\u5411\uff0c\u63d0\u5347\u6700\u574f\u60c5\u51b5\u4e0b\u7684\u7f51\u7edc\u9c81\u68d2\u6027\u3002", "motivation": "\u5728\u96c6\u6210\u611f\u77e5\u4e0e\u901a\u4fe1\u7f51\u7edc\u4e2d\uff0c\u4e0d\u540c\u4f4d\u7f6e\u5bfc\u5411\u77e2\u91cf\u7684\u76f8\u5173\u6027\u4f1a\u5f71\u54cd\u5b9a\u4f4d\u6027\u80fd\uff0c\u9700\u8981\u8bbe\u8ba1\u9c81\u68d2\u7684\u8282\u70b9\u90e8\u7f72\u7b56\u7565\u6765\u63d0\u5347\u591a\u8282\u70b9\u534f\u540c\u5b9a\u4f4d\u7684\u53ef\u9760\u6027\u3002", "method": "\u9996\u5148\u5206\u6790\u5bfc\u5411\u77e2\u91cf\u76f8\u5173\u6027\u5bf9\u5b9a\u4f4d\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u63d0\u51fa\u8ddd\u79bb\u52a0\u6743\u76f8\u5173\u6027\u5ea6\u91cf\uff1b\u7136\u540e\u5efa\u7acb\u90e8\u7f72\u4f18\u5316\u6846\u67b6\uff0c\u540c\u65f6\u4f18\u5316\u8282\u70b9\u4f4d\u7f6e\u548c\u9635\u5217\u65b9\u5411\u4ee5\u6700\u5c0f\u5316\u6700\u5927\u52a0\u6743\u76f8\u5173\u6027\uff1b\u6700\u540e\u5f00\u53d1\u9057\u4f20\u7b97\u6cd5\u6c42\u89e3\u8be5min-max\u4f18\u5316\u95ee\u9898\u3002", "result": "\u901a\u8fc7MUSIC\u548c\u795e\u7ecf\u7f51\u7edc\u5b9a\u4f4d\u65b9\u6cd5\u7684\u5e7f\u6cdb\u4eff\u771f\u9a8c\u8bc1\uff0c\u6240\u63d0\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u9c81\u68d2\u5b9a\u4f4d\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u4f18\u5316\u90e8\u7f72\u7b56\u7565\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684\u8282\u70b9\u90e8\u7f72\u4f18\u5316\u6846\u67b6\u80fd\u591f\u6709\u6548\u63d0\u5347ISAC\u7f51\u7edc\u4e2d\u591a\u8282\u70b9\u534f\u540c\u5b9a\u4f4d\u7684\u9c81\u68d2\u6027\uff0c\u4e3a\u5b9e\u9645\u7f51\u7edc\u90e8\u7f72\u63d0\u4f9b\u4e86\u7406\u8bba\u6307\u5bfc\u548c\u6280\u672f\u65b9\u6848\u3002"}}
{"id": "2601.01391", "categories": ["eess.AS", "cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2601.01391", "abs": "https://arxiv.org/abs/2601.01391", "authors": ["Ian Jacob Cabansag", "Paul Ntegeka"], "title": "Bayesian Negative Binomial Regression of Afrobeats Chart Persistence", "comment": null, "summary": "Afrobeats songs compete for attention on streaming platforms, where chart visibility can influence both revenue and cultural impact. This paper examines whether collaborations help songs remain on the charts longer, using daily Nigeria Spotify Top 200 data from 2024. Each track is summarized by the number of days it appears in the Top 200 during the year and its total annual streams in Nigeria. A Bayesian negative binomial regression is applied, with days on chart as the outcome and collaboration status (solo versus multi-artist) and log total streams as predictors. This approach is well suited for overdispersed count data and allows the effect of collaboration to be interpreted while controlling for overall popularity. Posterior inference is conducted using Markov chain Monte Carlo, and results are assessed using rate ratios, posterior probabilities, and predictive checks. The findings indicate that, after accounting for total streams, collaboration tracks tend to spend slightly fewer days on the chart than comparable solo tracks.", "AI": {"tldr": "\u7814\u7a76\u4f7f\u7528\u8d1d\u53f6\u65af\u8d1f\u4e8c\u9879\u56de\u5f52\u5206\u6790\u5c3c\u65e5\u5229\u4e9aSpotify\u699c\u5355\u6570\u636e\uff0c\u53d1\u73b0\u5408\u4f5c\u6b4c\u66f2\u5728\u63a7\u5236\u603b\u64ad\u653e\u91cf\u540e\uff0c\u6bd4\u5355\u4eba\u6b4c\u66f2\u5728\u699c\u5355\u4e0a\u505c\u7559\u65f6\u95f4\u7a0d\u77ed", "motivation": "\u5728\u6d41\u5a92\u4f53\u5e73\u53f0\u4e0a\uff0c\u6b4c\u66f2\u5728\u699c\u5355\u4e0a\u7684\u505c\u7559\u65f6\u95f4\u5f71\u54cd\u6536\u5165\u548c\u4f20\u64ad\u6548\u679c\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7a76\u5408\u4f5c\u6b4c\u66f2\u662f\u5426\u6bd4\u5355\u4eba\u6b4c\u66f2\u5728\u699c\u5355\u4e0a\u505c\u7559\u66f4\u957f\u65f6\u95f4\uff0c\u4ece\u800c\u83b7\u5f97\u66f4\u597d\u7684\u66dd\u5149\u548c\u5546\u4e1a\u6210\u529f\u3002", "method": "\u4f7f\u75282024\u5e74\u5c3c\u65e5\u5229\u4e9aSpotify Top 200\u6bcf\u65e5\u6570\u636e\uff0c\u8bb0\u5f55\u6bcf\u9996\u6b4c\u66f2\u5728\u699c\u5355\u4e0a\u7684\u5929\u6570\u548c\u5e74\u5ea6\u603b\u64ad\u653e\u91cf\u3002\u91c7\u7528\u8d1d\u53f6\u65af\u8d1f\u4e8c\u9879\u56de\u5f52\u6a21\u578b\uff0c\u4ee5\u699c\u5355\u505c\u7559\u5929\u6570\u4e3a\u56e0\u53d8\u91cf\uff0c\u5408\u4f5c\u72b6\u6001\uff08\u5355\u4ebavs\u591a\u827a\u4eba\uff09\u548c\u5bf9\u6570\u603b\u64ad\u653e\u91cf\u4e3a\u81ea\u53d8\u91cf\uff0c\u4f7f\u7528\u9a6c\u5c14\u53ef\u592b\u94fe\u8499\u7279\u5361\u6d1b\u8fdb\u884c\u540e\u9a8c\u63a8\u65ad\u3002", "result": "\u5728\u63a7\u5236\u603b\u64ad\u653e\u91cf\u540e\uff0c\u5408\u4f5c\u6b4c\u66f2\u5728\u699c\u5355\u4e0a\u7684\u505c\u7559\u65f6\u95f4\u6bd4\u76f8\u4f3c\u64ad\u653e\u91cf\u7684\u5355\u4eba\u6b4c\u66f2\u7a0d\u77ed\uff0c\u8fd9\u4e00\u53d1\u73b0\u4e0e\u9884\u671f\u76f8\u53cd\u3002", "conclusion": "\u5408\u4f5c\u5e76\u4e0d\u4e00\u5b9a\u5ef6\u957f\u6b4c\u66f2\u5728\u699c\u5355\u4e0a\u7684\u505c\u7559\u65f6\u95f4\uff0c\u5728\u8003\u8651\u6b4c\u66f2\u6574\u4f53\u6d41\u884c\u5ea6\u540e\uff0c\u5355\u4eba\u6b4c\u66f2\u53ef\u80fd\u5177\u6709\u66f4\u5f3a\u7684\u699c\u5355\u6301\u4e45\u529b\u3002"}}
{"id": "2601.01239", "categories": ["cs.SD", "cs.CR", "cs.MM", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.01239", "abs": "https://arxiv.org/abs/2601.01239", "authors": ["Jiajie Zhu", "Xia Du", "Xiaoyuan Liu", "Jizhe Zhou", "Qizhen Xu", "Zheng Lin", "Chi-Man Pun"], "title": "IO-RAE: Information-Obfuscation Reversible Adversarial Example for Audio Privacy Protection", "comment": "10 pages, 5 figures", "summary": "The rapid advancements in artificial intelligence have significantly accelerated the adoption of speech recognition technology, leading to its widespread integration across various applications. However, this surge in usage also highlights a critical issue: audio data is highly vulnerable to unauthorized exposure and analysis, posing significant privacy risks for businesses and individuals. This paper introduces an Information-Obfuscation Reversible Adversarial Example (IO-RAE) framework, the pioneering method designed to safeguard audio privacy using reversible adversarial examples. IO-RAE leverages large language models to generate misleading yet contextually coherent content, effectively preventing unauthorized eavesdropping by humans and Automatic Speech Recognition (ASR) systems. Additionally, we propose the Cumulative Signal Attack technique, which mitigates high-frequency noise and enhances attack efficacy by targeting low-frequency signals. Our approach ensures the protection of audio data without degrading its quality or our ability. Experimental evaluations demonstrate the superiority of our method, achieving a targeted misguidance rate of 96.5% and a remarkable 100% untargeted misguidance rate in obfuscating target keywords across multiple ASR models, including a commercial black-box system from Google. Furthermore, the quality of the recovered audio, measured by the Perceptual Evaluation of Speech Quality score, reached 4.45, comparable to high-quality original recordings. Notably, the recovered audio processed by ASR systems exhibited an error rate of 0%, indicating nearly lossless recovery. These results highlight the practical applicability and effectiveness of our IO-RAE framework in protecting sensitive audio privacy.", "AI": {"tldr": "IO-RAE\u6846\u67b6\u5229\u7528\u53ef\u9006\u5bf9\u6297\u6837\u672c\u4fdd\u62a4\u97f3\u9891\u9690\u79c1\uff0c\u901a\u8fc7LLM\u751f\u6210\u8bef\u5bfc\u5185\u5bb9\u9632\u6b62\u7a83\u542c\uff0c\u540c\u65f6\u4fdd\u6301\u97f3\u9891\u8d28\u91cf\u5e76\u5b9e\u73b0\u8fd1\u4e4e\u65e0\u635f\u6062\u590d\u3002", "motivation": "\u968f\u7740\u8bed\u97f3\u8bc6\u522b\u6280\u672f\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u97f3\u9891\u6570\u636e\u9762\u4e34\u4e25\u91cd\u7684\u9690\u79c1\u6cc4\u9732\u98ce\u9669\uff0c\u9700\u8981\u5728\u4e0d\u5f71\u54cd\u97f3\u9891\u8d28\u91cf\u7684\u524d\u63d0\u4e0b\u4fdd\u62a4\u654f\u611f\u4fe1\u606f\u3002", "method": "\u63d0\u51faIO-RAE\u6846\u67b6\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u4e0a\u4e0b\u6587\u8fde\u8d2f\u7684\u8bef\u5bfc\u5185\u5bb9\uff0c\u7ed3\u5408\u7d2f\u79ef\u4fe1\u53f7\u653b\u51fb\u6280\u672f\u9488\u5bf9\u4f4e\u9891\u4fe1\u53f7\uff0c\u751f\u6210\u53ef\u9006\u5bf9\u6297\u6837\u672c\u3002", "result": "\u5728\u591a\u4e2aASR\u6a21\u578b\u4e0a\u5b9e\u73b096.5%\u7684\u76ee\u6807\u8bef\u5bfc\u7387\u548c100%\u7684\u975e\u76ee\u6807\u8bef\u5bfc\u7387\uff0c\u6062\u590d\u97f3\u9891\u7684PESQ\u8bc4\u5206\u8fbe4.45\uff0cASR\u9519\u8bef\u7387\u4e3a0%\uff0c\u8fd1\u4e4e\u65e0\u635f\u6062\u590d\u3002", "conclusion": "IO-RAE\u6846\u67b6\u80fd\u6709\u6548\u4fdd\u62a4\u97f3\u9891\u9690\u79c1\uff0c\u9632\u6b62\u4eba\u7c7b\u548cASR\u7cfb\u7edf\u7a83\u542c\uff0c\u540c\u65f6\u4fdd\u6301\u97f3\u9891\u9ad8\u8d28\u91cf\u548c\u53ef\u6062\u590d\u6027\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2601.01229", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01229", "abs": "https://arxiv.org/abs/2601.01229", "authors": ["Furkan Gen\u00e7", "Boran \u0130smet Macun", "Sait Sarper \u00d6zaslan", "Emine U. Saritas", "Tolga \u00c7ukur"], "title": "NeuroSSM: Multiscale Differential State-Space Modeling for Context-Aware fMRI Analysis", "comment": null, "summary": "Accurate fMRI analysis requires sensitivity to temporal structure across multiple scales, as BOLD signals encode cognitive processes that emerge from fast transient dynamics to slower, large-scale fluctuations. Existing deep learning (DL) approaches to temporal modeling face challenges in jointly capturing these dynamics over long fMRI time series. Among current DL models, transformers address long-range dependencies by explicitly modeling pairwise interactions through attention, but the associated quadratic computational cost limits effective integration of temporal dependencies across long fMRI sequences. Selective state-space models (SSMs) instead model long-range temporal dependencies implicitly through latent state evolution in a dynamical system, enabling efficient propagation of dependencies over time. However, recent SSM-based approaches for fMRI commonly operate on derived functional connectivity representations and employ single-scale temporal processing. These design choices constrain the ability to jointly represent fast transient dynamics and slower global trends within a single model. We propose NeuroSSM, a selective state-space architecture designed for end-to-end analysis of raw BOLD signals in fMRI time series. NeuroSSM addresses the above limitations through two complementary design components: a multiscale state-space backbone that captures fast and slow dynamics concurrently, and a parallel differencing branch that increases sensitivity to transient state changes. Experiments on clinical and non-clinical datasets demonstrate that NeuroSSM achieves competitive performance and efficiency against state-of-the-art fMRI analysis methods.", "AI": {"tldr": "NeuroSSM\uff1a\u4e00\u79cd\u7528\u4e8e\u539f\u59cbBOLD\u4fe1\u53f7fMRI\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u7684\u9009\u62e9\u6027\u72b6\u6001\u7a7a\u95f4\u67b6\u6784\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea6\u72b6\u6001\u7a7a\u95f4\u9aa8\u5e72\u7f51\u7edc\u548c\u5e76\u884c\u5dee\u5206\u5206\u652f\uff0c\u540c\u65f6\u6355\u6349\u5feb\u901f\u77ac\u6001\u52a8\u6001\u548c\u7f13\u6162\u5168\u5c40\u8d8b\u52bf\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728fMRI\u5206\u6790\u4e2d\u9762\u4e34\u6311\u6218\uff1a\u9700\u8981\u540c\u65f6\u6355\u6349\u4ece\u5feb\u901f\u77ac\u6001\u52a8\u6001\u5230\u7f13\u6162\u5927\u89c4\u6a21\u6ce2\u52a8\u7684\u591a\u5c3a\u5ea6\u65f6\u95f4\u7ed3\u6784\u3002Transformer\u867d\u7136\u80fd\u5904\u7406\u957f\u7a0b\u4f9d\u8d56\u4f46\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u800c\u73b0\u6709\u7684SSM\u65b9\u6cd5\u901a\u5e38\u57fa\u4e8e\u529f\u80fd\u8fde\u63a5\u8868\u793a\u4e14\u91c7\u7528\u5355\u5c3a\u5ea6\u5904\u7406\uff0c\u65e0\u6cd5\u5728\u5355\u4e2a\u6a21\u578b\u4e2d\u8054\u5408\u8868\u793a\u5feb\u901f\u548c\u7f13\u6162\u52a8\u6001\u3002", "method": "\u63d0\u51faNeuroSSM\u67b6\u6784\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1) \u591a\u5c3a\u5ea6\u72b6\u6001\u7a7a\u95f4\u9aa8\u5e72\u7f51\u7edc\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u540c\u65f6\u6355\u6349\u5feb\u901f\u548c\u7f13\u6162\u52a8\u6001\uff1b2) \u5e76\u884c\u5dee\u5206\u5206\u652f\uff0c\u589e\u5f3a\u5bf9\u77ac\u6001\u72b6\u6001\u53d8\u5316\u7684\u654f\u611f\u6027\u3002\u8be5\u67b6\u6784\u76f4\u63a5\u5904\u7406\u539f\u59cbBOLD\u4fe1\u53f7\uff0c\u5b9e\u73b0\u7aef\u5230\u7aef\u5206\u6790\u3002", "result": "\u5728\u4e34\u5e8a\u548c\u975e\u4e34\u5e8a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cNeuroSSM\u5728\u6027\u80fd\u548c\u6548\u7387\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u7684fMRI\u5206\u6790\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u6355\u6349\u591a\u5c3a\u5ea6\u65f6\u95f4\u52a8\u6001\u3002", "conclusion": "NeuroSSM\u901a\u8fc7\u591a\u5c3a\u5ea6\u72b6\u6001\u7a7a\u95f4\u5efa\u6a21\u548c\u5dee\u5206\u589e\u5f3a\u673a\u5236\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u8054\u5408\u6355\u6349\u5feb\u901f\u77ac\u6001\u52a8\u6001\u548c\u7f13\u6162\u5168\u5c40\u8d8b\u52bf\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u4e3afMRI\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.01852", "categories": ["eess.AS", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01852", "abs": "https://arxiv.org/abs/2601.01852", "authors": ["Xiaoxue Gao", "Zexin Li", "Yiming Chen", "Nancy F. Chen"], "title": "MORE: Multi-Objective Adversarial Attacks on Speech Recognition", "comment": "19 pages", "summary": "The emergence of large-scale automatic speech recognition (ASR) models such as Whisper has greatly expanded their adoption across diverse real-world applications. Ensuring robustness against even minor input perturbations is therefore critical for maintaining reliable performance in real-time environments. While prior work has mainly examined accuracy degradation under adversarial attacks, robustness with respect to efficiency remains largely unexplored. This narrow focus provides only a partial understanding of ASR model vulnerabilities. To address this gap, we conduct a comprehensive study of ASR robustness under multiple attack scenarios. We introduce MORE, a multi-objective repetitive doubling encouragement attack, which jointly degrades recognition accuracy and inference efficiency through a hierarchical staged repulsion-anchoring mechanism. Specifically, we reformulate multi-objective adversarial optimization into a hierarchical framework that sequentially achieves the dual objectives. To further amplify effectiveness, we propose a novel repetitive encouragement doubling objective (REDO) that induces duplicative text generation by maintaining accuracy degradation and periodically doubling the predicted sequence length. Overall, MORE compels ASR models to produce incorrect transcriptions at a substantially higher computational cost, triggered by a single adversarial input. Experiments show that MORE consistently yields significantly longer transcriptions while maintaining high word error rates compared to existing baselines, underscoring its effectiveness in multi-objective adversarial attack.", "AI": {"tldr": "\u63d0\u51fa\u4e86MORE\u653b\u51fb\u65b9\u6cd5\uff0c\u540c\u65f6\u964d\u4f4eASR\u6a21\u578b\u7684\u8bc6\u522b\u51c6\u786e\u7387\u548c\u63a8\u7406\u6548\u7387\uff0c\u901a\u8fc7\u5c42\u6b21\u5316\u9636\u6bb5\u6392\u65a5-\u951a\u5b9a\u673a\u5236\u5b9e\u73b0\u591a\u76ee\u6807\u5bf9\u6297\u653b\u51fb\u3002", "motivation": "\u5f53\u524dASR\u6a21\u578b\uff08\u5982Whisper\uff09\u5728\u73b0\u5b9e\u5e94\u7528\u4e2d\u5e7f\u6cdb\u4f7f\u7528\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u5bf9\u6297\u653b\u51fb\u4e0b\u7684\u51c6\u786e\u7387\u4e0b\u964d\uff0c\u800c\u5ffd\u7565\u4e86\u6548\u7387\u65b9\u9762\u7684\u9c81\u68d2\u6027\u3002\u8fd9\u79cd\u5c40\u9650\u6027\u5bfc\u81f4\u5bf9ASR\u6a21\u578b\u6f0f\u6d1e\u7684\u7406\u89e3\u4e0d\u5168\u9762\u3002", "method": "\u63d0\u51fa\u4e86MORE\uff08\u591a\u76ee\u6807\u91cd\u590d\u52a0\u500d\u9f13\u52b1\u653b\u51fb\uff09\uff0c\u91c7\u7528\u5c42\u6b21\u5316\u9636\u6bb5\u6392\u65a5-\u951a\u5b9a\u673a\u5236\uff0c\u5c06\u591a\u76ee\u6807\u5bf9\u6297\u4f18\u5316\u91cd\u65b0\u8868\u8ff0\u4e3a\u5c42\u6b21\u5316\u6846\u67b6\u3002\u5f15\u5165\u65b0\u9896\u7684\u91cd\u590d\u9f13\u52b1\u52a0\u500d\u76ee\u6807\uff08REDO\uff09\uff0c\u901a\u8fc7\u4fdd\u6301\u51c6\u786e\u7387\u4e0b\u964d\u5e76\u5468\u671f\u6027\u5730\u52a0\u500d\u9884\u6d4b\u5e8f\u5217\u957f\u5ea6\uff0c\u8bf1\u5bfc\u91cd\u590d\u6587\u672c\u751f\u6210\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cMORE\u76f8\u6bd4\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u80fd\u591f\u6301\u7eed\u4ea7\u751f\u663e\u8457\u66f4\u957f\u7684\u8f6c\u5f55\u6587\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u8f83\u9ad8\u7684\u8bcd\u9519\u8bef\u7387\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u591a\u76ee\u6807\u5bf9\u6297\u653b\u51fb\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "MORE\u653b\u51fb\u80fd\u591f\u8feb\u4f7fASR\u6a21\u578b\u5728\u5355\u4e2a\u5bf9\u6297\u8f93\u5165\u4e0b\u4ea7\u751f\u9519\u8bef\u8f6c\u5f55\uff0c\u540c\u65f6\u663e\u8457\u589e\u52a0\u8ba1\u7b97\u6210\u672c\uff0c\u4e3a\u5168\u9762\u7406\u89e3ASR\u6a21\u578b\u9c81\u68d2\u6027\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2601.01294", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.01294", "abs": "https://arxiv.org/abs/2601.01294", "authors": ["Ching Ho Lee", "Javier Nistal", "Stefan Lattner", "Marco Pasini", "George Fazekas"], "title": "Diffusion Timbre Transfer Via Mutual Information Guided Inpainting", "comment": "6 pages, 2 figures, 3 tables", "summary": "We study timbre transfer as an inference-time editing problem for music audio. Starting from a strong pre-trained latent diffusion model, we introduce a lightweight procedure that requires no additional training: (i) a dimension-wise noise injection that targets latent channels most informative of instrument identity, and (ii) an early-step clamping mechanism that re-imposes the input's melodic and rhythmic structure during reverse diffusion. The method operates directly on audio latents and is compatible with text/audio conditioning (e.g., CLAP). We discuss design choices,analyze trade-offs between timbral change and structural preservation, and show that simple inference-time controls can meaningfully steer pre-trained models for style-transfer use cases.", "AI": {"tldr": "\u65e0\u9700\u989d\u5916\u8bad\u7ec3\uff0c\u901a\u8fc7\u63a8\u7406\u65f6\u7f16\u8f91\u5b9e\u73b0\u97f3\u8272\u8f6c\u6362\uff1a\u5728\u9884\u8bad\u7ec3\u6f5c\u5728\u6269\u6563\u6a21\u578b\u4e0a\u5f15\u5165\u7ef4\u5ea6\u566a\u58f0\u6ce8\u5165\u548c\u65e9\u671f\u6b65\u957f\u94b3\u5236\u673a\u5236", "motivation": "\u5c06\u97f3\u8272\u8f6c\u6362\u89c6\u4e3a\u97f3\u4e50\u97f3\u9891\u7684\u63a8\u7406\u65f6\u7f16\u8f91\u95ee\u9898\uff0c\u5229\u7528\u5f3a\u5927\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u907f\u514d\u989d\u5916\u8bad\u7ec3\u6210\u672c", "method": "1) \u9488\u5bf9\u4e50\u5668\u8eab\u4efd\u4fe1\u606f\u6700\u4e30\u5bcc\u7684\u6f5c\u5728\u901a\u9053\u8fdb\u884c\u7ef4\u5ea6\u566a\u58f0\u6ce8\u5165\uff1b2) \u5728\u53cd\u5411\u6269\u6563\u8fc7\u7a0b\u4e2d\u901a\u8fc7\u65e9\u671f\u6b65\u957f\u94b3\u5236\u91cd\u65b0\u65bd\u52a0\u8f93\u5165\u7684\u65cb\u5f8b\u548c\u8282\u594f\u7ed3\u6784", "result": "\u65b9\u6cd5\u76f4\u63a5\u5728\u97f3\u9891\u6f5c\u5728\u7a7a\u95f4\u64cd\u4f5c\uff0c\u517c\u5bb9\u6587\u672c/\u97f3\u9891\u6761\u4ef6\uff08\u5982CLAP\uff09\uff0c\u901a\u8fc7\u7b80\u5355\u63a8\u7406\u65f6\u63a7\u5236\u6709\u6548\u5f15\u5bfc\u9884\u8bad\u7ec3\u6a21\u578b\u5b9e\u73b0\u98ce\u683c\u8f6c\u6362", "conclusion": "\u5c55\u793a\u4e86\u7b80\u5355\u63a8\u7406\u65f6\u63a7\u5236\u80fd\u6709\u6548\u5f15\u5bfc\u9884\u8bad\u7ec3\u6a21\u578b\u7528\u4e8e\u98ce\u683c\u8f6c\u6362\uff0c\u5206\u6790\u4e86\u97f3\u8272\u53d8\u5316\u4e0e\u7ed3\u6784\u4fdd\u6301\u4e4b\u95f4\u7684\u6743\u8861"}}
{"id": "2601.01232", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.01232", "abs": "https://arxiv.org/abs/2601.01232", "authors": ["Yu Xue", "Kwantae Kim"], "title": "A 2.5 $\u03bc$W 30 nV/$\\surd$Hz Instrumentation Amplifier for Bioimpedance Sensors with Source Degenerated Current Mirror and DTMOS Transistor", "comment": "11 pages, 14 figures", "summary": "This paper proposes a low-power and low-noise instrumentation amplifier (IA) tailored for bioimpedance sensing applications. The design originates from a gain-boosted flipped voltage follower (FVF) transconductance (TC) stage and integrates two complementary circuit techniques to improve the noise performance. To achieve an optimal balance between input-referred noise and available voltage headroom, a source-degenerated current mirror (SDCM) is adopted, resulting in reducing the input-referred noise by 7.95% compared with a conventional current mirror structure. In addition, a dynamic threshold MOSFET (DTMOS) scheme is employed to enhance the effective transconductance, leading to a further 11.66% reduction in input-referred noise. Simulated in a 28 nm CMOS process demonstrate that the proposed IA achieves an input-referred noise floor of 30 nV/$\\surd$Hz and a bandwidth of 1.44 MHz, while consuming only 2.5 $\u03bc$W from a 0.8 V supply. Compared to the baseline design, the proposed approach achieves a 32.4% reduction in power consumption without degrading noise performance. The complete design parameters are open-sourced in this paper, to ensure reproducibility and facilitate future developments.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7528\u4e8e\u751f\u7269\u963b\u6297\u4f20\u611f\u7684\u4f4e\u529f\u8017\u4f4e\u566a\u58f0\u4eea\u8868\u653e\u5927\u5668\uff0c\u91c7\u7528\u589e\u76ca\u589e\u5f3a\u578b\u7ffb\u8f6c\u7535\u538b\u8ddf\u968f\u5668\u7ed3\u6784\uff0c\u7ed3\u5408\u6e90\u9000\u5316\u7535\u6d41\u955c\u548c\u52a8\u6001\u9608\u503cMOSFET\u6280\u672f\u964d\u4f4e\u566a\u58f0\uff0c\u572828nm CMOS\u5de5\u827a\u4e0b\u5b9e\u73b030nV/\u221aHz\u566a\u58f0\u548c2.5\u03bcW\u529f\u8017\u3002", "motivation": "\u751f\u7269\u963b\u6297\u4f20\u611f\u5e94\u7528\u9700\u8981\u4f4e\u529f\u8017\u3001\u4f4e\u566a\u58f0\u7684\u4eea\u8868\u653e\u5927\u5668\uff0c\u4f20\u7edf\u8bbe\u8ba1\u5728\u566a\u58f0\u6027\u80fd\u548c\u7535\u538b\u88d5\u5ea6\u4e4b\u95f4\u5b58\u5728\u6743\u8861\uff0c\u9700\u8981\u521b\u65b0\u7535\u8def\u6280\u672f\u6765\u4f18\u5316\u8fd9\u4e00\u5e73\u8861\u3002", "method": "\u57fa\u4e8e\u589e\u76ca\u589e\u5f3a\u578b\u7ffb\u8f6c\u7535\u538b\u8ddf\u968f\u5668\u8de8\u5bfc\u7ea7\uff0c\u91c7\u7528\u4e24\u79cd\u4e92\u8865\u6280\u672f\uff1a1) \u6e90\u9000\u5316\u7535\u6d41\u955c\u4f18\u5316\u566a\u58f0\u4e0e\u7535\u538b\u88d5\u5ea6\u5e73\u8861\uff0c2) \u52a8\u6001\u9608\u503cMOSFET\u65b9\u6848\u589e\u5f3a\u6709\u6548\u8de8\u5bfc\u3002\u572828nm CMOS\u5de5\u827a\u4e2d\u5b9e\u73b0\u3002", "result": "\u8f93\u5165\u53c2\u8003\u566a\u58f030nV/\u221aHz\uff0c\u5e26\u5bbd1.44MHz\uff0c\u529f\u8017\u4ec52.5\u03bcW\uff080.8V\u7535\u6e90\uff09\u3002\u76f8\u6bd4\u57fa\u51c6\u8bbe\u8ba1\uff0c\u529f\u8017\u964d\u4f4e32.4%\u4e14\u566a\u58f0\u6027\u80fd\u4e0d\u53d8\u3002\u6e90\u9000\u5316\u7535\u6d41\u955c\u964d\u4f4e\u566a\u58f07.95%\uff0c\u52a8\u6001\u9608\u503cMOSFET\u8fdb\u4e00\u6b65\u964d\u4f4e11.66%\u3002", "conclusion": "\u63d0\u51fa\u7684\u4eea\u8868\u653e\u5927\u5668\u6210\u529f\u5b9e\u73b0\u4e86\u751f\u7269\u963b\u6297\u4f20\u611f\u6240\u9700\u7684\u4f4e\u529f\u8017\u4f4e\u566a\u58f0\u6027\u80fd\uff0c\u901a\u8fc7\u521b\u65b0\u7535\u8def\u6280\u672f\u4f18\u5316\u4e86\u566a\u58f0\u4e0e\u529f\u8017\u7684\u6743\u8861\uff0c\u8bbe\u8ba1\u53c2\u6570\u5df2\u5f00\u6e90\u4ee5\u786e\u4fdd\u53ef\u91cd\u590d\u6027\u548c\u4fc3\u8fdb\u672a\u6765\u53d1\u5c55\u3002"}}
{"id": "2601.02073", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2601.02073", "abs": "https://arxiv.org/abs/2601.02073", "authors": ["Abhijit Mohanta", "Remruatpuii", "Priyankoo Sarmah", "Rohit Sinha", "Wendy Lalhminghlui"], "title": "Towards Prosodically Informed Mizo TTS without Explicit Tone Markings", "comment": null, "summary": "This paper reports on the development of a text-to-speech (TTS) system for Mizo, a low-resource, tonal, and Tibeto-Burman language spoken primarily in the Indian state of Mizoram. The TTS was built with only 5.18 hours of data; however, in terms of subjective and objective evaluations, the outputs were considered perceptually acceptable and intelligible. A baseline model using Tacotron2 was built, and then, with the same data, another TTS model was built with VITS. In both subjective and objective evaluations, the VITS model outperformed the Tacotron2 model. In terms of tone synthesis, the VITS model showed significantly lower tone errors than the Tacotron2 model. The paper demonstrates that a non-autoregressive, end-to-end framework can achieve synthesis of acceptable perceptual quality and intelligibility.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u4ec55.18\u5c0f\u65f6\u6570\u636e\u7684\u7c73\u4f50\u8bedTTS\u7cfb\u7edf\uff0c\u4f7f\u7528Tacotron2\u548cVITS\u4e24\u79cd\u6a21\u578b\uff0cVITS\u5728\u4e3b\u89c2\u5ba2\u89c2\u8bc4\u4f30\u4e2d\u8868\u73b0\u66f4\u4f18\uff0c\u7279\u522b\u662f\u97f3\u8c03\u5408\u6210\u9519\u8bef\u663e\u8457\u964d\u4f4e\u3002", "motivation": "\u4e3a\u4f4e\u8d44\u6e90\u3001\u6709\u58f0\u8c03\u7684\u85cf\u7f05\u8bed\u7cfb\u8bed\u8a00\u7c73\u4f50\u8bed\u5f00\u53d1\u6587\u672c\u8f6c\u8bed\u97f3\u7cfb\u7edf\uff0c\u63a2\u7d22\u5728\u6709\u9650\u6570\u636e\u6761\u4ef6\u4e0b\u5b9e\u73b0\u53ef\u63a5\u53d7\u611f\u77e5\u8d28\u91cf\u548c\u53ef\u61c2\u5ea6\u7684\u5408\u6210\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u4ec55.18\u5c0f\u65f6\u7684\u7c73\u4f50\u8bed\u6570\u636e\uff0c\u5206\u522b\u6784\u5efa\u57fa\u4e8eTacotron2\u7684\u57fa\u7ebf\u6a21\u578b\u548c\u57fa\u4e8eVITS\u7684\u6a21\u578b\uff0c\u8fdb\u884c\u4e3b\u89c2\u548c\u5ba2\u89c2\u8bc4\u4f30\u5bf9\u6bd4\u3002", "result": "VITS\u6a21\u578b\u5728\u4e3b\u89c2\u548c\u5ba2\u89c2\u8bc4\u4f30\u4e2d\u5747\u4f18\u4e8eTacotron2\u6a21\u578b\uff0c\u7279\u522b\u662f\u5728\u97f3\u8c03\u5408\u6210\u65b9\u9762\u9519\u8bef\u7387\u663e\u8457\u66f4\u4f4e\uff0c\u7cfb\u7edf\u8f93\u51fa\u5728\u611f\u77e5\u4e0a\u53ef\u63a5\u53d7\u4e14\u53ef\u61c2\u3002", "conclusion": "\u975e\u81ea\u56de\u5f52\u7aef\u5230\u7aef\u6846\u67b6\uff08\u5982VITS\uff09\u80fd\u591f\u5728\u6709\u9650\u6570\u636e\u6761\u4ef6\u4e0b\u5b9e\u73b0\u53ef\u63a5\u53d7\u611f\u77e5\u8d28\u91cf\u548c\u53ef\u61c2\u5ea6\u7684\u6709\u58f0\u8c03\u8bed\u8a00\u5408\u6210\uff0c\u4e3a\u4f4e\u8d44\u6e90\u8bed\u8a00TTS\u5f00\u53d1\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84\u3002"}}
{"id": "2601.01373", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.01373", "abs": "https://arxiv.org/abs/2601.01373", "authors": ["Qundong Shi", "Jie Zhou", "Biyuan Lin", "Junbo Cui", "Guoyang Zeng", "Yixuan Zhou", "Ziyang Wang", "Xin Liu", "Zhen Luo", "Yudong Wang", "Zhiyuan Liu"], "title": "UltraEval-Audio: A Unified Framework for Comprehensive Evaluation of Audio Foundation Models", "comment": "13 pages, 2 figures", "summary": "The development of audio foundation models has accelerated rapidly since the emergence of GPT-4o. However, the lack of comprehensive evaluation has become a critical bottleneck for further progress in the field, particularly in audio generation. Current audio evaluation faces three major challenges: (1) audio evaluation lacks a unified framework, with datasets and code scattered across various sources, hindering fair and efficient cross-model comparison;(2) audio codecs, as a key component of audio foundation models, lack a widely accepted and holistic evaluation methodology; (3) existing speech benchmarks are heavily reliant on English, making it challenging to objectively assess models' performance on Chinese. To address the first issue, we introduce UltraEval-Audio, a unified evaluation framework for audio foundation models, specifically designed for both audio understanding and generation tasks. UltraEval-Audio features a modular architecture, supporting 10 languages and 14 core task categories, while seamlessly integrating 24 mainstream models and 36 authoritative benchmarks. To enhance research efficiency, the framework provides a one-command evaluation feature, accompanied by real-time public leaderboards. For the second challenge, UltraEval-Audio adopts a novel comprehensive evaluation scheme for audio codecs, evaluating performance across three key dimensions: semantic accuracy, timbre fidelity, and acoustic quality. To address the third issue, we propose two new Chinese benchmarks, SpeechCMMLU and SpeechHSK, designed to assess Chinese knowledge proficiency and language fluency. We wish that UltraEval-Audio will provide both academia and industry with a transparent, efficient, and fair platform for comparison of audio models. Our code, benchmarks, and leaderboards are available at https://github.com/OpenBMB/UltraEval-Audio.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86UltraEval-Audio\uff0c\u4e00\u4e2a\u7edf\u4e00\u7684\u97f3\u9891\u57fa\u7840\u6a21\u578b\u8bc4\u4f30\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u97f3\u9891\u8bc4\u4f30\u9886\u57df\u7f3a\u4e4f\u7edf\u4e00\u6807\u51c6\u3001\u97f3\u9891\u7f16\u89e3\u7801\u5668\u8bc4\u4f30\u65b9\u6cd5\u4e0d\u5b8c\u5584\u4ee5\u53ca\u4e2d\u6587\u8bed\u97f3\u57fa\u51c6\u4e0d\u8db3\u4e09\u5927\u6311\u6218\u3002", "motivation": "\u97f3\u9891\u57fa\u7840\u6a21\u578b\u53d1\u5c55\u8fc5\u901f\uff0c\u4f46\u7f3a\u4e4f\u5168\u9762\u7684\u8bc4\u4f30\u4f53\u7cfb\u5df2\u6210\u4e3a\u8be5\u9886\u57df\u8fdb\u4e00\u6b65\u53d1\u5c55\u7684\u5173\u952e\u74f6\u9888\u3002\u5f53\u524d\u97f3\u9891\u8bc4\u4f30\u9762\u4e34\u4e09\u5927\u6311\u6218\uff1a1) \u7f3a\u4e4f\u7edf\u4e00\u6846\u67b6\uff0c\u6570\u636e\u96c6\u548c\u4ee3\u7801\u5206\u6563\uff1b2) \u97f3\u9891\u7f16\u89e3\u7801\u5668\u7f3a\u4e4f\u5168\u9762\u8bc4\u4f30\u65b9\u6cd5\uff1b3) \u73b0\u6709\u8bed\u97f3\u57fa\u51c6\u8fc7\u5ea6\u4f9d\u8d56\u82f1\u8bed\uff0c\u96be\u4ee5\u5ba2\u89c2\u8bc4\u4f30\u4e2d\u6587\u6027\u80fd\u3002", "method": "\u63d0\u51faUltraEval-Audio\u7edf\u4e00\u8bc4\u4f30\u6846\u67b6\uff0c\u91c7\u7528\u6a21\u5757\u5316\u67b6\u6784\uff0c\u652f\u630110\u79cd\u8bed\u8a00\u548c14\u4e2a\u6838\u5fc3\u4efb\u52a1\u7c7b\u522b\uff0c\u96c6\u621024\u4e2a\u4e3b\u6d41\u6a21\u578b\u548c36\u4e2a\u6743\u5a01\u57fa\u51c6\u3002\u9488\u5bf9\u97f3\u9891\u7f16\u89e3\u7801\u5668\u63d0\u51fa\u5305\u542b\u8bed\u4e49\u51c6\u786e\u6027\u3001\u97f3\u8272\u4fdd\u771f\u5ea6\u548c\u58f0\u5b66\u8d28\u91cf\u4e09\u4e2a\u7ef4\u5ea6\u7684\u7efc\u5408\u8bc4\u4f30\u65b9\u6848\u3002\u9488\u5bf9\u4e2d\u6587\u8bc4\u4f30\u4e0d\u8db3\u95ee\u9898\uff0c\u63d0\u51faSpeechCMMLU\u548cSpeechHSK\u4e24\u4e2a\u65b0\u7684\u4e2d\u6587\u57fa\u51c6\u3002", "result": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u900f\u660e\u3001\u9ad8\u6548\u3001\u516c\u5e73\u7684\u97f3\u9891\u6a21\u578b\u6bd4\u8f83\u5e73\u53f0\uff0c\u63d0\u4f9b\u4e00\u952e\u8bc4\u4f30\u529f\u80fd\u548c\u5b9e\u65f6\u516c\u5171\u6392\u884c\u699c\u3002\u6846\u67b6\u652f\u6301\u591a\u8bed\u8a00\u3001\u591a\u4efb\u52a1\u8bc4\u4f30\uff0c\u5e76\u4e3a\u97f3\u9891\u7f16\u89e3\u7801\u5668\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "conclusion": "UltraEval-Audio\u4e3a\u5b66\u672f\u754c\u548c\u5de5\u4e1a\u754c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u97f3\u9891\u6a21\u578b\u8bc4\u4f30\u5e73\u53f0\uff0c\u89e3\u51b3\u4e86\u5f53\u524d\u97f3\u9891\u8bc4\u4f30\u9886\u57df\u7684\u5173\u952e\u74f6\u9888\u95ee\u9898\uff0c\u6709\u671b\u63a8\u52a8\u97f3\u9891\u57fa\u7840\u6a21\u578b\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002"}}
{"id": "2601.01277", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.01277", "abs": "https://arxiv.org/abs/2601.01277", "authors": ["Ximing Xie", "Fang Fang", "Zhiguo Ding", "Xianbin Wang"], "title": "Pinching Antennas in Blockage-Aware Environments: Modeling, Design, and Optimization", "comment": null, "summary": "Pinching-antenna (PA) systems have recently emerged as a promising member of the flexible-antenna family due to their ability to dynamically establish line-of-sight (LoS) links. While most existing studies assume ideal environments without obstacles, practical indoor deployments are often obstacle-rich, where LoS blockage significantly degrades performance. This paper investigates pinching-antenna systems in blockage-aware environments by developing a deterministic model for cylinder-shaped obstacles that precisely characterizes LoS conditions without relying on stochastic approximations. Based on this model, a special case is first studied where each PA serves a single user and can only be deployed at discrete positions along the waveguide. In this case, the waveguide-user assignment is obtained via the Hungarian algorithm, and PA positions are refined using a surrogate-assisted block-coordinate search. Then, a general case is considered where each PA serves all users and can be continuously placed along the waveguide. In this case, beamforming and PA positions are jointly optimized by a weighted minimum mean square error integrated deep deterministic policy gradient (WMMSE-DDPG) approach to address non-smooth LoS transitions. Simulation results demonstrate that the proposed algorithms significantly improve system throughput and LoS connectivity compared with benchmark methods. Moreover, the results reveal that pinching-antenna systems can effectively leverage obstacles to suppress co-channel interference, converting potential blockages into performance gains.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5728\u969c\u788d\u7269\u4e30\u5bcc\u7684\u5ba4\u5185\u73af\u5883\u4e2d\u5939\u6301\u5929\u7ebf\u7cfb\u7edf\uff0c\u5f00\u53d1\u4e86\u7cbe\u786e\u7684\u5706\u67f1\u5f62\u969c\u788d\u7269\u786e\u5b9a\u6027\u6a21\u578b\uff0c\u63d0\u51fa\u4e86\u9488\u5bf9\u5355\u7528\u6237\u548c\u591a\u7528\u6237\u573a\u666f\u7684\u4f18\u5316\u7b97\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7cfb\u7edf\u541e\u5410\u91cf\u548cLoS\u8fde\u63a5\u6027\u3002", "motivation": "\u73b0\u6709\u5939\u6301\u5929\u7ebf\u7cfb\u7edf\u7814\u7a76\u5927\u591a\u5047\u8bbe\u7406\u60f3\u65e0\u969c\u788d\u73af\u5883\uff0c\u4f46\u5b9e\u9645\u5ba4\u5185\u90e8\u7f72\u5e38\u5b58\u5728\u4e30\u5bcc\u969c\u788d\u7269\uff0c\u5bfc\u81f4\u89c6\u8ddd\u94fe\u8def\u963b\u585e\u4e25\u91cd\u5f71\u54cd\u6027\u80fd\u3002\u9700\u8981\u7814\u7a76\u969c\u788d\u7269\u611f\u77e5\u73af\u5883\u4e0b\u7684\u5939\u6301\u5929\u7ebf\u7cfb\u7edf\u4f18\u5316\u3002", "method": "1. \u5f00\u53d1\u5706\u67f1\u5f62\u969c\u788d\u7269\u7684\u786e\u5b9a\u6027\u6a21\u578b\u7cbe\u786e\u63cf\u8ff0LoS\u6761\u4ef6\uff1b2. \u5355\u7528\u6237\u573a\u666f\uff1a\u4f7f\u7528\u5308\u7259\u5229\u7b97\u6cd5\u8fdb\u884c\u6ce2\u5bfc-\u7528\u6237\u5206\u914d\uff0c\u91c7\u7528\u4ee3\u7406\u8f85\u52a9\u5757\u5750\u6807\u641c\u7d22\u4f18\u5316\u5929\u7ebf\u4f4d\u7f6e\uff1b3. \u591a\u7528\u6237\u573a\u666f\uff1a\u63d0\u51faWMMSE-DDPG\u65b9\u6cd5\u8054\u5408\u4f18\u5316\u6ce2\u675f\u6210\u5f62\u548c\u5929\u7ebf\u4f4d\u7f6e\uff0c\u5904\u7406\u975e\u5e73\u6ed1LoS\u8f6c\u6362\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u7b97\u6cd5\u76f8\u6bd4\u57fa\u51c6\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u7cfb\u7edf\u541e\u5410\u91cf\u548cLoS\u8fde\u63a5\u6027\u3002\u5939\u6301\u5929\u7ebf\u7cfb\u7edf\u80fd\u6709\u6548\u5229\u7528\u969c\u788d\u7269\u6291\u5236\u540c\u4fe1\u9053\u5e72\u6270\uff0c\u5c06\u6f5c\u5728\u963b\u585e\u8f6c\u5316\u4e3a\u6027\u80fd\u589e\u76ca\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u969c\u788d\u7269\u4e30\u5bcc\u7684\u5ba4\u5185\u73af\u5883\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u5939\u6301\u5929\u7ebf\u7cfb\u7edf\u4f18\u5316\u65b9\u6848\uff0c\u8bc1\u660e\u4e86\u969c\u788d\u7269\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u53ef\u8f6c\u5316\u4e3a\u6027\u80fd\u4f18\u52bf\u800c\u975e\u4ec5\u4ec5\u662f\u9650\u5236\u56e0\u7d20\u3002"}}
{"id": "2601.02231", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2601.02231", "abs": "https://arxiv.org/abs/2601.02231", "authors": ["Marc Deegen", "Tobias Gburrek", "Tobias Cord-Landwehr", "Thilo von Neumann", "Jiangyu Han", "Luk\u00e1\u0161 Burget", "Reinhold Haeb-Umbach"], "title": "On the Role of Spatial Features in Foundation-Model-Based Speaker Diarization", "comment": "Accepted at HSCMA 2026", "summary": "Recent advances in speaker diarization exploit large pretrained foundation models, such as WavLM, to achieve state-of-the-art performance on multiple datasets. Systems like DiariZen leverage these rich single-channel representations, but are limited to single-channel audio, preventing the use of spatial cues available in multi-channel recordings. This work analyzes the impact of incorporating spatial information into a state-of-the-art single-channel diarization system by evaluating several strategies for conditioning the model on multi-channel spatial features. Experiments on meeting-style datasets indicate that spatial information can improve diarization performance, but the overall improvement is smaller than expected for the proposed system, suggesting that the features aggregated over all WavLM layers already capture much of the information needed for accurate speaker discrimination, also in overlapping speech regions. These findings provide insight into the potential and limitations of using spatial cues to enhance foundation model-based diarization.", "AI": {"tldr": "\u7814\u7a76\u5206\u6790\u4e86\u5728\u591a\u901a\u9053\u5f55\u97f3\u4e2d\u5f15\u5165\u7a7a\u95f4\u4fe1\u606f\u5bf9\u57fa\u4e8eWavLM\u7b49\u57fa\u7840\u6a21\u578b\u7684\u8bf4\u8bdd\u4eba\u65e5\u5fd7\u7cfb\u7edf\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u7a7a\u95f4\u4fe1\u606f\u80fd\u63d0\u5347\u6027\u80fd\u4f46\u6539\u5584\u6709\u9650\uff0c\u8868\u660eWavLM\u591a\u5c42\u7279\u5f81\u5df2\u5305\u542b\u8db3\u591f\u7684\u8bf4\u8bdd\u4eba\u533a\u5206\u4fe1\u606f\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8eWavLM\u7b49\u57fa\u7840\u6a21\u578b\u7684\u8bf4\u8bdd\u4eba\u65e5\u5fd7\u7cfb\u7edf\uff08\u5982DiariZen\uff09\u4ec5\u9650\u4e8e\u5355\u901a\u9053\u97f3\u9891\uff0c\u65e0\u6cd5\u5229\u7528\u591a\u901a\u9053\u5f55\u97f3\u4e2d\u7684\u7a7a\u95f4\u7ebf\u7d22\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5c06\u7a7a\u95f4\u4fe1\u606f\u878d\u5165\u6700\u5148\u8fdb\u7684\u5355\u901a\u9053\u65e5\u5fd7\u7cfb\u7edf\u7684\u5f71\u54cd\u3002", "method": "\u8bc4\u4f30\u4e86\u591a\u79cd\u5c06\u591a\u901a\u9053\u7a7a\u95f4\u7279\u5f81\u6761\u4ef6\u5316\u5230\u6a21\u578b\u4e2d\u7684\u7b56\u7565\uff0c\u5728\u4f1a\u8bae\u98ce\u683c\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u5206\u6790\u7a7a\u95f4\u4fe1\u606f\u5bf9\u8bf4\u8bdd\u4eba\u65e5\u5fd7\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u7a7a\u95f4\u4fe1\u606f\u53ef\u4ee5\u6539\u5584\u8bf4\u8bdd\u4eba\u65e5\u5fd7\u6027\u80fd\uff0c\u4f46\u6574\u4f53\u6539\u8fdb\u5e45\u5ea6\u5c0f\u4e8e\u9884\u671f\u3002WavLM\u6240\u6709\u5c42\u805a\u5408\u7684\u7279\u5f81\u5df2\u7ecf\u6355\u83b7\u4e86\u51c6\u786e\u8bf4\u8bdd\u4eba\u533a\u5206\u6240\u9700\u7684\u5927\u90e8\u5206\u4fe1\u606f\uff0c\u5305\u62ec\u91cd\u53e0\u8bed\u97f3\u533a\u57df\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u4f7f\u7528\u7a7a\u95f4\u7ebf\u7d22\u589e\u5f3a\u57fa\u4e8e\u57fa\u7840\u6a21\u578b\u7684\u8bf4\u8bdd\u4eba\u65e5\u5fd7\u7cfb\u7edf\u7684\u6f5c\u529b\u548c\u5c40\u9650\u6027\uff0c\u4e3a\u672a\u6765\u591a\u901a\u9053\u65e5\u5fd7\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\u3002"}}
{"id": "2601.01392", "categories": ["cs.SD", "cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.01392", "abs": "https://arxiv.org/abs/2601.01392", "authors": ["Peidong Wang", "Zhiming Ma", "Xin Dai", "Yongkang Liu", "Shi Feng", "Xiaocui Yang", "Wenxing Hu", "Zhihao Wang", "Mingjun Pan", "Li Yuan", "Daling Wang"], "title": "SAFE-QAQ: End-to-End Slow-Thinking Audio-Text Fraud Detection via Reinforcement Learning", "comment": null, "summary": "Existing fraud detection methods predominantly rely on transcribed text, suffering from ASR errors and missing crucial acoustic cues like vocal tone and environmental context. This limits their effectiveness against complex deceptive strategies. To address these challenges, we first propose \\textbf{SAFE-QAQ}, an end-to-end comprehensive framework for audio-based slow-thinking fraud detection. First, the SAFE-QAQ framework eliminates the impact of transcription errors on detection performance. Secondly, we propose rule-based slow-thinking reward mechanisms that systematically guide the system to identify fraud-indicative patterns by accurately capturing fine-grained audio details, through hierarchical reasoning processes. Besides, our framework introduces a dynamic risk assessment framework during live calls, enabling early detection and prevention of fraud. Experiments on the TeleAntiFraud-Bench demonstrate that SAFE-QAQ achieves dramatic improvements over existing methods in multiple key dimensions, including accuracy, inference efficiency, and real-time processing capabilities. Currently deployed and analyzing over 70,000 calls daily, SAFE-QAQ effectively automates complex fraud detection, reducing human workload and financial losses. Code: https://anonymous.4open.science/r/SAFE-QAQ.", "AI": {"tldr": "SAFE-QAQ\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u97f3\u9891\u6b3a\u8bc8\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u6d88\u9664ASR\u8f6c\u5f55\u9519\u8bef\u5f71\u54cd\u3001\u57fa\u4e8e\u89c4\u5219\u7684\u6162\u601d\u8003\u5956\u52b1\u673a\u5236\u548c\u52a8\u6001\u98ce\u9669\u8bc4\u4f30\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6b3a\u8bc8\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u548c\u5b9e\u65f6\u5904\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u6b3a\u8bc8\u68c0\u6d4b\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u8f6c\u5f55\u6587\u672c\uff0c\u5bb9\u6613\u53d7\u5230ASR\u9519\u8bef\u5f71\u54cd\uff0c\u4e14\u5ffd\u7565\u4e86\u97f3\u9891\u4e2d\u7684\u5173\u952e\u58f0\u5b66\u7ebf\u7d22\uff08\u5982\u8bed\u97f3\u8bed\u8c03\u3001\u73af\u5883\u80cc\u666f\uff09\uff0c\u8fd9\u9650\u5236\u4e86\u5b83\u4eec\u5e94\u5bf9\u590d\u6742\u6b3a\u8bc8\u7b56\u7565\u7684\u80fd\u529b\u3002", "method": "1. \u7aef\u5230\u7aef\u97f3\u9891\u5904\u7406\u6846\u67b6\uff0c\u6d88\u9664\u8f6c\u5f55\u9519\u8bef\u5f71\u54cd\uff1b2. \u63d0\u51fa\u57fa\u4e8e\u89c4\u5219\u7684\u6162\u601d\u8003\u5956\u52b1\u673a\u5236\uff0c\u901a\u8fc7\u5206\u5c42\u63a8\u7406\u8fc7\u7a0b\u7cbe\u786e\u6355\u6349\u7ec6\u7c92\u5ea6\u97f3\u9891\u7ec6\u8282\uff1b3. \u5f15\u5165\u52a8\u6001\u98ce\u9669\u8bc4\u4f30\u6846\u67b6\uff0c\u5728\u5b9e\u65f6\u901a\u8bdd\u4e2d\u5b9e\u73b0\u65e9\u671f\u6b3a\u8bc8\u68c0\u6d4b\u548c\u9884\u9632\u3002", "result": "\u5728TeleAntiFraud-Bench\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0cSAFE-QAQ\u5728\u51c6\u786e\u6027\u3001\u63a8\u7406\u6548\u7387\u548c\u5b9e\u65f6\u5904\u7406\u80fd\u529b\u7b49\u591a\u4e2a\u5173\u952e\u7ef4\u5ea6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002\u76ee\u524d\u5df2\u90e8\u7f72\u5e76\u6bcf\u5929\u5206\u6790\u8d85\u8fc770,000\u4e2a\u901a\u8bdd\uff0c\u6709\u6548\u81ea\u52a8\u5316\u590d\u6742\u6b3a\u8bc8\u68c0\u6d4b\uff0c\u51cf\u5c11\u4eba\u5de5\u5de5\u4f5c\u91cf\u548c\u8d22\u52a1\u635f\u5931\u3002", "conclusion": "SAFE-QAQ\u901a\u8fc7\u76f4\u63a5\u5904\u7406\u97f3\u9891\u4fe1\u53f7\u3001\u5f15\u5165\u6162\u601d\u8003\u63a8\u7406\u673a\u5236\u548c\u52a8\u6001\u98ce\u9669\u8bc4\u4f30\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u4f20\u7edf\u57fa\u4e8e\u6587\u672c\u7684\u6b3a\u8bc8\u68c0\u6d4b\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u97f3\u9891\u6b3a\u8bc8\u68c0\u6d4b\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.01598", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.01598", "abs": "https://arxiv.org/abs/2601.01598", "authors": ["Anthony Joseph Perre", "Parker Huggins", "Alphan Sahin"], "title": "KAN-AE with Non-Linearity Score and Symbolic Regression for Energy-Efficient Channel Coding", "comment": "IEEE Consumer Communications & Networking Conference 2026 (IEEE CCNC 2026), 9-12 January 2026, Las Vegas, NV, USA", "summary": "In this paper, we investigate Kolmogorov-Arnold network-based autoencoders (KAN-AEs) with symbolic regression (SR) for energy-efficient channel coding. By using SR, we convert KAN-AEs into symbolic expressions, which enables low-complexity implementation and improved energy efficiency at the radios. To further enhance the efficiency, we introduce a new non-linearity score term in the SR process to help select lower-complexity equations when possible. Through numerical simulations, we demonstrate that KAN-AEs achieve competitive BLER performance while improving energy efficiency when paired with SR. We score the energy efficiency of a KAN-AE implementation using the proposed non-linearity metric and compare it to a multi-layer perceptron-based autoencoder (MLP-AE). Our experiment shows that the KAN-AE paired with SR uses 1.38 times less energy than the MLP-AE, supporting that KAN-AEs are a promising choice for energy-efficient deep learning-based channel coding.", "AI": {"tldr": "KAN-AEs\u7ed3\u5408\u7b26\u53f7\u56de\u5f52\u5b9e\u73b0\u80fd\u6548\u4fe1\u9053\u7f16\u7801\uff0c\u901a\u8fc7\u7b26\u53f7\u8868\u8fbe\u5f0f\u964d\u4f4e\u590d\u6742\u5ea6\uff0c\u6bd4MLP-AE\u8282\u80fd1.38\u500d", "motivation": "\u7814\u7a76\u5982\u4f55\u901a\u8fc7Kolmogorov-Arnold\u7f51\u7edc\u548c\u7b26\u53f7\u56de\u5f52\u6280\u672f\u5b9e\u73b0\u80fd\u91cf\u9ad8\u6548\u7684\u4fe1\u9053\u7f16\u7801\uff0c\u89e3\u51b3\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u65e0\u7ebf\u7535\u8bbe\u5907\u4e0a\u8ba1\u7b97\u590d\u6742\u3001\u80fd\u8017\u9ad8\u7684\u95ee\u9898", "method": "\u4f7f\u7528KAN-AEs\u7ed3\u5408\u7b26\u53f7\u56de\u5f52\uff0c\u5c06\u7f51\u7edc\u8f6c\u6362\u4e3a\u7b26\u53f7\u8868\u8fbe\u5f0f\uff1b\u5f15\u5165\u975e\u7ebf\u6027\u8bc4\u5206\u9879\u5728SR\u8fc7\u7a0b\u4e2d\u9009\u62e9\u4f4e\u590d\u6742\u5ea6\u65b9\u7a0b\uff1b\u4e0eMLP-AE\u8fdb\u884c\u5bf9\u6bd4\u5b9e\u9a8c", "result": "KAN-AEs\u5728\u4fdd\u6301\u7ade\u4e89\u6027BLER\u6027\u80fd\u7684\u540c\u65f6\uff0c\u901a\u8fc7SR\u5b9e\u73b0\u66f4\u9ad8\u80fd\u6548\uff1b\u4f7f\u7528\u63d0\u51fa\u7684\u975e\u7ebf\u6027\u5ea6\u91cf\u8bc4\u4f30\uff0cKAN-AE+SR\u6bd4MLP-AE\u8282\u80fd1.38\u500d", "conclusion": "KAN-AEs\u7ed3\u5408\u7b26\u53f7\u56de\u5f52\u662f\u80fd\u91cf\u9ad8\u6548\u6df1\u5ea6\u5b66\u4e60\u4fe1\u9053\u7f16\u7801\u7684\u6709\u524d\u666f\u9009\u62e9\uff0c\u901a\u8fc7\u7b26\u53f7\u8868\u8fbe\u5f0f\u5b9e\u73b0\u4f4e\u590d\u6742\u5ea6\u5b9e\u73b0\u548c\u80fd\u6548\u63d0\u5347"}}
{"id": "2601.01459", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.01459", "abs": "https://arxiv.org/abs/2601.01459", "authors": ["Yong Ren", "Jiangyan Yi", "Jianhua Tao", "Haiyang Sun", "Zhengqi Wen", "Hao Gu", "Le Xu", "Ye Bai"], "title": "OV-InstructTTS: Towards Open-Vocabulary Instruct Text-to-Speech", "comment": null, "summary": "Instruct Text-to-Speech (InstructTTS) leverages natural language descriptions as style prompts to guide speech synthesis. However, existing InstructTTS methods mainly rely on a direct combination of audio-related labels or their diverse rephrasings, making it difficult to handle flexible, high-level instructions. Such rigid control is insufficient for users such as content creators who wish to steer generation with descriptive instructions. To address these constraints, we introduce OV-InstructTTS, a new paradigm for open-vocabulary InstructTTS. We propose a comprehensive solution comprising a newly curated dataset, OV-Speech, and a novel reasoning-driven framework. The OV-Speech dataset pairs speech with open-vocabulary instructions, each augmented with a reasoning process that connects high-level instructions to acoustic features. The reasoning-driven framework infers emotional, acoustic, and paralinguistic information from open-vocabulary instructions before synthesizing speech. Evaluations show that this reasoning-driven approach significantly improves instruction-following fidelity and speech expressiveness. We believe this work can inspire the next user-friendly InstructTTS systems with stronger generalization and real-world applicability. The dataset and demos are publicly available on our project page.", "AI": {"tldr": "OV-InstructTTS\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5f00\u653e\u8bcd\u6c47\u6307\u4ee4TTS\u8303\u5f0f\uff0c\u901a\u8fc7\u63a8\u7406\u9a71\u52a8\u6846\u67b6\u5c06\u9ad8\u7ea7\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u8f6c\u6362\u4e3a\u8bed\u97f3\u5408\u6210\u63a7\u5236\u4fe1\u53f7\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6307\u4ee4\u8ddf\u968f\u80fd\u529b\u548c\u8bed\u97f3\u8868\u73b0\u529b\u3002", "motivation": "\u73b0\u6709InstructTTS\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u97f3\u9891\u76f8\u5173\u6807\u7b7e\u6216\u5176\u53d8\u4f53\uff0c\u96be\u4ee5\u5904\u7406\u7075\u6d3b\u7684\u9ad8\u7ea7\u6307\u4ee4\uff0c\u8fd9\u79cd\u521a\u6027\u63a7\u5236\u65e0\u6cd5\u6ee1\u8db3\u5185\u5bb9\u521b\u4f5c\u8005\u7b49\u7528\u6237\u5e0c\u671b\u901a\u8fc7\u63cf\u8ff0\u6027\u6307\u4ee4\u5f15\u5bfc\u751f\u6210\u7684\u9700\u6c42\u3002", "method": "\u63d0\u51fa\u4e86\u5305\u542b\u65b0\u6570\u636e\u96c6OV-Speech\u548c\u63a8\u7406\u9a71\u52a8\u6846\u67b6\u7684\u5b8c\u6574\u89e3\u51b3\u65b9\u6848\u3002OV-Speech\u6570\u636e\u96c6\u5c06\u8bed\u97f3\u4e0e\u5f00\u653e\u8bcd\u6c47\u6307\u4ee4\u914d\u5bf9\uff0c\u6bcf\u4e2a\u6307\u4ee4\u90fd\u5e26\u6709\u8fde\u63a5\u9ad8\u7ea7\u6307\u4ee4\u5230\u58f0\u5b66\u7279\u5f81\u7684\u63a8\u7406\u8fc7\u7a0b\u3002\u63a8\u7406\u9a71\u52a8\u6846\u67b6\u5728\u5408\u6210\u8bed\u97f3\u524d\u4ece\u5f00\u653e\u8bcd\u6c47\u6307\u4ee4\u63a8\u65ad\u60c5\u611f\u3001\u58f0\u5b66\u548c\u526f\u8bed\u8a00\u4fe1\u606f\u3002", "result": "\u8bc4\u4f30\u663e\u793a\u8fd9\u79cd\u63a8\u7406\u9a71\u52a8\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u6307\u4ee4\u8ddf\u968f\u4fdd\u771f\u5ea6\u548c\u8bed\u97f3\u8868\u73b0\u529b\uff0c\u80fd\u591f\u5b9e\u73b0\u66f4\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u548c\u5b9e\u9645\u5e94\u7528\u6027\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u53ef\u4ee5\u542f\u53d1\u4e0b\u4e00\u4ee3\u7528\u6237\u53cb\u597d\u7684InstructTTS\u7cfb\u7edf\uff0c\u5177\u6709\u66f4\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u548c\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002\u6570\u636e\u96c6\u548c\u6f14\u793a\u5df2\u5728\u9879\u76ee\u9875\u9762\u516c\u5f00\u3002"}}
{"id": "2601.01773", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.01773", "abs": "https://arxiv.org/abs/2601.01773", "authors": ["Chengwang Ji", "Haiquan Lu", "Qiaoyan Peng", "Jintao Wang", "Shaodan Ma"], "title": "Joint Sparsity and Beamforming Design for RDARS-Aided Systems", "comment": null, "summary": "Reconfigurable distributed antennas and reflecting surface (RDARS) has emerged as a promising architecture for communication and sensing performance enhancement. In particular, the new selection gain can be achieved by leveraging the dynamic working mode selection between connection and reflection modes, whereas low-complexity element configuration remains an open issue. In this paper, we consider a RDARS-assisted communication system, where the connected elements are formed as a uniform sparse array for simplified mode configuration while achieving enlarged physical array aperture. The sum rate maximization problem is then formulated by jointly optimizing the active and passive beamforming matrices and sparsity of connected element array. For the special cases of a single user equipment (UE) and two UEs, the optimal sparsity designs are derived in closed-form. Then, for an arbitrary number of UEs, a weighted minimum mean-square error-based alternating optimization (AO) algorithm is proposed to tackle the non-convex optimization problem. Numerical results demonstrate the importance of optimizing the sparsity and the effectiveness of low-complexity sparsity optimization.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86RDARS\u8f85\u52a9\u901a\u4fe1\u7cfb\u7edf\u4e2d\u8fde\u63a5\u5143\u4ef6\u7684\u7a00\u758f\u9635\u5217\u8bbe\u8ba1\uff0c\u901a\u8fc7\u4f18\u5316\u7a00\u758f\u5ea6\u3001\u4e3b\u52a8\u548c\u88ab\u52a8\u6ce2\u675f\u6210\u5f62\u6765\u6700\u5927\u5316\u7cfb\u7edf\u548c\u901f\u7387\u3002", "motivation": "RDARS\u67b6\u6784\u5728\u901a\u4fe1\u548c\u611f\u77e5\u6027\u80fd\u589e\u5f3a\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u52a8\u6001\u5de5\u4f5c\u6a21\u5f0f\u9009\u62e9\u548c\u4f4e\u590d\u6742\u5ea6\u5143\u4ef6\u914d\u7f6e\u65b9\u9762\u5b58\u5728\u6311\u6218\u3002\u7279\u522b\u662f\u8fde\u63a5\u5143\u4ef6\u7684\u914d\u7f6e\u590d\u6742\u5ea6\u95ee\u9898\u5c1a\u672a\u5f97\u5230\u5145\u5206\u89e3\u51b3\u3002", "method": "\u63d0\u51fa\u5c06\u8fde\u63a5\u5143\u4ef6\u5f62\u6210\u5747\u5300\u7a00\u758f\u9635\u5217\u4ee5\u7b80\u5316\u6a21\u5f0f\u914d\u7f6e\u5e76\u6269\u5927\u7269\u7406\u9635\u5217\u5b54\u5f84\u3002\u901a\u8fc7\u8054\u5408\u4f18\u5316\u4e3b\u52a8/\u88ab\u52a8\u6ce2\u675f\u6210\u5f62\u77e9\u9635\u548c\u8fde\u63a5\u5143\u4ef6\u9635\u5217\u7684\u7a00\u758f\u5ea6\u6765\u6700\u5927\u5316\u7cfb\u7edf\u548c\u901f\u7387\u3002\u9488\u5bf9\u5355\u7528\u6237\u548c\u53cc\u7528\u6237\u573a\u666f\u63a8\u5bfc\u4e86\u95ed\u5f0f\u6700\u4f18\u7a00\u758f\u8bbe\u8ba1\uff0c\u9488\u5bf9\u591a\u7528\u6237\u573a\u666f\u63d0\u51fa\u4e86\u57fa\u4e8e\u52a0\u6743\u6700\u5c0f\u5747\u65b9\u8bef\u5dee\u7684\u4ea4\u66ff\u4f18\u5316\u7b97\u6cd5\u3002", "result": "\u6570\u503c\u7ed3\u679c\u8868\u660e\u4f18\u5316\u7a00\u758f\u5ea6\u7684\u91cd\u8981\u6027\u4ee5\u53ca\u4f4e\u590d\u6742\u5ea6\u7a00\u758f\u5ea6\u4f18\u5316\u7684\u6709\u6548\u6027\u3002\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u663e\u8457\u63d0\u5347\u7cfb\u7edf\u6027\u80fd\u3002", "conclusion": "RDARS\u8f85\u52a9\u901a\u4fe1\u7cfb\u7edf\u4e2d\u8fde\u63a5\u5143\u4ef6\u7684\u7a00\u758f\u9635\u5217\u8bbe\u8ba1\u80fd\u591f\u6709\u6548\u5e73\u8861\u6027\u80fd\u4e0e\u590d\u6742\u5ea6\uff0c\u63d0\u51fa\u7684\u4f18\u5316\u65b9\u6cd5\u4e3a\u5b9e\u9645\u7cfb\u7edf\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.01554", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.01554", "abs": "https://arxiv.org/abs/2601.01554", "authors": ["Donghua Yu", "Zhengyuan Lin", "Chen Yang", "Yiyang Zhang", "Zhaoye Fei", "Hanfu Chen", "Jingqi Chen", "Ke Chen", "Qinyuan Cheng", "Liwei Fan", "Yi Jiang", "Jie Zhu", "Muchen Li", "Shimin Li", "Wenxuan Wang", "Yang Wang", "Zhe Xu", "Yitian Gong", "Yuqian Zhang"], "title": "MOSS Transcribe Diarize: Accurate Transcription with Speaker Diarization", "comment": null, "summary": "Speaker-Attributed, Time-Stamped Transcription (SATS) aims to transcribe what is said and to precisely determine the timing of each speaker, which is particularly valuable for meeting transcription. Existing SATS systems rarely adopt an end-to-end formulation and are further constrained by limited context windows, weak long-range speaker memory, and the inability to output timestamps. To address these limitations, we present MOSS Transcribe Diarize, a unified multimodal large language model that jointly performs Speaker-Attributed, Time-Stamped Transcription in an end-to-end paradigm. Trained on extensive real wild data and equipped with a 128k context window for up to 90-minute inputs, MOSS Transcribe Diarize scales well and generalizes robustly. Across comprehensive evaluations, it outperforms state-of-the-art commercial systems on multiple public and in-house benchmarks.", "AI": {"tldr": "MOSS Transcribe Diarize\uff1a\u4e00\u4e2a\u7edf\u4e00\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u4ee5\u7aef\u5230\u7aef\u65b9\u5f0f\u8054\u5408\u6267\u884c\u8bf4\u8bdd\u4eba\u5f52\u5c5e\u3001\u65f6\u95f4\u6233\u8f6c\u5f55\u4efb\u52a1\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u73b0\u6709\u5546\u4e1a\u7cfb\u7edf\u3002", "motivation": "\u73b0\u6709SATS\u7cfb\u7edf\u5f88\u5c11\u91c7\u7528\u7aef\u5230\u7aef\u6846\u67b6\uff0c\u4e14\u53d7\u9650\u4e8e\u6709\u9650\u7684\u4e0a\u4e0b\u6587\u7a97\u53e3\u3001\u5f31\u7684\u957f\u8ddd\u79bb\u8bf4\u8bdd\u4eba\u8bb0\u5fc6\u80fd\u529b\u4ee5\u53ca\u65e0\u6cd5\u8f93\u51fa\u65f6\u95f4\u6233\u3002\u9700\u8981\u89e3\u51b3\u8fd9\u4e9b\u9650\u5236\u6765\u6539\u8fdb\u4f1a\u8bae\u8f6c\u5f55\u8d28\u91cf\u3002", "method": "\u63d0\u51faMOSS Transcribe Diarize\uff0c\u4e00\u4e2a\u7edf\u4e00\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u91c7\u7528\u7aef\u5230\u7aef\u8303\u5f0f\u8054\u5408\u6267\u884c\u8bf4\u8bdd\u4eba\u5f52\u5c5e\u548c\u65f6\u95f4\u6233\u8f6c\u5f55\u3002\u4f7f\u7528\u5927\u91cf\u771f\u5b9e\u6570\u636e\u8bad\u7ec3\uff0c\u5177\u5907128k\u4e0a\u4e0b\u6587\u7a97\u53e3\uff0c\u652f\u6301\u957f\u8fbe90\u5206\u949f\u7684\u8f93\u5165\u3002", "result": "\u5728\u5168\u9762\u7684\u8bc4\u4f30\u4e2d\uff0cMOSS Transcribe Diarize\u5728\u591a\u4e2a\u516c\u5f00\u548c\u5185\u90e8\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u4e86\u6700\u5148\u8fdb\u7684\u5546\u4e1a\u7cfb\u7edf\uff0c\u8868\u73b0\u51fa\u826f\u597d\u7684\u6269\u5c55\u6027\u548c\u9c81\u68d2\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "MOSS Transcribe Diarize\u901a\u8fc7\u7aef\u5230\u7aef\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709SATS\u7cfb\u7edf\u7684\u5c40\u9650\u6027\uff0c\u5728\u8bf4\u8bdd\u4eba\u5f52\u5c5e\u548c\u65f6\u95f4\u6233\u8f6c\u5f55\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb\u3002"}}
{"id": "2601.01791", "categories": ["eess.SP", "cs.IT", "cs.NI"], "pdf": "https://arxiv.org/pdf/2601.01791", "abs": "https://arxiv.org/abs/2601.01791", "authors": ["Shunpu Tang", "Yuanyuan Jia", "Zijiu Yang", "Qianqian Yang", "Ruichen Zhang", "Jun Du", "Jihong Park", "Zhiguo Shi", "Khaled B. Letaief"], "title": "Rethinking Secure Semantic Communications in the Age of Generative and Agentic AI: Threats and Opportunities", "comment": null, "summary": "Semantic communication (SemCom) improves communication efficiency by transmitting task-relevant information instead of raw bits and is expected to be a key technology for 6G networks. Recent advances in generative AI (GenAI) further enhance SemCom by enabling robust semantic encoding and decoding under limited channel conditions. However, these efficiency gains also introduce new security and privacy vulnerabilities. Due to the broadcast nature of wireless channels, eavesdroppers can also use powerful GenAI-based semantic decoders to recover private information from intercepted signals. Moreover, rapid advances in agentic AI enable eavesdroppers to perform long-term and adaptive inference through the integration of memory, external knowledge, and reasoning capabilities. This allows eavesdroppers to further infer user private behavior and intent beyond the transmitted content. Motivated by these emerging challenges, this paper comprehensively rethinks the security and privacy of SemCom systems in the age of generative and agentic AI. We first present a systematic taxonomy of eavesdropping threat models in SemCom systems. Then, we provide insights into how GenAI and agentic AI can enhance eavesdropping threats. Meanwhile, we also highlight potential opportunities for leveraging GenAI and agentic AI to design privacy-preserving SemCom systems.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u5206\u6790\u4e86\u751f\u6210\u5f0fAI\u548c\u667a\u80fd\u4f53AI\u65f6\u4ee3\u8bed\u4e49\u901a\u4fe1\u7cfb\u7edf\u7684\u5b89\u5168\u4e0e\u9690\u79c1\u95ee\u9898\uff0c\u65e2\u63a2\u8ba8\u4e86AI\u589e\u5f3a\u7684\u7a83\u542c\u5a01\u80c1\uff0c\u4e5f\u63d0\u51fa\u4e86AI\u8d4b\u80fd\u7684\u9690\u79c1\u4fdd\u62a4\u673a\u4f1a\u3002", "motivation": "\u8bed\u4e49\u901a\u4fe1\u901a\u8fc7\u4f20\u8f93\u4efb\u52a1\u76f8\u5173\u4fe1\u606f\u800c\u975e\u539f\u59cb\u6bd4\u7279\u6765\u63d0\u9ad8\u901a\u4fe1\u6548\u7387\uff0c\u662f6G\u7f51\u7edc\u7684\u5173\u952e\u6280\u672f\u3002\u751f\u6210\u5f0fAI\u8fdb\u4e00\u6b65\u589e\u5f3a\u4e86\u8bed\u4e49\u901a\u4fe1\u7684\u9c81\u68d2\u6027\uff0c\u4f46\u8fd9\u4e9b\u6548\u7387\u63d0\u5347\u4e5f\u5e26\u6765\u4e86\u65b0\u7684\u5b89\u5168\u548c\u9690\u79c1\u6f0f\u6d1e\u3002\u7531\u4e8e\u65e0\u7ebf\u4fe1\u9053\u7684\u5e7f\u64ad\u7279\u6027\uff0c\u7a83\u542c\u8005\u53ef\u4ee5\u4f7f\u7528\u5f3a\u5927\u7684\u751f\u6210\u5f0fAI\u8bed\u4e49\u89e3\u7801\u5668\u4ece\u622a\u83b7\u4fe1\u53f7\u4e2d\u6062\u590d\u79c1\u5bc6\u4fe1\u606f\u3002\u6b64\u5916\uff0c\u667a\u80fd\u4f53AI\u7684\u5feb\u901f\u53d1\u5c55\u4f7f\u7a83\u542c\u8005\u80fd\u591f\u901a\u8fc7\u6574\u5408\u8bb0\u5fc6\u3001\u5916\u90e8\u77e5\u8bc6\u548c\u63a8\u7406\u80fd\u529b\u8fdb\u884c\u957f\u671f\u81ea\u9002\u5e94\u63a8\u65ad\uff0c\u8fdb\u4e00\u6b65\u63a8\u65ad\u7528\u6237\u79c1\u5bc6\u884c\u4e3a\u548c\u610f\u56fe\u3002", "method": "1. \u63d0\u51fa\u8bed\u4e49\u901a\u4fe1\u7cfb\u7edf\u4e2d\u7a83\u542c\u5a01\u80c1\u6a21\u578b\u7684\u7cfb\u7edf\u5206\u7c7b\u6cd5\uff1b2. \u5206\u6790\u751f\u6210\u5f0fAI\u548c\u667a\u80fd\u4f53AI\u5982\u4f55\u589e\u5f3a\u7a83\u542c\u5a01\u80c1\uff1b3. \u63a2\u8ba8\u5229\u7528\u751f\u6210\u5f0fAI\u548c\u667a\u80fd\u4f53AI\u8bbe\u8ba1\u9690\u79c1\u4fdd\u62a4\u8bed\u4e49\u901a\u4fe1\u7cfb\u7edf\u7684\u6f5c\u5728\u673a\u4f1a\u3002", "result": "\u672c\u6587\u5168\u9762\u91cd\u65b0\u601d\u8003\u4e86\u751f\u6210\u5f0f\u548c\u667a\u80fd\u4f53AI\u65f6\u4ee3\u8bed\u4e49\u901a\u4fe1\u7cfb\u7edf\u7684\u5b89\u5168\u4e0e\u9690\u79c1\u95ee\u9898\uff0c\u5efa\u7acb\u4e86\u7cfb\u7edf\u7684\u5a01\u80c1\u6a21\u578b\u5206\u7c7b\u6846\u67b6\uff0c\u63ed\u793a\u4e86AI\u6280\u672f\u5bf9\u7a83\u542c\u5a01\u80c1\u7684\u53cc\u91cd\u5f71\u54cd\u2014\u2014\u65e2\u589e\u5f3a\u4e86\u7a83\u542c\u80fd\u529b\uff0c\u4e5f\u4e3a\u9690\u79c1\u4fdd\u62a4\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u3002", "conclusion": "\u5728\u751f\u6210\u5f0f\u548c\u667a\u80fd\u4f53AI\u5feb\u901f\u53d1\u5c55\u7684\u80cc\u666f\u4e0b\uff0c\u8bed\u4e49\u901a\u4fe1\u7cfb\u7edf\u9762\u4e34\u65b0\u7684\u5b89\u5168\u4e0e\u9690\u79c1\u6311\u6218\u3002\u9700\u8981\u7cfb\u7edf\u6027\u5730\u91cd\u65b0\u601d\u8003\u5b89\u5168\u9632\u62a4\u7b56\u7565\uff0c\u65e2\u8981\u8ba4\u8bc6\u5230AI\u589e\u5f3a\u7684\u7a83\u542c\u5a01\u80c1\uff0c\u4e5f\u8981\u79ef\u6781\u5229\u7528AI\u6280\u672f\u8bbe\u8ba1\u66f4\u5f3a\u5927\u7684\u9690\u79c1\u4fdd\u62a4\u673a\u5236\uff0c\u5b9e\u73b0\u5b89\u5168\u4e0e\u6548\u7387\u7684\u5e73\u8861\u3002"}}
{"id": "2601.01568", "categories": ["cs.SD", "cs.AI", "cs.CV", "cs.MM", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.01568", "abs": "https://arxiv.org/abs/2601.01568", "authors": ["Chunyu Qiang", "Jun Wang", "Xiaopeng Wang", "Kang Yin", "Yuxin Guo", "Xijuan Zeng", "Nan Li", "Zihan Li", "Yuzhe Liang", "Ziyu Zhang", "Teng Ma", "Yushen Chen", "Zhongliang Liu", "Feng Deng", "Chen Zhang", "Pengfei Wan"], "title": "MM-Sonate: Multimodal Controllable Audio-Video Generation with Zero-Shot Voice Cloning", "comment": null, "summary": "Joint audio-video generation aims to synthesize synchronized multisensory content, yet current unified models struggle with fine-grained acoustic control, particularly for identity-preserving speech. Existing approaches either suffer from temporal misalignment due to cascaded generation or lack the capability to perform zero-shot voice cloning within a joint synthesis framework. In this work, we present MM-Sonate, a multimodal flow-matching framework that unifies controllable audio-video joint generation with zero-shot voice cloning capabilities. Unlike prior works that rely on coarse semantic descriptions, MM-Sonate utilizes a unified instruction-phoneme input to enforce strict linguistic and temporal alignment. To enable zero-shot voice cloning, we introduce a timbre injection mechanism that effectively decouples speaker identity from linguistic content. Furthermore, addressing the limitations of standard classifier-free guidance in multimodal settings, we propose a noise-based negative conditioning strategy that utilizes natural noise priors to significantly enhance acoustic fidelity. Empirical evaluations demonstrate that MM-Sonate establishes new state-of-the-art performance in joint generation benchmarks, significantly outperforming baselines in lip synchronization and speech intelligibility, while achieving voice cloning fidelity comparable to specialized Text-to-Speech systems.", "AI": {"tldr": "MM-Sonate\uff1a\u7edf\u4e00\u7684\u591a\u6a21\u6001\u6d41\u5339\u914d\u6846\u67b6\uff0c\u5b9e\u73b0\u53ef\u63a7\u7684\u97f3\u9891-\u89c6\u9891\u8054\u5408\u751f\u6210\u4e0e\u96f6\u6837\u672c\u8bed\u97f3\u514b\u9686\uff0c\u901a\u8fc7\u6307\u4ee4-\u97f3\u7d20\u8f93\u5165\u786e\u4fdd\u65f6\u5e8f\u5bf9\u9f50\uff0c\u5f15\u5165\u97f3\u8272\u6ce8\u5165\u673a\u5236\u5206\u79bb\u8bf4\u8bdd\u4eba\u8eab\u4efd\u4e0e\u5185\u5bb9\uff0c\u4f7f\u7528\u566a\u58f0\u8d1f\u6761\u4ef6\u7b56\u7565\u63d0\u5347\u97f3\u9891\u4fdd\u771f\u5ea6\u3002", "motivation": "\u5f53\u524d\u8054\u5408\u97f3\u9891-\u89c6\u9891\u751f\u6210\u6a21\u578b\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1\uff09\u7531\u4e8e\u7ea7\u8054\u751f\u6210\u5bfc\u81f4\u65f6\u5e8f\u4e0d\u5bf9\u9f50\uff1b2\uff09\u7f3a\u4e4f\u5728\u8054\u5408\u5408\u6210\u6846\u67b6\u5185\u5b9e\u73b0\u96f6\u6837\u672c\u8bed\u97f3\u514b\u9686\u7684\u80fd\u529b\u3002\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u4f9d\u8d56\u7c97\u7c92\u5ea6\u8bed\u4e49\u63cf\u8ff0\uff0c\u8981\u4e48\u65e0\u6cd5\u5728\u7edf\u4e00\u6846\u67b6\u4e2d\u5b9e\u73b0\u7cbe\u786e\u7684\u8bed\u97f3\u63a7\u5236\u3002", "method": "\u63d0\u51faMM-Sonate\u591a\u6a21\u6001\u6d41\u5339\u914d\u6846\u67b6\uff0c\u91c7\u7528\u7edf\u4e00\u7684\u6307\u4ee4-\u97f3\u7d20\u8f93\u5165\u786e\u4fdd\u8bed\u8a00\u548c\u65f6\u5e8f\u5bf9\u9f50\uff1b\u5f15\u5165\u97f3\u8272\u6ce8\u5165\u673a\u5236\u89e3\u8026\u8bf4\u8bdd\u4eba\u8eab\u4efd\u4e0e\u8bed\u8a00\u5185\u5bb9\uff1b\u63d0\u51fa\u57fa\u4e8e\u566a\u58f0\u7684\u8d1f\u6761\u4ef6\u7b56\u7565\uff0c\u5229\u7528\u81ea\u7136\u566a\u58f0\u5148\u9a8c\u589e\u5f3a\u97f3\u9891\u4fdd\u771f\u5ea6\uff0c\u514b\u670d\u6807\u51c6\u65e0\u5206\u7c7b\u5668\u5f15\u5bfc\u5728\u591a\u6a21\u6001\u8bbe\u7f6e\u4e2d\u7684\u5c40\u9650\u6027\u3002", "result": "\u5728\u8054\u5408\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u65b0\u7684SOTA\u6027\u80fd\uff0c\u5728\u5507\u90e8\u540c\u6b65\u548c\u8bed\u97f3\u6e05\u6670\u5ea6\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u540c\u65f6\u5b9e\u73b0\u4e86\u4e0e\u4e13\u4e1a\u6587\u672c\u5230\u8bed\u97f3\u7cfb\u7edf\u76f8\u5f53\u7684\u8bed\u97f3\u514b\u9686\u4fdd\u771f\u5ea6\u3002", "conclusion": "MM-Sonate\u6210\u529f\u7edf\u4e00\u4e86\u53ef\u63a7\u7684\u97f3\u9891-\u89c6\u9891\u8054\u5408\u751f\u6210\u4e0e\u96f6\u6837\u672c\u8bed\u97f3\u514b\u9686\u529f\u80fd\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u8f93\u5165\u8868\u793a\u3001\u97f3\u8272\u89e3\u8026\u673a\u5236\u548c\u566a\u58f0\u6761\u4ef6\u7b56\u7565\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u65f6\u5e8f\u5bf9\u9f50\u548c\u8bed\u97f3\u63a7\u5236\u65b9\u9762\u7684\u5173\u952e\u9650\u5236\u3002"}}
{"id": "2601.01834", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.01834", "abs": "https://arxiv.org/abs/2601.01834", "authors": ["Tianyu Fang", "Xiaohua Zhou", "Yijie Mao"], "title": "On the Performance of Lossless Reciprocal MiLAC Architectures in Multi-User Networks", "comment": null, "summary": "Microwave linear analog computer (MiLAC)-aided beamforming, which processes the transmitted symbols fully in the analog domain, has recently emerged as a promising alternative to fully digital and hybrid beamforming architectures for multiple-input multiple-output (MIMO) systems. While prior studies have shown that lossless and reciprocal MiLAC can achieve the same capacity as digital beamforming in a single-user MIMO network, its performance in multi-user scenarios remains unknown. To answer this question, in this work, we establish a downlink multi-user multiple-input single-output (MU-MISO) network with a MiLAC-aided transmitter, and investigate its sum-rate performance. Based on the microwave network theory, we first prove that lossless and reciprocal MiLAC cannot achieve the same performance as digital beamforming in a general MU-MISO network. Then, we formulate a sum-rate maximization problem and develop an efficient optimization framework to jointly optimize the power allocation and the scattering matrix for MiLAC. Numerical results validate our theoretical analysis and demonstrate that MiLAC is a promising architecture for future extremely large-scale MIMO systems.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5fae\u6ce2\u7ebf\u6027\u6a21\u62df\u8ba1\u7b97\u673a\u8f85\u52a9\u6ce2\u675f\u6210\u5f62\u5728\u591a\u7528\u6237MISO\u7f51\u7edc\u4e2d\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u65e0\u635f\u4e92\u6613MiLAC\u65e0\u6cd5\u8fbe\u5230\u6570\u5b57\u6ce2\u675f\u6210\u5f62\u7684\u6027\u80fd\uff0c\u5e76\u63d0\u51fa\u4e86\u8054\u5408\u4f18\u5316\u529f\u7387\u5206\u914d\u548c\u6563\u5c04\u77e9\u9635\u7684\u6846\u67b6\u3002", "motivation": "\u5fae\u6ce2\u7ebf\u6027\u6a21\u62df\u8ba1\u7b97\u673a\u8f85\u52a9\u6ce2\u675f\u6210\u5f62\u4f5c\u4e3a\u5168\u6570\u5b57\u548c\u6df7\u5408\u6ce2\u675f\u6210\u5f62\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u5728\u5355\u7528\u6237MIMO\u7f51\u7edc\u4e2d\u80fd\u8fbe\u5230\u4e0e\u6570\u5b57\u6ce2\u675f\u6210\u5f62\u76f8\u540c\u7684\u5bb9\u91cf\uff0c\u4f46\u5176\u5728\u591a\u7528\u6237\u573a\u666f\u4e0b\u7684\u6027\u80fd\u5c1a\u4e0d\u660e\u786e\u3002", "method": "\u57fa\u4e8e\u5fae\u6ce2\u7f51\u7edc\u7406\u8bba\uff0c\u9996\u5148\u8bc1\u660e\u65e0\u635f\u4e92\u6613MiLAC\u5728\u4e00\u822cMU-MISO\u7f51\u7edc\u4e2d\u65e0\u6cd5\u8fbe\u5230\u6570\u5b57\u6ce2\u675f\u6210\u5f62\u7684\u6027\u80fd\uff1b\u7136\u540e\u6784\u5efa\u548c\u901f\u7387\u6700\u5927\u5316\u95ee\u9898\uff0c\u5f00\u53d1\u8054\u5408\u4f18\u5316\u529f\u7387\u5206\u914d\u548cMiLAC\u6563\u5c04\u77e9\u9635\u7684\u9ad8\u6548\u4f18\u5316\u6846\u67b6\u3002", "result": "\u6570\u503c\u7ed3\u679c\u9a8c\u8bc1\u4e86\u7406\u8bba\u5206\u6790\uff0c\u8868\u660eMiLAC\u662f\u672a\u6765\u6781\u5927\u89c4\u6a21MIMO\u7cfb\u7edf\u7684\u6709\u524d\u666f\u67b6\u6784\uff0c\u4f46\u9700\u8981\u9002\u5f53\u7684\u4f18\u5316\u624d\u80fd\u5b9e\u73b0\u826f\u597d\u6027\u80fd\u3002", "conclusion": "MiLAC\u5728\u591a\u7528\u6237MISO\u7f51\u7edc\u4e2d\u65e0\u6cd5\u8fbe\u5230\u6570\u5b57\u6ce2\u675f\u6210\u5f62\u7684\u6027\u80fd\uff0c\u4f46\u901a\u8fc7\u63d0\u51fa\u7684\u4f18\u5316\u6846\u67b6\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u4e3a\u672a\u6765\u5927\u89c4\u6a21MIMO\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u6a21\u62df\u5904\u7406\u65b9\u6848\u3002"}}
{"id": "2601.02099", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2601.02099", "abs": "https://arxiv.org/abs/2601.02099", "authors": ["Ji Yeoung Sim", "Rebecca Moranis", "Johanna Devaney"], "title": "BeatlesFC: Harmonic function annotations of Isophonics' The Beatles dataset", "comment": "International Society for Music Information Retrieval, Late-Breaking Demo 2024", "summary": "This paper presents BeatlesFC, a set of harmonic function annotations for Isophonics' The Beatles dataset. Harmonic function annotations characterize chord labels as stable (tonic) or unstable (predominant, dominant). They operate at the level of musical phrases, serving as a link between chord labels and higher-level formal structures.", "AI": {"tldr": "BeatlesFC\u4e3a\u62ab\u5934\u58eb\u6570\u636e\u96c6\u63d0\u4f9b\u548c\u58f0\u529f\u80fd\u6807\u6ce8\uff0c\u5c06\u548c\u5f26\u6807\u8bb0\u4e3a\u7a33\u5b9a\uff08\u4e3b\u97f3\uff09\u6216\u4e0d\u7a33\u5b9a\uff08\u4e0b\u5c5e\u3001\u5c5e\u97f3\uff09\uff0c\u5728\u4e50\u53e5\u5c42\u9762\u8fde\u63a5\u548c\u5f26\u6807\u7b7e\u4e0e\u66f4\u9ad8\u5c42\u6b21\u97f3\u4e50\u7ed3\u6784\u3002", "motivation": "\u73b0\u6709\u62ab\u5934\u58eb\u6570\u636e\u96c6\u7f3a\u4e4f\u548c\u58f0\u529f\u80fd\u6807\u6ce8\uff0c\u800c\u548c\u58f0\u529f\u80fd\u5206\u6790\u5bf9\u4e8e\u7406\u89e3\u97f3\u4e50\u7ed3\u6784\u548c\u8fde\u63a5\u4e0d\u540c\u5c42\u6b21\u97f3\u4e50\u4fe1\u606f\u81f3\u5173\u91cd\u8981\u3002", "method": "\u4e3aIsophonics\u62ab\u5934\u58eb\u6570\u636e\u96c6\u521b\u5efa\u548c\u58f0\u529f\u80fd\u6807\u6ce8\u96c6\uff0c\u5c06\u548c\u5f26\u6807\u7b7e\u5206\u7c7b\u4e3a\u7a33\u5b9a\uff08\u4e3b\u97f3\uff09\u6216\u4e0d\u7a33\u5b9a\uff08\u4e0b\u5c5e\u3001\u5c5e\u97f3\uff09\u529f\u80fd\uff0c\u5e76\u5728\u4e50\u53e5\u5c42\u9762\u8fdb\u884c\u6807\u6ce8\u3002", "result": "\u521b\u5efa\u4e86BeatlesFC\u6807\u6ce8\u96c6\uff0c\u63d0\u4f9b\u4e86\u62ab\u5934\u58eb\u6b4c\u66f2\u7684\u548c\u58f0\u529f\u80fd\u5206\u6790\uff0c\u586b\u8865\u4e86\u73b0\u6709\u6570\u636e\u96c6\u7684\u7a7a\u767d\uff0c\u4e3a\u97f3\u4e50\u5206\u6790\u548c\u8ba1\u7b97\u97f3\u4e50\u5b66\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u8d44\u6e90\u3002", "conclusion": "BeatlesFC\u6807\u6ce8\u96c6\u4e3a\u62ab\u5934\u58eb\u97f3\u4e50\u7684\u548c\u58f0\u5206\u6790\u63d0\u4f9b\u4e86\u91cd\u8981\u5de5\u5177\uff0c\u6709\u52a9\u4e8e\u8fde\u63a5\u548c\u5f26\u7ea7\u5206\u6790\u4e0e\u66f4\u9ad8\u5c42\u6b21\u97f3\u4e50\u5f62\u5f0f\u7ed3\u6784\u7684\u7814\u7a76\u3002"}}
{"id": "2601.01956", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.01956", "abs": "https://arxiv.org/abs/2601.01956", "authors": ["Tang Shuntian", "Wu Xiaomei", "Wang Xinyi", "Zhao Le", "Yang Guang", "Liu Zilong", "Liu Fan", "Fei Zesong"], "title": "Doppler-Resilient LEO Satellite OFDM Transmission with Affine Frequency Domain Pilot", "comment": "6 pages, 4 figures, submitted to 2026 ICC Workshops", "summary": "Orthogonal frequency division multiplexing (OFDM) based low Earth orbit (LEO) satellite communication system suffers from severe Doppler shifts, while {the Doppler-resilient affine frequency-division multiplexing (AFDM) transmission suffers from significantly high processing complexity in data detection}. In this paper, we explore the channel estimation gain of affine frequency (AF) domain pilot to enhance the OFDM transmission under high mobility. Specifically, we propose a novel AF domain pilot embedding scheme for satellite-ground downlink OFDM systems for capturing the channel characteristics. By exploiting the autoregressive (AR) property of adjacent channels, a long short-term memory (LSTM) based predictor is designed to replace conventional interpolation operation in OFDM channel estimation. Simulation results show that the proposed transmission scheme significantly outperforms conventional OFDM scheme in terms of bit error rate (BER) under high Doppler scenarios, thus paving a new way for the design of next generation non-terrestrial network (NTN) communication systems.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u4eff\u5c04\u9891\u7387\u57df\u5bfc\u9891\u7684OFDM\u536b\u661f\u901a\u4fe1\u65b9\u6848\uff0c\u5229\u7528LSTM\u9884\u6d4b\u5668\u66ff\u4ee3\u4f20\u7edf\u63d2\u503c\uff0c\u663e\u8457\u63d0\u5347\u9ad8\u591a\u666e\u52d2\u573a\u666f\u4e0b\u7684\u6027\u80fd", "motivation": "LEO\u536b\u661fOFDM\u7cfb\u7edf\u9762\u4e34\u4e25\u91cd\u591a\u666e\u52d2\u9891\u79fb\u95ee\u9898\uff0c\u800c\u73b0\u6709AFDM\u65b9\u6848\u867d\u7136\u6297\u591a\u666e\u52d2\u4f46\u5904\u7406\u590d\u6742\u5ea6\u9ad8\uff0c\u9700\u8981\u5bfb\u627e\u66f4\u4f18\u7684\u89e3\u51b3\u65b9\u6848", "method": "\u63d0\u51faAF\u57df\u5bfc\u9891\u5d4c\u5165\u65b9\u6848\uff0c\u5229\u7528\u76f8\u90bb\u4fe1\u9053\u7684\u81ea\u56de\u5f52\u7279\u6027\uff0c\u8bbe\u8ba1LSTM\u9884\u6d4b\u5668\u66ff\u4ee3\u4f20\u7edf\u63d2\u503c\u8fdb\u884c\u4fe1\u9053\u4f30\u8ba1", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6848\u5728\u9ad8\u591a\u666e\u52d2\u573a\u666f\u4e0b\u7684\u8bef\u7801\u7387\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u4f20\u7edfOFDM\u65b9\u6848", "conclusion": "\u4e3a\u4e0b\u4e00\u4ee3\u975e\u5730\u9762\u7f51\u7edc\u901a\u4fe1\u7cfb\u7edf\u8bbe\u8ba1\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84"}}
{"id": "2601.02101", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2601.02101", "abs": "https://arxiv.org/abs/2601.02101", "authors": ["Chunyu Yuan", "Johanna Devaney"], "title": "A Mamba-Based Model for Automatic Chord Recognition", "comment": "International Society of Music Information Retrieval, Late-Breaking Demo 2024", "summary": "In this work, we propose a new efficient solution, which is a Mamba-based model named BMACE (Bidirectional Mamba-based network, for Automatic Chord Estimation), which utilizes selective structured state-space models in a bidirectional Mamba layer to effectively model temporal dependencies. Our model achieves high prediction performance comparable to state-of-the-art models, with the advantage of requiring fewer parameters and lower computational resources", "AI": {"tldr": "\u63d0\u51faBMACE\u6a21\u578b\uff0c\u57fa\u4e8e\u53cc\u5411Mamba\u5c42\u548c\u9009\u62e9\u6027\u7ed3\u6784\u5316\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff0c\u7528\u4e8e\u81ea\u52a8\u548c\u5f26\u4f30\u8ba1\uff0c\u5728\u4fdd\u6301\u9ad8\u6027\u80fd\u7684\u540c\u65f6\u51cf\u5c11\u53c2\u6570\u548c\u8ba1\u7b97\u8d44\u6e90", "motivation": "\u73b0\u6709\u81ea\u52a8\u548c\u5f26\u4f30\u8ba1\u6a21\u578b\u901a\u5e38\u9700\u8981\u5927\u91cf\u53c2\u6570\u548c\u8ba1\u7b97\u8d44\u6e90\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848", "method": "\u4f7f\u7528\u57fa\u4e8e\u9009\u62e9\u6027\u7ed3\u6784\u5316\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u7684\u53cc\u5411Mamba\u5c42\u6765\u6709\u6548\u5efa\u6a21\u65f6\u95f4\u4f9d\u8d56\u5173\u7cfb", "result": "\u6a21\u578b\u8fbe\u5230\u4e0e\u6700\u5148\u8fdb\u6a21\u578b\u76f8\u5f53\u7684\u9884\u6d4b\u6027\u80fd\uff0c\u540c\u65f6\u9700\u8981\u66f4\u5c11\u7684\u53c2\u6570\u548c\u66f4\u4f4e\u7684\u8ba1\u7b97\u8d44\u6e90", "conclusion": "BMACE\u6a21\u578b\u4e3a\u81ea\u52a8\u548c\u5f26\u4f30\u8ba1\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u6027\u80fd\u4f18\u8d8a\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2601.02219", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.02219", "abs": "https://arxiv.org/abs/2601.02219", "authors": ["Zihao Zhou", "Zhaolin Wang", "Yuanwei Liu"], "title": "Beam-Brainstorm: A Generative Site-Specific Beamforming Approach", "comment": null, "summary": "Accurately understanding the propagation environment is a fundamental challenge in site-specific beamforming (SSBF). This paper proposes a novel generative SSBF (GenSSBF) solution, which represents a paradigm shift from conventional unstructured prediction to joint-structure modeling. First, considering the fundamental differences between beam generation and conventional image synthesis, a unified GenSSBF framework is proposed, which includes a site profile, a wireless prompting module, and a generator. Second, a beam-brainstorm (BBS) solution is proposed as an instantiation of this GenSSBF framework. Specifically, the site profile is configured by transforming channel data from spatial domain to a reversible latent space via discrete Fourier transform (DFT). To facilitate practical deployment, the wireless prompt is constructed from the reference signal received power (RSRP) measured using a small number of DFT-beams. Finally, the generator is developed using a customized conditional diffusion model. Rather than relying on a meticulously designed global codebook, BBS directly generates diverse and high-fidelity user-specific beams guided by the wireless prompts. Simulation results on accurate ray-tracing datasets demonstrate that BBS can achieve near-optimal beamforming gain while drastically reducing the beam sweeping overhead, even in low signal-to-noise ratio (SNR) environments.", "AI": {"tldr": "\u63d0\u51faGenSSBF\u6846\u67b6\uff0c\u901a\u8fc7\u8054\u5408\u7ed3\u6784\u5efa\u6a21\u5b9e\u73b0\u7ad9\u70b9\u7279\u5b9a\u6ce2\u675f\u6210\u5f62\uff0c\u4f7f\u7528\u5b9a\u5236\u6761\u4ef6\u6269\u6563\u6a21\u578b\u751f\u6210\u7528\u6237\u7279\u5b9a\u6ce2\u675f\uff0c\u663e\u8457\u964d\u4f4e\u6ce2\u675f\u626b\u63cf\u5f00\u9500", "motivation": "\u51c6\u786e\u7406\u89e3\u4f20\u64ad\u73af\u5883\u662f\u7ad9\u70b9\u7279\u5b9a\u6ce2\u675f\u6210\u5f62\uff08SSBF\uff09\u7684\u57fa\u672c\u6311\u6218\u3002\u4f20\u7edf\u975e\u7ed3\u6784\u5316\u9884\u6d4b\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u65b0\u7684\u8054\u5408\u7ed3\u6784\u5efa\u6a21\u65b9\u6cd5", "method": "\u63d0\u51faGenSSBF\u7edf\u4e00\u6846\u67b6\uff0c\u5305\u542b\u7ad9\u70b9\u5256\u9762\u3001\u65e0\u7ebf\u63d0\u793a\u6a21\u5757\u548c\u751f\u6210\u5668\u3002\u5177\u4f53\u5b9e\u73b0BBS\u65b9\u6848\uff1a\u901a\u8fc7DFT\u5c06\u4fe1\u9053\u6570\u636e\u8f6c\u6362\u5230\u53ef\u9006\u6f5c\u5728\u7a7a\u95f4\u6784\u5efa\u7ad9\u70b9\u5256\u9762\uff1b\u4f7f\u7528\u5c11\u91cfDFT\u6ce2\u675f\u6d4b\u91cf\u7684RSRP\u6784\u5efa\u65e0\u7ebf\u63d0\u793a\uff1b\u91c7\u7528\u5b9a\u5236\u6761\u4ef6\u6269\u6563\u6a21\u578b\u4f5c\u4e3a\u751f\u6210\u5668", "result": "\u5728\u7cbe\u786e\u5c04\u7ebf\u8ffd\u8e2a\u6570\u636e\u96c6\u4e0a\u7684\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0cBBS\u80fd\u591f\u5b9e\u73b0\u63a5\u8fd1\u6700\u4f18\u7684\u6ce2\u675f\u6210\u5f62\u589e\u76ca\uff0c\u540c\u65f6\u5927\u5e45\u51cf\u5c11\u6ce2\u675f\u626b\u63cf\u5f00\u9500\uff0c\u5373\u4f7f\u5728\u4f4e\u4fe1\u566a\u6bd4\u73af\u5883\u4e0b\u4e5f\u80fd\u4fdd\u6301\u6027\u80fd", "conclusion": "GenSSBF\u6846\u67b6\u4ee3\u8868\u4e86\u4ece\u4f20\u7edf\u975e\u7ed3\u6784\u5316\u9884\u6d4b\u5230\u8054\u5408\u7ed3\u6784\u5efa\u6a21\u7684\u8303\u5f0f\u8f6c\u53d8\uff0cBBS\u4f5c\u4e3a\u5176\u5b9e\u4f8b\u5316\u65b9\u6848\u80fd\u591f\u76f4\u63a5\u751f\u6210\u591a\u6837\u5316\u3001\u9ad8\u4fdd\u771f\u7684\u7528\u6237\u7279\u5b9a\u6ce2\u675f\uff0c\u663e\u8457\u63d0\u5347\u6ce2\u675f\u6210\u5f62\u6548\u7387"}}
{"id": "2601.02357", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.02357", "abs": "https://arxiv.org/abs/2601.02357", "authors": ["Trey Brosnan"], "title": "DARC: Drum accompaniment generation with fine-grained rhythm control", "comment": null, "summary": "In music creation, rapid prototyping is essential for exploring and refining ideas, yet existing generative tools often fall short when users require both structural control and stylistic flexibility. Prior approaches in stem-to-stem generation can condition on other musical stems but offer limited control over rhythm, and timbre-transfer methods allow users to specify specific rhythms, but cannot condition on musical context. We introduce DARC, a generative drum accompaniment model that conditions both on musical context from other stems and explicit rhythm prompts such as beatboxing or tapping tracks. Using parameter-efficient fine-tuning, we augment STAGE, a state-of-the-art drum stem generator, with fine-grained rhythm control while maintaining musical context awareness.", "AI": {"tldr": "DARC\u662f\u4e00\u4e2a\u9f13\u4f34\u594f\u751f\u6210\u6a21\u578b\uff0c\u901a\u8fc7\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u589e\u5f3aSTAGE\u6a21\u578b\uff0c\u5b9e\u73b0\u97f3\u4e50\u4e0a\u4e0b\u6587\u611f\u77e5\u548c\u7ec6\u7c92\u5ea6\u8282\u594f\u63a7\u5236", "motivation": "\u73b0\u6709\u97f3\u4e50\u751f\u6210\u5de5\u5177\u5728\u7ed3\u6784\u63a7\u5236\u548c\u98ce\u683c\u7075\u6d3b\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff1astem-to-stem\u751f\u6210\u65b9\u6cd5\u5bf9\u8282\u594f\u63a7\u5236\u6709\u9650\uff0c\u800c\u97f3\u8272\u8f6c\u6362\u65b9\u6cd5\u867d\u7136\u80fd\u6307\u5b9a\u8282\u594f\u4f46\u65e0\u6cd5\u8003\u8651\u97f3\u4e50\u4e0a\u4e0b\u6587", "method": "\u4f7f\u7528\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u6280\u672f\u589e\u5f3aSTAGE\uff08\u6700\u5148\u8fdb\u7684\u9f13stem\u751f\u6210\u5668\uff09\uff0c\u4f7f\u5176\u65e2\u80fd\u57fa\u4e8e\u5176\u4ed6stem\u7684\u97f3\u4e50\u4e0a\u4e0b\u6587\uff0c\u53c8\u80fd\u63a5\u53d7\u660e\u786e\u7684\u8282\u594f\u63d0\u793a\uff08\u5982beatboxing\u6216\u6572\u51fb\u97f3\u8f68\uff09", "result": "\u5f00\u53d1\u51faDARC\u6a21\u578b\uff0c\u5728\u4fdd\u6301\u97f3\u4e50\u4e0a\u4e0b\u6587\u611f\u77e5\u80fd\u529b\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u7ec6\u7c92\u5ea6\u7684\u8282\u594f\u63a7\u5236", "conclusion": "DARC\u901a\u8fc7\u7ed3\u5408\u97f3\u4e50\u4e0a\u4e0b\u6587\u548c\u660e\u786e\u8282\u594f\u63d0\u793a\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u7ed3\u6784\u63a7\u5236\u548c\u98ce\u683c\u7075\u6d3b\u6027\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u97f3\u4e50\u521b\u4f5c\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u5feb\u901f\u539f\u578b\u5de5\u5177"}}
{"id": "2601.02225", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.02225", "abs": "https://arxiv.org/abs/2601.02225", "authors": ["Yunping Mu", "Gongpu Wang", "Ruisi He", "Theodoros A. Tsiftsis", "Saman Atapattu", "Chintha Tellambura"], "title": "Backscatter-Assisted High-Speed Rail Communications in Straight Tunnel Environments: Effects of Tag Number and Phase Control", "comment": "6 pages, 5 figures", "summary": "Backscatter communication is a promising technology to enhance the signal strength received by the receiver in straight tunnel environments. The impact of the number of tags and their phase adjustment on system performance remains a challenging issue though. Therefore, in this paper, we investigate the channel gain of backscatter-assisted communication with multiple tags in straight tunnels. In particular, we derive the probabilities that the backscatter link gain is greater than the direct link under adjustable and random phase assumptions by applying the Gaussian and Gamma approximations to derive tractable expressions. The simulation results show that phaseadjustable tags significantly improve the channel gain of the backscatter links compared to the random phase case. Moreover, the number of tags has an upper threshold for an effective tag deployment pattern. These insights provide valuable guidelines for the efficient design of backscatter communication systems in tunnel environments.", "AI": {"tldr": "\u7814\u7a76\u96a7\u9053\u73af\u5883\u4e2d\u591a\u6807\u7b7e\u53cd\u5411\u6563\u5c04\u901a\u4fe1\u7684\u4fe1\u9053\u589e\u76ca\uff0c\u5206\u6790\u6807\u7b7e\u6570\u91cf\u548c\u76f8\u4f4d\u8c03\u6574\u5bf9\u7cfb\u7edf\u6027\u80fd\u7684\u5f71\u54cd", "motivation": "\u53cd\u5411\u6563\u5c04\u901a\u4fe1\u80fd\u589e\u5f3a\u96a7\u9053\u73af\u5883\u4e2d\u7684\u4fe1\u53f7\u5f3a\u5ea6\uff0c\u4f46\u6807\u7b7e\u6570\u91cf\u548c\u76f8\u4f4d\u8c03\u6574\u5bf9\u7cfb\u7edf\u6027\u80fd\u7684\u5f71\u54cd\u4ecd\u662f\u4e00\u4e2a\u6311\u6218\u6027\u95ee\u9898", "method": "\u901a\u8fc7\u9ad8\u65af\u548c\u4f3d\u9a6c\u8fd1\u4f3c\u63a8\u5bfc\u53ef\u8c03\u76f8\u4f4d\u548c\u968f\u673a\u76f8\u4f4d\u60c5\u51b5\u4e0b\u53cd\u5411\u6563\u5c04\u94fe\u8def\u589e\u76ca\u5927\u4e8e\u76f4\u63a5\u94fe\u8def\u7684\u6982\u7387\uff0c\u83b7\u5f97\u53ef\u5904\u7406\u7684\u8868\u8fbe\u5f0f", "result": "\u53ef\u8c03\u76f8\u4f4d\u6807\u7b7e\u76f8\u6bd4\u968f\u673a\u76f8\u4f4d\u663e\u8457\u63d0\u9ad8\u53cd\u5411\u6563\u5c04\u94fe\u8def\u7684\u4fe1\u9053\u589e\u76ca\uff1b\u6807\u7b7e\u6570\u91cf\u5b58\u5728\u6709\u6548\u90e8\u7f72\u6a21\u5f0f\u7684\u4e0a\u9650\u9608\u503c", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u96a7\u9053\u73af\u5883\u4e2d\u53cd\u5411\u6563\u5c04\u901a\u4fe1\u7cfb\u7edf\u7684\u9ad8\u6548\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u6307\u5bfc\u539f\u5219"}}
{"id": "2601.02227", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.02227", "abs": "https://arxiv.org/abs/2601.02227", "authors": ["Hanyeol Ryu", "Sangkil Kim"], "title": "Ultra-low-power Monostatic Backscatter Platform with Phase-Aware Channel Estimation and System-Level Validation", "comment": "19 pages, 18 figures", "summary": "This paper presents a novel channel-estimation (CE) method that mitigates residual phase drifts in backscatter links and a full hardware and signal-processing pipeline for a single-antenna monostatic system. The platform comprises a semi-passive tag, a software-defined radio (SDR) reader, and a 2x1 planar Yagi-Uda array (7 dBi with higher than 30 dB isolation) operating at 2.4 ~ 2.5 GHz. The developed backscatter fading model accounts for round-trip propagation and temporal correlation, and employs an analytically derived resource-optimal pilot allocation strategy. At the receiver, optimized least square (LS) and linear minimum mean square error (LMMSE) CE with pilot-aided carrier frequency offset (CFO) compensation feed a zero-forcing (ZF) equalizer to suppress ISI. The prototype delivers 500 kbps at 1 m with power of 158 uW (SDR baseband) and 10 uW (RF switch), yielding 320 pJ/bit. OOK and BPSK modulations achieve measured EVMs of 2.97 % and 4.02 %, respectively. Performance is validated by BER measurements and successful reconstruction of a full-color image in an over-the-air experiment. The results demonstrate an ultra-low-power, multimedia-capable backscatter IoT link and provide practical hardware-software co-design guidance for scalable deployments.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7528\u4e8e\u5355\u5929\u7ebf\u5355\u7ad9\u7cfb\u7edf\u7684\u4fe1\u9053\u4f30\u8ba1\u65b9\u6cd5\uff0c\u901a\u8fc7\u8865\u507f\u6b8b\u7559\u76f8\u4f4d\u6f02\u79fb\u548c\u4f18\u5316\u5bfc\u9891\u5206\u914d\uff0c\u5b9e\u73b0\u8d85\u4f4e\u529f\u8017\uff08320 pJ/bit\uff09\u7684\u591a\u5a92\u4f53\u7ea7\u53cd\u5411\u6563\u5c04\u7269\u8054\u7f51\u94fe\u8def", "motivation": "\u89e3\u51b3\u53cd\u5411\u6563\u5c04\u94fe\u8def\u4e2d\u7684\u6b8b\u7559\u76f8\u4f4d\u6f02\u79fb\u95ee\u9898\uff0c\u5f00\u53d1\u786c\u4ef6-\u8f6f\u4ef6\u534f\u540c\u8bbe\u8ba1\u5e73\u53f0\uff0c\u5b9e\u73b0\u8d85\u4f4e\u529f\u8017\u3001\u591a\u5a92\u4f53\u80fd\u529b\u7684\u7269\u8054\u7f51\u901a\u4fe1", "method": "1) \u5f00\u53d1\u5305\u542b\u534a\u65e0\u6e90\u6807\u7b7e\u3001SDR\u9605\u8bfb\u5668\u548c2x1\u5e73\u9762\u516b\u6728\u5929\u7ebf\u7684\u786c\u4ef6\u5e73\u53f0\uff1b2) \u5efa\u7acb\u8003\u8651\u5f80\u8fd4\u4f20\u64ad\u548c\u65f6\u95f4\u76f8\u5173\u6027\u7684\u53cd\u5411\u6563\u5c04\u8870\u843d\u6a21\u578b\uff1b3) \u91c7\u7528\u5206\u6790\u63a8\u5bfc\u7684\u8d44\u6e90\u6700\u4f18\u5bfc\u9891\u5206\u914d\u7b56\u7565\uff1b4) \u63a5\u6536\u7aef\u4f7f\u7528\u4f18\u5316\u7684LS\u548cLMMSE\u4fe1\u9053\u4f30\u8ba1\uff0c\u7ed3\u5408\u5bfc\u9891\u8f85\u52a9CFO\u8865\u507f\uff0c\u6700\u540e\u901a\u8fc7ZF\u5747\u8861\u5668\u6291\u5236ISI", "result": "\u57281\u7c73\u8ddd\u79bb\u5b9e\u73b0500 kbps\u4f20\u8f93\u901f\u7387\uff0c\u603b\u529f\u8017168 \u03bcW\uff08SDR\u57fa\u5e26158 \u03bcW + RF\u5f00\u517310 \u03bcW\uff09\uff0c\u80fd\u91cf\u6548\u7387\u8fbe320 pJ/bit\u3002OOK\u548cBPSK\u8c03\u5236\u7684EVM\u5206\u522b\u4e3a2.97%\u548c4.02%\uff0c\u901a\u8fc7BER\u6d4b\u91cf\u548c\u5168\u5f69\u8272\u56fe\u50cf\u4f20\u8f93\u9a8c\u8bc1\u6027\u80fd", "conclusion": "\u8be5\u5de5\u4f5c\u5c55\u793a\u4e86\u8d85\u4f4e\u529f\u8017\u3001\u591a\u5a92\u4f53\u80fd\u529b\u7684\u53cd\u5411\u6563\u5c04\u7269\u8054\u7f51\u94fe\u8def\uff0c\u4e3a\u53ef\u6269\u5c55\u90e8\u7f72\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u786c\u4ef6-\u8f6f\u4ef6\u534f\u540c\u8bbe\u8ba1\u6307\u5bfc"}}

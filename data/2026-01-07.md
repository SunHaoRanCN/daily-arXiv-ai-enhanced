<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 9]
- [eess.AS](#eess.AS) [Total: 3]
- [cs.SD](#cs.SD) [Total: 15]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [Hydrodynamic Whispering: Enabling Near-Field Silent Communication via Artificial Lateral Line Arrays](https://arxiv.org/abs/2601.02394)
*Yuan-Jie Chen*

Main category: eess.SP

TL;DR: 提出"水动力耳语"近场无声通信范式，利用人工侧线阵列通过局部水动力压力波动实现安全短距离水下网络通信


<details>
  <summary>Details</summary>
Motivation: 解决水下群体隐蔽协调的迫切需求，开发具有低截获概率的安全水下通信系统

Method: 基于势流理论建模振荡偶极子源，采用BPSK调制方案，设计24传感器共形阵列，开发时空联合处理框架和空间匹配场波束成形

Result: 系统获得约13.8dB阵列增益，在有效范围内保持接近零的误码率，验证了利用局部水动力压力波动进行可靠安全短距离水下网络的可行性

Conclusion: 研究验证了利用局部水动力压力波动进行可靠安全短距离水下网络通信的可行性，为隐蔽水下群体协调提供了新解决方案

Abstract: To address the imperative for covert underwater swarm coordination, this paper introduces "Hydrodynamic Whispering," a near-field silent communication paradigm utilizing Artificial Lateral Line (ALL) arrays. Grounded in potential flow theory, we model the transmitter as an oscillating dipole source. The resulting pressure field exhibits steep nearfield attenuation (scaling with 1/r^2, naturally delimiting a secure "communication bubble" with intrinsic Low Probability of Interception (LPI) properties. We propose a transceiver architecture featuring a Binary Phase Shift Keying (BPSK) modulation scheme adapted for mechanical actuator inertia, coupled with a bio-inspired 24-sensor conformal array. To mitigate low Signal-to-Noise Ratio (SNR) in turbulent environments,a Spatio-Temporal Joint Processing framework incorporating Spatial Matched-Field Beamforming is developed. Simulation results demonstrate that the system achieves an array gain of approximately 13.8 dB and maintains a near-zero Bit Error Rate (BER) within the effective range. This study validates the feasibility of utilizing localized hydrodynamic pressure fluctuations for reliable and secure short-range underwater networking.

</details>


### [2] [Beyond Path Loss: Altitude-Dependent Spectral Structure Modeling for UAV Measurements](https://arxiv.org/abs/2601.02605)
*Amir Hossein Fahim Raouf,İsmail Güvenç*

Main category: eess.SP

TL;DR: 该论文提出了一个基于测量的框架，用于表征系留式Helikite无人机接收信号的随高度变化的光谱行为，通过多年度频谱测量数据建立了高度依赖的光谱结构模型。


<details>
  <summary>Details</summary>
Motivation: 传统基于功率或阈值的频谱占用模型无法捕捉无人机通信中随高度变化的复杂频谱结构，需要新的建模框架来支持频谱感知的无人机传感和频段选择决策。

Method: 使用系留式Helikite无人机在室外城市环境中进行多年频谱测量，收集89 MHz-6 GHz范围内的功率谱密度快照，提取三个高度依赖的光谱指标：平均功率、光谱熵和光谱稀疏度，并建立高度依赖的光谱结构模型（ADSSM）。

Result: ADSSM模型在所有频段和年份中均实现了较低的均方根误差和较高的决定系数，结果显示功率变化发生在较窄的低空区域，而熵和稀疏度在更宽的、频段依赖的高度范围内演变。

Conclusion: 高度依赖的频谱行为本质上是多维的，所提出的框架能够明确建模光谱结构随高度的变化，超越了传统的功率或阈值模型，实现了频谱感知的无人机传感和频段选择决策。

Abstract: This paper presents a measurement-based framework for characterizing altitude-dependent spectral behavior of signals received by a tethered Helikite unmanned aerial vehicle (UAV). Using a multi-year spectrum measurement campaign in an outdoor urban environment, power spectral density snapshots are collected over the 89 MHz--6 GHz range. Three altitude-dependent spectral metrics are extracted: band-average power, spectral entropy, and spectral sparsity. We introduce the Altitude-Dependent Spectral Structure Model (ADSSM) to characterize the spectral power and entropy using first-order altitude-domain differential equations, and spectral sparsity using a logistic function, yielding closed-form expressions with physically consistent asymptotic behavior. The model is fitted to altitude-binned measurements from three annual campaigns at the AERPAW testbed across six licensed and unlicensed sub-6 GHz bands. Across all bands and years, the ADSSM achieves low root-mean-square error and high coefficients of determination. Results indicate that power transitions occur over narrow low-altitude regions, while entropy and sparsity evolve over broader, band-dependent altitude ranges, demonstrating that altitude-dependent spectrum behavior is inherently multidimensional. By explicitly modeling altitude-dependent transitions in spectral structure beyond received power, the proposed framework enables spectrum-aware UAV sensing and band selection decisions that are not achievable with conventional power- or threshold-based occupancy models.

</details>


### [3] [AI-Native 6G Physical Layer with Cross-Module Optimization and Cooperative Control Agents](https://arxiv.org/abs/2601.02827)
*Xufei Zheng,Han Xiao,Shi Jin,Zhiqin Wang,Wenqiang Tian,Wendong Liu,Jianfei Cao,Jia Shen,Zhihua Shi,Zhi Zhang,Ning Yang*

Main category: eess.SP

TL;DR: 提出AI原生跨模块优化物理层框架，通过协同控制代理实现全局AI/ML模块优化，包含上行联合信源信道编码调制、下行调制预编码检测的端到端优化，以及多个增强机制和基于AI的控制策略。


<details>
  <summary>Details</summary>
Motivation: 传统物理层模块间存在信息壁垒，难以实现端到端全局优化。需要打破模块隔离，通过AI/ML技术实现跨模块协同优化，提升系统整体性能。

Method: 1) AI原生跨模块优化框架；2) 上行AI/ML联合信源信道编码调制优化；3) 下行AI/ML调制预编码检测优化；4) 跨层调制方法；5) 效用导向预编码构造；6) 调制集成CSI反馈；7) AI/ML控制代理实现模型切换。

Result: 仿真结果表明所提方案在BLER和吞吐量方面具有优越性，采用符合3GPP要求的实际假设，为未来标准化提供有价值参考。

Conclusion: 提出的AI原生跨模块优化框架通过打破传统模块隔离，实现端到端全局优化，结合多种增强机制和智能控制策略，显著提升物理层性能，为6G及未来通信系统提供新思路。

Abstract: In this article, a framework of AI-native cross-module optimized physical layer with cooperative control agents is proposed, which involves optimization across global AI/ML modules of the physical layer with innovative design of multiple enhancement mechanisms and control strategies. Specifically, it achieves simultaneous optimization across global modules of uplink AI/ML-based joint source-channel coding with modulation, and downlink AI/ML-based modulation with precoding and corresponding data detection, reducing traditional inter-module information barriers to facilitate end-to-end optimization toward global objectives. Moreover, multiple enhancement mechanisms are also proposed, including i) an AI/ML-based cross-layer modulation approach with theoretical analysis for downlink transmission that breaks the isolation of inter-layer features to expand the solution space for determining improved constellation, ii) a utility-oriented precoder construction method that shifts the role of the AI/ML-based CSI feedback decoder from recovering the original CSI to directly generating precoding matrices aiming to improve end-to-end performance, and iii) incorporating modulation into AI/ML-based CSI feedback to bypass bit-level bottlenecks that introduce quantization errors, non-differentiable gradients, and limitations in constellation solution spaces. Furthermore, AI/ML based control agents for optimized transmission schemes are proposed that leverage AI/ML to perform model switching according to channel state, thereby enabling integrated control for global throughput optimization. Finally, simulation results demonstrate the superiority of the proposed solutions in terms of BLER and throughput. These extensive simulations employ more practical assumptions that are aligned with the requirements of the 3GPP, which hopefully provides valuable insights for future standardization discussions.

</details>


### [4] [Transparent and Resilient Activity Recognition via Attention-Based Distributed Radar Sensing](https://arxiv.org/abs/2601.02874)
*Mina Shahbazifar,Zolfa Zeinalpour-Yazdi,Matthias Hollick,Arash Asadi,Vahid Jamali*

Main category: eess.SP

TL;DR: 提出一个端到端的分布式雷达感知框架，使用轻量级CNN提取局部特征，通过自注意力融合块建模节点间关系，实现高效、低开销、可解释的人类活动识别。


<details>
  <summary>Details</summary>
Motivation: 分布式雷达传感器能够实现鲁棒的人类活动识别，但随着协调节点数量的增加，面临从大数据集中提取特征和透明数据融合的挑战。需要解决通信开销、延迟和可解释性问题。

Method: 1) 每个雷达节点使用轻量级2D CNN从原始雷达数据中提取局部特征；2) 自注意力融合块建模节点间关系并进行自适应信息融合；3) 采用混合监督对比损失提高特征可分性，特别是针对细粒度和不平衡的活动类别。

Result: 在真实世界分布式超宽带雷达数据上的实验表明：局部特征提取将输入维度降低480倍，显著减少通信开销和延迟；模型复杂度降低70.8%；平均准确率高于基线方法；注意力机制提供可解释性，量化每个雷达节点的贡献。

Conclusion: 该框架实现了透明、高效、低开销的分布式雷达感知，解决了大规模分布式雷达系统中的特征提取、数据融合和可解释性问题，为实际应用提供了可行的解决方案。

Abstract: Distributed radar sensors enable robust human activity recognition. However, scaling the number of coordinated nodes introduces challenges in feature extraction from large datasets, and transparent data fusion. We propose an end-to-end framework that operates directly on raw radar data. Each radar node employs a lightweight 2D Convolutional Neural Network (CNN) to extract local features. A self-attention fusion block then models inter-node relationships and performs adaptive information fusion. Local feature extraction reduces the input dimensionality by up to 480x. This significantly lowers communication overhead and latency. The attention mechanism provides inherent interpretability by quantifying the contribution of each radar node. A hybrid supervised contrastive loss further improves feature separability, especially for fine-grained and imbalanced activity classes. Experiments on real-world distributed Ultra Wide Band (UWB) radar data demonstrate that the proposed method reduces model complexity by 70.8\%, while achieving higher average accuracy than baseline approaches. Overall, the framework enables transparent, efficient, and low-overhead distributed radar sensing.

</details>


### [5] [Study of Class-Incremental Radio Frequency Fingerprint Recognition Without Storing Exemplars](https://arxiv.org/abs/2601.03063)
*Rundong Jiang,Jun Hu,Yunqi Song,Zhiyuan Xie,Shiyou Xu*

Main category: eess.SP

TL;DR: 提出了一种无需存储原始样本的类增量学习框架，用于射频指纹识别，通过冻结骨干网络、训练轻量适配器，并使用高斯混合模型生成伪特征来避免遗忘，在资源受限环境中实现高效部署。


<details>
  <summary>Details</summary>
Motivation: 无线设备激增使得身份认证至关重要，射频指纹识别利用物理层特征进行设备识别，但实际部署中面临新设备不断加入、每类训练数据有限的问题。传统静态训练和样本重放方法不切实际，因为类别数量增长、存储成本高且存在隐私问题。

Method: 基于预训练特征提取器，在增量阶段冻结骨干网络，仅训练分类器和轻量适配器模块。为每个类别拟合对角高斯混合模型到骨干特征，从拟合分布中采样伪特征来复习旧类别而不存储原始信号。引入时域随机掩码增强，并采用多教师蒸馏方案将阶段适配器压缩为单个推理适配器。

Result: 在自收集的ADS-B数据集上评估，骨干网络在2,175个类别上预训练，增量实验在669个不相交类别上进行。相比多个基线方法，该方法始终获得更高的平均准确率和更低的遗忘率，同时使用显著更少的存储且避免原始数据保留。

Conclusion: 该框架为资源受限和隐私敏感环境中的射频指纹识别部署提供了实用、低存储的解决方案，具有可重复性，平衡了准确性和运行时效率。

Abstract: The rapid proliferation of wireless devices makes robust identity authentication essential. Radio Frequency Fingerprinting (RFF) exploits device-specific, hard-to-forge physical-layer impairments for identification, and is promising for IoT and unmanned systems. In practice, however, new devices continuously join deployed systems while per-class training data are limited. Conventional static training and naive replay of stored exemplars are impractical due to growing class cardinality, storage cost, and privacy concerns.
  We propose an exemplar-free class-incremental learning framework tailored to RFF recognition. Starting from a pretrained feature extractor, we freeze the backbone during incremental stages and train only a classifier together with lightweight Adapter modules that perform small task-specific feature adjustments. For each class we fit a diagonal Gaussian Mixture Model (GMM) to the backbone features and sample pseudo-features from these fitted distributions to rehearse past classes without storing raw signals. To improve robustness under few-shot conditions we introduce a time-domain random-masking augmentation and adopt a multi-teacher distillation scheme to compress stage-wise Adapters into a single inference Adapter, trading off accuracy and runtime efficiency.
  We evaluate the method on large, self-collected ADS-B datasets: the backbone is pretrained on 2,175 classes and incremental experiments are run on a disjoint set of 669 classes with multiple rounds and step sizes. Against several representative baselines, our approach consistently yields higher average accuracy and lower forgetting, while using substantially less storage and avoiding raw-data retention.
  The proposed pipeline is reproducible and provides a practical, low-storage solution for RFF deployment in resource- and privacy-constrained environments.

</details>


### [6] [A Conditional Variational Framework for Channel Prediction in High-Mobility 6G OTFS Networks](https://arxiv.org/abs/2601.03084)
*Mohsen Kazemian,Jürgen Jasperneite*

Main category: eess.SP

TL;DR: 提出基于条件变分自编码器(CVAE)的OTFS信道预测方法，在高移动性场景下优于现有学习基线


<details>
  <summary>Details</summary>
Motivation: 在高移动性OTFS系统中，多普勒扩展和时变多径传播导致信道快速去相关，传统基于导频的信道估计方法容易产生过时的信道状态信息和过大开销，因此需要可靠的信道预测方法来支持鲁棒的检测和解码

Method: 提出条件变分自编码器用于信道预测(CVAE4CP)方法，通过学习给定物理系统和移动参数条件下OTFS时延多普勒信道系数的条件分布，将这些参数作为条件信息，在信道实际实现前预测未来信道系数，同时通过低维潜在表示考虑信道固有不确定性

Result: 在高移动性条件下进行广泛仿真评估，数值结果表明CVAE4CP在归一化均方误差(NMSE)方面始终优于竞争的学习基线方法，特别是在高多普勒频率和扩展预测范围时

Conclusion: 该方法在快速时变OTFS系统中具有有效性和鲁棒性，能够可靠预测信道

Abstract: This paper proposes a machine learning (ML) based method for channel prediction in high mobility orthogonal time frequency space (OTFS) channels. In these scenarios, rapid variations caused by Doppler spread and time varying multipath propagation lead to fast channel decorrelation, making conventional pilot based channel estimation methods prone to outdated channel state information (CSI) and excessive overhead. Therefore, reliable channel prediction methods become essential to support robust detection and decoding in OTFS systems. In this paper, we propose conditional variational autoencoder for channel prediction (CVAE4CP) method, which learns the conditional distribution of OTFS delay Doppler channel coefficients given physical system and mobility parameters. By incorporating these parameters as conditioning information, the proposed method enables the prediction of future channel coefficients before their actual realization, while accounting for inherent channel uncertainty through a low dimensional latent representation. The proposed framework is evaluated through extensive simulations under high mobility conditions. Numerical results demonstrate that CVAE4CP consistently outperforms a competing learning based baseline in terms of normalized mean squared error (NMSE), particularly at high Doppler frequencies and extended prediction horizons. These results confirm the effectiveness and robustness of the proposed approach for channel prediction in rapidly time varying OTFS systems.

</details>


### [7] [Post-Decision State-Based Online Learning for Delay-Energy-Aware Flow Allocation in Wireless Systems](https://arxiv.org/abs/2601.03108)
*Mahesh Ganesh Bhat,Shana Moothedath,Prasanna Chaporkar*

Main category: eess.SP

TL;DR: 提出一种基于后决策状态的结构感知强化学习方法，用于5G UPF中的延迟和能量感知流分配，相比标准Q学习收敛更快且长期成本更低。


<details>
  <summary>Details</summary>
Motivation: 5G用户平面功能需要高效的流分配机制来处理异构UPF能力和随机流到达，传统方法难以处理未知统计特性，需要能适应动态环境的智能资源分配方案。

Method: 将系统建模为马尔可夫决策过程，提出基于后决策状态的价值迭代算法，通过分离动作控制动态和外生因素，利用MDP底层结构实现高效自适应流分配。

Result: 仿真结果表明，所提方法比标准Q学习收敛更快，且能实现更低的长期成本，验证了PDS-based RL在无线网络资源分配中的有效性。

Conclusion: 基于后决策状态的结构感知强化学习为5G UPF中的延迟和能量感知流分配提供了有效解决方案，即使在缺乏外生变量统计知识的情况下也能实现高效自适应分配。

Abstract: We develop a structure-aware reinforcement learning (RL) approach for delay- and energy-aware flow allocation in 5G User Plane Functions (UPFs). We consider a dynamic system with $K$ heterogeneous UPFs of varying capacities that handle stochastic arrivals of $M$ flow types, each with distinct rate requirements. We model the system as a Markov decision process (MDP) to capture the stochastic nature of flow arrivals and departures (possibly unknown), as well as the impact of flow allocation in the system. To solve this problem, we propose a post-decision state (PDS) based value iteration algorithm that exploits the underlying structure of the MDP. By separating action-controlled dynamics from exogenous factors, PDS enables faster convergence and efficient adaptive flow allocation, even in the absence of statistical knowledge about exogenous variables. Simulation results demonstrate that the proposed method converges faster and achieves lower long-term cost than standard Q-learning, highlighting the effectiveness of PDS-based RL for resource allocation in wireless networks.

</details>


### [8] [Spectral-Efficient LoRa with Low Complexity Detection](https://arxiv.org/abs/2601.03148)
*Alireza Maleki,Ebrahim Bedeer,Robert Barton*

Main category: eess.SP

TL;DR: 提出SE-LoRa调制方案，通过SIC检测器显著提升LoRa频谱效率，在保持可接受误码性能的同时，复杂度与传统LoRa相当。


<details>
  <summary>Details</summary>
Motivation: 传统LoRa调制频谱效率较低，需要提升其频谱效率同时保持合理的误码性能和复杂度。

Method: 提出SE-LoRa调制方案，推导联合ML检测规则，并设计基于频域特性的低复杂度SIC检测器。

Result: SE-LoRa在SF为7、9、11时分别提升频谱效率445.45%、1011.11%、1071.88%，在SER=10⁻³时误码性能与传统LoRa相差小于3dB。

Conclusion: SE-LoRa方案显著提升LoRa频谱效率，同时保持可接受的误码性能和低复杂度，适用于高SF场景。

Abstract: In this paper, we propose a spectral-efficient LoRa (SE-LoRa) modulation scheme with a low complexity successive interference cancellation (SIC)-based detector. The proposed communication scheme significantly improves the spectral efficiency of LoRa modulation, while achieving an acceptable error performance compared to conventional LoRa modulation, especially in higher spreading factor (SF) settings. We derive the joint maximum likelihood (ML) detection rule for the SE-LoRa transmission scheme that turns out to be of high computational complexity. To overcome this issue, and by exploiting the frequency-domain characteristics of the dechirped SE-LoRa signal, we propose a low complexity SIC-based detector with a computation complexity at the order of conventional LoRa detection. By computer simulations, we show that the proposed SE-LoRa with low complexity SIC-based detector can improve the spectral efficiency of LoRa modulation up to $445.45\%$, $1011.11\%$, and $1071.88\%$ for SF values of $7$, $9$, and $11$, respectively, while maintaining the error performance within less than $3$ dB of conventional LoRa at symbol error rate (SER) of $10^{-3}$ in Rician channel conditions.

</details>


### [9] [Inter-Year Transfer of Altitude-Dependent Spectrum Activity Models Using Minimal Calibration](https://arxiv.org/abs/2601.03234)
*Amir Hossein Fahim Raouf,İsmail Güvenc*

Main category: eess.SP

TL;DR: 研究跨年度的海拔依赖频谱活动模型与测量的可迁移性，提出基于物理的随机几何模型，分析不同频段的海拔结构稳定性，评估最小化校准的模型复用方法。


<details>
  <summary>Details</summary>
Motivation: 研究频谱活动模型在不同年份间的可迁移性，旨在降低频谱监测和建模的成本，特别是在航空场景中需要理解海拔依赖的干扰特性。

Method: 提出基于物理的随机几何模型，将聚合干扰建模为海拔分箱接收功率，提取三个可解释参数。分析2023-2025年多个sub-6 GHz频段的航空频谱测量数据，评估不同频段的海拔结构稳定性。开发最小化校准方法，固定参考年的过渡海拔，仅用目标年的两个海拔分箱估计其余参数。

Result: 下行链路和共享接入频段保持稳定的几何驱动海拔结构，跨年一致性高；上行链路频段海拔依赖性弱，干扰主要由活动动态而非传播几何主导。最小化校准方法在下行链路和CBRS频段提供准确预测，表明在稳定环境中可实现低成本模型迁移。

Conclusion: 下行链路和共享接入频段的几何驱动海拔结构具有跨年稳定性，支持低成本模型复用；上行链路频段更适合活动动态模型而非平均场模型。该方法为稳定环境中的频谱监测提供了实用的模型迁移框架。

Abstract: This paper studies the transferability of altitude-dependent spectrum activity models and measurements across years. We introduce a physics-informed, mean-only stochastic-geometry model of aggregate interference to altitude-binned received power, yielding three interpretable parameters for a given band and campaign: 1) line-of-sight transition slope, 2) transition altitude, and 3) effective activity constant. Analysis of aerial spectrum measurements collected from 2023 to 2025 across multiple sub-6 GHz bands reveals that downlink (DL) and shared-access bands preserve a persistent geometry-driven altitude structure that is stable across years. In contrast, uplink (UL) bands exhibit weak altitude dependence with no identifiable transition, indicating that interference is dominated by activity dynamics rather than propagation geometry. To quantify the practical limits of model reuse, we evaluate a minimal-calibration method in which the transition altitude is fixed from a reference year and the remaining parameters are estimated from only two altitude bins in the target year. The results further indicate that the proposed approach provides accurate predictions for DL and CBRS bands, suggesting the feasibility of low-cost model transfer in stable environments, while highlighting the reduced applicability of mean-field models for UL scenarios.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [10] [Vclip: Face-based Speaker Generation by Face-voice Association Learning](https://arxiv.org/abs/2601.02753)
*Yao Shi,Yunfei Xu,Hongbin Suo,Yulong Wan,Haifeng Liu*

Main category: eess.AS

TL;DR: Vclip是一种基于人脸进行语音合成的新方法，利用CLIP编码器在嘈杂的视听数据上学习人脸与声音的关联，通过检索策略和GMM说话人生成模块实现高质量的人脸驱动语音合成。


<details>
  <summary>Details</summary>
Motivation: 基于人脸的语音合成任务中，由于缺乏高质量的视听语料库，现有方法存在合成质量低或知识迁移导致的领域不匹配问题。需要一种能有效学习人脸与声音关联的方法。

Method: 提出Vclip方法：1) 利用CLIP编码器在嘈杂的视听数据上学习人脸与声音的语义关联；2) 采用基于检索的策略，结合GMM说话人生成模块为下游TTS系统生成可能的目标说话人；3) 利用下游TTS的反馈信息来优化合成声音与人脸的匹配度。

Result: 在Voxceleb测试集上达到89.63%的跨模态验证AUC分数。实验表明Vclip结合检索步骤能够弥合人脸与声音特征之间的差距，利用下游TTS的反馈信息可以合成与参考人脸紧密匹配的声音。

Conclusion: Vclip方法通过有效学习人脸与声音的关联，结合检索策略和TTS反馈，成功实现了高质量的人脸驱动语音合成，解决了现有方法的质量和领域不匹配问题。

Abstract: This paper discusses the task of face-based speech synthesis, a kind of personalized speech synthesis where the synthesized voices are constrained to perceptually match with a reference face image. Due to the lack of TTS-quality audio-visual corpora, previous approaches suffer from either low synthesis quality or domain mismatch induced by a knowledge transfer scheme. This paper proposes a new approach called Vclip that utilizes the facial-semantic knowledge of the CLIP encoder on noisy audio-visual data to learn the association between face and voice efficiently, achieving 89.63% cross-modal verification AUC score on Voxceleb testset. The proposed method then uses a retrieval-based strategy, combined with GMM-based speaker generation module for a downstream TTS system, to produce probable target speakers given reference images. Experimental results demonstrate that the proposed Vclip system in conjunction with the retrieval step can bridge the gap between face and voice features for face-based speech synthesis. And using the feedback information distilled from downstream TTS helps to synthesize voices that match closely with reference faces. Demos available at sos1sos2sixteen.github.io/vclip.

</details>


### [11] [XLSR-MamBo: Scaling the Hybrid Mamba-Attention Backbone for Audio Deepfake Detection](https://arxiv.org/abs/2601.02944)
*Kwok-Ho Ng,Tingting Song,Yongdong WU,Zhihua Xia*

Main category: eess.AS

TL;DR: 提出XLSR-MamBo混合架构，结合XLSR前端与Mamba-Attention骨干网络，在音频深度伪造检测任务中实现竞争性性能


<details>
  <summary>Details</summary>
Motivation: 高级语音合成技术带来安全风险，需要研究音频深度伪造检测。纯因果状态空间模型在处理全局频域伪影方面存在局限，因此探索混合架构的扩展特性

Method: 提出XLSR-MamBo模块化框架，集成XLSR前端与Mamba-Attention骨干网络。系统评估四种拓扑设计，使用Mamba、Mamba2、Hydra和Gated DeltaNet等先进SSM变体

Result: MamBo-3-Hydra-N3配置在ASVspoof 2021 LA、DF和In-the-Wild基准测试中达到竞争性性能。在DFADD数据集上对未见过的扩散和流匹配合成方法展现鲁棒泛化能力

Conclusion: 混合框架能有效捕捉伪造语音信号中的伪影，为音频深度伪造检测提供有效方法。增加骨干网络深度可缓解性能波动和不稳定性

Abstract: Advanced speech synthesis technologies have enabled highly realistic speech generation, posing security risks that motivate research into audio deepfake detection (ADD). While state space models (SSMs) offer linear complexity, pure causal SSMs architectures often struggle with the content-based retrieval required to capture global frequency-domain artifacts. To address this, we explore the scaling properties of hybrid architectures by proposing XLSR-MamBo, a modular framework integrating an XLSR front-end with synergistic Mamba-Attention backbones. We systematically evaluate four topological designs using advanced SSM variants, Mamba, Mamba2, Hydra, and Gated DeltaNet. Experimental results demonstrate that the MamBo-3-Hydra-N3 configuration achieves competitive performance compared to other state-of-the-art systems on the ASVspoof 2021 LA, DF, and In-the-Wild benchmarks. This performance benefits from Hydra's native bidirectional modeling, which captures holistic temporal dependencies more efficiently than the heuristic dual-branch strategies employed in prior works. Furthermore, evaluations on the DFADD dataset demonstrate robust generalization to unseen diffusion- and flow-matching-based synthesis methods. Crucially, our analysis reveals that scaling backbone depth effectively mitigates the performance variance and instability observed in shallower models. These results demonstrate the hybrid framework's ability to capture artifacts in spoofed speech signals, providing an effective method for ADD.

</details>


### [12] [Towards Fine-Grained and Multi-Granular Contrastive Language-Speech Pre-training](https://arxiv.org/abs/2601.03065)
*Yifan Yang,Bing Han,Hui Wang,Wei Wang,Ziyang Ma,Long Zhou,Zengrui Jin,Guanrou Yang,Tianrui Wang,Xu Tan,Xie Chen*

Main category: eess.AS

TL;DR: FCaps数据集提供47k小时语音和19M细粒度风格描述，CLSP模型通过对比学习实现多粒度语音-文本表示学习，在多项任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有语音-文本模型通常使用粗粒度标注或任务特定监督进行训练，缺乏细粒度说话风格建模能力，且可扩展的细粒度风格标注数据不可得。

Method: 提出FCaps数据集，通过端到端管道直接基于音频生成详细标注，避免级联管道中的LLM重写错误传播。基于FCaps提出CLSP对比学习模型，整合全局和细粒度监督。

Result: FCaps标注在正确性、覆盖范围和自然度上优于现有级联标注。CLSP学习到的细粒度多粒度表示在全局/细粒度语音-文本检索、零样本副语言分类和语音风格相似度评分等任务中表现可靠，与人类判断高度一致。

Conclusion: FCaps数据集和CLSP模型为细粒度说话风格建模提供了有效解决方案，实现了多粒度语音-文本表示学习，在多项任务中展现强大性能，所有资源将公开可用。

Abstract: Modeling fine-grained speaking styles remains challenging for language-speech representation pre-training, as existing speech-text models are typically trained with coarse captions or task-specific supervision, and scalable fine-grained style annotations are unavailable. We present FCaps, a large-scale dataset with fine-grained free-text style descriptions, encompassing 47k hours of speech and 19M fine-grained captions annotated via a novel end-to-end pipeline that directly grounds detailed captions in audio, thereby avoiding the error propagation caused by LLM-based rewriting in existing cascaded pipelines. Evaluations using LLM-as-a-judge demonstrate that our annotations surpass existing cascaded annotations in terms of correctness, coverage, and naturalness. Building on FCaps, we propose CLSP, a contrastive language-speech pre-trained model that integrates global and fine-grained supervision, enabling unified representations across multiple granularities. Extensive experiments demonstrate that CLSP learns fine-grained and multi-granular speech-text representations that perform reliably across global and fine-grained speech-text retrieval, zero-shot paralinguistic classification, and speech style similarity scoring, with strong alignment to human judgments. All resources will be made publicly available.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [13] [Quantifying Quanvolutional Neural Networks Robustness for Speech in Healthcare Applications](https://arxiv.org/abs/2601.02432)
*Ha Tran,Bipasha Kashyap,Pubudu N. Pathirana*

Main category: cs.SD

TL;DR: 量子卷积神经网络（QNNs）在语音情感识别和病理检测中，对音高偏移、时间偏移和速度变化等声学干扰表现出比传统CNN更强的鲁棒性，但对高斯噪声仍较敏感。


<details>
  <summary>Details</summary>
Motivation: 基于语音的机器学习系统对噪声敏感，限制了其在情感识别和语音病理检测中的可靠部署。需要评估量子机器学习模型在声学干扰下的鲁棒性。

Method: 在干净训练/干扰测试模式下，使用AVFAD（语音病理）和TESS（语音情感）数据集，比较三种QNN模型（随机、基础、强纠缠）与简单CNN基线、ResNet-18和VGG-16。评估四种声学干扰（高斯噪声、音高偏移、时间偏移、速度变化），使用准确率和干扰度量（CE、mCE、RCE、RmCE），并分析电路复杂度、收敛速度和不同情感的鲁棒性。

Result: QNNs在音高偏移、时间偏移和速度变化干扰下普遍优于CNN基线（在严重时间偏移下CE/RCE降低达22%），但对高斯噪声CNN基线更鲁棒。QNN-Basic在AVFAD上整体鲁棒性最佳，QNN-Random在TESS上表现最强。恐惧情感最鲁棒（严重干扰下80-90%准确率），中性情感在高斯噪声下可能崩溃（5.5%准确率），快乐情感对音高、时间和速度干扰最脆弱。QNNs收敛速度比CNN基线快达6倍。

Conclusion: 浅层纠缠量子前端可以提高语音处理系统对常见非对抗性声学干扰的鲁棒性，但对加性噪声的敏感性仍是挑战。这是首个系统研究QNN在语音声学干扰下鲁棒性的工作。

Abstract: Speech-based machine learning systems are sensitive to noise, complicating reliable deployment in emotion recognition and voice pathology detection. We evaluate the robustness of a hybrid quantum machine learning model, quanvolutional neural networks (QNNs) against classical convolutional neural networks (CNNs) under four acoustic corruptions (Gaussian noise, pitch shift, temporal shift, and speed variation) in a clean-train/corrupted-test regime. Using AVFAD (voice pathology) and TESS (speech emotion), we compare three QNN models (Random, Basic, Strongly) to a simple CNN baseline (CNN-Base), ResNet-18 and VGG-16 using accuracy and corruption metrics (CE, mCE, RCE, RmCE), and analyze architectural factors (circuit complexity or depth, convergence) alongside per-emotion robustness. QNNs generally outperform the CNN-Base under pitch shift, temporal shift, and speed variation (up to 22% lower CE/RCE at severe temporal shift), while the CNN-Base remains more resilient to Gaussian noise. Among quantum circuits, QNN-Basic achieves the best overall robustness on AVFAD, and QNN-Random performs strongest on TESS. Emotion-wise, fear is most robust (80-90% accuracy under severe corruptions), neutral can collapse under strong Gaussian noise (5.5% accuracy), and happy is most vulnerable to pitch, temporal, and speed distortions. QNNs also converge up to six times faster than the CNN-Base. To our knowledge, this is a systematic study of QNN robustness for speech under common non-adversarial acoustic corruptions, indicating that shallow entangling quantum front-ends can improve noise resilience while sensitivity to additive noise remains a challenge.

</details>


### [14] [VocalBridge: Latent Diffusion-Bridge Purification for Defeating Perturbation-Based Voiceprint Defenses](https://arxiv.org/abs/2601.02444)
*Maryam Abbasihafshejani,AHM Nazmus Sakib,Murtuza Jadliwala*

Main category: cs.SD

TL;DR: 本文提出VocalBridge框架，通过扩散模型在EnCodec潜在空间中学习从受扰动语音到干净语音的映射，有效对抗语音克隆防御，恢复可克隆的说话人特征。


<details>
  <summary>Details</summary>
Motivation: 当前语音克隆防御通过在语音中嵌入保护性扰动来隐藏说话人身份，但攻击者可以使用净化技术移除这些扰动，恢复原始声学特征并重新生成可克隆语音。现有净化方法主要针对ASR系统的对抗噪声，对说话人验证或语音克隆攻击效果有限，需要更有效的净化方法。

Method: 提出Diffusion-Bridge (VocalBridge)净化框架：1) 在EnCodec潜在空间中学习从受扰动语音到干净语音的潜在映射；2) 使用时间条件1D U-Net和余弦噪声调度；3) 引入Whisper引导的音素变体，提供轻量级时间引导而无需真实转录文本。

Result: 实验结果表明，该方法在从受保护语音中恢复可克隆语音方面持续优于现有净化方法，证明了当前基于扰动的防御的脆弱性。

Conclusion: 现有基于扰动的语音克隆防御在面对自适应净化攻击时不够鲁棒，需要开发更强大的保护机制来应对不断进化的语音克隆和说话人验证威胁。

Abstract: The rapid advancement of speech synthesis technologies, including text-to-speech (TTS) and voice conversion (VC), has intensified security and privacy concerns related to voice cloning. Recent defenses attempt to prevent unauthorized cloning by embedding protective perturbations into speech to obscure speaker identity while maintaining intelligibility. However, adversaries can apply advanced purification techniques to remove these perturbations, recover authentic acoustic characteristics, and regenerate cloneable voices. Despite the growing realism of such attacks, the robustness of existing defenses under adaptive purification remains insufficiently studied.
  Most existing purification methods are designed to counter adversarial noise in automatic speech recognition (ASR) systems rather than speaker verification or voice cloning pipelines. As a result, they fail to suppress the fine-grained acoustic cues that define speaker identity and are often ineffective against speaker verification attacks (SVA). To address these limitations, we propose Diffusion-Bridge (VocalBridge), a purification framework that learns a latent mapping from perturbed to clean speech in the EnCodec latent space. Using a time-conditioned 1D U-Net with a cosine noise schedule, the model enables efficient, transcript-free purification while preserving speaker-discriminative structure. We further introduce a Whisper-guided phoneme variant that incorporates lightweight temporal guidance without requiring ground-truth transcripts. Experimental results show that our approach consistently outperforms existing purification methods in recovering cloneable voices from protected speech. Our findings demonstrate the fragility of current perturbation-based defenses and highlight the need for more robust protection mechanisms against evolving voice-cloning and speaker verification threats.

</details>


### [15] [Dynamic Quantization Error Propagation in Encoder-Decoder ASR Quantization](https://arxiv.org/abs/2601.02455)
*Xinyu Wang,Yajie Luo,Yihong Wu,Liheng Ma,Ziyu Zhao,Jingrui Tian,Lei Ding,Yufei Cui,Xiao-Wen Chang*

Main category: cs.SD

TL;DR: FADE方法通过细粒度动态量化误差传播，在ASR模型压缩中平衡跨层误差校正与局部量化，显著提升稳定性和性能


<details>
  <summary>Details</summary>
Motivation: 在内存受限的边缘设备上运行ASR模型需要高效压缩，但现有的层后量化方法在编码器-解码器架构中存在误差累积问题，而现有解决方案如QEP由于ASR模型的异构性（编码器处理声学特征、解码器生成文本）而效果不佳

Method: 提出FADE（细粒度动态量化误差传播）方法，自适应控制跨层误差校正与局部量化之间的权衡

Result: 实验显示FADE显著提高了稳定性，减少了运行间的性能方差，同时在平均WER上超越了基线方法

Conclusion: FADE方法有效解决了ASR模型量化中的误差累积问题，为异构编码器-解码器架构提供了更优的压缩解决方案

Abstract: Running Automatic Speech Recognition (ASR) models on memory-constrained edge devices requires efficient compression. While layer-wise post-training quantization is effective, it suffers from error accumulation, especially in encoder-decoder architectures. Existing solutions like Quantization Error Propagation (QEP) are suboptimal for ASR due to the model's heterogeneity, processing acoustic features in the encoder while generating text in the decoder. To address this, we propose Fine-grained Alpha for Dynamic Quantization Error Propagation (FADE), which adaptively controls the trade-off between cross-layer error correction and local quantization. Experiments show that FADE significantly improves stability by reducing performance variance across runs, while simultaneously surpassing baselines in mean WER.

</details>


### [16] [Understanding Human Perception of Music Plagiarism Through a Computational Approach](https://arxiv.org/abs/2601.02586)
*Daeun Hwang,Hyeonbin Hwang*

Main category: cs.SD

TL;DR: 该研究探讨人类感知音乐抄袭的关键标准，提出基于LLM的评估框架，通过提取旋律、节奏、和弦进行等高阶特征来模拟人类判断。


<details>
  <summary>Details</summary>
Motivation: 现有音乐相似性检测算法多样，但现实中的音乐抄袭讨论常基于听众感知。需要研究人类感知音乐抄袭的关键标准，以弥合算法与人类判断之间的差距。

Method: 首先研究人类对音乐相似性的感知标准，识别关键特征（旋律、节奏、和弦进行）和变异程度。然后提出LLM-as-a-judge框架，采用系统化、分步方法，利用提取这些高阶特征的模块进行评估。

Result: 研究识别了人类判断音乐相似性的关键特征和变异阈值，并成功构建了基于LLM的评估框架，能够模拟人类对音乐抄袭的感知判断。

Conclusion: 该研究为音乐抄袭检测提供了更符合人类感知的评估方法，通过LLM框架能够更准确地模拟人类对音乐相似性的判断，有助于弥合算法与人类感知之间的差距。

Abstract: There is a wide variety of music similarity detection algorithms, while discussions about music plagiarism in the real world are often based on audience perceptions. Therefore, we aim to conduct a study to examine the key criteria of human perception of music plagiarism, focusing on the three commonly used musical features in similarity analysis: melody, rhythm, and chord progression. After identifying the key features and levels of variation humans use in perceiving musical similarity, we propose a LLM-as-a-judge framework that applies a systematic, step-by-step approach, drawing on modules that extract such high-level attributes.

</details>


### [17] [A Music Information Retrieval Approach to Classify Sub-Genres in Role Playing Games](https://arxiv.org/abs/2601.02591)
*Daeun Hwang,Xuyuan Cai,Edward F. Melcer,Elin Carstensdottir*

Main category: cs.SD

TL;DR: 该研究首次系统分析了角色扮演游戏（RPG）三个子类型中视频游戏音乐（VGM）的可量化音乐特征，探索这些特征如何与游戏类型的感知和表现相关联。


<details>
  <summary>Details</summary>
Motivation: 目前视频游戏音乐研究多沿用电影音乐的分析框架，主要关注其理论功能与媒体类型的对应关系。然而，至今缺乏系统方法分析不同游戏类型中VGM的可量化音乐特征。研究者希望填补这一空白，探索音乐特征与游戏类型感知之间的关联。

Method: 从RPG三个子类型的游戏中提取VGM的音乐特征，然后假设不同音乐特征如何与每个游戏类型的感知和表现相关联。通过分析这些特征与游戏类型的相关性，探索音乐特征在游戏叙事元素和玩法机制中的作用。

Result: 研究发现音乐特征与RPG子类型的感知和表现存在相关性。这种相关性表明特定音乐特征可能与各子类型预期的叙事元素或玩法机制相关，为游戏音乐设计提供了实证依据。

Conclusion: 该研究为视频游戏音乐分析提供了新的系统方法，揭示了音乐特征与游戏类型之间的量化关系。这些发现有助于游戏开发者更好地设计符合游戏类型特征的音乐，增强玩家的沉浸感和游戏体验。

Abstract: Video game music (VGM) is often studied under the same lens as film music, which largely focuses on its theoretical functionality with relation to the identified genres of the media. However, till date, we are unaware of any systematic approach that analyzes the quantifiable musical features in VGM across several identified game genres. Therefore, we extracted musical features from VGM in games from three sub-genres of Role-Playing Games (RPG), and then hypothesized how different musical features are correlated to the perceptions and portrayals of each genre. This observed correlation may be used to further suggest such features are relevant to the expected storytelling elements or play mechanics associated with the sub-genre.

</details>


### [18] [Multi-channel multi-speaker transformer for speech recognition](https://arxiv.org/abs/2601.02688)
*Guo Yifan,Tian Yao,Suo Hongbin,Wan Yulong*

Main category: cs.SD

TL;DR: 提出M2Former用于远场多说话人语音识别，相比现有方法显著降低词错误率


<details>
  <summary>Details</summary>
Motivation: 随着远程会议和车载语音助手的发展，远场多说话人语音识别成为研究热点。现有MCT方法无法从混合音频中为每个说话人编码高维声学特征，因为说话人之间存在干扰。

Method: 提出多通道多说话人变换器（M2Former），专门用于远场多说话人ASR，能够处理说话人之间的干扰问题。

Result: 在SMS-WSJ基准测试中，M2Former相比神经波束形成器、MCT、双路径RNN和基于多通道深度聚类的端到端系统，相对词错误率分别降低9.2%、14.3%、24.9%和52.2%。

Conclusion: M2Former在远场多说话人语音识别任务上表现出色，显著优于现有方法，为解决说话人干扰问题提供了有效方案。

Abstract: With the development of teleconferencing and in-vehicle voice assistants, far-field multi-speaker speech recognition has become a hot research topic. Recently, a multi-channel transformer (MCT) has been proposed, which demonstrates the ability of the transformer to model far-field acoustic environments. However, MCT cannot encode high-dimensional acoustic features for each speaker from mixed input audio because of the interference between speakers. Based on these, we propose the multi-channel multi-speaker transformer (M2Former) for far-field multi-speaker ASR in this paper. Experiments on the SMS-WSJ benchmark show that the M2Former outperforms the neural beamformer, MCT, dual-path RNN with transform-average-concatenate and multi-channel deep clustering based end-to-end systems by 9.2%, 14.3%, 24.9%, and 52.2% respectively, in terms of relative word error rate reduction.

</details>


### [19] [Omni2Sound: Towards Unified Video-Text-to-Audio Generation](https://arxiv.org/abs/2601.02731)
*Yusheng Dai,Zehua Chen,Yuxuan Jiang,Baolong Gao,Qiuhong Ke,Jun Zhu,Jianfei Cai*

Main category: cs.SD

TL;DR: Omni2Sound：首个统一视频-文本-音频生成模型，通过SoundAtlas数据集解决多模态对齐难题，采用三阶段训练策略消除任务竞争，在V2A、T2A、VT2A三项任务上均达到SOTA性能


<details>
  <summary>Details</summary>
Motivation: 现有统一模型面临两大挑战：1）高质量音频标注数据稀缺导致多模态条件语义冲突；2）跨任务和任务内竞争导致V2A-T2A性能权衡和模态偏差。需要同时解决数据质量和模型训练难题

Method: 1）构建SoundAtlas数据集（47万对），采用视觉到语言压缩、初级-高级代理交接、后验过滤等创新流程；2）设计Omni2Sound统一扩散模型，采用三阶段多任务渐进训练策略，将跨任务竞争转化为联合优化

Result: 在VGGSound-Omni基准测试中，使用标准DiT骨干网络，Omni2Sound在V2A、T2A、VT2A三项任务上均达到统一SOTA性能，展现出对异构输入条件的强大泛化能力

Conclusion: 通过高质量数据集SoundAtlas和创新的三阶段训练策略，成功解决了多模态生成中的语义对齐和任务竞争问题，实现了首个在三个音频生成任务上均达到SOTA的统一模型

Abstract: Training a unified model integrating video-to-audio (V2A), text-to-audio (T2A), and joint video-text-to-audio (VT2A) generation offers significant application flexibility, yet faces two unexplored foundational challenges: (1) the scarcity of high-quality audio captions with tight A-V-T alignment, leading to severe semantic conflict between multimodal conditions, and (2) cross-task and intra-task competition, manifesting as an adverse V2A-T2A performance trade-off and modality bias in the VT2A task. First, to address data scarcity, we introduce SoundAtlas, a large-scale dataset (470k pairs) that significantly outperforms existing benchmarks and even human experts in quality. Powered by a novel agentic pipeline, it integrates Vision-to-Language Compression to mitigate visual bias of MLLMs, a Junior-Senior Agent Handoff for a 5 times cost reduction, and rigorous Post-hoc Filtering to ensure fidelity. Consequently, SoundAtlas delivers semantically rich and temporally detailed captions with tight V-A-T alignment. Second, we propose Omni2Sound, a unified VT2A diffusion model supporting flexible input modalities. To resolve the inherent cross-task and intra-task competition, we design a three-stage multi-task progressive training schedule that converts cross-task competition into joint optimization and mitigates modality bias in the VT2A task, maintaining both audio-visual alignment and off-screen audio generation faithfulness. Finally, we construct VGGSound-Omni, a comprehensive benchmark for unified evaluation, including challenging off-screen tracks. With a standard DiT backbone, Omni2Sound achieves unified SOTA performance across all three tasks within a single model, demonstrating strong generalization across benchmarks with heterogeneous input conditions. The project page is at https://swapforward.github.io/Omni2Sound.

</details>


### [20] [SPO-CLAPScore: Enhancing CLAP-based alignment prediction system with Standardize Preference Optimization, for the first XACLE Challenge](https://arxiv.org/abs/2601.02900)
*Taisei Takano,Ryoya Yoshida*

Main category: cs.SD

TL;DR: 本文介绍了参加XACLE挑战赛的Takano_UTokyo_03系统，该系统采用CLAPScore架构结合标准化偏好优化(SPO)训练方法，通过标准化听众评分来学习相对偏好并减少个体评分偏差，最终获得第六名成绩。


<details>
  <summary>Details</summary>
Motivation: XACLE挑战赛旨在解决音频-文本语义对齐自动评估指标的需求，这些指标需要与人类感知相关性高。当前需要能够有效评估音频和文本语义对齐的自动化方法。

Method: 系统基于CLAPScore架构，引入标准化偏好优化(SPO)训练方法：1) 标准化每个听众提供的原始对齐分数；2) 使模型学习相对偏好而非绝对分数；3) 减少个体评分偏差影响。同时采用听众筛选机制，排除评分不一致的听众。

Result: 实验表明SPO和听众筛选都能有效提升与人类判断的相关性。系统在挑战赛中获得第六名，Spearman秩相关系数(SRCC)为0.6142，与排名靠前的系统差距很小，表现出竞争力。

Conclusion: 提出的标准化偏好优化(SPO)方法和听众筛选机制能有效提高音频-文本对齐评估的准确性，系统在XACLE挑战赛中取得有竞争力的结果，代码已开源供进一步研究。

Abstract: The first XACLE Challenge (x-to-audio alignment challenge) addresses the critical need for automatic evaluation metrics that correlate with human perception of audio-text semantic alignment. In this paper, we describe the "Takano_UTokyo_03" system submitted to XACLE Challenge. Our approach leverages a CLAPScore-based architecture integrated with a novel training method called Standardized Preference Optimization (SPO). SPO standardizes the raw alignment scores provided by each listener, enabling the model to learn relative preferences and mitigate the impact of individual scoring biases. Additionally, we employ listener screening to exclude listeners with inconsistent ratings. Experimental evaluations demonstrate that both SPO and listener screening effectively improve the correlation with human judgment. Our system achieved 6th place in the challenge with a Spearman's rank correlation coefficient (SRCC) of 0.6142, demonstrating competitive performance within a marginal gap from the top-ranked systems. The code is available at https://github.com/ttakano398/SPO-CLAPScore.

</details>


### [21] [UniSRCodec: Unified and Low-Bitrate Single Codebook Codec with Sub-Band Reconstruction](https://arxiv.org/abs/2601.02776)
*Zhisheng Zhang,Xiang Li,Yixuan Zhou,Jing Peng,Shengbo Cai,Guoyang Zeng,Zhiyong Wu*

Main category: cs.SD

TL;DR: UniSRCodec是一种单码本神经音频编解码器，支持高采样率、低带宽、高保真度的统一音频压缩，通过Mel谱图压缩和子带重建技术实现高质量音频重建。


<details>
  <summary>Details</summary>
Motivation: 现有神经音频编解码器存在局限性：多码本编解码器结构复杂且难以适应下游任务，单码本编解码器虽然结构简单但保真度低、无法有效建模统一音频、不支持高频音频建模。需要一种既能保持单码本简单结构又能实现高质量压缩的解决方案。

Method: 1. 分析波形压缩的低效性，采用Mel谱图进行时频压缩；2. 配合声码器恢复原始音频的相位信息；3. 提出子带重建技术，实现低频和高频带的高质量压缩；4. 设计单码本架构，支持高采样率音频。

Result: UniSRCodec在仅40个令牌率下，在跨域单码本编解码器中达到最先进性能，重建质量可与某些多码本方法相媲美。主观和客观实验结果均验证了其优越性。

Conclusion: UniSRCodec成功解决了单码本编解码器的局限性，实现了高采样率、低带宽、高保真度的统一音频压缩，为神经音频编解码器提供了一种结构简单但性能优越的解决方案。

Abstract: Neural Audio Codecs (NACs) can reduce transmission overhead by performing compact compression and reconstruction, which also aim to bridge the gap between continuous and discrete signals. Existing NACs can be divided into two categories: multi-codebook and single-codebook codecs. Multi-codebook codecs face challenges such as structural complexity and difficulty in adapting to downstream tasks, while single-codebook codecs, though structurally simpler, suffer from low-fidelity, ineffective modeling of unified audio, and an inability to support modeling of high-frequency audio. We propose the UniSRCodec, a single-codebook codec capable of supporting high sampling rate, low-bandwidth, high fidelity, and unified. We analyze the inefficiency of waveform-based compression and introduce the time and frequency compression method using the Mel-spectrogram, and cooperate with a Vocoder to recover the phase information of the original audio. Moreover, we propose a sub-band reconstruction technique to achieve high-quality compression across both low and high frequency bands. Subjective and objective experimental results demonstrate that UniSRCodec achieves state-of-the-art (SOTA) performance among cross-domain single-codebook codecs with only a token rate of 40, and its reconstruction quality is comparable to that of certain multi-codebook methods. Our demo page is available at https://wxzyd123.github.io/unisrcodec.

</details>


### [22] [MoE Adapter for Large Audio Language Models: Sparsity, Disentanglement, and Gradient-Conflict-Free](https://arxiv.org/abs/2601.02967)
*Yishu Lei,Shuwei He,Jing Hu,Dan Zhang,Xianlong Luo,Danxiang Zhu,Shikun Feng,Rui Liu,Jingzhou He,Yu Sun,Hua Wu,Haifeng Wang*

Main category: cs.SD

TL;DR: MoE-Adapter：一种稀疏混合专家架构，用于解耦音频信息，通过动态门控机制将音频token路由到专门专家，缓解梯度冲突，在音频语义和副语言任务上优于密集线性基线。


<details>
  <summary>Details</summary>
Motivation: 音频信息本质上是异质的，包含语音、音乐、环境等多种属性。现有研究使用密集参数共享适配器建模这些不同模式，导致优化过程中出现梯度冲突，因为不同属性所需的参数更新相互矛盾。

Method: 提出MoE-Adapter，一种稀疏混合专家架构，采用动态门控机制将音频token路由到捕获互补特征子空间的专门专家，同时保留共享专家用于全局上下文，从而缓解梯度冲突并实现细粒度特征学习。

Result: 在音频语义和副语言任务上，MoE-Adapter均取得优越性能，在计算成本相当的情况下持续优于密集线性基线。

Conclusion: MoE-Adapter通过稀疏专家架构有效解耦异质音频信息，缓解梯度冲突问题，为音频多模态感知提供了更优的适配器设计。作者将发布相关代码和模型以促进未来研究。

Abstract: Extending the input modality of Large Language Models~(LLMs) to the audio domain is essential for achieving comprehensive multimodal perception. However, it is well-known that acoustic information is intrinsically \textit{heterogeneous}, entangling attributes such as speech, music, and environmental context. Existing research is limited to a dense, parameter-shared adapter to model these diverse patterns, which induces \textit{gradient conflict} during optimization, as parameter updates required for distinct attributes contradict each other. To address this limitation, we introduce the \textit{\textbf{MoE-Adapter}}, a sparse Mixture-of-Experts~(MoE) architecture designed to decouple acoustic information. Specifically, it employs a dynamic gating mechanism that routes audio tokens to specialized experts capturing complementary feature subspaces while retaining shared experts for global context, thereby mitigating gradient conflicts and enabling fine-grained feature learning. Comprehensive experiments show that the MoE-Adapter achieves superior performance on both audio semantic and paralinguistic tasks, consistently outperforming dense linear baselines with comparable computational costs. Furthermore, we will release the related code and models to facilitate future research.

</details>


### [23] [Vulnerabilities of Audio-Based Biometric Authentication Systems Against Deepfake Speech Synthesis](https://arxiv.org/abs/2601.02914)
*Mengze Hong,Di Jiang,Zeying Xie,Weiwei Zhao,Guan Wang,Chen Jason Zhang*

Main category: cs.SD

TL;DR: 语音合成技术快速发展，现代语音克隆模型仅需少量样本即可绕过商业说话人验证系统，反欺骗检测器在不同音频合成方法间泛化能力差，暴露出严重安全漏洞。


<details>
  <summary>Details</summary>
Motivation: 随着音频深度伪造技术从研究走向商业应用，高风险行业中的生物特征认证面临紧迫安全威胁。需要系统评估现有说话人认证系统在语音合成攻击下的安全性。

Method: 基于大规模语音合成数据集，对最先进的说话人认证系统进行系统性实证评估，分析语音克隆模型和反欺骗检测器的安全漏洞。

Result: 发现两大安全漏洞：1) 现代语音克隆模型仅需极少样本即可成功绕过商业说话人验证系统；2) 反欺骗检测器在不同音频合成方法间泛化能力差，实际场景性能与实验室性能存在显著差距。

Conclusion: 现有安全措施需要重新评估，亟需架构创新、自适应防御机制，并应向多因素认证过渡以应对音频深度伪造威胁。

Abstract: As audio deepfakes transition from research artifacts to widely available commercial tools, robust biometric authentication faces pressing security threats in high-stakes industries. This paper presents a systematic empirical evaluation of state-of-the-art speaker authentication systems based on a large-scale speech synthesis dataset, revealing two major security vulnerabilities: 1) modern voice cloning models trained on very small samples can easily bypass commercial speaker verification systems; and 2) anti-spoofing detectors struggle to generalize across different methods of audio synthesis, leading to a significant gap between in-domain performance and real-world robustness. These findings call for a reconsideration of security measures and stress the need for architectural innovations, adaptive defenses, and the transition towards multi-factor authentication.

</details>


### [24] [The World is Not Mono: Enabling Spatial Understanding in Large Audio-Language Models](https://arxiv.org/abs/2601.02954)
*Yuhuan You,Lai Wei,Xihong Wu,Tianshu Qu*

Main category: cs.SD

TL;DR: 该论文提出了一个分层听觉场景分析框架，通过构建大规模合成双耳音频数据集、设计混合特征投影器和渐进式训练课程，使大型音频语言模型具备空间感知能力，从"单声道"语义识别提升到空间智能。


<details>
  <summary>Details</summary>
Motivation: 现有大型音频语言模型将世界感知为"单声道"——忽略空间维度（"哪里"）的单一音频流，而空间维度是通用声学场景分析的关键。为了弥补这一差距，需要让模型理解和推理复杂的声学世界。

Method: 1. 构建大规模合成双耳音频数据集提供丰富的空间线索；2. 设计混合特征投影器，使用并行语义和空间编码器提取解耦表示，通过密集融合机制整合；3. 采用渐进式训练课程，从监督微调（SFT）到通过组相对策略优化（GRPO）的强化学习，显式发展模型的推理能力。

Result: 在综合基准测试中，模型展现出相对较强的空间理解能力。通过启用空间感知，为利用大型模型的强大推理能力进行整体声学场景分析提供了清晰路径。

Conclusion: 该工作通过分层听觉场景分析框架，使模型能够理解和推理复杂的声学世界，从"单声道"语义识别推进到空间智能，为整体声学场景分析提供了新途径。

Abstract: Existing large audio-language models perceive the world as "mono" -- a single stream of audio that ignores the critical spatial dimension ("where") required for universal acoustic scene analysis. To bridge this gap, we first introduce a hierarchical framework for Auditory Scene Analysis (ASA). Guided by this framework, we introduce a system that enables models like Qwen2-Audio to understand and reason about the complex acoustic world. Our framework achieves this through three core contributions: First, we build a large-scale, synthesized binaural audio dataset to provide the rich spatial cues. Second, we design a hybrid feature projector, which leverages parallel semantic and spatial encoders to extract decoupled representations. These distinct streams are integrated via a dense fusion mechanism, ensuring the model receives a holistic view of the acoustic scene. Finally, we employ a progressive training curriculum, advancing from supervised fine-tuning (SFT) to reinforcement learning via Group Relative Policy Optimization (GRPO), to explicitly evolve the model's capabilities towards reasoning. On our comprehensive benchmark, the model demonstrates comparatively strong capability for spatial understanding. By enabling this spatial perception, our work provides a clear pathway for leveraging the powerful reasoning abilities of large models towards holistic acoustic scene analysis, advancing from "mono" semantic recognition to spatial intelligence.

</details>


### [25] [Interpretable All-Type Audio Deepfake Detection with Audio LLMs via Frequency-Time Reinforcement Learning](https://arxiv.org/abs/2601.02983)
*Yuankun Xie,Xiaoxuan Guo,Jiayi Zhou,Tao Wang,Jian Liu,Ruibo Fu,Xiaopeng Wang,Haonan Cheng,Long Ye*

Main category: cs.SD

TL;DR: 提出FT-GRPO方法，通过频率-时间结构化思维链和两阶段训练，实现跨音频类型的深度伪造检测，同时保持可解释性。


<details>
  <summary>Details</summary>
Motivation: 音频大语言模型使高质量合成音频广泛可用，增加了恶意音频深度伪造的风险。现实世界的音频深度伪造检测需要能够跨异构音频类型泛化并提供可解释决策的检测器。

Method: 1. 提出自动标注和精炼管道，构建频率-时间结构化思维链推理，生成约34万冷启动演示数据；2. 提出频率时间-组相对策略优化（FT-GRPO），采用两阶段训练：先用监督微调冷启动模型，然后在基于规则的频率-时间约束下应用GRPO。

Result: FT-GRPO在全类型音频深度伪造检测上达到最先进性能，同时产生可解释的、基于频率-时间基础的推理过程。

Conclusion: 通过结合结构化思维链和约束强化学习，FT-GRPO实现了高性能且可解释的全类型音频深度伪造检测，解决了传统方法在可解释性和泛化能力方面的局限性。

Abstract: Recent advances in audio large language models (ALLMs) have made high-quality synthetic audio widely accessible, increasing the risk of malicious audio deepfakes across speech, environmental sounds, singing voice, and music. Real-world audio deepfake detection (ADD) therefore requires all-type detectors that generalize across heterogeneous audio and provide interpretable decisions. Given the strong multi-task generalization ability of ALLMs, we first investigate their performance on all-type ADD under both supervised fine-tuning (SFT) and reinforcement fine-tuning (RFT). However, SFT using only binary real/fake labels tends to reduce the model to a black-box classifier, sacrificing interpretability. Meanwhile, vanilla RFT under sparse supervision is prone to reward hacking and can produce hallucinated, ungrounded rationales. To address this, we propose an automatic annotation and polishing pipeline that constructs Frequency-Time structured chain-of-thought (CoT) rationales, producing ~340K cold-start demonstrations. Building on CoT data, we propose Frequency Time-Group Relative Policy Optimization (FT-GRPO), a two-stage training paradigm that cold-starts ALLMs with SFT and then applies GRPO under rule-based frequency-time constraints. Experiments demonstrate that FT-GRPO achieves state-of-the-art performance on all-type ADD while producing interpretable, FT-grounded rationales. The data and code are available online.

</details>


### [26] [Segment-Aware Conditioning for Training-Free Intra-Utterance Emotion and Duration Control in Text-to-Speech](https://arxiv.org/abs/2601.03170)
*Qifan Liang,Yuansen Liu,Ruixin Wei,Nan Lu,Junchuan Zhao,Ye Wang*

Main category: cs.SD

TL;DR: 提出无需训练的零样本TTS可控框架，实现语句内情感和时长表达控制，通过分段感知的情感调节和时长引导策略，结合LLM自动提示构建，在保持语音质量的同时达到最先进的语句内一致性控制。


<details>
  <summary>Details</summary>
Motivation: 现有可控TTS方法大多局限于语句间控制，难以实现细粒度的语句内表达控制，且依赖非公开数据集或复杂的多阶段训练流程。

Method: 提出分段感知的情感调节策略（结合因果掩码和单调流对齐过滤）和分段感知的时长引导策略（结合局部时长嵌入引导和全局EOS对数调制），并构建3万样本的多情感时长标注数据集用于LLM自动提示构建。

Result: 实验表明该方法在无需训练的情况下，在多情感和时长控制方面达到了最先进的语句内一致性，同时保持了基础TTS模型的语音质量水平。

Conclusion: 该训练免费框架成功实现了零样本TTS的细粒度语句内情感和时长表达控制，解决了现有方法在语句内控制方面的局限性。

Abstract: While controllable Text-to-Speech (TTS) has achieved notable progress, most existing methods remain limited to inter-utterance-level control, making fine-grained intra-utterance expression challenging due to their reliance on non-public datasets or complex multi-stage training. In this paper, we propose a training-free controllable framework for pretrained zero-shot TTS to enable intra-utterance emotion and duration expression. Specifically, we propose a segment-aware emotion conditioning strategy that combines causal masking with monotonic stream alignment filtering to isolate emotion conditioning and schedule mask transitions, enabling smooth intra-utterance emotion shifts while preserving global semantic coherence. Based on this, we further propose a segment-aware duration steering strategy to combine local duration embedding steering with global EOS logit modulation, allowing local duration adjustment while ensuring globally consistent termination. To eliminate the need for segment-level manual prompt engineering, we construct a 30,000-sample multi-emotion and duration-annotated text dataset to enable LLM-based automatic prompt construction. Extensive experiments demonstrate that our training-free method not only achieves state-of-the-art intra-utterance consistency in multi-emotion and duration control, but also maintains baseline-level speech quality of the underlying TTS model. Audio samples are available at https://aclanonymous111.github.io/TED-TTS-DemoPage/.

</details>


### [27] [The Sonar Moment: Benchmarking Audio-Language Models in Audio Geo-Localization](https://arxiv.org/abs/2601.03227)
*Ruixing Zhang,Zihan Liu,Leilei Sun,Tongyu Zhu,Weifeng Lv*

Main category: cs.SD

TL;DR: 提出了首个音频地理定位基准AGL1K，包含72个国家/地区的1444个音频片段，评估了16个音频语言模型的地理定位能力，发现闭源模型优于开源模型，语言线索主导预测。


<details>
  <summary>Details</summary>
Motivation: 音频地理定位在计算机视觉中已有基准，但音频领域缺乏高质量音频-位置配对数据，限制了该领域的发展。

Method: 1) 创建AGL1K基准，涵盖72个国家/地区；2) 提出音频可定位性指标筛选可靠样本；3) 评估16个音频语言模型的地理定位能力。

Result: 1) 音频语言模型已具备音频地理定位能力；2) 闭源模型显著优于开源模型；3) 语言线索常作为预测的主要依据；4) 分析了模型的推理轨迹、区域偏见、错误原因和可定位性指标的可解释性。

Conclusion: AGL1K为音频地理定位建立了首个基准，有望推动音频语言模型在空间推理能力方面的进步。

Abstract: Geo-localization aims to infer the geographic origin of a given signal. In computer vision, geo-localization has served as a demanding benchmark for compositional reasoning and is relevant to public safety. In contrast, progress on audio geo-localization has been constrained by the lack of high-quality audio-location pairs. To address this gap, we introduce AGL1K, the first audio geo-localization benchmark for audio language models (ALMs), spanning 72 countries and territories. To extract reliably localizable samples from a crowd-sourced platform, we propose the Audio Localizability metric that quantifies the informativeness of each recording, yielding 1,444 curated audio clips. Evaluations on 16 ALMs show that ALMs have emerged with audio geo-localization capability. We find that closed-source models substantially outperform open-source models, and that linguistic clues often dominate as a scaffold for prediction. We further analyze ALMs' reasoning traces, regional bias, error causes, and the interpretability of the localizability metric. Overall, AGL1K establishes a benchmark for audio geo-localization and may advance ALMs with better geospatial reasoning capability.

</details>

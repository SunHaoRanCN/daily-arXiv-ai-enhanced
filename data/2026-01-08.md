<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 12]
- [eess.AS](#eess.AS) [Total: 5]
- [cs.SD](#cs.SD) [Total: 5]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [SEP Analysis of a Low-Resolution SIMO System with M-PSK over Fading Channels](https://arxiv.org/abs/2601.03387)
*Amila Ravinath,Minhua Ding,Bikshapathi Gouda,Italo Atzeni,Antti Tölli*

Main category: eess.SP

TL;DR: 分析了相位量化SIMO系统在瑞利衰落和AWGN下的平均符号错误概率，推导了QPSK调制的精确SEP表达式，揭示了SIMO-MRC与相位量化MISO系统之间的对偶性，并扩展到有限CSI情况。


<details>
  <summary>Details</summary>
Motivation: 研究相位量化SIMO系统在M-PSK调制下的性能分析，特别是在瑞利衰落和加性高斯白噪声信道条件下，需要推导精确的SEP表达式并理解系统的分集和编码增益特性。

Method: 采用新颖的分析方法，推导QPSK调制的n位相位量化SIMO-MRC系统的精确SEP表达式；对于2位相位量化SIMO选择合并系统，获得任意接收天线数下的分集和编码增益；揭示SIMO-MRC与相位量化MISO系统之间的对偶性；扩展到有限CSI情况。

Result: 获得了QPSK调制n位相位量化SIMO-MRC系统的精确SEP表达式及高SNR下的分集和编码增益特性；发现了SIMO-MRC与相位量化MISO系统之间的对偶关系；在有限CSI情况下，分集增益进一步减半。

Conclusion: 提出的分析方法成功推导了相位量化SIMO系统的精确SEP表达式，揭示了系统间的对偶关系，为理解相位量化通信系统的性能提供了理论框架，并扩展到实际有限CSI场景。

Abstract: In this paper, the average symbol error probability (SEP) of a phase-quantized single-input multiple-output (SIMO) system with M-ary phase-shift keying (PSK) modulation is analyzed under Rayleigh fading and additive white Gaussian noise. By leveraging a novel method, we derive exact SEP expressions for a quadrature PSK (QPSK)-modulated n-bit phase-quantized SIMO system with maximum ratio combining (SIMO-MRC), along with the corresponding high signal-to-noise ratio (SNR) characterizations in terms of diversity and coding gains. For a QPSK-modulated 2-bit phase-quantized SIMO system with selection combining, the diversity and coding gains are further obtained for an arbitrary number of receive antennas, complementing existing results. Interestingly, the proposed method also reveals a duality between a SIMO-MRC system and a phase-quantized multiple-input single-output (MISO) system with maximum ratio transmission, when the modulation order, phase-quantization resolution, antenna configuration, and the channel state information (CSI) conditions are reciprocal. This duality enables direct inference to obtain the diversity of a general M-PSK-modulated n-bit phase-quantized SIMO-MRC system, and extends the results to its MISO counterpart. All the above results have been obtained assuming perfect CSI at the receiver (CSIR). Finally, the SEP analysis of a QPSK-modulated 2-bit phase-quantized SIMO system is extended to the limited CSIR case, where the CSI at each receive antenna is represented by only 2 bits of channel phase information. In this scenario, the diversity gain is shown to be further halved in general.

</details>


### [2] [Foundation Model-Aided Hierarchical Control for Robust RIS-Assisted Near-Field Communications](https://arxiv.org/abs/2601.03427)
*Mohammad Ghassemi,Han Zhang,Ali Afana,Akram Bin Sediq,Melike Erol-Kantarci*

Main category: eess.SP

TL;DR: 提出DT-HDRL框架，通过双变压器模型处理不同时间尺度的CSI估计和遮挡预测，在6G ELAA近场通信中提升频谱效率


<details>
  <summary>Details</summary>
Motivation: 6G网络中部署极大孔径阵列(ELAA)使通信进入近场区域，信号呈现球面波传播。RIS可支持近场波束聚焦，但需要快速CSI估计和遮挡缓解，这两者在不同时间尺度上变化，传统单级控制算法难以应对

Method: 提出双变压器分层框架(DT-HDRL)：快速时间尺度变压器处理射线追踪数据进行CSI估计；视觉变压器(ViT)分析视觉数据预测遮挡。HDRL中高层控制器选择LoS或RIS辅助NLoS传输路径，低层控制器优化BS波束聚焦和RIS相位偏移

Result: 仿真结果显示，相比单时间尺度基线方法，频谱效率提升约18%；遮挡预测器F1分数达到0.92，在动态场景中提供769毫秒的提前预警窗口

Conclusion: DT-HDRL框架有效协调不同时间尺度的控制任务，在动态条件下最大化频谱效率并确保鲁棒性能，为6G近场通信中的RIS应用提供了有效解决方案

Abstract: The deployment of extremely large aperture arrays (ELAAs) in sixth-generation (6G) networks could shift communication into the near-field communication (NFC) regime. In this regime, signals exhibit spherical wave propagation, unlike the planar waves in conventional far-field systems. Reconfigurable intelligent surfaces (RISs) can dynamically adjust phase shifts to support NFC beamfocusing, concentrating signal energy at specific spatial coordinates. However, effective RIS utilization depends on both rapid channel state information (CSI) estimation and proactive blockage mitigation, which occur on inherently different timescales. CSI varies at millisecond intervals due to small-scale fading, while blockage events evolve over seconds, posing challenges for conventional single-level control algorithms. To address this issue, we propose a dual-transformer (DT) hierarchical framework that integrates two specialized transformer models within a hierarchical deep reinforcement learning (HDRL) architecture, referred to as the DT-HDRL framework. A fast-timescale transformer processes ray-tracing data for rapid CSI estimation, while a vision transformer (ViT) analyzes visual data to predict impending blockages. In HDRL, the high-level controller selects line-of-sight (LoS) or RIS-assisted non-line-of-sight (NLoS) transmission paths and sets goals, while the low-level controller optimizes base station (BS) beamfocusing and RIS phase shifts using instantaneous CSI. This dual-timescale coordination maximizes spectral efficiency (SE) while ensuring robust performance under dynamic conditions. Simulation results demonstrate that our approach improves SE by approximately 18% compared to single-timescale baselines, while the proposed blockage predictor achieves an F1-score of 0.92, providing a 769 ms advance warning window in dynamic scenarios.

</details>


### [3] [Energy Harvesting in High Altitude Platform Station Enabled Sensor Networks](https://arxiv.org/abs/2601.03446)
*Melek Tuylu,Eylem Erdogan*

Main category: eess.SP

TL;DR: HAPS系统在VHetNet架构中面临能耗挑战，本文提出一种HAPS间的能量收集策略，通过分析中断概率、遍历容量和吞吐量验证其可行性。


<details>
  <summary>Details</summary>
Motivation: 高空平台站(HAPS)系统在未来无线通信网络中扮演关键角色，但作为垂直异构网络(VHetNet)核心组件时面临严重能耗约束，需要解决其能源供应问题。

Method: 提出HAPS间的能量收集策略，使受能量限制的HAPS能从不受能量限制的HAPS收集能量，通过推导中断概率、遍历容量理论公式，并用蒙特卡洛模拟验证。

Result: 分析结果显示，能量收集策略能有效满足HAPS系统的能源需求，通过理论推导和仿真验证了系统性能指标，并探讨了吞吐量表现。

Conclusion: 充分利用能量收集潜力是满足HAPS系统能源需求的可行途径，为VHetNet架构中的HAPS集成提供了能源解决方案。

Abstract: High altitude platform station (HAPS) systems are becoming crucial facilitators for future wireless communication networks, enhancing connectivity across all vertical communication layers, including small Internet of Things (IoT) sensors and devices, terrestrial users, and aerial devices. In the context of the widely recognized vertical heterogeneous network (VHetNet) architecture, HAPS systems can provide service to both aerial and ground users. However, integrating HAPS systems as a core element in the VHetNet architecture presents a considerable energy challenge, marking a prominent constraint for their operation. Driven by this challenge, we introduce an energy harvesting (EH) strategy tailored for HAPS systems, enabling a HAPS system to gather energy from another HAPS system, which is not constrained by energy limitations. To assess the performance capabilities of the proposed model, we derive outage probability (OP), ergodic capacity (EC) and verify them by using Monte Carlo (MC) simulations. Moreover, we explore the system in terms of throughput. The findings reveal that harnessing full potential of EH stands as a viable approach to meet the energy demands of HAPS systems.

</details>


### [4] [Intensity Fluctuation Dynamics in XPM](https://arxiv.org/abs/2601.03527)
*Ravneel Prasad,Emanuele Viterbo*

Main category: eess.SP

TL;DR: 本文提出了一种增强型XPM模型，明确考虑光纤中频域强度波动增长，改进了先前主要关注时域脉冲形变的模型，能更准确预测XPM相位失真和系统性能。


<details>
  <summary>Details</summary>
Motivation: 在高容量WDM系统中，交叉相位调制(XPM)是关键的非线性损伤，主要由色散引起的强度波动驱动。现有模型主要关注时域脉冲形变，未能充分描述频域强度波动增长对XPM的影响，需要更精确的建模方法。

Method: 提出增强型XPM模型，明确纳入光纤中频域强度波动增长，建立频域增长与XPM诱导相位失真之间的直接关联，通过仿真验证模型准确性，并推导相位方差来预测系统性能。

Result: 结果显示强度波动演化（特别是低频部分）显著影响XPM相位波动频谱和相位方差。模型能准确预测不同系统参数下的频谱特性，推导的相位方差能精确预测系统误码率性能。

Conclusion: 为准确表征XPM损伤，必须建模频域强度波动演化。该模型为先进光网络设计提供指导，强调了频域分析在XPM建模中的重要性。

Abstract: Cross-Phase Modulation (XPM) constitutes a critical nonlinear impairment in high-capacity Wavelength Division Multiplexing (WDM) systems, significantly driven by intensity fluctuations (IFs) that evolve due to chromatic dispersion. This paper presents an enhanced XPM model that explicitly incorporates frequency-domain IF growth along the fiber, improving upon prior models that focused primarily on temporal pulse deformation. A direct correlation between this frequency-domain growth and XPM-induced phase distortions is established and analyzed. Results demonstrate that IF evolution, particularly at lower frequencies, profoundly affects XPM phase fluctuation spectra and phase variance. Validated through simulations, the model accurately predicts these spectral characteristics across various system parameters. Furthermore, the derived phase variance enables accurate prediction of system performance in terms of Bit Error Ratio (BER). These findings highlight the necessity of modeling frequency-domain IF evolution to accurately characterize XPM impairments, offering guidance for the design of advanced optical networks.

</details>


### [5] [ASVspoof 5: Evaluation of Spoofing, Deepfake, and Adversarial Attack Detection Using Crowdsourced Speech](https://arxiv.org/abs/2601.03944)
*Xin Wang,Héctor Delgado,Nicholas Evans,Xuechen Liu,Tomi Kinnunen,Hemlata Tak,Kong Aik Lee,Ivan Kukanov,Md Sahidullah,Massimiliano Todisco,Junichi Yamagishi*

Main category: eess.SP

TL;DR: ASVspoof 5挑战赛是第五版语音欺骗检测竞赛，使用新的众包数据库，包含更多说话者和多样录音条件，评估了53个团队的方案，发现性能在对抗攻击和神经编码/压缩下会下降。


<details>
  <summary>Details</summary>
Motivation: 推动语音欺骗和深度伪造检测技术的研究，通过挑战赛形式促进该领域发展，使用更真实、多样化的数据集来评估现有方法。

Method: 创建新的众包数据库，包含更多说话者和多样录音条件，混合前沿和传统生成语音技术，组织挑战赛收集53个团队的解决方案进行评估。

Result: 许多解决方案表现良好，但在对抗攻击和神经编码/压缩方案应用下性能会下降，挑战赛结果显示了当前方法的局限性。

Conclusion: ASVspoof 5挑战赛展示了语音欺骗检测的进展和挑战，提出了未来研究方向路线图，包括校准研究和其他主要挑战。

Abstract: ASVspoof 5 is the fifth edition in a series of challenges which promote the study of speech spoofing and deepfake detection solutions. A significant change from previous challenge editions is a new crowdsourced database collected from a substantially greater number of speakers under diverse recording conditions, and a mix of cutting-edge and legacy generative speech technology. With the new database described elsewhere, we provide in this paper an overview of the ASVspoof 5 challenge results for the submissions of 53 participating teams. While many solutions perform well, performance degrades under adversarial attacks and the application of neural encoding/compression schemes. Together with a review of post-challenge results, we also report a study of calibration in addition to other principal challenges and outline a road-map for the future of ASVspoof.

</details>


### [6] [OpenISAC: An Open-Source Real-Time Experimentation Platform for OFDM-ISAC with Over-the-Air Synchronization](https://arxiv.org/abs/2601.03535)
*Zhiwen Zhou,Chaoyue Zhang,Xiaoli Xu,Yong Zeng*

Main category: eess.SP

TL;DR: OpenISAC是一个开源实时ISAC实验平台，采用OFDM波形，支持单站和双站时延-多普勒感知，无需节点间有线连接的OTA同步机制，基于USRP硬件和开源软件构建。


<details>
  <summary>Details</summary>
Motivation: ISAC是6G的关键应用场景，但理论研究进展受限于缺乏可访问、开源、实时的实验平台，阻碍了ISAC的进一步发展。

Method: 构建基于OFDM波形的开源平台，实现单站和双站时延-多普勒感知功能，采用创新的OTA同步机制支持稳健的双站操作，物理层调制解调器用C++实现高速处理，感知数据流式传输到Python环境提供用户友好接口。

Result: 开发出OpenISAC平台，支持从低成本USRP B200系列到高性能X400系列的多种软件定义无线电，无需商业许可证，提供灵活参数选择和实时通信感知操作。

Conclusion: OpenISAC作为强大且可访问的工具，为学术界和研究社区探索和创新OFDM-ISAC领域提供了有力支持。

Abstract: Integrated sensing and communication (ISAC) is envisioned to be one of the key usage scenarios for the sixth generation (6G) mobile communication networks. While significant progresses have been achieved for the theoretical studies, the further advancement of ISAC is hampered by the lack of accessible, open-source, and real-time experimental platforms. To address this gap, we introduce OpenISAC, a versatile and high-performance open-source platform for real-time ISAC experimentation. OpenISAC utilizes orthogonal frequency division multiplexing (OFDM) waveform and implements crucial sensing functionalities, including both monostatic and bistatic delay-Doppler sensing. A key feature of our platform is a novel over-the-air (OTA) synchronization mechanism that enables robust bistatic operations without requiring a wired connection between nodes. The platform is built entirely on open-source software, leveraging the universal software radio peripheral (USRP) hardware driver (UHD) library, thus eliminating the need for any commercial licenses. It supports a wide range of software-defined radios, from the cost-effective USRP B200 series to the high-performance X400 series. The physical layer modulator and demodulator are implemented with C++ for high-speed processing, while the sensing data is streamed to a Python environment, providing a user-friendly interface for rapid prototyping and validation of sensing signal processing algorithms. With flexible parameter selection and real-time communication and sensing operation, OpenISAC serves as a powerful and accessible tool for the academic and research communities to explore and innovate within the field of OFDM-ISAC.

</details>


### [7] [F$^4$-CKM: Learning Channel Knowledge Map with Radio Frequency Radiance Field Rendering](https://arxiv.org/abs/2601.03601)
*Kequan Zhou,Guangyi Zhang,Hanlei Li,Yunlong Cai,Shengli Liu,Guanding Yu*

Main category: eess.SP

TL;DR: F⁴-CKM：一种基于辐射场渲染的6G信道知识地图构建框架，通过无线辐射表示网络捕捉空间-频率特性，实现高效信道预测


<details>
  <summary>Details</summary>
Motivation: 6G移动通信中，天线阵列规模和带宽增长使得获取准确及时的信道状态信息(CSI)愈发困难。传统CSI反馈负担重，信道知识地图(CKM)通过环境感知技术仅基于用户位置预测CSI，但如何有效构建CKM仍是一个开放问题。

Method: 提出F⁴-CKM框架，具有四个特征：辐射场渲染、空间-频率感知、位置无关使用、快速学习。核心是将计算机视觉中的辐射场渲染技术适配到射频域，通过新型无线辐射表示(WiRARE)网络捕捉无线信道的空间-频率特性。还引入了新型整形滤波模块和角度采样策略。

Result: 大量实验表明，F⁴-CKM在无线信道预测准确性和效率方面显著优于现有基线方法。

Conclusion: F⁴-CKM为6G信道知识地图构建提供了一种创新框架，通过辐射场渲染技术和空间-频率感知网络，有效解决了大规模天线系统下的信道预测问题，具有重要的实际应用价值。

Abstract: In 6G mobile communications, acquiring accurate and timely channel state information (CSI) becomes increasingly challenging due to the growing antenna array size and bandwidth. To alleviate the CSI feedback burden, the channel knowledge map (CKM) has emerged as a promising approach by leveraging environment-aware techniques to predict CSI based solely on user locations. However, how to effectively construct a CKM remains an open issue. In this paper, we propose F$^4$-CKM, a novel CKM construction framework characterized by four distinctive features: radiance Field rendering, spatial-Frequency-awareness, location-Free usage, and Fast learning. Central to our design is the adaptation of radiance field rendering techniques from computer vision to the radio frequency (RF) domain, enabled by a novel Wireless Radiator Representation (WiRARE) network that captures the spatial-frequency characteristics of wireless channels. Additionally, a novel shaping filter module and an angular sampling strategy are introduced to facilitate CKM construction. Extensive experiments demonstrate that F$^4$-CKM significantly outperforms existing baselines in terms of wireless channel prediction accuracy and efficiency.

</details>


### [8] [Zak-OTFS ISAC with Bistatic Sensing via Semi-Blind Atomic Norm Denoising Scheme](https://arxiv.org/abs/2601.03639)
*Kecheng Zhang,Weijie Yuan,Maria Sabrina Greco*

Main category: eess.SP

TL;DR: 提出了一种用于Zak-OTFS ISAC系统的半盲原子范数去噪方案，通过原子范数优化实现双基地感知中的高精度信道估计和数据检测


<details>
  <summary>Details</summary>
Motivation: 在双弥散环境中，分数延迟-多普勒偏移导致严重的信道扩展，使得精确的信道估计成为Zak-OTFS ISAC系统实现准确双基地感知和鲁棒通信的关键挑战

Method: 首先推导分数延迟-多普勒偏移和矩形窗下Zak-OTFS的离散时间输入输出关系，然后将联合信道参数估计和数据检测任务表述为原子范数去噪问题，使用负平方惩罚方法处理非凸离散星座约束，最后开发了结合主化最小化、加速投影梯度和非精确加速近端梯度方法的迭代算法

Result: 仿真结果表明，所提方案实现了超分辨率感知精度，通信性能接近完美信道状态信息下界

Conclusion: 提出的半盲原子范数去噪方案有效解决了Zak-OTFS ISAC系统在双弥散环境中的信道估计难题，为高移动性场景下的集成感知与通信提供了有前景的解决方案

Abstract: Integrated sensing and communication (ISAC) through Zak-transform-based orthogonal time frequency space (Zak-OTFS) modulation is a promising solution for high-mobility scenarios. Realizing accurate bistatic sensing and robust communication necessitates precise channel estimation; however, this remains a formidable challenge in doubly dispersive environments, where fractional delay-Doppler shifts induce severe channel spreading. This paper proposes a semi-blind atomic norm denoising scheme for Zak-OTFS ISAC with bistatic sensing. We first derive the discrete-time input-output (I/O) relationship of Zak-OTFS under fractional delay-Doppler shifts and rectangular windowing. Based on this I/O relation, we formulate the joint channel parameter estimation and data detection task as an atomic norm denoising problem, utilizing the negative square penalty method to handle the non-convex discrete constellation constraints. To solve this problem efficiently, we develop an accelerated iterative algorithm that integrates majorization-minimization, accelerated projected gradient, and inexact accelerated proximal gradient methods. We provide a rigorous convergence proof for the proposed algorithm. Simulation results demonstrate that the proposed scheme achieves super-resolution sensing accuracy and communication performance approaching the perfect channel state information lower bound.

</details>


### [9] [Cramer-Rao Bound for Angle of Arrival Estimates in True-Time-Delay Systems](https://arxiv.org/abs/2601.03735)
*Carl Collmann,Ahmad Nimr,Gerhard Fettweis*

Main category: eess.SP

TL;DR: 该论文推导了典型TTD系统中角度估计的克拉美-罗下界(CRB)，研究了Fisher信息的特性，并利用ML等估计方法解决ULA的角度估计问题。


<details>
  <summary>Details</summary>
Motivation: 在联合通信与感知(JC&S)背景下，获取准确的参数估计（如AoA）对于解决初始接入、干扰抑制、用户定位、环境监测和MIMO系统同步等问题至关重要。TTD系统因其在快速波束训练和波束斜视抑制方面的优势而受到关注。

Method: 推导了典型TTD系统中角度估计的克拉美-罗下界(CRB)，研究了Fisher信息的特性并进行数值评估。使用最大似然(ML)和已有估计方法来解决均匀线性阵列的角度估计问题。

Result: 论文得出了TTD系统中角度估计的理论性能界限（CRB），分析了Fisher信息的性质，并通过数值评估验证了理论结果。展示了ML等估计方法在均匀线性阵列上的应用效果。

Conclusion: 该研究为TTD系统中的角度估计提供了理论性能界限和分析框架，有助于优化JC&S系统中的参数估计性能，特别是在快速波束训练和波束斜视抑制方面具有应用价值。

Abstract: In the context of joint communication and sensing JC&S, the challenge of obtaining accurate parameter estimates is of interest. Parameter estimates, such as the AoA can be utilized for solving the initial access problem, interference mitigation, localization of users or monitoring of the environment and synchronization of MIMO systems. Recently, TTD systems have gained attention for fast beam training during initial access and mitigation of beam squinting. This work derives the CRB for angle estimates in typical TTD systems. Properties of the CRB and the Fisher information are investigated and numerically evaluated. Finally, methods for angle estimation such as ML and established estimators are utilized to solve the angle estimation problem using a uniform linear array.

</details>


### [10] [Two-stage Multi-beam Training for Multiuser Millimeter-Wave Communications](https://arxiv.org/abs/2601.03745)
*Weijia Wang,Changsheng You,Xiaodan Shao,Rui Zhang*

Main category: eess.SP

TL;DR: 提出一种用于毫米波通信系统的高效多波束训练方法，通过两阶段设计平衡训练开销与波束识别成功率


<details>
  <summary>Details</summary>
Motivation: 传统单波束训练方法依赖穷举搜索，而多波束训练面临训练开销与波束识别成功率之间的权衡挑战，特别是在严重波束间干扰的情况下

Method: 提出两阶段多波束训练方法：第一阶段将天线阵列划分为稀疏子阵列生成高增益多波束识别候选用户角度；第二阶段重新划分为密集子阵列生成灵活转向的宽波束，采用交叉验证方法解决第一阶段剩余的角模糊问题

Result: 数值结果表明，相比现有多波束训练方法，该方法显著提高了波束识别成功率，同时保持甚至减少了所需的波束训练开销

Conclusion: 该方法有效解决了多波束训练中训练开销与识别成功率之间的权衡问题，为毫米波多用户通信系统提供了高效可靠的波束训练解决方案

Abstract: In this letter, we study an efficient multi-beam training method for multiuser millimeter-wave communication systems. Unlike the conventional single-beam training method that relies on exhaustive search, multi-beam training design faces a key challenge in balancing the trade-off between beam training overhead and success beam-identification rate, exacerbated by severe inter-beam interference. To tackle this challenge, we propose a new two-stage multi-beam training method with two distinct multi-beam patterns to enable fast and accurate user angle identification. Specifically, in the first stage, the antenna array is divided into sparse subarrays to generate multiple beams (with high array gains), for identifying candidate user angles. In the second stage, the array is redivided into dense subarrays to generate flexibly steered wide beams, for which a cross-validation method is employed to effectively resolve the remaining angular ambiguity in the first stage. Last, numerical results demonstrate that the proposed method significantly improves the success beam-identification rate compared to existing multi-beam training methods, while retaining or even reducing the required beam training overhead.

</details>


### [11] [CSI-MAE: A Masked Autoencoder-based Channel Foundation Model](https://arxiv.org/abs/2601.03789)
*Jun Jiang,Xiaolong Ruan,Shugong Xu*

Main category: eess.SP

TL;DR: CSI-MAE：基于掩码自编码器的通用信道基础模型，通过CSI感知和生成整合感知与通信，在跨场景任务中表现出色，具备零样本迁移能力


<details>
  <summary>Details</summary>
Motivation: 现有信道基础模型存在三大问题：过度依赖场景特定数据导致泛化能力差、专注于单/双任务、缺乏零样本学习能力。需要开发能够跨场景泛化的通用模型

Method: 提出CSI-MAE模型，基于掩码自编码器架构，在3GPP信道模型数据集上训练，整合CSI感知和生成功能。采用轻量级解码器微调策略降低训练成本

Result: CSI-MAE在匹配或超越监督模型性能的同时，具备出色的零样本迁移能力。通过全参数微调达到SOTA性能，在跨场景应用中与监督技术相媲美

Conclusion: CSI-MAE作为通用信道基础模型，通过自监督学习和掩码自编码器架构解决了现有模型的局限性，推动了无线通信领域的创新

Abstract: Self-Supervised Learning (SSL) has emerged as a key technique in machine learning, tackling challenges such as limited labeled data, high annotation costs, and variable wireless channel conditions. It is essential for developing Channel Foundation Models (CFMs), which extract latent features from channel state information (CSI) and adapt to different wireless settings. Yet, existing CFMs have notable drawbacks: heavy reliance on scenario-specific data hinders generalization, they focus on single/dual tasks, and lack zero-shot learning ability. In this paper, we propose CSI-MAE, a generalized CFM leveraging masked autoencoder for cross-scenario generalization. Trained on 3GPP channel model datasets, it integrates sensing and communication via CSI perception and generation, proven effective across diverse tasks. A lightweight decoder finetuning strategy cuts training costs while maintaining competitive performance. Under this approach, CSI-MAE matches or surpasses supervised models. With full-parameter finetuning, it achieves the state-of-the-art performance. Its exceptional zero-shot transferability also rivals supervised techniques in cross-scenario applications, driving wireless communication innovation.

</details>


### [12] [Hybrid Downlink Beamforming with Outage Constraints under Imperfect CSI using Model-Driven Deep Learning](https://arxiv.org/abs/2601.04069)
*Lukas Schynol,Marius Pesavento*

Main category: eess.SP

TL;DR: 提出基于贪婪选择算法的轻量级模型辅助深度学习架构，用于不完美CSI下的能效多用户混合下行波束成形和功率分配，通过自适应退火近似实现概率中断约束，在CSI误差下达到标称中断水平且分配更少功率。


<details>
  <summary>Details</summary>
Motivation: 传统优化方法在计算上成本高昂，而通用深度网络架构缺乏可解释性且需要大量训练数据。需要一种轻量级、可解释且能处理不完美CSI和概率中断约束的解决方案。

Method: 提出基于贪婪选择算法的模型辅助深度学习架构，通过实例自适应增强信号模型来估计CSI误差影响。采用新颖高效的隐式表示方法处理嵌套约束波束成形问题，并使用自适应退火近似概率中断约束的损失函数。

Result: 仿真验证表明，所提出的深度网络在信道估计和信道压缩导致的CSI误差下能够达到标称中断水平，同时比基准方法分配更少功率。单个训练模型能够泛化到不同用户数、QoS要求和CSI质量水平。

Conclusion: 提出的轻量级模型辅助深度学习架构有效解决了不完美CSI下的能效多用户混合波束成形问题，自适应退火损失函数加速了训练并实现了更好的功率-中断权衡，具有良好泛化能力。

Abstract: We consider energy-efficient multi-user hybrid downlink beamforming (BF) and power allocation under imperfect channel state information (CSI) and probabilistic outage constraints. In this domain, classical optimization methods resort to computationally costly conic optimization problems. Meanwhile, generic deep network (DN) architectures lack interpretability and require large training data sets to generalize well. In this paper, we therefore propose a lightweight model-aided deep learning architecture based on a greedy selection algorithm for analog beam codewords. The architecture relies on an instance-adaptive augmentation of the signal model to estimate the impact of the CSI error. To learn the DN parameters, we derive a novel and efficient implicit representation of the nested constrained BF problem and prove sufficient conditions for the existence of the corresponding gradient. In the loss function, we utilize an annealing-based approximation of the outage compared to conventional quantile-based loss terms. This approximation adaptively anneals towards the exact probabilistic constraint depending on the current level of quality of service (QoS) violation. Simulations validate that the proposed DN can achieve the nominal outage level under CSI error due to channel estimation and channel compression, while allocating less power than benchmarks. Thereby, a single trained model generalizes to different numbers of users, QoS requirements and levels of CSI quality. We further show that the adaptive annealing-based loss function can accelerate the training and yield a better power-outage trade-off.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [13] [Discriminating real and synthetic super-resolved audio samples using embedding-based classifiers](https://arxiv.org/abs/2601.03443)
*Mikhail Silaev,Konstantinos Drossos,Tuomas Virtanen*

Main category: eess.AS

TL;DR: 该论文发现，尽管GAN和扩散模型在音频超分辨率上取得了感知质量上的优异表现，但通过嵌入空间分类器分析显示，真实音频与合成音频在分布上仍存在明显可分离性，揭示了感知质量与真实分布保真度之间的差距。


<details>
  <summary>Details</summary>
Motivation: 现有音频超分辨率模型的评估主要依赖信号级或感知度量，但缺乏对合成超分辨率音频与真实宽带音频分布匹配程度的深入分析。论文旨在填补这一空白，研究生成音频与真实音频在分布上的差异。

Method: 通过分析真实音频与超分辨率音频在各种嵌入空间中的可分离性，考虑中频段（4→16kHz）和全频段（16→48kHz）上采样任务，训练线性分类器基于多种音频嵌入区分真实与合成样本。

Result: 嵌入基分类器实现了近乎完美的分离效果，即使生成的音频具有高感知质量和最先进的度量分数。这种行为在不同数据集和模型（包括最近的扩散方法）中保持一致，突显了感知质量与真实分布保真度之间的持续差距。

Conclusion: 音频超分辨率模型在感知质量上表现出色，但在分布保真度方面仍存在明显不足，表明当前评估指标无法完全捕捉生成音频与真实音频在分布层面的差异。

Abstract: Generative adversarial networks (GANs) and diffusion models have recently achieved state-of-the-art performance in audio super-resolution (ADSR), producing perceptually convincing wideband audio from narrowband inputs. However, existing evaluations primarily rely on signal-level or perceptual metrics, leaving open the question of how closely the distributions of synthetic super-resolved and real wideband audio match. Here we address this problem by analyzing the separability of real and super-resolved audio in various embedding spaces. We consider both middle-band ($4\to 16$~kHz) and full-band ($16\to 48$~kHz) upsampling tasks for speech and music, training linear classifiers to distinguish real from synthetic samples based on multiple types of audio embeddings. Comparisons with objective metrics and subjective listening tests reveal that embedding-based classifiers achieve near-perfect separation, even when the generated audio attains high perceptual quality and state-of-the-art metric scores. This behavior is consistent across datasets and models, including recent diffusion-based approaches, highlighting a persistent gap between perceptual quality and true distributional fidelity in ADSR models.

</details>


### [14] [Learning from Limited Labels: Transductive Graph Label Propagation for Indian Music Analysis](https://arxiv.org/abs/2601.03626)
*Parampreet Singh,Akshay Raina,Sayeedul Islam Sheikh,Vipul Arora*

Main category: eess.AS

TL;DR: 该研究探索使用标签传播（LP）这一基于图的半监督学习技术，自动标注音频和音乐领域的未标记数据，显著降低标注成本并提高标注质量。


<details>
  <summary>Details</summary>
Motivation: 音频和音乐领域缺乏大规模标注数据集，因为标注这些录音资源密集、劳动量大且需要专业知识。需要一种方法来减少标注开销并加速音乐信息检索的进展。

Method: 采用基于图的半监督学习技术——标签传播（LP），通过构建音频嵌入的相似性图，在转导式半监督设置中将有限标签信息从小型标注子集传播到更大的未标注语料库。应用于印度艺术音乐的两个任务：拉格识别和乐器分类，整合多个公共数据集和Prasar Bharati档案馆的额外录音。

Result: 标签传播显著减少了标注开销，相比传统基线方法（包括基于预训练归纳模型的方法）产生了更高质量的标注结果。

Conclusion: 基于图的半监督学习有潜力民主化数据标注并加速音乐信息检索的进展，特别是在资源有限的音频和音乐领域。

Abstract: Supervised machine learning frameworks rely on extensive labeled datasets for robust performance on real-world tasks. However, there is a lack of large annotated datasets in audio and music domains, as annotating such recordings is resource-intensive, laborious, and often require expert domain knowledge. In this work, we explore the use of label propagation (LP), a graph-based semi-supervised learning technique, for automatically labeling the unlabeled set in an unsupervised manner. By constructing a similarity graph over audio embeddings, we propagate limited label information from a small annotated subset to a larger unlabeled corpus in a transductive, semi-supervised setting. We apply this method to two tasks in Indian Art Music (IAM): Raga identification and Instrument classification. For both these tasks, we integrate multiple public datasets along with additional recordings we acquire from Prasar Bharati Archives to perform LP. Our experiments demonstrate that LP significantly reduces labeling overhead and produces higher-quality annotations compared to conventional baseline methods, including those based on pretrained inductive models. These results highlight the potential of graph-based semi-supervised learning to democratize data annotation and accelerate progress in music information retrieval.

</details>


### [15] [ReStyle-TTS: Relative and Continuous Style Control for Zero-Shot Speech Synthesis](https://arxiv.org/abs/2601.03632)
*Haitao Li,Chunxiang Jin,Chenglin Li,Wenhao Guan,Zhengxing Huang,Xie Chen*

Main category: eess.AS

TL;DR: ReStyle-TTS：一个支持连续、相对风格控制的零样本文本转语音框架，通过解耦分类器自由引导减少对参考音频风格的依赖，实现音高、能量和情感的连续控制。


<details>
  <summary>Details</summary>
Motivation: 现有的零样本文本转语音模型虽然能从短参考音频克隆说话人音色，但会强烈继承参考音频中的说话风格。当只有有限或不匹配的参考音频时，合成具有特定风格的语音需要精心选择参考音频，这在实际应用中不切实际。现有可控TTS方法通常依赖绝对风格目标和离散文本提示，不支持连续和相对参考的风格控制。

Method: 提出ReStyle-TTS框架：1）引入解耦分类器自由引导（DCFG），独立控制文本和参考引导，减少对参考风格的依赖同时保持文本保真度；2）应用风格特定的LoRA配合正交LoRA融合，实现连续且解耦的多属性控制；3）引入音色一致性优化模块，缓解因减弱参考引导导致的音色漂移。

Result: 实验表明，ReStyle-TTS能够实现用户友好的连续相对控制，覆盖音高、能量和多种情感，同时保持可懂度和说话人音色。在具有挑战性的不匹配参考-目标风格场景中表现稳健。

Conclusion: ReStyle-TTS成功解决了零样本文本转语音中连续相对风格控制的难题，通过减少模型对参考风格的隐式依赖并引入显式控制机制，实现了灵活的风格调整能力，为实际应用提供了实用解决方案。

Abstract: Zero-shot text-to-speech models can clone a speaker's timbre from a short reference audio, but they also strongly inherit the speaking style present in the reference. As a result, synthesizing speech with a desired style often requires carefully selecting reference audio, which is impractical when only limited or mismatched references are available. While recent controllable TTS methods attempt to address this issue, they typically rely on absolute style targets and discrete textual prompts, and therefore do not support continuous and reference-relative style control. We propose ReStyle-TTS, a framework that enables continuous and reference-relative style control in zero-shot TTS. Our key insight is that effective style control requires first reducing the model's implicit dependence on reference style before introducing explicit control mechanisms. To this end, we introduce Decoupled Classifier-Free Guidance (DCFG), which independently controls text and reference guidance, reducing reliance on reference style while preserving text fidelity. On top of this, we apply style-specific LoRAs together with Orthogonal LoRA Fusion to enable continuous and disentangled multi-attribute control, and introduce a Timbre Consistency Optimization module to mitigate timbre drift caused by weakened reference guidance. Experiments show that ReStyle-TTS enables user-friendly, continuous, and relative control over pitch, energy, and multiple emotions while maintaining intelligibility and speaker timbre, and performs robustly in challenging mismatched reference-target style scenarios.

</details>


### [16] [TellWhisper: Tell Whisper Who Speaks When](https://arxiv.org/abs/2601.03712)
*Yifan Hu,Peiji Yang,Zhisheng Wang,Yicheng Zhong,Rui Liu*

Main category: eess.AS

TL;DR: TellWhisper：统一框架，通过时间-说话人旋转位置编码(TS-RoPE)在语音编码器中联合建模说话人身份和时间信息，解决多说话人ASR中"谁在何时说了什么"的问题。


<details>
  <summary>Details</summary>
Motivation: 现有多说话人ASR方法将时间建模和说话人建模解耦：有些在编码前注入说话人线索（如说话人掩码），可能导致不可逆信息损失；有些在编码后融合身份（如混合说话人后验），可能使声学内容与说话人身份纠缠。这种分离在快速话轮转换和重叠语音中表现脆弱，导致性能下降。

Method: 1. 提出TellWhisper统一框架，在语音编码器中联合建模说话人身份和时间信息；2. 设计TS-RoPE（时间-说话人旋转位置编码）：时间坐标来自帧索引，说话人坐标来自说话人活动和停顿线索，通过区域特定旋转角度使注意力机制同时关注"何时"和"谁"；3. 开发Hyper-SD：在双曲空间中进行说话人分类，增强类间分离并细化说话人活动估计。

Result: 大量实验证明了所提方法的有效性。

Conclusion: TellWhisper通过统一的时间-说话人建模框架，解决了现有方法在快速话轮转换和重叠语音中的局限性，实现了更好的多说话人ASR性能。

Abstract: Multi-speaker automatic speech recognition (MASR) aims to predict ''who spoke when and what'' from multi-speaker speech, a key technology for multi-party dialogue understanding. However, most existing approaches decouple temporal modeling and speaker modeling when addressing ''when'' and ''who'': some inject speaker cues before encoding (e.g., speaker masking), which can cause irreversible information loss; others fuse identity by mixing speaker posteriors after encoding, which may entangle acoustic content with speaker identity. This separation is brittle under rapid turn-taking and overlapping speech, often leading to degraded performance. To address these limitations, we propose TellWhisper, a unified framework that jointly models speaker identity and temporal within the speech encoder. Specifically, we design TS-RoPE, a time-speaker rotary positional encoding: time coordinates are derived from frame indices, while speaker coordinates are derived from speaker activity and pause cues. By applying region-specific rotation angles, the model explicitly captures per-speaker continuity, speaker-turn transitions, and state dynamics, enabling the attention mechanism to simultaneously attend to ''when'' and ''who''. Moreover, to estimate frame-level speaker activity, we develop Hyper-SD, which casts speaker classification in hyperbolic space to enhance inter-class separation and refine speaker-activity estimates. Extensive experiments demonstrate the effectiveness of the proposed approach.

</details>


### [17] [Sound Event Detection with Boundary-Aware Optimization and Inference](https://arxiv.org/abs/2601.04178)
*Florian Schmid,Chi Ian Tang,Sanjeel Parekh,Vamsi Krishna Ithapu,Juan Azcarreta Ortiz,Giacomo Ferroni,Yijun Qian,Arnoldas Jasonas,Cosmin Frateanu,Camilla Clark,Gerhard Widmer,Çağdaş Bilen*

Main category: eess.AS

TL;DR: 提出新的时间事件检测方法，通过显式建模事件开始和结束边界，结合边界感知优化和推理策略，显著提升时间事件检测性能


<details>
  <summary>Details</summary>
Motivation: 时间检测问题在时间序列估计、活动识别和声音事件检测等领域普遍存在，现有方法在时间事件建模方面存在局限性，需要更精确的边界检测

Method: 提出Recurrent Event Detection (RED)和Event Proposal Network (EPN)两个新的时间建模层，结合定制的损失函数，采用边界感知的优化和推理策略

Result: 在AudioSet强标注子集上的实验表明，该方法不仅优于传统帧级SED模型加最先进后处理的方法，还消除了后处理超参数调优的需求，在所有AudioSet强类别上达到新的最先进性能

Conclusion: 提出的边界感知时间事件建模方法显著提升了时间事件检测的精度和效果，为时间检测问题提供了新的有效解决方案

Abstract: Temporal detection problems appear in many fields including time-series estimation, activity recognition and sound event detection (SED). In this work, we propose a new approach to temporal event modeling by explicitly modeling event onsets and offsets, and by introducing boundary-aware optimization and inference strategies that substantially enhance temporal event detection. The presented methodology incorporates new temporal modeling layers - Recurrent Event Detection (RED) and Event Proposal Network (EPN) - which, together with tailored loss functions, enable more effective and precise temporal event detection. We evaluate the proposed method in the SED domain using a subset of the temporally-strongly annotated portion of AudioSet. Experimental results show that our approach not only outperforms traditional frame-wise SED models with state-of-the-art post-processing, but also removes the need for post-processing hyperparameter tuning, and scales to achieve new state-of-the-art performance across all AudioSet Strong classes.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [18] [Investigation into respiratory sound classification for an imbalanced data set using hybrid LSTM-KAN architectures](https://arxiv.org/abs/2601.03610)
*Nithinkumar K.,Anand R*

Main category: cs.SD

TL;DR: 提出混合LSTM-KAN模型，结合焦点损失、SMOTE等技术，解决呼吸音分类中的严重类别不平衡问题，在COPD占86%的极端不平衡数据集上取得94.6%准确率和0.703宏平均F1分数。


<details>
  <summary>Details</summary>
Motivation: 呼吸音听诊对肺部疾病诊断至关重要，但自动分类面临两大挑战：1）声音的细微声学差异难以捕捉；2）临床数据集存在严重的类别不平衡问题，影响少数类别的识别性能。

Method: 提出混合深度学习模型：LSTM网络用于序列特征编码 + Kolmogorov-Arnold Network (KAN)用于分类。结合全面的特征提取流程和针对性的不平衡缓解策略，包括焦点损失、类别特定的数据增强和SMOTE过采样技术。

Result: 在包含6个类别且高度偏斜的公共呼吸音数据库上，模型整体准确率达到94.6%，宏平均F1分数为0.703。尽管COPD类别占数据86%以上，但相比基线方法，少数类别的检测性能显著提升。

Conclusion: 提出的混合LSTM-KAN架构结合不平衡缓解策略，能有效处理极端不平衡的呼吸音分类任务，提升少数类别的识别能力，为临床呼吸音自动诊断提供了有效解决方案。

Abstract: Respiratory sounds captured via auscultation contain critical clues for diagnosing pulmonary conditions. Automated classification of these sounds faces challenges due to subtle acoustic differences and severe class imbalance in clinical datasets. This study investigates respiratory sound classification with a focus on mitigating pronounced class imbalance. We propose a hybrid deep learning model that combines a Long Short-Term Memory (LSTM) network for sequential feature encoding with a Kolmogorov-Arnold Network (KAN) for classification. The model is integrated with a comprehensive feature extraction pipeline and targeted imbalance mitigation strategies. Experiments were conducted on a public respiratory sound database comprising six classes with a highly skewed distribution. Techniques such as focal loss, class-specific data augmentation, and Synthetic Minority Over-sampling Technique (SMOTE) were employed to enhance minority class recognition. The proposed Hybrid LSTM-KAN model achieves an overall accuracy of 94.6 percent and a macro-averaged F1 score of 0.703, despite the dominant COPD class accounting for over 86 percent of the data. Improved detection performance is observed for minority classes compared to baseline approaches, demonstrating the effectiveness of the proposed architecture for imbalanced respiratory sound classification.

</details>


### [19] [Domain Adaptation of the Pyannote Diarization Pipeline for Conversational Indonesian Audio](https://arxiv.org/abs/2601.03684)
*Muhammad Daffa'i Rafi Prasetyo,Ramadhan Andika Putra,Zaidan Naufal Ilmi,Kurniawati Azizah*

Main category: cs.SD

TL;DR: 该研究提出了一种针对印尼语对话音频的说话人日志领域自适应方法，通过神经TTS生成合成数据，将英语为中心的日志系统适配到低资源语言，显著降低了错误率。


<details>
  <summary>Details</summary>
Motivation: 解决将英语为中心的说话人日志系统直接应用于低资源印尼语时性能不佳的问题，特别是针对零样本迁移时高达53.47%的DER错误率。

Method: 采用领域自适应方法，使用神经文本到语音技术生成合成印尼语数据，构建小数据集（171样本）和大数据集（25小时合成语音），在不同训练配置下微调pyannote/segmentation-3.0模型。

Result: 小数据集模型将DER降至34.31%-34.81%，25小时大数据集模型达到最佳性能：DER为29.24%（比基线提升13.68%），召回率99.06%，F1分数87.14%。

Conclusion: 合成数据驱动的领域自适应能有效提升说话人日志系统在低资源语言上的性能，神经TTS生成的数据可作为实际标注数据的有效替代，为低资源语言语音处理提供可行方案。

Abstract: This study presents a domain adaptation approach for speaker diarization targeting conversational Indonesian audio. We address the challenge of adapting an English-centric diarization pipeline to a low-resource language by employing synthetic data generation using neural Text-to-Speech technology. Experiments were conducted with varying training configurations, a small dataset (171 samples) and a large dataset containing 25 hours of synthetic speech. Results demonstrate that the baseline \texttt{pyannote/segmentation-3.0} model, trained on the AMI Corpus, achieves a Diarization Error Rate (DER) of 53.47\% when applied zero-shot to Indonesian. Domain adaptation significantly improves performance, with the small dataset models reducing DER to 34.31\% (1 epoch) and 34.81\% (2 epochs). The model trained on the 25-hour dataset achieves the best performance with a DER of 29.24\%, representing a 13.68\% absolute improvement over the baseline while maintaining 99.06\% Recall and 87.14\% F1-Score.

</details>


### [20] [IndexTTS 2.5 Technical Report](https://arxiv.org/abs/2601.03888)
*Yunpei Li,Xun Zhou,Jinchao Wang,Lu Wang,Yong Wu,Siyi Zhou,Yiquan Zhou,Jingchen Shu*

Main category: cs.SD

TL;DR: IndexTTS 2.5在IndexTTS 2基础上，通过语义编解码压缩、架构升级、多语言扩展和强化学习优化四大改进，显著提升了多语言覆盖、推理速度和合成质量，支持中英日西四种语言的零样本情感语音合成。


<details>
  <summary>Details</summary>
Motivation: 在IndexTTS 2的基础上，需要进一步提升多语言覆盖范围、推理速度和整体合成质量，同时保持零样本情感复制的优势。

Method: 1) 语义编解码压缩：将语义编解码帧率从50Hz降至25Hz，减少序列长度；2) 架构升级：将S2M模块的U-DiT骨干替换为更高效的Zipformer架构；3) 多语言扩展：提出边界感知对齐、token级拼接和指令引导生成三种跨语言建模策略；4) 强化学习优化：在T2S模块后训练中应用GRPO提升发音准确性和自然度。

Result: IndexTTS 2.5支持更广泛的语言覆盖，在零样本设置下能够复制未见语言的情感韵律，推理速度提升2.28倍，同时保持与IndexTTS 2相当的词错误率和说话人相似度。

Conclusion: IndexTTS 2.5通过四项关键技术改进，实现了多语言覆盖、推理速度和合成质量的显著提升，为零样本多语言情感TTS建立了实用的设计原则。

Abstract: In prior work, we introduced IndexTTS 2, a zero-shot neural text-to-speech foundation model comprising two core components: a transformer-based Text-to-Semantic (T2S) module and a non-autoregressive Semantic-to-Mel (S2M) module, which together enable faithful emotion replication and establish the first autoregressive duration-controllable generative paradigm. Building upon this, we present IndexTTS 2.5, which significantly enhances multilingual coverage, inference speed, and overall synthesis quality through four key improvements: 1) Semantic Codec Compression: we reduce the semantic codec frame rate from 50 Hz to 25 Hz, halving sequence length and substantially lowering both training and inference costs; 2) Architectural Upgrade: we replace the U-DiT-based backbone of the S2M module with a more efficient Zipformer-based modeling architecture, achieving notable parameter reduction and faster mel-spectrogram generation; 3) Multilingual Extension: We propose three explicit cross-lingual modeling strategies, boundary-aware alignment, token-level concatenation, and instruction-guided generation, establishing practical design principles for zero-shot multilingual emotional TTS that supports Chinese, English, Japanese, and Spanish, and enables robust emotion transfer even without target-language emotional training data; 4) Reinforcement Learning Optimization: we apply GRPO in post-training of the T2S module, improving pronunciation accuracy and natrualness. Experiments show that IndexTTS 2.5 not only supports broader language coverage but also replicates emotional prosody in unseen languages under the same zero-shot setting. IndexTTS 2.5 achieves a 2.28 times improvement in RTF while maintaining comparable WER and speaker similarity to IndexTTS 2.

</details>


### [21] [Lightweight and perceptually-guided voice conversion for electro-laryngeal speech](https://arxiv.org/abs/2601.03892)
*Benedikt Mayrhofer,Franz Pernkopf,Philipp Aichinger,Martin Hagmüller*

Main category: cs.SD

TL;DR: 提出轻量级StreamVC框架适配电喉语音转换，通过移除音高/能量模块、结合自监督预训练和监督微调，显著提升电喉语音的自然度和可懂度


<details>
  <summary>Details</summary>
Motivation: 电喉语音存在音高恒定、韵律有限、机械噪声等问题，导致自然度和可懂度降低，需要有效的语音转换技术进行语音康复

Method: 轻量级StreamVC框架适配：移除音高和能量模块，结合自监督预训练和监督微调，使用感知损失和可懂度损失指导训练，基于WavLM特征和人类反馈预测

Result: 最佳模型(+WavLM+HF)显著降低字符错误率(CER)，将自然度平均意见得分(nMOS)从1.1提升至3.3，在所有评估指标上缩小与健康语音的差距

Conclusion: 证明了轻量级语音转换架构适配电喉语音康复的可行性，同时指出韵律生成和可懂度提升仍是主要瓶颈

Abstract: Electro-laryngeal (EL) speech is characterized by constant pitch, limited prosody, and mechanical noise, reducing naturalness and intelligibility. We propose a lightweight adaptation of the state-of-the-art StreamVC framework to this setting by removing pitch and energy modules and combining self-supervised pretraining with supervised fine-tuning on parallel EL and healthy (HE) speech data, guided by perceptual and intelligibility losses. Objective and subjective evaluations across different loss configurations confirm their influence: the best model variant, based on WavLM features and human-feedback predictions (+WavLM+HF), drastically reduces character error rate (CER) of EL inputs, raises naturalness mean opinion score (nMOS) from 1.1 to 3.3, and consistently narrows the gap to HE ground-truth speech in all evaluated metrics. These findings demonstrate the feasibility of adapting lightweight voice conversion architectures to EL voice rehabilitation while also identifying prosody generation and intelligibility improvements as the main remaining bottlenecks.

</details>


### [22] [Muse: Towards Reproducible Long-Form Song Generation with Fine-Grained Style Control](https://arxiv.org/abs/2601.03973)
*Changhao Jiang,Jiahao Chen,Zhenghao Xiang,Zhixiong Yang,Hanchen Wang,Jiabao Zhuang,Xinmeng Che,Jiajun Sun,Hui Li,Yifei Cao,Shihan Dou,Ming Zhang,Junjie Ye,Tao Ji,Tao Gui,Qi Zhang,Xuanjing Huang*

Main category: cs.SD

TL;DR: 开源长歌曲生成系统Muse：包含授权合成数据集、训练评估流程和模型，支持细粒度风格控制，性能接近商业系统


<details>
  <summary>Details</summary>
Motivation: 当前商业系统（如Suno）在长歌曲生成方面表现出色，但学术研究因缺乏公开训练数据而难以复现，阻碍了公平比较和进展。需要开源系统来推动可控长歌曲生成研究。

Method: 1. 发布包含11.6万首授权合成歌曲的数据集，包含自动生成的歌词和风格描述，音频由SunoV5合成；2. 训练Muse模型：基于Qwen语言模型，使用MuCodec扩展离散音频token，通过单阶段监督微调，无需任务特定损失或额外架构组件

Result: 尽管数据规模和模型大小适中，Muse在音素错误率、文本-音乐风格相似度和音频美学质量方面表现具有竞争力，同时支持跨不同音乐结构的可控分段生成

Conclusion: 开源了完整的数据、模型权重、训练和评估流程，为可控长歌曲生成研究的持续进展铺平了道路

Abstract: Recent commercial systems such as Suno demonstrate strong capabilities in long-form song generation, while academic research remains largely non-reproducible due to the lack of publicly available training data, hindering fair comparison and progress. To this end, we release a fully open-source system for long-form song generation with fine-grained style conditioning, including a licensed synthetic dataset, training and evaluation pipelines, and Muse, an easy-to-deploy song generation model. The dataset consists of 116k fully licensed synthetic songs with automatically generated lyrics and style descriptions paired with audio synthesized by SunoV5. We train Muse via single-stage supervised finetuning of a Qwen-based language model extended with discrete audio tokens using MuCodec, without task-specific losses, auxiliary objectives, or additional architectural components. Our evaluations find that although Muse is trained with a modest data scale and model size, it achieves competitive performance on phoneme error rate, text--music style similarity, and audio aesthetic quality, while enabling controllable segment-level generation across different musical structures. All data, model weights, and training and evaluation pipelines will be publicly released, paving the way for continued progress in controllable long-form song generation research. The project repository is available at https://github.com/yuhui1038/Muse.

</details>

{"id": "2601.03610", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.03610", "abs": "https://arxiv.org/abs/2601.03610", "authors": ["Nithinkumar K.", "Anand R"], "title": "Investigation into respiratory sound classification for an imbalanced data set using hybrid LSTM-KAN architectures", "comment": null, "summary": "Respiratory sounds captured via auscultation contain critical clues for diagnosing pulmonary conditions. Automated classification of these sounds faces challenges due to subtle acoustic differences and severe class imbalance in clinical datasets. This study investigates respiratory sound classification with a focus on mitigating pronounced class imbalance. We propose a hybrid deep learning model that combines a Long Short-Term Memory (LSTM) network for sequential feature encoding with a Kolmogorov-Arnold Network (KAN) for classification. The model is integrated with a comprehensive feature extraction pipeline and targeted imbalance mitigation strategies. Experiments were conducted on a public respiratory sound database comprising six classes with a highly skewed distribution. Techniques such as focal loss, class-specific data augmentation, and Synthetic Minority Over-sampling Technique (SMOTE) were employed to enhance minority class recognition. The proposed Hybrid LSTM-KAN model achieves an overall accuracy of 94.6 percent and a macro-averaged F1 score of 0.703, despite the dominant COPD class accounting for over 86 percent of the data. Improved detection performance is observed for minority classes compared to baseline approaches, demonstrating the effectiveness of the proposed architecture for imbalanced respiratory sound classification.", "AI": {"tldr": "\u63d0\u51fa\u6df7\u5408LSTM-KAN\u6a21\u578b\uff0c\u7ed3\u5408\u7126\u70b9\u635f\u5931\u3001SMOTE\u7b49\u6280\u672f\uff0c\u89e3\u51b3\u547c\u5438\u97f3\u5206\u7c7b\u4e2d\u7684\u4e25\u91cd\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u5728COPD\u536086%\u7684\u6781\u7aef\u4e0d\u5e73\u8861\u6570\u636e\u96c6\u4e0a\u53d6\u5f9794.6%\u51c6\u786e\u7387\u548c0.703\u5b8f\u5e73\u5747F1\u5206\u6570\u3002", "motivation": "\u547c\u5438\u97f3\u542c\u8bca\u5bf9\u80ba\u90e8\u75be\u75c5\u8bca\u65ad\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u81ea\u52a8\u5206\u7c7b\u9762\u4e34\u4e24\u5927\u6311\u6218\uff1a1\uff09\u58f0\u97f3\u7684\u7ec6\u5fae\u58f0\u5b66\u5dee\u5f02\u96be\u4ee5\u6355\u6349\uff1b2\uff09\u4e34\u5e8a\u6570\u636e\u96c6\u5b58\u5728\u4e25\u91cd\u7684\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u5f71\u54cd\u5c11\u6570\u7c7b\u522b\u7684\u8bc6\u522b\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u6df7\u5408\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff1aLSTM\u7f51\u7edc\u7528\u4e8e\u5e8f\u5217\u7279\u5f81\u7f16\u7801 + Kolmogorov-Arnold Network (KAN)\u7528\u4e8e\u5206\u7c7b\u3002\u7ed3\u5408\u5168\u9762\u7684\u7279\u5f81\u63d0\u53d6\u6d41\u7a0b\u548c\u9488\u5bf9\u6027\u7684\u4e0d\u5e73\u8861\u7f13\u89e3\u7b56\u7565\uff0c\u5305\u62ec\u7126\u70b9\u635f\u5931\u3001\u7c7b\u522b\u7279\u5b9a\u7684\u6570\u636e\u589e\u5f3a\u548cSMOTE\u8fc7\u91c7\u6837\u6280\u672f\u3002", "result": "\u5728\u5305\u542b6\u4e2a\u7c7b\u522b\u4e14\u9ad8\u5ea6\u504f\u659c\u7684\u516c\u5171\u547c\u5438\u97f3\u6570\u636e\u5e93\u4e0a\uff0c\u6a21\u578b\u6574\u4f53\u51c6\u786e\u7387\u8fbe\u523094.6%\uff0c\u5b8f\u5e73\u5747F1\u5206\u6570\u4e3a0.703\u3002\u5c3d\u7ba1COPD\u7c7b\u522b\u5360\u6570\u636e86%\u4ee5\u4e0a\uff0c\u4f46\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5c11\u6570\u7c7b\u522b\u7684\u68c0\u6d4b\u6027\u80fd\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u63d0\u51fa\u7684\u6df7\u5408LSTM-KAN\u67b6\u6784\u7ed3\u5408\u4e0d\u5e73\u8861\u7f13\u89e3\u7b56\u7565\uff0c\u80fd\u6709\u6548\u5904\u7406\u6781\u7aef\u4e0d\u5e73\u8861\u7684\u547c\u5438\u97f3\u5206\u7c7b\u4efb\u52a1\uff0c\u63d0\u5347\u5c11\u6570\u7c7b\u522b\u7684\u8bc6\u522b\u80fd\u529b\uff0c\u4e3a\u4e34\u5e8a\u547c\u5438\u97f3\u81ea\u52a8\u8bca\u65ad\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.03684", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2601.03684", "abs": "https://arxiv.org/abs/2601.03684", "authors": ["Muhammad Daffa'i Rafi Prasetyo", "Ramadhan Andika Putra", "Zaidan Naufal Ilmi", "Kurniawati Azizah"], "title": "Domain Adaptation of the Pyannote Diarization Pipeline for Conversational Indonesian Audio", "comment": "Experiments conducted using synthetic Indonesian conversational speech for domain adaptation", "summary": "This study presents a domain adaptation approach for speaker diarization targeting conversational Indonesian audio. We address the challenge of adapting an English-centric diarization pipeline to a low-resource language by employing synthetic data generation using neural Text-to-Speech technology. Experiments were conducted with varying training configurations, a small dataset (171 samples) and a large dataset containing 25 hours of synthetic speech. Results demonstrate that the baseline \\texttt{pyannote/segmentation-3.0} model, trained on the AMI Corpus, achieves a Diarization Error Rate (DER) of 53.47\\% when applied zero-shot to Indonesian. Domain adaptation significantly improves performance, with the small dataset models reducing DER to 34.31\\% (1 epoch) and 34.81\\% (2 epochs). The model trained on the 25-hour dataset achieves the best performance with a DER of 29.24\\%, representing a 13.68\\% absolute improvement over the baseline while maintaining 99.06\\% Recall and 87.14\\% F1-Score.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u5370\u5c3c\u8bed\u5bf9\u8bdd\u97f3\u9891\u7684\u8bf4\u8bdd\u4eba\u65e5\u5fd7\u9886\u57df\u81ea\u9002\u5e94\u65b9\u6cd5\uff0c\u901a\u8fc7\u795e\u7ecfTTS\u751f\u6210\u5408\u6210\u6570\u636e\uff0c\u5c06\u82f1\u8bed\u4e3a\u4e2d\u5fc3\u7684\u65e5\u5fd7\u7cfb\u7edf\u9002\u914d\u5230\u4f4e\u8d44\u6e90\u8bed\u8a00\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u9519\u8bef\u7387\u3002", "motivation": "\u89e3\u51b3\u5c06\u82f1\u8bed\u4e3a\u4e2d\u5fc3\u7684\u8bf4\u8bdd\u4eba\u65e5\u5fd7\u7cfb\u7edf\u76f4\u63a5\u5e94\u7528\u4e8e\u4f4e\u8d44\u6e90\u5370\u5c3c\u8bed\u65f6\u6027\u80fd\u4e0d\u4f73\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u9488\u5bf9\u96f6\u6837\u672c\u8fc1\u79fb\u65f6\u9ad8\u8fbe53.47%\u7684DER\u9519\u8bef\u7387\u3002", "method": "\u91c7\u7528\u9886\u57df\u81ea\u9002\u5e94\u65b9\u6cd5\uff0c\u4f7f\u7528\u795e\u7ecf\u6587\u672c\u5230\u8bed\u97f3\u6280\u672f\u751f\u6210\u5408\u6210\u5370\u5c3c\u8bed\u6570\u636e\uff0c\u6784\u5efa\u5c0f\u6570\u636e\u96c6\uff08171\u6837\u672c\uff09\u548c\u5927\u6570\u636e\u96c6\uff0825\u5c0f\u65f6\u5408\u6210\u8bed\u97f3\uff09\uff0c\u5728\u4e0d\u540c\u8bad\u7ec3\u914d\u7f6e\u4e0b\u5fae\u8c03pyannote/segmentation-3.0\u6a21\u578b\u3002", "result": "\u5c0f\u6570\u636e\u96c6\u6a21\u578b\u5c06DER\u964d\u81f334.31%-34.81%\uff0c25\u5c0f\u65f6\u5927\u6570\u636e\u96c6\u6a21\u578b\u8fbe\u5230\u6700\u4f73\u6027\u80fd\uff1aDER\u4e3a29.24%\uff08\u6bd4\u57fa\u7ebf\u63d0\u534713.68%\uff09\uff0c\u53ec\u56de\u738799.06%\uff0cF1\u5206\u657087.14%\u3002", "conclusion": "\u5408\u6210\u6570\u636e\u9a71\u52a8\u7684\u9886\u57df\u81ea\u9002\u5e94\u80fd\u6709\u6548\u63d0\u5347\u8bf4\u8bdd\u4eba\u65e5\u5fd7\u7cfb\u7edf\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e0a\u7684\u6027\u80fd\uff0c\u795e\u7ecfTTS\u751f\u6210\u7684\u6570\u636e\u53ef\u4f5c\u4e3a\u5b9e\u9645\u6807\u6ce8\u6570\u636e\u7684\u6709\u6548\u66ff\u4ee3\uff0c\u4e3a\u4f4e\u8d44\u6e90\u8bed\u8a00\u8bed\u97f3\u5904\u7406\u63d0\u4f9b\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2601.03888", "categories": ["cs.SD", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.03888", "abs": "https://arxiv.org/abs/2601.03888", "authors": ["Yunpei Li", "Xun Zhou", "Jinchao Wang", "Lu Wang", "Yong Wu", "Siyi Zhou", "Yiquan Zhou", "Jingchen Shu"], "title": "IndexTTS 2.5 Technical Report", "comment": "11 pages, 4 figures", "summary": "In prior work, we introduced IndexTTS 2, a zero-shot neural text-to-speech foundation model comprising two core components: a transformer-based Text-to-Semantic (T2S) module and a non-autoregressive Semantic-to-Mel (S2M) module, which together enable faithful emotion replication and establish the first autoregressive duration-controllable generative paradigm. Building upon this, we present IndexTTS 2.5, which significantly enhances multilingual coverage, inference speed, and overall synthesis quality through four key improvements: 1) Semantic Codec Compression: we reduce the semantic codec frame rate from 50 Hz to 25 Hz, halving sequence length and substantially lowering both training and inference costs; 2) Architectural Upgrade: we replace the U-DiT-based backbone of the S2M module with a more efficient Zipformer-based modeling architecture, achieving notable parameter reduction and faster mel-spectrogram generation; 3) Multilingual Extension: We propose three explicit cross-lingual modeling strategies, boundary-aware alignment, token-level concatenation, and instruction-guided generation, establishing practical design principles for zero-shot multilingual emotional TTS that supports Chinese, English, Japanese, and Spanish, and enables robust emotion transfer even without target-language emotional training data; 4) Reinforcement Learning Optimization: we apply GRPO in post-training of the T2S module, improving pronunciation accuracy and natrualness. Experiments show that IndexTTS 2.5 not only supports broader language coverage but also replicates emotional prosody in unseen languages under the same zero-shot setting. IndexTTS 2.5 achieves a 2.28 times improvement in RTF while maintaining comparable WER and speaker similarity to IndexTTS 2.", "AI": {"tldr": "IndexTTS 2.5\u5728IndexTTS 2\u57fa\u7840\u4e0a\uff0c\u901a\u8fc7\u8bed\u4e49\u7f16\u89e3\u7801\u538b\u7f29\u3001\u67b6\u6784\u5347\u7ea7\u3001\u591a\u8bed\u8a00\u6269\u5c55\u548c\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u56db\u5927\u6539\u8fdb\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u8bed\u8a00\u8986\u76d6\u3001\u63a8\u7406\u901f\u5ea6\u548c\u5408\u6210\u8d28\u91cf\uff0c\u652f\u6301\u4e2d\u82f1\u65e5\u897f\u56db\u79cd\u8bed\u8a00\u7684\u96f6\u6837\u672c\u60c5\u611f\u8bed\u97f3\u5408\u6210\u3002", "motivation": "\u5728IndexTTS 2\u7684\u57fa\u7840\u4e0a\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u63d0\u5347\u591a\u8bed\u8a00\u8986\u76d6\u8303\u56f4\u3001\u63a8\u7406\u901f\u5ea6\u548c\u6574\u4f53\u5408\u6210\u8d28\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u96f6\u6837\u672c\u60c5\u611f\u590d\u5236\u7684\u4f18\u52bf\u3002", "method": "1) \u8bed\u4e49\u7f16\u89e3\u7801\u538b\u7f29\uff1a\u5c06\u8bed\u4e49\u7f16\u89e3\u7801\u5e27\u7387\u4ece50Hz\u964d\u81f325Hz\uff0c\u51cf\u5c11\u5e8f\u5217\u957f\u5ea6\uff1b2) \u67b6\u6784\u5347\u7ea7\uff1a\u5c06S2M\u6a21\u5757\u7684U-DiT\u9aa8\u5e72\u66ff\u6362\u4e3a\u66f4\u9ad8\u6548\u7684Zipformer\u67b6\u6784\uff1b3) \u591a\u8bed\u8a00\u6269\u5c55\uff1a\u63d0\u51fa\u8fb9\u754c\u611f\u77e5\u5bf9\u9f50\u3001token\u7ea7\u62fc\u63a5\u548c\u6307\u4ee4\u5f15\u5bfc\u751f\u6210\u4e09\u79cd\u8de8\u8bed\u8a00\u5efa\u6a21\u7b56\u7565\uff1b4) \u5f3a\u5316\u5b66\u4e60\u4f18\u5316\uff1a\u5728T2S\u6a21\u5757\u540e\u8bad\u7ec3\u4e2d\u5e94\u7528GRPO\u63d0\u5347\u53d1\u97f3\u51c6\u786e\u6027\u548c\u81ea\u7136\u5ea6\u3002", "result": "IndexTTS 2.5\u652f\u6301\u66f4\u5e7f\u6cdb\u7684\u8bed\u8a00\u8986\u76d6\uff0c\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u80fd\u591f\u590d\u5236\u672a\u89c1\u8bed\u8a00\u7684\u60c5\u611f\u97f5\u5f8b\uff0c\u63a8\u7406\u901f\u5ea6\u63d0\u53472.28\u500d\uff0c\u540c\u65f6\u4fdd\u6301\u4e0eIndexTTS 2\u76f8\u5f53\u7684\u8bcd\u9519\u8bef\u7387\u548c\u8bf4\u8bdd\u4eba\u76f8\u4f3c\u5ea6\u3002", "conclusion": "IndexTTS 2.5\u901a\u8fc7\u56db\u9879\u5173\u952e\u6280\u672f\u6539\u8fdb\uff0c\u5b9e\u73b0\u4e86\u591a\u8bed\u8a00\u8986\u76d6\u3001\u63a8\u7406\u901f\u5ea6\u548c\u5408\u6210\u8d28\u91cf\u7684\u663e\u8457\u63d0\u5347\uff0c\u4e3a\u96f6\u6837\u672c\u591a\u8bed\u8a00\u60c5\u611fTTS\u5efa\u7acb\u4e86\u5b9e\u7528\u7684\u8bbe\u8ba1\u539f\u5219\u3002"}}
{"id": "2601.03892", "categories": ["cs.SD", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.03892", "abs": "https://arxiv.org/abs/2601.03892", "authors": ["Benedikt Mayrhofer", "Franz Pernkopf", "Philipp Aichinger", "Martin Hagm\u00fcller"], "title": "Lightweight and perceptually-guided voice conversion for electro-laryngeal speech", "comment": "5 pages, 5 figures. Audio samples available at https://spsc-tugraz.github.io/lw-elvc-icassp26/ Preprint submitted to ICASSP", "summary": "Electro-laryngeal (EL) speech is characterized by constant pitch, limited prosody, and mechanical noise, reducing naturalness and intelligibility. We propose a lightweight adaptation of the state-of-the-art StreamVC framework to this setting by removing pitch and energy modules and combining self-supervised pretraining with supervised fine-tuning on parallel EL and healthy (HE) speech data, guided by perceptual and intelligibility losses. Objective and subjective evaluations across different loss configurations confirm their influence: the best model variant, based on WavLM features and human-feedback predictions (+WavLM+HF), drastically reduces character error rate (CER) of EL inputs, raises naturalness mean opinion score (nMOS) from 1.1 to 3.3, and consistently narrows the gap to HE ground-truth speech in all evaluated metrics. These findings demonstrate the feasibility of adapting lightweight voice conversion architectures to EL voice rehabilitation while also identifying prosody generation and intelligibility improvements as the main remaining bottlenecks.", "AI": {"tldr": "\u63d0\u51fa\u8f7b\u91cf\u7ea7StreamVC\u6846\u67b6\u9002\u914d\u7535\u5589\u8bed\u97f3\u8f6c\u6362\uff0c\u901a\u8fc7\u79fb\u9664\u97f3\u9ad8/\u80fd\u91cf\u6a21\u5757\u3001\u7ed3\u5408\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u548c\u76d1\u7763\u5fae\u8c03\uff0c\u663e\u8457\u63d0\u5347\u7535\u5589\u8bed\u97f3\u7684\u81ea\u7136\u5ea6\u548c\u53ef\u61c2\u5ea6", "motivation": "\u7535\u5589\u8bed\u97f3\u5b58\u5728\u97f3\u9ad8\u6052\u5b9a\u3001\u97f5\u5f8b\u6709\u9650\u3001\u673a\u68b0\u566a\u58f0\u7b49\u95ee\u9898\uff0c\u5bfc\u81f4\u81ea\u7136\u5ea6\u548c\u53ef\u61c2\u5ea6\u964d\u4f4e\uff0c\u9700\u8981\u6709\u6548\u7684\u8bed\u97f3\u8f6c\u6362\u6280\u672f\u8fdb\u884c\u8bed\u97f3\u5eb7\u590d", "method": "\u8f7b\u91cf\u7ea7StreamVC\u6846\u67b6\u9002\u914d\uff1a\u79fb\u9664\u97f3\u9ad8\u548c\u80fd\u91cf\u6a21\u5757\uff0c\u7ed3\u5408\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u548c\u76d1\u7763\u5fae\u8c03\uff0c\u4f7f\u7528\u611f\u77e5\u635f\u5931\u548c\u53ef\u61c2\u5ea6\u635f\u5931\u6307\u5bfc\u8bad\u7ec3\uff0c\u57fa\u4e8eWavLM\u7279\u5f81\u548c\u4eba\u7c7b\u53cd\u9988\u9884\u6d4b", "result": "\u6700\u4f73\u6a21\u578b(+WavLM+HF)\u663e\u8457\u964d\u4f4e\u5b57\u7b26\u9519\u8bef\u7387(CER)\uff0c\u5c06\u81ea\u7136\u5ea6\u5e73\u5747\u610f\u89c1\u5f97\u5206(nMOS)\u4ece1.1\u63d0\u5347\u81f33.3\uff0c\u5728\u6240\u6709\u8bc4\u4f30\u6307\u6807\u4e0a\u7f29\u5c0f\u4e0e\u5065\u5eb7\u8bed\u97f3\u7684\u5dee\u8ddd", "conclusion": "\u8bc1\u660e\u4e86\u8f7b\u91cf\u7ea7\u8bed\u97f3\u8f6c\u6362\u67b6\u6784\u9002\u914d\u7535\u5589\u8bed\u97f3\u5eb7\u590d\u7684\u53ef\u884c\u6027\uff0c\u540c\u65f6\u6307\u51fa\u97f5\u5f8b\u751f\u6210\u548c\u53ef\u61c2\u5ea6\u63d0\u5347\u4ecd\u662f\u4e3b\u8981\u74f6\u9888"}}
{"id": "2601.03387", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.03387", "abs": "https://arxiv.org/abs/2601.03387", "authors": ["Amila Ravinath", "Minhua Ding", "Bikshapathi Gouda", "Italo Atzeni", "Antti T\u00f6lli"], "title": "SEP Analysis of a Low-Resolution SIMO System with M-PSK over Fading Channels", "comment": "13 pages, 8 figures, Submitted to IEEE Transactions on Communications", "summary": "In this paper, the average symbol error probability (SEP) of a phase-quantized single-input multiple-output (SIMO) system with M-ary phase-shift keying (PSK) modulation is analyzed under Rayleigh fading and additive white Gaussian noise. By leveraging a novel method, we derive exact SEP expressions for a quadrature PSK (QPSK)-modulated n-bit phase-quantized SIMO system with maximum ratio combining (SIMO-MRC), along with the corresponding high signal-to-noise ratio (SNR) characterizations in terms of diversity and coding gains. For a QPSK-modulated 2-bit phase-quantized SIMO system with selection combining, the diversity and coding gains are further obtained for an arbitrary number of receive antennas, complementing existing results. Interestingly, the proposed method also reveals a duality between a SIMO-MRC system and a phase-quantized multiple-input single-output (MISO) system with maximum ratio transmission, when the modulation order, phase-quantization resolution, antenna configuration, and the channel state information (CSI) conditions are reciprocal. This duality enables direct inference to obtain the diversity of a general M-PSK-modulated n-bit phase-quantized SIMO-MRC system, and extends the results to its MISO counterpart. All the above results have been obtained assuming perfect CSI at the receiver (CSIR). Finally, the SEP analysis of a QPSK-modulated 2-bit phase-quantized SIMO system is extended to the limited CSIR case, where the CSI at each receive antenna is represented by only 2 bits of channel phase information. In this scenario, the diversity gain is shown to be further halved in general.", "AI": {"tldr": "\u5206\u6790\u4e86\u76f8\u4f4d\u91cf\u5316SIMO\u7cfb\u7edf\u5728\u745e\u5229\u8870\u843d\u548cAWGN\u4e0b\u7684\u5e73\u5747\u7b26\u53f7\u9519\u8bef\u6982\u7387\uff0c\u63a8\u5bfc\u4e86QPSK\u8c03\u5236\u7684\u7cbe\u786eSEP\u8868\u8fbe\u5f0f\uff0c\u63ed\u793a\u4e86SIMO-MRC\u4e0e\u76f8\u4f4d\u91cf\u5316MISO\u7cfb\u7edf\u4e4b\u95f4\u7684\u5bf9\u5076\u6027\uff0c\u5e76\u6269\u5c55\u5230\u6709\u9650CSI\u60c5\u51b5\u3002", "motivation": "\u7814\u7a76\u76f8\u4f4d\u91cf\u5316SIMO\u7cfb\u7edf\u5728M-PSK\u8c03\u5236\u4e0b\u7684\u6027\u80fd\u5206\u6790\uff0c\u7279\u522b\u662f\u5728\u745e\u5229\u8870\u843d\u548c\u52a0\u6027\u9ad8\u65af\u767d\u566a\u58f0\u4fe1\u9053\u6761\u4ef6\u4e0b\uff0c\u9700\u8981\u63a8\u5bfc\u7cbe\u786e\u7684SEP\u8868\u8fbe\u5f0f\u5e76\u7406\u89e3\u7cfb\u7edf\u7684\u5206\u96c6\u548c\u7f16\u7801\u589e\u76ca\u7279\u6027\u3002", "method": "\u91c7\u7528\u65b0\u9896\u7684\u5206\u6790\u65b9\u6cd5\uff0c\u63a8\u5bfcQPSK\u8c03\u5236\u7684n\u4f4d\u76f8\u4f4d\u91cf\u5316SIMO-MRC\u7cfb\u7edf\u7684\u7cbe\u786eSEP\u8868\u8fbe\u5f0f\uff1b\u5bf9\u4e8e2\u4f4d\u76f8\u4f4d\u91cf\u5316SIMO\u9009\u62e9\u5408\u5e76\u7cfb\u7edf\uff0c\u83b7\u5f97\u4efb\u610f\u63a5\u6536\u5929\u7ebf\u6570\u4e0b\u7684\u5206\u96c6\u548c\u7f16\u7801\u589e\u76ca\uff1b\u63ed\u793aSIMO-MRC\u4e0e\u76f8\u4f4d\u91cf\u5316MISO\u7cfb\u7edf\u4e4b\u95f4\u7684\u5bf9\u5076\u6027\uff1b\u6269\u5c55\u5230\u6709\u9650CSI\u60c5\u51b5\u3002", "result": "\u83b7\u5f97\u4e86QPSK\u8c03\u5236n\u4f4d\u76f8\u4f4d\u91cf\u5316SIMO-MRC\u7cfb\u7edf\u7684\u7cbe\u786eSEP\u8868\u8fbe\u5f0f\u53ca\u9ad8SNR\u4e0b\u7684\u5206\u96c6\u548c\u7f16\u7801\u589e\u76ca\u7279\u6027\uff1b\u53d1\u73b0\u4e86SIMO-MRC\u4e0e\u76f8\u4f4d\u91cf\u5316MISO\u7cfb\u7edf\u4e4b\u95f4\u7684\u5bf9\u5076\u5173\u7cfb\uff1b\u5728\u6709\u9650CSI\u60c5\u51b5\u4e0b\uff0c\u5206\u96c6\u589e\u76ca\u8fdb\u4e00\u6b65\u51cf\u534a\u3002", "conclusion": "\u63d0\u51fa\u7684\u5206\u6790\u65b9\u6cd5\u6210\u529f\u63a8\u5bfc\u4e86\u76f8\u4f4d\u91cf\u5316SIMO\u7cfb\u7edf\u7684\u7cbe\u786eSEP\u8868\u8fbe\u5f0f\uff0c\u63ed\u793a\u4e86\u7cfb\u7edf\u95f4\u7684\u5bf9\u5076\u5173\u7cfb\uff0c\u4e3a\u7406\u89e3\u76f8\u4f4d\u91cf\u5316\u901a\u4fe1\u7cfb\u7edf\u7684\u6027\u80fd\u63d0\u4f9b\u4e86\u7406\u8bba\u6846\u67b6\uff0c\u5e76\u6269\u5c55\u5230\u5b9e\u9645\u6709\u9650CSI\u573a\u666f\u3002"}}
{"id": "2601.03443", "categories": ["eess.AS", "cs.AI", "cs.SD", "eess.SP"], "pdf": "https://arxiv.org/pdf/2601.03443", "abs": "https://arxiv.org/abs/2601.03443", "authors": ["Mikhail Silaev", "Konstantinos Drossos", "Tuomas Virtanen"], "title": "Discriminating real and synthetic super-resolved audio samples using embedding-based classifiers", "comment": "Accepted for publication in Workshop Proceedingsof the 2026 IEEE International Conference on Acoustics, Speech, and Signal Processing", "summary": "Generative adversarial networks (GANs) and diffusion models have recently achieved state-of-the-art performance in audio super-resolution (ADSR), producing perceptually convincing wideband audio from narrowband inputs. However, existing evaluations primarily rely on signal-level or perceptual metrics, leaving open the question of how closely the distributions of synthetic super-resolved and real wideband audio match. Here we address this problem by analyzing the separability of real and super-resolved audio in various embedding spaces. We consider both middle-band ($4\\to 16$~kHz) and full-band ($16\\to 48$~kHz) upsampling tasks for speech and music, training linear classifiers to distinguish real from synthetic samples based on multiple types of audio embeddings. Comparisons with objective metrics and subjective listening tests reveal that embedding-based classifiers achieve near-perfect separation, even when the generated audio attains high perceptual quality and state-of-the-art metric scores. This behavior is consistent across datasets and models, including recent diffusion-based approaches, highlighting a persistent gap between perceptual quality and true distributional fidelity in ADSR models.", "AI": {"tldr": "\u8be5\u8bba\u6587\u53d1\u73b0\uff0c\u5c3d\u7ba1GAN\u548c\u6269\u6563\u6a21\u578b\u5728\u97f3\u9891\u8d85\u5206\u8fa8\u7387\u4e0a\u53d6\u5f97\u4e86\u611f\u77e5\u8d28\u91cf\u4e0a\u7684\u4f18\u5f02\u8868\u73b0\uff0c\u4f46\u901a\u8fc7\u5d4c\u5165\u7a7a\u95f4\u5206\u7c7b\u5668\u5206\u6790\u663e\u793a\uff0c\u771f\u5b9e\u97f3\u9891\u4e0e\u5408\u6210\u97f3\u9891\u5728\u5206\u5e03\u4e0a\u4ecd\u5b58\u5728\u660e\u663e\u53ef\u5206\u79bb\u6027\uff0c\u63ed\u793a\u4e86\u611f\u77e5\u8d28\u91cf\u4e0e\u771f\u5b9e\u5206\u5e03\u4fdd\u771f\u5ea6\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "motivation": "\u73b0\u6709\u97f3\u9891\u8d85\u5206\u8fa8\u7387\u6a21\u578b\u7684\u8bc4\u4f30\u4e3b\u8981\u4f9d\u8d56\u4fe1\u53f7\u7ea7\u6216\u611f\u77e5\u5ea6\u91cf\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u5408\u6210\u8d85\u5206\u8fa8\u7387\u97f3\u9891\u4e0e\u771f\u5b9e\u5bbd\u5e26\u97f3\u9891\u5206\u5e03\u5339\u914d\u7a0b\u5ea6\u7684\u6df1\u5165\u5206\u6790\u3002\u8bba\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u7814\u7a76\u751f\u6210\u97f3\u9891\u4e0e\u771f\u5b9e\u97f3\u9891\u5728\u5206\u5e03\u4e0a\u7684\u5dee\u5f02\u3002", "method": "\u901a\u8fc7\u5206\u6790\u771f\u5b9e\u97f3\u9891\u4e0e\u8d85\u5206\u8fa8\u7387\u97f3\u9891\u5728\u5404\u79cd\u5d4c\u5165\u7a7a\u95f4\u4e2d\u7684\u53ef\u5206\u79bb\u6027\uff0c\u8003\u8651\u4e2d\u9891\u6bb5\uff084\u219216kHz\uff09\u548c\u5168\u9891\u6bb5\uff0816\u219248kHz\uff09\u4e0a\u91c7\u6837\u4efb\u52a1\uff0c\u8bad\u7ec3\u7ebf\u6027\u5206\u7c7b\u5668\u57fa\u4e8e\u591a\u79cd\u97f3\u9891\u5d4c\u5165\u533a\u5206\u771f\u5b9e\u4e0e\u5408\u6210\u6837\u672c\u3002", "result": "\u5d4c\u5165\u57fa\u5206\u7c7b\u5668\u5b9e\u73b0\u4e86\u8fd1\u4e4e\u5b8c\u7f8e\u7684\u5206\u79bb\u6548\u679c\uff0c\u5373\u4f7f\u751f\u6210\u7684\u97f3\u9891\u5177\u6709\u9ad8\u611f\u77e5\u8d28\u91cf\u548c\u6700\u5148\u8fdb\u7684\u5ea6\u91cf\u5206\u6570\u3002\u8fd9\u79cd\u884c\u4e3a\u5728\u4e0d\u540c\u6570\u636e\u96c6\u548c\u6a21\u578b\uff08\u5305\u62ec\u6700\u8fd1\u7684\u6269\u6563\u65b9\u6cd5\uff09\u4e2d\u4fdd\u6301\u4e00\u81f4\uff0c\u7a81\u663e\u4e86\u611f\u77e5\u8d28\u91cf\u4e0e\u771f\u5b9e\u5206\u5e03\u4fdd\u771f\u5ea6\u4e4b\u95f4\u7684\u6301\u7eed\u5dee\u8ddd\u3002", "conclusion": "\u97f3\u9891\u8d85\u5206\u8fa8\u7387\u6a21\u578b\u5728\u611f\u77e5\u8d28\u91cf\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u5206\u5e03\u4fdd\u771f\u5ea6\u65b9\u9762\u4ecd\u5b58\u5728\u660e\u663e\u4e0d\u8db3\uff0c\u8868\u660e\u5f53\u524d\u8bc4\u4f30\u6307\u6807\u65e0\u6cd5\u5b8c\u5168\u6355\u6349\u751f\u6210\u97f3\u9891\u4e0e\u771f\u5b9e\u97f3\u9891\u5728\u5206\u5e03\u5c42\u9762\u7684\u5dee\u5f02\u3002"}}
{"id": "2601.03973", "categories": ["cs.SD", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.03973", "abs": "https://arxiv.org/abs/2601.03973", "authors": ["Changhao Jiang", "Jiahao Chen", "Zhenghao Xiang", "Zhixiong Yang", "Hanchen Wang", "Jiabao Zhuang", "Xinmeng Che", "Jiajun Sun", "Hui Li", "Yifei Cao", "Shihan Dou", "Ming Zhang", "Junjie Ye", "Tao Ji", "Tao Gui", "Qi Zhang", "Xuanjing Huang"], "title": "Muse: Towards Reproducible Long-Form Song Generation with Fine-Grained Style Control", "comment": null, "summary": "Recent commercial systems such as Suno demonstrate strong capabilities in long-form song generation, while academic research remains largely non-reproducible due to the lack of publicly available training data, hindering fair comparison and progress. To this end, we release a fully open-source system for long-form song generation with fine-grained style conditioning, including a licensed synthetic dataset, training and evaluation pipelines, and Muse, an easy-to-deploy song generation model. The dataset consists of 116k fully licensed synthetic songs with automatically generated lyrics and style descriptions paired with audio synthesized by SunoV5. We train Muse via single-stage supervised finetuning of a Qwen-based language model extended with discrete audio tokens using MuCodec, without task-specific losses, auxiliary objectives, or additional architectural components. Our evaluations find that although Muse is trained with a modest data scale and model size, it achieves competitive performance on phoneme error rate, text--music style similarity, and audio aesthetic quality, while enabling controllable segment-level generation across different musical structures. All data, model weights, and training and evaluation pipelines will be publicly released, paving the way for continued progress in controllable long-form song generation research. The project repository is available at https://github.com/yuhui1038/Muse.", "AI": {"tldr": "\u5f00\u6e90\u957f\u6b4c\u66f2\u751f\u6210\u7cfb\u7edfMuse\uff1a\u5305\u542b\u6388\u6743\u5408\u6210\u6570\u636e\u96c6\u3001\u8bad\u7ec3\u8bc4\u4f30\u6d41\u7a0b\u548c\u6a21\u578b\uff0c\u652f\u6301\u7ec6\u7c92\u5ea6\u98ce\u683c\u63a7\u5236\uff0c\u6027\u80fd\u63a5\u8fd1\u5546\u4e1a\u7cfb\u7edf", "motivation": "\u5f53\u524d\u5546\u4e1a\u7cfb\u7edf\uff08\u5982Suno\uff09\u5728\u957f\u6b4c\u66f2\u751f\u6210\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5b66\u672f\u7814\u7a76\u56e0\u7f3a\u4e4f\u516c\u5f00\u8bad\u7ec3\u6570\u636e\u800c\u96be\u4ee5\u590d\u73b0\uff0c\u963b\u788d\u4e86\u516c\u5e73\u6bd4\u8f83\u548c\u8fdb\u5c55\u3002\u9700\u8981\u5f00\u6e90\u7cfb\u7edf\u6765\u63a8\u52a8\u53ef\u63a7\u957f\u6b4c\u66f2\u751f\u6210\u7814\u7a76\u3002", "method": "1. \u53d1\u5e03\u5305\u542b11.6\u4e07\u9996\u6388\u6743\u5408\u6210\u6b4c\u66f2\u7684\u6570\u636e\u96c6\uff0c\u5305\u542b\u81ea\u52a8\u751f\u6210\u7684\u6b4c\u8bcd\u548c\u98ce\u683c\u63cf\u8ff0\uff0c\u97f3\u9891\u7531SunoV5\u5408\u6210\uff1b2. \u8bad\u7ec3Muse\u6a21\u578b\uff1a\u57fa\u4e8eQwen\u8bed\u8a00\u6a21\u578b\uff0c\u4f7f\u7528MuCodec\u6269\u5c55\u79bb\u6563\u97f3\u9891token\uff0c\u901a\u8fc7\u5355\u9636\u6bb5\u76d1\u7763\u5fae\u8c03\uff0c\u65e0\u9700\u4efb\u52a1\u7279\u5b9a\u635f\u5931\u6216\u989d\u5916\u67b6\u6784\u7ec4\u4ef6", "result": "\u5c3d\u7ba1\u6570\u636e\u89c4\u6a21\u548c\u6a21\u578b\u5927\u5c0f\u9002\u4e2d\uff0cMuse\u5728\u97f3\u7d20\u9519\u8bef\u7387\u3001\u6587\u672c-\u97f3\u4e50\u98ce\u683c\u76f8\u4f3c\u5ea6\u548c\u97f3\u9891\u7f8e\u5b66\u8d28\u91cf\u65b9\u9762\u8868\u73b0\u5177\u6709\u7ade\u4e89\u529b\uff0c\u540c\u65f6\u652f\u6301\u8de8\u4e0d\u540c\u97f3\u4e50\u7ed3\u6784\u7684\u53ef\u63a7\u5206\u6bb5\u751f\u6210", "conclusion": "\u5f00\u6e90\u4e86\u5b8c\u6574\u7684\u6570\u636e\u3001\u6a21\u578b\u6743\u91cd\u3001\u8bad\u7ec3\u548c\u8bc4\u4f30\u6d41\u7a0b\uff0c\u4e3a\u53ef\u63a7\u957f\u6b4c\u66f2\u751f\u6210\u7814\u7a76\u7684\u6301\u7eed\u8fdb\u5c55\u94fa\u5e73\u4e86\u9053\u8def"}}
{"id": "2601.03427", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.03427", "abs": "https://arxiv.org/abs/2601.03427", "authors": ["Mohammad Ghassemi", "Han Zhang", "Ali Afana", "Akram Bin Sediq", "Melike Erol-Kantarci"], "title": "Foundation Model-Aided Hierarchical Control for Robust RIS-Assisted Near-Field Communications", "comment": "12 pages, 8 figures,", "summary": "The deployment of extremely large aperture arrays (ELAAs) in sixth-generation (6G) networks could shift communication into the near-field communication (NFC) regime. In this regime, signals exhibit spherical wave propagation, unlike the planar waves in conventional far-field systems. Reconfigurable intelligent surfaces (RISs) can dynamically adjust phase shifts to support NFC beamfocusing, concentrating signal energy at specific spatial coordinates. However, effective RIS utilization depends on both rapid channel state information (CSI) estimation and proactive blockage mitigation, which occur on inherently different timescales. CSI varies at millisecond intervals due to small-scale fading, while blockage events evolve over seconds, posing challenges for conventional single-level control algorithms. To address this issue, we propose a dual-transformer (DT) hierarchical framework that integrates two specialized transformer models within a hierarchical deep reinforcement learning (HDRL) architecture, referred to as the DT-HDRL framework. A fast-timescale transformer processes ray-tracing data for rapid CSI estimation, while a vision transformer (ViT) analyzes visual data to predict impending blockages. In HDRL, the high-level controller selects line-of-sight (LoS) or RIS-assisted non-line-of-sight (NLoS) transmission paths and sets goals, while the low-level controller optimizes base station (BS) beamfocusing and RIS phase shifts using instantaneous CSI. This dual-timescale coordination maximizes spectral efficiency (SE) while ensuring robust performance under dynamic conditions. Simulation results demonstrate that our approach improves SE by approximately 18% compared to single-timescale baselines, while the proposed blockage predictor achieves an F1-score of 0.92, providing a 769 ms advance warning window in dynamic scenarios.", "AI": {"tldr": "\u63d0\u51faDT-HDRL\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u53d8\u538b\u5668\u6a21\u578b\u5904\u7406\u4e0d\u540c\u65f6\u95f4\u5c3a\u5ea6\u7684CSI\u4f30\u8ba1\u548c\u906e\u6321\u9884\u6d4b\uff0c\u57286G ELAA\u8fd1\u573a\u901a\u4fe1\u4e2d\u63d0\u5347\u9891\u8c31\u6548\u7387", "motivation": "6G\u7f51\u7edc\u4e2d\u90e8\u7f72\u6781\u5927\u5b54\u5f84\u9635\u5217(ELAA)\u4f7f\u901a\u4fe1\u8fdb\u5165\u8fd1\u573a\u533a\u57df\uff0c\u4fe1\u53f7\u5448\u73b0\u7403\u9762\u6ce2\u4f20\u64ad\u3002RIS\u53ef\u652f\u6301\u8fd1\u573a\u6ce2\u675f\u805a\u7126\uff0c\u4f46\u9700\u8981\u5feb\u901fCSI\u4f30\u8ba1\u548c\u906e\u6321\u7f13\u89e3\uff0c\u8fd9\u4e24\u8005\u5728\u4e0d\u540c\u65f6\u95f4\u5c3a\u5ea6\u4e0a\u53d8\u5316\uff0c\u4f20\u7edf\u5355\u7ea7\u63a7\u5236\u7b97\u6cd5\u96be\u4ee5\u5e94\u5bf9", "method": "\u63d0\u51fa\u53cc\u53d8\u538b\u5668\u5206\u5c42\u6846\u67b6(DT-HDRL)\uff1a\u5feb\u901f\u65f6\u95f4\u5c3a\u5ea6\u53d8\u538b\u5668\u5904\u7406\u5c04\u7ebf\u8ffd\u8e2a\u6570\u636e\u8fdb\u884cCSI\u4f30\u8ba1\uff1b\u89c6\u89c9\u53d8\u538b\u5668(ViT)\u5206\u6790\u89c6\u89c9\u6570\u636e\u9884\u6d4b\u906e\u6321\u3002HDRL\u4e2d\u9ad8\u5c42\u63a7\u5236\u5668\u9009\u62e9LoS\u6216RIS\u8f85\u52a9NLoS\u4f20\u8f93\u8def\u5f84\uff0c\u4f4e\u5c42\u63a7\u5236\u5668\u4f18\u5316BS\u6ce2\u675f\u805a\u7126\u548cRIS\u76f8\u4f4d\u504f\u79fb", "result": "\u4eff\u771f\u7ed3\u679c\u663e\u793a\uff0c\u76f8\u6bd4\u5355\u65f6\u95f4\u5c3a\u5ea6\u57fa\u7ebf\u65b9\u6cd5\uff0c\u9891\u8c31\u6548\u7387\u63d0\u5347\u7ea618%\uff1b\u906e\u6321\u9884\u6d4b\u5668F1\u5206\u6570\u8fbe\u52300.92\uff0c\u5728\u52a8\u6001\u573a\u666f\u4e2d\u63d0\u4f9b769\u6beb\u79d2\u7684\u63d0\u524d\u9884\u8b66\u7a97\u53e3", "conclusion": "DT-HDRL\u6846\u67b6\u6709\u6548\u534f\u8c03\u4e0d\u540c\u65f6\u95f4\u5c3a\u5ea6\u7684\u63a7\u5236\u4efb\u52a1\uff0c\u5728\u52a8\u6001\u6761\u4ef6\u4e0b\u6700\u5927\u5316\u9891\u8c31\u6548\u7387\u5e76\u786e\u4fdd\u9c81\u68d2\u6027\u80fd\uff0c\u4e3a6G\u8fd1\u573a\u901a\u4fe1\u4e2d\u7684RIS\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2601.03626", "categories": ["eess.AS", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.03626", "abs": "https://arxiv.org/abs/2601.03626", "authors": ["Parampreet Singh", "Akshay Raina", "Sayeedul Islam Sheikh", "Vipul Arora"], "title": "Learning from Limited Labels: Transductive Graph Label Propagation for Indian Music Analysis", "comment": "Published at Journal of Acoustical Society of India, 2025", "summary": "Supervised machine learning frameworks rely on extensive labeled datasets for robust performance on real-world tasks. However, there is a lack of large annotated datasets in audio and music domains, as annotating such recordings is resource-intensive, laborious, and often require expert domain knowledge. In this work, we explore the use of label propagation (LP), a graph-based semi-supervised learning technique, for automatically labeling the unlabeled set in an unsupervised manner. By constructing a similarity graph over audio embeddings, we propagate limited label information from a small annotated subset to a larger unlabeled corpus in a transductive, semi-supervised setting. We apply this method to two tasks in Indian Art Music (IAM): Raga identification and Instrument classification. For both these tasks, we integrate multiple public datasets along with additional recordings we acquire from Prasar Bharati Archives to perform LP. Our experiments demonstrate that LP significantly reduces labeling overhead and produces higher-quality annotations compared to conventional baseline methods, including those based on pretrained inductive models. These results highlight the potential of graph-based semi-supervised learning to democratize data annotation and accelerate progress in music information retrieval.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u7d22\u4f7f\u7528\u6807\u7b7e\u4f20\u64ad\uff08LP\uff09\u8fd9\u4e00\u57fa\u4e8e\u56fe\u7684\u534a\u76d1\u7763\u5b66\u4e60\u6280\u672f\uff0c\u81ea\u52a8\u6807\u6ce8\u97f3\u9891\u548c\u97f3\u4e50\u9886\u57df\u7684\u672a\u6807\u8bb0\u6570\u636e\uff0c\u663e\u8457\u964d\u4f4e\u6807\u6ce8\u6210\u672c\u5e76\u63d0\u9ad8\u6807\u6ce8\u8d28\u91cf\u3002", "motivation": "\u97f3\u9891\u548c\u97f3\u4e50\u9886\u57df\u7f3a\u4e4f\u5927\u89c4\u6a21\u6807\u6ce8\u6570\u636e\u96c6\uff0c\u56e0\u4e3a\u6807\u6ce8\u8fd9\u4e9b\u5f55\u97f3\u8d44\u6e90\u5bc6\u96c6\u3001\u52b3\u52a8\u91cf\u5927\u4e14\u9700\u8981\u4e13\u4e1a\u77e5\u8bc6\u3002\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u51cf\u5c11\u6807\u6ce8\u5f00\u9500\u5e76\u52a0\u901f\u97f3\u4e50\u4fe1\u606f\u68c0\u7d22\u7684\u8fdb\u5c55\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u56fe\u7684\u534a\u76d1\u7763\u5b66\u4e60\u6280\u672f\u2014\u2014\u6807\u7b7e\u4f20\u64ad\uff08LP\uff09\uff0c\u901a\u8fc7\u6784\u5efa\u97f3\u9891\u5d4c\u5165\u7684\u76f8\u4f3c\u6027\u56fe\uff0c\u5728\u8f6c\u5bfc\u5f0f\u534a\u76d1\u7763\u8bbe\u7f6e\u4e2d\u5c06\u6709\u9650\u6807\u7b7e\u4fe1\u606f\u4ece\u5c0f\u578b\u6807\u6ce8\u5b50\u96c6\u4f20\u64ad\u5230\u66f4\u5927\u7684\u672a\u6807\u6ce8\u8bed\u6599\u5e93\u3002\u5e94\u7528\u4e8e\u5370\u5ea6\u827a\u672f\u97f3\u4e50\u7684\u4e24\u4e2a\u4efb\u52a1\uff1a\u62c9\u683c\u8bc6\u522b\u548c\u4e50\u5668\u5206\u7c7b\uff0c\u6574\u5408\u591a\u4e2a\u516c\u5171\u6570\u636e\u96c6\u548cPrasar Bharati\u6863\u6848\u9986\u7684\u989d\u5916\u5f55\u97f3\u3002", "result": "\u6807\u7b7e\u4f20\u64ad\u663e\u8457\u51cf\u5c11\u4e86\u6807\u6ce8\u5f00\u9500\uff0c\u76f8\u6bd4\u4f20\u7edf\u57fa\u7ebf\u65b9\u6cd5\uff08\u5305\u62ec\u57fa\u4e8e\u9884\u8bad\u7ec3\u5f52\u7eb3\u6a21\u578b\u7684\u65b9\u6cd5\uff09\u4ea7\u751f\u4e86\u66f4\u9ad8\u8d28\u91cf\u7684\u6807\u6ce8\u7ed3\u679c\u3002", "conclusion": "\u57fa\u4e8e\u56fe\u7684\u534a\u76d1\u7763\u5b66\u4e60\u6709\u6f5c\u529b\u6c11\u4e3b\u5316\u6570\u636e\u6807\u6ce8\u5e76\u52a0\u901f\u97f3\u4e50\u4fe1\u606f\u68c0\u7d22\u7684\u8fdb\u5c55\uff0c\u7279\u522b\u662f\u5728\u8d44\u6e90\u6709\u9650\u7684\u97f3\u9891\u548c\u97f3\u4e50\u9886\u57df\u3002"}}
{"id": "2601.03446", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.03446", "abs": "https://arxiv.org/abs/2601.03446", "authors": ["Melek Tuylu", "Eylem Erdogan"], "title": "Energy Harvesting in High Altitude Platform Station Enabled Sensor Networks", "comment": "PDF-only submission approved by arXiv support due to unresolved TeX compilation issues", "summary": "High altitude platform station (HAPS) systems are becoming crucial facilitators for future wireless communication networks, enhancing connectivity across all vertical communication layers, including small Internet of Things (IoT) sensors and devices, terrestrial users, and aerial devices. In the context of the widely recognized vertical heterogeneous network (VHetNet) architecture, HAPS systems can provide service to both aerial and ground users. However, integrating HAPS systems as a core element in the VHetNet architecture presents a considerable energy challenge, marking a prominent constraint for their operation. Driven by this challenge, we introduce an energy harvesting (EH) strategy tailored for HAPS systems, enabling a HAPS system to gather energy from another HAPS system, which is not constrained by energy limitations. To assess the performance capabilities of the proposed model, we derive outage probability (OP), ergodic capacity (EC) and verify them by using Monte Carlo (MC) simulations. Moreover, we explore the system in terms of throughput. The findings reveal that harnessing full potential of EH stands as a viable approach to meet the energy demands of HAPS systems.", "AI": {"tldr": "HAPS\u7cfb\u7edf\u5728VHetNet\u67b6\u6784\u4e2d\u9762\u4e34\u80fd\u8017\u6311\u6218\uff0c\u672c\u6587\u63d0\u51fa\u4e00\u79cdHAPS\u95f4\u7684\u80fd\u91cf\u6536\u96c6\u7b56\u7565\uff0c\u901a\u8fc7\u5206\u6790\u4e2d\u65ad\u6982\u7387\u3001\u904d\u5386\u5bb9\u91cf\u548c\u541e\u5410\u91cf\u9a8c\u8bc1\u5176\u53ef\u884c\u6027\u3002", "motivation": "\u9ad8\u7a7a\u5e73\u53f0\u7ad9(HAPS)\u7cfb\u7edf\u5728\u672a\u6765\u65e0\u7ebf\u901a\u4fe1\u7f51\u7edc\u4e2d\u626e\u6f14\u5173\u952e\u89d2\u8272\uff0c\u4f46\u4f5c\u4e3a\u5782\u76f4\u5f02\u6784\u7f51\u7edc(VHetNet)\u6838\u5fc3\u7ec4\u4ef6\u65f6\u9762\u4e34\u4e25\u91cd\u80fd\u8017\u7ea6\u675f\uff0c\u9700\u8981\u89e3\u51b3\u5176\u80fd\u6e90\u4f9b\u5e94\u95ee\u9898\u3002", "method": "\u63d0\u51faHAPS\u95f4\u7684\u80fd\u91cf\u6536\u96c6\u7b56\u7565\uff0c\u4f7f\u53d7\u80fd\u91cf\u9650\u5236\u7684HAPS\u80fd\u4ece\u4e0d\u53d7\u80fd\u91cf\u9650\u5236\u7684HAPS\u6536\u96c6\u80fd\u91cf\uff0c\u901a\u8fc7\u63a8\u5bfc\u4e2d\u65ad\u6982\u7387\u3001\u904d\u5386\u5bb9\u91cf\u7406\u8bba\u516c\u5f0f\uff0c\u5e76\u7528\u8499\u7279\u5361\u6d1b\u6a21\u62df\u9a8c\u8bc1\u3002", "result": "\u5206\u6790\u7ed3\u679c\u663e\u793a\uff0c\u80fd\u91cf\u6536\u96c6\u7b56\u7565\u80fd\u6709\u6548\u6ee1\u8db3HAPS\u7cfb\u7edf\u7684\u80fd\u6e90\u9700\u6c42\uff0c\u901a\u8fc7\u7406\u8bba\u63a8\u5bfc\u548c\u4eff\u771f\u9a8c\u8bc1\u4e86\u7cfb\u7edf\u6027\u80fd\u6307\u6807\uff0c\u5e76\u63a2\u8ba8\u4e86\u541e\u5410\u91cf\u8868\u73b0\u3002", "conclusion": "\u5145\u5206\u5229\u7528\u80fd\u91cf\u6536\u96c6\u6f5c\u529b\u662f\u6ee1\u8db3HAPS\u7cfb\u7edf\u80fd\u6e90\u9700\u6c42\u7684\u53ef\u884c\u9014\u5f84\uff0c\u4e3aVHetNet\u67b6\u6784\u4e2d\u7684HAPS\u96c6\u6210\u63d0\u4f9b\u4e86\u80fd\u6e90\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.03632", "categories": ["eess.AS", "cs.AI", "cs.SD"], "pdf": "https://arxiv.org/pdf/2601.03632", "abs": "https://arxiv.org/abs/2601.03632", "authors": ["Haitao Li", "Chunxiang Jin", "Chenglin Li", "Wenhao Guan", "Zhengxing Huang", "Xie Chen"], "title": "ReStyle-TTS: Relative and Continuous Style Control for Zero-Shot Speech Synthesis", "comment": null, "summary": "Zero-shot text-to-speech models can clone a speaker's timbre from a short reference audio, but they also strongly inherit the speaking style present in the reference. As a result, synthesizing speech with a desired style often requires carefully selecting reference audio, which is impractical when only limited or mismatched references are available. While recent controllable TTS methods attempt to address this issue, they typically rely on absolute style targets and discrete textual prompts, and therefore do not support continuous and reference-relative style control. We propose ReStyle-TTS, a framework that enables continuous and reference-relative style control in zero-shot TTS. Our key insight is that effective style control requires first reducing the model's implicit dependence on reference style before introducing explicit control mechanisms. To this end, we introduce Decoupled Classifier-Free Guidance (DCFG), which independently controls text and reference guidance, reducing reliance on reference style while preserving text fidelity. On top of this, we apply style-specific LoRAs together with Orthogonal LoRA Fusion to enable continuous and disentangled multi-attribute control, and introduce a Timbre Consistency Optimization module to mitigate timbre drift caused by weakened reference guidance. Experiments show that ReStyle-TTS enables user-friendly, continuous, and relative control over pitch, energy, and multiple emotions while maintaining intelligibility and speaker timbre, and performs robustly in challenging mismatched reference-target style scenarios.", "AI": {"tldr": "ReStyle-TTS\uff1a\u4e00\u4e2a\u652f\u6301\u8fde\u7eed\u3001\u76f8\u5bf9\u98ce\u683c\u63a7\u5236\u7684\u96f6\u6837\u672c\u6587\u672c\u8f6c\u8bed\u97f3\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u8026\u5206\u7c7b\u5668\u81ea\u7531\u5f15\u5bfc\u51cf\u5c11\u5bf9\u53c2\u8003\u97f3\u9891\u98ce\u683c\u7684\u4f9d\u8d56\uff0c\u5b9e\u73b0\u97f3\u9ad8\u3001\u80fd\u91cf\u548c\u60c5\u611f\u7684\u8fde\u7eed\u63a7\u5236\u3002", "motivation": "\u73b0\u6709\u7684\u96f6\u6837\u672c\u6587\u672c\u8f6c\u8bed\u97f3\u6a21\u578b\u867d\u7136\u80fd\u4ece\u77ed\u53c2\u8003\u97f3\u9891\u514b\u9686\u8bf4\u8bdd\u4eba\u97f3\u8272\uff0c\u4f46\u4f1a\u5f3a\u70c8\u7ee7\u627f\u53c2\u8003\u97f3\u9891\u4e2d\u7684\u8bf4\u8bdd\u98ce\u683c\u3002\u5f53\u53ea\u6709\u6709\u9650\u6216\u4e0d\u5339\u914d\u7684\u53c2\u8003\u97f3\u9891\u65f6\uff0c\u5408\u6210\u5177\u6709\u7279\u5b9a\u98ce\u683c\u7684\u8bed\u97f3\u9700\u8981\u7cbe\u5fc3\u9009\u62e9\u53c2\u8003\u97f3\u9891\uff0c\u8fd9\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u4e0d\u5207\u5b9e\u9645\u3002\u73b0\u6709\u53ef\u63a7TTS\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u7edd\u5bf9\u98ce\u683c\u76ee\u6807\u548c\u79bb\u6563\u6587\u672c\u63d0\u793a\uff0c\u4e0d\u652f\u6301\u8fde\u7eed\u548c\u76f8\u5bf9\u53c2\u8003\u7684\u98ce\u683c\u63a7\u5236\u3002", "method": "\u63d0\u51faReStyle-TTS\u6846\u67b6\uff1a1\uff09\u5f15\u5165\u89e3\u8026\u5206\u7c7b\u5668\u81ea\u7531\u5f15\u5bfc\uff08DCFG\uff09\uff0c\u72ec\u7acb\u63a7\u5236\u6587\u672c\u548c\u53c2\u8003\u5f15\u5bfc\uff0c\u51cf\u5c11\u5bf9\u53c2\u8003\u98ce\u683c\u7684\u4f9d\u8d56\u540c\u65f6\u4fdd\u6301\u6587\u672c\u4fdd\u771f\u5ea6\uff1b2\uff09\u5e94\u7528\u98ce\u683c\u7279\u5b9a\u7684LoRA\u914d\u5408\u6b63\u4ea4LoRA\u878d\u5408\uff0c\u5b9e\u73b0\u8fde\u7eed\u4e14\u89e3\u8026\u7684\u591a\u5c5e\u6027\u63a7\u5236\uff1b3\uff09\u5f15\u5165\u97f3\u8272\u4e00\u81f4\u6027\u4f18\u5316\u6a21\u5757\uff0c\u7f13\u89e3\u56e0\u51cf\u5f31\u53c2\u8003\u5f15\u5bfc\u5bfc\u81f4\u7684\u97f3\u8272\u6f02\u79fb\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cReStyle-TTS\u80fd\u591f\u5b9e\u73b0\u7528\u6237\u53cb\u597d\u7684\u8fde\u7eed\u76f8\u5bf9\u63a7\u5236\uff0c\u8986\u76d6\u97f3\u9ad8\u3001\u80fd\u91cf\u548c\u591a\u79cd\u60c5\u611f\uff0c\u540c\u65f6\u4fdd\u6301\u53ef\u61c2\u5ea6\u548c\u8bf4\u8bdd\u4eba\u97f3\u8272\u3002\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u4e0d\u5339\u914d\u53c2\u8003-\u76ee\u6807\u98ce\u683c\u573a\u666f\u4e2d\u8868\u73b0\u7a33\u5065\u3002", "conclusion": "ReStyle-TTS\u6210\u529f\u89e3\u51b3\u4e86\u96f6\u6837\u672c\u6587\u672c\u8f6c\u8bed\u97f3\u4e2d\u8fde\u7eed\u76f8\u5bf9\u98ce\u683c\u63a7\u5236\u7684\u96be\u9898\uff0c\u901a\u8fc7\u51cf\u5c11\u6a21\u578b\u5bf9\u53c2\u8003\u98ce\u683c\u7684\u9690\u5f0f\u4f9d\u8d56\u5e76\u5f15\u5165\u663e\u5f0f\u63a7\u5236\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u7075\u6d3b\u7684\u98ce\u683c\u8c03\u6574\u80fd\u529b\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.03527", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.03527", "abs": "https://arxiv.org/abs/2601.03527", "authors": ["Ravneel Prasad", "Emanuele Viterbo"], "title": "Intensity Fluctuation Dynamics in XPM", "comment": null, "summary": "Cross-Phase Modulation (XPM) constitutes a critical nonlinear impairment in high-capacity Wavelength Division Multiplexing (WDM) systems, significantly driven by intensity fluctuations (IFs) that evolve due to chromatic dispersion. This paper presents an enhanced XPM model that explicitly incorporates frequency-domain IF growth along the fiber, improving upon prior models that focused primarily on temporal pulse deformation. A direct correlation between this frequency-domain growth and XPM-induced phase distortions is established and analyzed. Results demonstrate that IF evolution, particularly at lower frequencies, profoundly affects XPM phase fluctuation spectra and phase variance. Validated through simulations, the model accurately predicts these spectral characteristics across various system parameters. Furthermore, the derived phase variance enables accurate prediction of system performance in terms of Bit Error Ratio (BER). These findings highlight the necessity of modeling frequency-domain IF evolution to accurately characterize XPM impairments, offering guidance for the design of advanced optical networks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u589e\u5f3a\u578bXPM\u6a21\u578b\uff0c\u660e\u786e\u8003\u8651\u5149\u7ea4\u4e2d\u9891\u57df\u5f3a\u5ea6\u6ce2\u52a8\u589e\u957f\uff0c\u6539\u8fdb\u4e86\u5148\u524d\u4e3b\u8981\u5173\u6ce8\u65f6\u57df\u8109\u51b2\u5f62\u53d8\u7684\u6a21\u578b\uff0c\u80fd\u66f4\u51c6\u786e\u9884\u6d4bXPM\u76f8\u4f4d\u5931\u771f\u548c\u7cfb\u7edf\u6027\u80fd\u3002", "motivation": "\u5728\u9ad8\u5bb9\u91cfWDM\u7cfb\u7edf\u4e2d\uff0c\u4ea4\u53c9\u76f8\u4f4d\u8c03\u5236(XPM)\u662f\u5173\u952e\u7684\u975e\u7ebf\u6027\u635f\u4f24\uff0c\u4e3b\u8981\u7531\u8272\u6563\u5f15\u8d77\u7684\u5f3a\u5ea6\u6ce2\u52a8\u9a71\u52a8\u3002\u73b0\u6709\u6a21\u578b\u4e3b\u8981\u5173\u6ce8\u65f6\u57df\u8109\u51b2\u5f62\u53d8\uff0c\u672a\u80fd\u5145\u5206\u63cf\u8ff0\u9891\u57df\u5f3a\u5ea6\u6ce2\u52a8\u589e\u957f\u5bf9XPM\u7684\u5f71\u54cd\uff0c\u9700\u8981\u66f4\u7cbe\u786e\u7684\u5efa\u6a21\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u589e\u5f3a\u578bXPM\u6a21\u578b\uff0c\u660e\u786e\u7eb3\u5165\u5149\u7ea4\u4e2d\u9891\u57df\u5f3a\u5ea6\u6ce2\u52a8\u589e\u957f\uff0c\u5efa\u7acb\u9891\u57df\u589e\u957f\u4e0eXPM\u8bf1\u5bfc\u76f8\u4f4d\u5931\u771f\u4e4b\u95f4\u7684\u76f4\u63a5\u5173\u8054\uff0c\u901a\u8fc7\u4eff\u771f\u9a8c\u8bc1\u6a21\u578b\u51c6\u786e\u6027\uff0c\u5e76\u63a8\u5bfc\u76f8\u4f4d\u65b9\u5dee\u6765\u9884\u6d4b\u7cfb\u7edf\u6027\u80fd\u3002", "result": "\u7ed3\u679c\u663e\u793a\u5f3a\u5ea6\u6ce2\u52a8\u6f14\u5316\uff08\u7279\u522b\u662f\u4f4e\u9891\u90e8\u5206\uff09\u663e\u8457\u5f71\u54cdXPM\u76f8\u4f4d\u6ce2\u52a8\u9891\u8c31\u548c\u76f8\u4f4d\u65b9\u5dee\u3002\u6a21\u578b\u80fd\u51c6\u786e\u9884\u6d4b\u4e0d\u540c\u7cfb\u7edf\u53c2\u6570\u4e0b\u7684\u9891\u8c31\u7279\u6027\uff0c\u63a8\u5bfc\u7684\u76f8\u4f4d\u65b9\u5dee\u80fd\u7cbe\u786e\u9884\u6d4b\u7cfb\u7edf\u8bef\u7801\u7387\u6027\u80fd\u3002", "conclusion": "\u4e3a\u51c6\u786e\u8868\u5f81XPM\u635f\u4f24\uff0c\u5fc5\u987b\u5efa\u6a21\u9891\u57df\u5f3a\u5ea6\u6ce2\u52a8\u6f14\u5316\u3002\u8be5\u6a21\u578b\u4e3a\u5148\u8fdb\u5149\u7f51\u7edc\u8bbe\u8ba1\u63d0\u4f9b\u6307\u5bfc\uff0c\u5f3a\u8c03\u4e86\u9891\u57df\u5206\u6790\u5728XPM\u5efa\u6a21\u4e2d\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2601.03712", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2601.03712", "abs": "https://arxiv.org/abs/2601.03712", "authors": ["Yifan Hu", "Peiji Yang", "Zhisheng Wang", "Yicheng Zhong", "Rui Liu"], "title": "TellWhisper: Tell Whisper Who Speaks When", "comment": "14 pages, 6 figures, 8 tables, submitted to ACL 2026", "summary": "Multi-speaker automatic speech recognition (MASR) aims to predict ''who spoke when and what'' from multi-speaker speech, a key technology for multi-party dialogue understanding. However, most existing approaches decouple temporal modeling and speaker modeling when addressing ''when'' and ''who'': some inject speaker cues before encoding (e.g., speaker masking), which can cause irreversible information loss; others fuse identity by mixing speaker posteriors after encoding, which may entangle acoustic content with speaker identity. This separation is brittle under rapid turn-taking and overlapping speech, often leading to degraded performance. To address these limitations, we propose TellWhisper, a unified framework that jointly models speaker identity and temporal within the speech encoder. Specifically, we design TS-RoPE, a time-speaker rotary positional encoding: time coordinates are derived from frame indices, while speaker coordinates are derived from speaker activity and pause cues. By applying region-specific rotation angles, the model explicitly captures per-speaker continuity, speaker-turn transitions, and state dynamics, enabling the attention mechanism to simultaneously attend to ''when'' and ''who''. Moreover, to estimate frame-level speaker activity, we develop Hyper-SD, which casts speaker classification in hyperbolic space to enhance inter-class separation and refine speaker-activity estimates. Extensive experiments demonstrate the effectiveness of the proposed approach.", "AI": {"tldr": "TellWhisper\uff1a\u7edf\u4e00\u6846\u67b6\uff0c\u901a\u8fc7\u65f6\u95f4-\u8bf4\u8bdd\u4eba\u65cb\u8f6c\u4f4d\u7f6e\u7f16\u7801(TS-RoPE)\u5728\u8bed\u97f3\u7f16\u7801\u5668\u4e2d\u8054\u5408\u5efa\u6a21\u8bf4\u8bdd\u4eba\u8eab\u4efd\u548c\u65f6\u95f4\u4fe1\u606f\uff0c\u89e3\u51b3\u591a\u8bf4\u8bdd\u4ebaASR\u4e2d\"\u8c01\u5728\u4f55\u65f6\u8bf4\u4e86\u4ec0\u4e48\"\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u591a\u8bf4\u8bdd\u4ebaASR\u65b9\u6cd5\u5c06\u65f6\u95f4\u5efa\u6a21\u548c\u8bf4\u8bdd\u4eba\u5efa\u6a21\u89e3\u8026\uff1a\u6709\u4e9b\u5728\u7f16\u7801\u524d\u6ce8\u5165\u8bf4\u8bdd\u4eba\u7ebf\u7d22\uff08\u5982\u8bf4\u8bdd\u4eba\u63a9\u7801\uff09\uff0c\u53ef\u80fd\u5bfc\u81f4\u4e0d\u53ef\u9006\u4fe1\u606f\u635f\u5931\uff1b\u6709\u4e9b\u5728\u7f16\u7801\u540e\u878d\u5408\u8eab\u4efd\uff08\u5982\u6df7\u5408\u8bf4\u8bdd\u4eba\u540e\u9a8c\uff09\uff0c\u53ef\u80fd\u4f7f\u58f0\u5b66\u5185\u5bb9\u4e0e\u8bf4\u8bdd\u4eba\u8eab\u4efd\u7ea0\u7f20\u3002\u8fd9\u79cd\u5206\u79bb\u5728\u5feb\u901f\u8bdd\u8f6e\u8f6c\u6362\u548c\u91cd\u53e0\u8bed\u97f3\u4e2d\u8868\u73b0\u8106\u5f31\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002", "method": "1. \u63d0\u51faTellWhisper\u7edf\u4e00\u6846\u67b6\uff0c\u5728\u8bed\u97f3\u7f16\u7801\u5668\u4e2d\u8054\u5408\u5efa\u6a21\u8bf4\u8bdd\u4eba\u8eab\u4efd\u548c\u65f6\u95f4\u4fe1\u606f\uff1b2. \u8bbe\u8ba1TS-RoPE\uff08\u65f6\u95f4-\u8bf4\u8bdd\u4eba\u65cb\u8f6c\u4f4d\u7f6e\u7f16\u7801\uff09\uff1a\u65f6\u95f4\u5750\u6807\u6765\u81ea\u5e27\u7d22\u5f15\uff0c\u8bf4\u8bdd\u4eba\u5750\u6807\u6765\u81ea\u8bf4\u8bdd\u4eba\u6d3b\u52a8\u548c\u505c\u987f\u7ebf\u7d22\uff0c\u901a\u8fc7\u533a\u57df\u7279\u5b9a\u65cb\u8f6c\u89d2\u5ea6\u4f7f\u6ce8\u610f\u529b\u673a\u5236\u540c\u65f6\u5173\u6ce8\"\u4f55\u65f6\"\u548c\"\u8c01\"\uff1b3. \u5f00\u53d1Hyper-SD\uff1a\u5728\u53cc\u66f2\u7a7a\u95f4\u4e2d\u8fdb\u884c\u8bf4\u8bdd\u4eba\u5206\u7c7b\uff0c\u589e\u5f3a\u7c7b\u95f4\u5206\u79bb\u5e76\u7ec6\u5316\u8bf4\u8bdd\u4eba\u6d3b\u52a8\u4f30\u8ba1\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\u4e86\u6240\u63d0\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "TellWhisper\u901a\u8fc7\u7edf\u4e00\u7684\u65f6\u95f4-\u8bf4\u8bdd\u4eba\u5efa\u6a21\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u5feb\u901f\u8bdd\u8f6e\u8f6c\u6362\u548c\u91cd\u53e0\u8bed\u97f3\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u591a\u8bf4\u8bdd\u4ebaASR\u6027\u80fd\u3002"}}
{"id": "2601.03944", "categories": ["eess.SP", "cs.SD"], "pdf": "https://arxiv.org/pdf/2601.03944", "abs": "https://arxiv.org/abs/2601.03944", "authors": ["Xin Wang", "H\u00e9ctor Delgado", "Nicholas Evans", "Xuechen Liu", "Tomi Kinnunen", "Hemlata Tak", "Kong Aik Lee", "Ivan Kukanov", "Md Sahidullah", "Massimiliano Todisco", "Junichi Yamagishi"], "title": "ASVspoof 5: Evaluation of Spoofing, Deepfake, and Adversarial Attack Detection Using Crowdsourced Speech", "comment": "Submitted", "summary": "ASVspoof 5 is the fifth edition in a series of challenges which promote the study of speech spoofing and deepfake detection solutions. A significant change from previous challenge editions is a new crowdsourced database collected from a substantially greater number of speakers under diverse recording conditions, and a mix of cutting-edge and legacy generative speech technology. With the new database described elsewhere, we provide in this paper an overview of the ASVspoof 5 challenge results for the submissions of 53 participating teams. While many solutions perform well, performance degrades under adversarial attacks and the application of neural encoding/compression schemes. Together with a review of post-challenge results, we also report a study of calibration in addition to other principal challenges and outline a road-map for the future of ASVspoof.", "AI": {"tldr": "ASVspoof 5\u6311\u6218\u8d5b\u662f\u7b2c\u4e94\u7248\u8bed\u97f3\u6b3a\u9a97\u68c0\u6d4b\u7ade\u8d5b\uff0c\u4f7f\u7528\u65b0\u7684\u4f17\u5305\u6570\u636e\u5e93\uff0c\u5305\u542b\u66f4\u591a\u8bf4\u8bdd\u8005\u548c\u591a\u6837\u5f55\u97f3\u6761\u4ef6\uff0c\u8bc4\u4f30\u4e8653\u4e2a\u56e2\u961f\u7684\u65b9\u6848\uff0c\u53d1\u73b0\u6027\u80fd\u5728\u5bf9\u6297\u653b\u51fb\u548c\u795e\u7ecf\u7f16\u7801/\u538b\u7f29\u4e0b\u4f1a\u4e0b\u964d\u3002", "motivation": "\u63a8\u52a8\u8bed\u97f3\u6b3a\u9a97\u548c\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u6280\u672f\u7684\u7814\u7a76\uff0c\u901a\u8fc7\u6311\u6218\u8d5b\u5f62\u5f0f\u4fc3\u8fdb\u8be5\u9886\u57df\u53d1\u5c55\uff0c\u4f7f\u7528\u66f4\u771f\u5b9e\u3001\u591a\u6837\u5316\u7684\u6570\u636e\u96c6\u6765\u8bc4\u4f30\u73b0\u6709\u65b9\u6cd5\u3002", "method": "\u521b\u5efa\u65b0\u7684\u4f17\u5305\u6570\u636e\u5e93\uff0c\u5305\u542b\u66f4\u591a\u8bf4\u8bdd\u8005\u548c\u591a\u6837\u5f55\u97f3\u6761\u4ef6\uff0c\u6df7\u5408\u524d\u6cbf\u548c\u4f20\u7edf\u751f\u6210\u8bed\u97f3\u6280\u672f\uff0c\u7ec4\u7ec7\u6311\u6218\u8d5b\u6536\u96c653\u4e2a\u56e2\u961f\u7684\u89e3\u51b3\u65b9\u6848\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u8bb8\u591a\u89e3\u51b3\u65b9\u6848\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u5bf9\u6297\u653b\u51fb\u548c\u795e\u7ecf\u7f16\u7801/\u538b\u7f29\u65b9\u6848\u5e94\u7528\u4e0b\u6027\u80fd\u4f1a\u4e0b\u964d\uff0c\u6311\u6218\u8d5b\u7ed3\u679c\u663e\u793a\u4e86\u5f53\u524d\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "conclusion": "ASVspoof 5\u6311\u6218\u8d5b\u5c55\u793a\u4e86\u8bed\u97f3\u6b3a\u9a97\u68c0\u6d4b\u7684\u8fdb\u5c55\u548c\u6311\u6218\uff0c\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u8def\u7ebf\u56fe\uff0c\u5305\u62ec\u6821\u51c6\u7814\u7a76\u548c\u5176\u4ed6\u4e3b\u8981\u6311\u6218\u3002"}}
{"id": "2601.03535", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.03535", "abs": "https://arxiv.org/abs/2601.03535", "authors": ["Zhiwen Zhou", "Chaoyue Zhang", "Xiaoli Xu", "Yong Zeng"], "title": "OpenISAC: An Open-Source Real-Time Experimentation Platform for OFDM-ISAC with Over-the-Air Synchronization", "comment": "Submitted to IEEE Transactions on Wireless Communications for possible publication", "summary": "Integrated sensing and communication (ISAC) is envisioned to be one of the key usage scenarios for the sixth generation (6G) mobile communication networks. While significant progresses have been achieved for the theoretical studies, the further advancement of ISAC is hampered by the lack of accessible, open-source, and real-time experimental platforms. To address this gap, we introduce OpenISAC, a versatile and high-performance open-source platform for real-time ISAC experimentation. OpenISAC utilizes orthogonal frequency division multiplexing (OFDM) waveform and implements crucial sensing functionalities, including both monostatic and bistatic delay-Doppler sensing. A key feature of our platform is a novel over-the-air (OTA) synchronization mechanism that enables robust bistatic operations without requiring a wired connection between nodes. The platform is built entirely on open-source software, leveraging the universal software radio peripheral (USRP) hardware driver (UHD) library, thus eliminating the need for any commercial licenses. It supports a wide range of software-defined radios, from the cost-effective USRP B200 series to the high-performance X400 series. The physical layer modulator and demodulator are implemented with C++ for high-speed processing, while the sensing data is streamed to a Python environment, providing a user-friendly interface for rapid prototyping and validation of sensing signal processing algorithms. With flexible parameter selection and real-time communication and sensing operation, OpenISAC serves as a powerful and accessible tool for the academic and research communities to explore and innovate within the field of OFDM-ISAC.", "AI": {"tldr": "OpenISAC\u662f\u4e00\u4e2a\u5f00\u6e90\u5b9e\u65f6ISAC\u5b9e\u9a8c\u5e73\u53f0\uff0c\u91c7\u7528OFDM\u6ce2\u5f62\uff0c\u652f\u6301\u5355\u7ad9\u548c\u53cc\u7ad9\u65f6\u5ef6-\u591a\u666e\u52d2\u611f\u77e5\uff0c\u65e0\u9700\u8282\u70b9\u95f4\u6709\u7ebf\u8fde\u63a5\u7684OTA\u540c\u6b65\u673a\u5236\uff0c\u57fa\u4e8eUSRP\u786c\u4ef6\u548c\u5f00\u6e90\u8f6f\u4ef6\u6784\u5efa\u3002", "motivation": "ISAC\u662f6G\u7684\u5173\u952e\u5e94\u7528\u573a\u666f\uff0c\u4f46\u7406\u8bba\u7814\u7a76\u8fdb\u5c55\u53d7\u9650\u4e8e\u7f3a\u4e4f\u53ef\u8bbf\u95ee\u3001\u5f00\u6e90\u3001\u5b9e\u65f6\u7684\u5b9e\u9a8c\u5e73\u53f0\uff0c\u963b\u788d\u4e86ISAC\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002", "method": "\u6784\u5efa\u57fa\u4e8eOFDM\u6ce2\u5f62\u7684\u5f00\u6e90\u5e73\u53f0\uff0c\u5b9e\u73b0\u5355\u7ad9\u548c\u53cc\u7ad9\u65f6\u5ef6-\u591a\u666e\u52d2\u611f\u77e5\u529f\u80fd\uff0c\u91c7\u7528\u521b\u65b0\u7684OTA\u540c\u6b65\u673a\u5236\u652f\u6301\u7a33\u5065\u7684\u53cc\u7ad9\u64cd\u4f5c\uff0c\u7269\u7406\u5c42\u8c03\u5236\u89e3\u8c03\u5668\u7528C++\u5b9e\u73b0\u9ad8\u901f\u5904\u7406\uff0c\u611f\u77e5\u6570\u636e\u6d41\u5f0f\u4f20\u8f93\u5230Python\u73af\u5883\u63d0\u4f9b\u7528\u6237\u53cb\u597d\u63a5\u53e3\u3002", "result": "\u5f00\u53d1\u51faOpenISAC\u5e73\u53f0\uff0c\u652f\u6301\u4ece\u4f4e\u6210\u672cUSRP B200\u7cfb\u5217\u5230\u9ad8\u6027\u80fdX400\u7cfb\u5217\u7684\u591a\u79cd\u8f6f\u4ef6\u5b9a\u4e49\u65e0\u7ebf\u7535\uff0c\u65e0\u9700\u5546\u4e1a\u8bb8\u53ef\u8bc1\uff0c\u63d0\u4f9b\u7075\u6d3b\u53c2\u6570\u9009\u62e9\u548c\u5b9e\u65f6\u901a\u4fe1\u611f\u77e5\u64cd\u4f5c\u3002", "conclusion": "OpenISAC\u4f5c\u4e3a\u5f3a\u5927\u4e14\u53ef\u8bbf\u95ee\u7684\u5de5\u5177\uff0c\u4e3a\u5b66\u672f\u754c\u548c\u7814\u7a76\u793e\u533a\u63a2\u7d22\u548c\u521b\u65b0OFDM-ISAC\u9886\u57df\u63d0\u4f9b\u4e86\u6709\u529b\u652f\u6301\u3002"}}
{"id": "2601.04178", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2601.04178", "abs": "https://arxiv.org/abs/2601.04178", "authors": ["Florian Schmid", "Chi Ian Tang", "Sanjeel Parekh", "Vamsi Krishna Ithapu", "Juan Azcarreta Ortiz", "Giacomo Ferroni", "Yijun Qian", "Arnoldas Jasonas", "Cosmin Frateanu", "Camilla Clark", "Gerhard Widmer", "\u00c7a\u011fda\u015f Bilen"], "title": "Sound Event Detection with Boundary-Aware Optimization and Inference", "comment": "Submitted to IEEE Signal Processing Letters", "summary": "Temporal detection problems appear in many fields including time-series estimation, activity recognition and sound event detection (SED). In this work, we propose a new approach to temporal event modeling by explicitly modeling event onsets and offsets, and by introducing boundary-aware optimization and inference strategies that substantially enhance temporal event detection. The presented methodology incorporates new temporal modeling layers - Recurrent Event Detection (RED) and Event Proposal Network (EPN) - which, together with tailored loss functions, enable more effective and precise temporal event detection. We evaluate the proposed method in the SED domain using a subset of the temporally-strongly annotated portion of AudioSet. Experimental results show that our approach not only outperforms traditional frame-wise SED models with state-of-the-art post-processing, but also removes the need for post-processing hyperparameter tuning, and scales to achieve new state-of-the-art performance across all AudioSet Strong classes.", "AI": {"tldr": "\u63d0\u51fa\u65b0\u7684\u65f6\u95f4\u4e8b\u4ef6\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u4e8b\u4ef6\u5f00\u59cb\u548c\u7ed3\u675f\u8fb9\u754c\uff0c\u7ed3\u5408\u8fb9\u754c\u611f\u77e5\u4f18\u5316\u548c\u63a8\u7406\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u65f6\u95f4\u4e8b\u4ef6\u68c0\u6d4b\u6027\u80fd", "motivation": "\u65f6\u95f4\u68c0\u6d4b\u95ee\u9898\u5728\u65f6\u95f4\u5e8f\u5217\u4f30\u8ba1\u3001\u6d3b\u52a8\u8bc6\u522b\u548c\u58f0\u97f3\u4e8b\u4ef6\u68c0\u6d4b\u7b49\u9886\u57df\u666e\u904d\u5b58\u5728\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u65f6\u95f4\u4e8b\u4ef6\u5efa\u6a21\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u66f4\u7cbe\u786e\u7684\u8fb9\u754c\u68c0\u6d4b", "method": "\u63d0\u51faRecurrent Event Detection (RED)\u548cEvent Proposal Network (EPN)\u4e24\u4e2a\u65b0\u7684\u65f6\u95f4\u5efa\u6a21\u5c42\uff0c\u7ed3\u5408\u5b9a\u5236\u7684\u635f\u5931\u51fd\u6570\uff0c\u91c7\u7528\u8fb9\u754c\u611f\u77e5\u7684\u4f18\u5316\u548c\u63a8\u7406\u7b56\u7565", "result": "\u5728AudioSet\u5f3a\u6807\u6ce8\u5b50\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u4f18\u4e8e\u4f20\u7edf\u5e27\u7ea7SED\u6a21\u578b\u52a0\u6700\u5148\u8fdb\u540e\u5904\u7406\u7684\u65b9\u6cd5\uff0c\u8fd8\u6d88\u9664\u4e86\u540e\u5904\u7406\u8d85\u53c2\u6570\u8c03\u4f18\u7684\u9700\u6c42\uff0c\u5728\u6240\u6709AudioSet\u5f3a\u7c7b\u522b\u4e0a\u8fbe\u5230\u65b0\u7684\u6700\u5148\u8fdb\u6027\u80fd", "conclusion": "\u63d0\u51fa\u7684\u8fb9\u754c\u611f\u77e5\u65f6\u95f4\u4e8b\u4ef6\u5efa\u6a21\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u65f6\u95f4\u4e8b\u4ef6\u68c0\u6d4b\u7684\u7cbe\u5ea6\u548c\u6548\u679c\uff0c\u4e3a\u65f6\u95f4\u68c0\u6d4b\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u7684\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2601.03601", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.03601", "abs": "https://arxiv.org/abs/2601.03601", "authors": ["Kequan Zhou", "Guangyi Zhang", "Hanlei Li", "Yunlong Cai", "Shengli Liu", "Guanding Yu"], "title": "F$^4$-CKM: Learning Channel Knowledge Map with Radio Frequency Radiance Field Rendering", "comment": null, "summary": "In 6G mobile communications, acquiring accurate and timely channel state information (CSI) becomes increasingly challenging due to the growing antenna array size and bandwidth. To alleviate the CSI feedback burden, the channel knowledge map (CKM) has emerged as a promising approach by leveraging environment-aware techniques to predict CSI based solely on user locations. However, how to effectively construct a CKM remains an open issue. In this paper, we propose F$^4$-CKM, a novel CKM construction framework characterized by four distinctive features: radiance Field rendering, spatial-Frequency-awareness, location-Free usage, and Fast learning. Central to our design is the adaptation of radiance field rendering techniques from computer vision to the radio frequency (RF) domain, enabled by a novel Wireless Radiator Representation (WiRARE) network that captures the spatial-frequency characteristics of wireless channels. Additionally, a novel shaping filter module and an angular sampling strategy are introduced to facilitate CKM construction. Extensive experiments demonstrate that F$^4$-CKM significantly outperforms existing baselines in terms of wireless channel prediction accuracy and efficiency.", "AI": {"tldr": "F\u2074-CKM\uff1a\u4e00\u79cd\u57fa\u4e8e\u8f90\u5c04\u573a\u6e32\u67d3\u76846G\u4fe1\u9053\u77e5\u8bc6\u5730\u56fe\u6784\u5efa\u6846\u67b6\uff0c\u901a\u8fc7\u65e0\u7ebf\u8f90\u5c04\u8868\u793a\u7f51\u7edc\u6355\u6349\u7a7a\u95f4-\u9891\u7387\u7279\u6027\uff0c\u5b9e\u73b0\u9ad8\u6548\u4fe1\u9053\u9884\u6d4b", "motivation": "6G\u79fb\u52a8\u901a\u4fe1\u4e2d\uff0c\u5929\u7ebf\u9635\u5217\u89c4\u6a21\u548c\u5e26\u5bbd\u589e\u957f\u4f7f\u5f97\u83b7\u53d6\u51c6\u786e\u53ca\u65f6\u7684\u4fe1\u9053\u72b6\u6001\u4fe1\u606f(CSI)\u6108\u53d1\u56f0\u96be\u3002\u4f20\u7edfCSI\u53cd\u9988\u8d1f\u62c5\u91cd\uff0c\u4fe1\u9053\u77e5\u8bc6\u5730\u56fe(CKM)\u901a\u8fc7\u73af\u5883\u611f\u77e5\u6280\u672f\u4ec5\u57fa\u4e8e\u7528\u6237\u4f4d\u7f6e\u9884\u6d4bCSI\uff0c\u4f46\u5982\u4f55\u6709\u6548\u6784\u5efaCKM\u4ecd\u662f\u4e00\u4e2a\u5f00\u653e\u95ee\u9898\u3002", "method": "\u63d0\u51faF\u2074-CKM\u6846\u67b6\uff0c\u5177\u6709\u56db\u4e2a\u7279\u5f81\uff1a\u8f90\u5c04\u573a\u6e32\u67d3\u3001\u7a7a\u95f4-\u9891\u7387\u611f\u77e5\u3001\u4f4d\u7f6e\u65e0\u5173\u4f7f\u7528\u3001\u5feb\u901f\u5b66\u4e60\u3002\u6838\u5fc3\u662f\u5c06\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u7684\u8f90\u5c04\u573a\u6e32\u67d3\u6280\u672f\u9002\u914d\u5230\u5c04\u9891\u57df\uff0c\u901a\u8fc7\u65b0\u578b\u65e0\u7ebf\u8f90\u5c04\u8868\u793a(WiRARE)\u7f51\u7edc\u6355\u6349\u65e0\u7ebf\u4fe1\u9053\u7684\u7a7a\u95f4-\u9891\u7387\u7279\u6027\u3002\u8fd8\u5f15\u5165\u4e86\u65b0\u578b\u6574\u5f62\u6ee4\u6ce2\u6a21\u5757\u548c\u89d2\u5ea6\u91c7\u6837\u7b56\u7565\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cF\u2074-CKM\u5728\u65e0\u7ebf\u4fe1\u9053\u9884\u6d4b\u51c6\u786e\u6027\u548c\u6548\u7387\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "F\u2074-CKM\u4e3a6G\u4fe1\u9053\u77e5\u8bc6\u5730\u56fe\u6784\u5efa\u63d0\u4f9b\u4e86\u4e00\u79cd\u521b\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u8f90\u5c04\u573a\u6e32\u67d3\u6280\u672f\u548c\u7a7a\u95f4-\u9891\u7387\u611f\u77e5\u7f51\u7edc\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5927\u89c4\u6a21\u5929\u7ebf\u7cfb\u7edf\u4e0b\u7684\u4fe1\u9053\u9884\u6d4b\u95ee\u9898\uff0c\u5177\u6709\u91cd\u8981\u7684\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2601.03639", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.03639", "abs": "https://arxiv.org/abs/2601.03639", "authors": ["Kecheng Zhang", "Weijie Yuan", "Maria Sabrina Greco"], "title": "Zak-OTFS ISAC with Bistatic Sensing via Semi-Blind Atomic Norm Denoising Scheme", "comment": "Submitted to IEEE for possible publication", "summary": "Integrated sensing and communication (ISAC) through Zak-transform-based orthogonal time frequency space (Zak-OTFS) modulation is a promising solution for high-mobility scenarios. Realizing accurate bistatic sensing and robust communication necessitates precise channel estimation; however, this remains a formidable challenge in doubly dispersive environments, where fractional delay-Doppler shifts induce severe channel spreading. This paper proposes a semi-blind atomic norm denoising scheme for Zak-OTFS ISAC with bistatic sensing. We first derive the discrete-time input-output (I/O) relationship of Zak-OTFS under fractional delay-Doppler shifts and rectangular windowing. Based on this I/O relation, we formulate the joint channel parameter estimation and data detection task as an atomic norm denoising problem, utilizing the negative square penalty method to handle the non-convex discrete constellation constraints. To solve this problem efficiently, we develop an accelerated iterative algorithm that integrates majorization-minimization, accelerated projected gradient, and inexact accelerated proximal gradient methods. We provide a rigorous convergence proof for the proposed algorithm. Simulation results demonstrate that the proposed scheme achieves super-resolution sensing accuracy and communication performance approaching the perfect channel state information lower bound.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8eZak-OTFS ISAC\u7cfb\u7edf\u7684\u534a\u76f2\u539f\u5b50\u8303\u6570\u53bb\u566a\u65b9\u6848\uff0c\u901a\u8fc7\u539f\u5b50\u8303\u6570\u4f18\u5316\u5b9e\u73b0\u53cc\u57fa\u5730\u611f\u77e5\u4e2d\u7684\u9ad8\u7cbe\u5ea6\u4fe1\u9053\u4f30\u8ba1\u548c\u6570\u636e\u68c0\u6d4b", "motivation": "\u5728\u53cc\u5f25\u6563\u73af\u5883\u4e2d\uff0c\u5206\u6570\u5ef6\u8fdf-\u591a\u666e\u52d2\u504f\u79fb\u5bfc\u81f4\u4e25\u91cd\u7684\u4fe1\u9053\u6269\u5c55\uff0c\u4f7f\u5f97\u7cbe\u786e\u7684\u4fe1\u9053\u4f30\u8ba1\u6210\u4e3aZak-OTFS ISAC\u7cfb\u7edf\u5b9e\u73b0\u51c6\u786e\u53cc\u57fa\u5730\u611f\u77e5\u548c\u9c81\u68d2\u901a\u4fe1\u7684\u5173\u952e\u6311\u6218", "method": "\u9996\u5148\u63a8\u5bfc\u5206\u6570\u5ef6\u8fdf-\u591a\u666e\u52d2\u504f\u79fb\u548c\u77e9\u5f62\u7a97\u4e0bZak-OTFS\u7684\u79bb\u6563\u65f6\u95f4\u8f93\u5165\u8f93\u51fa\u5173\u7cfb\uff0c\u7136\u540e\u5c06\u8054\u5408\u4fe1\u9053\u53c2\u6570\u4f30\u8ba1\u548c\u6570\u636e\u68c0\u6d4b\u4efb\u52a1\u8868\u8ff0\u4e3a\u539f\u5b50\u8303\u6570\u53bb\u566a\u95ee\u9898\uff0c\u4f7f\u7528\u8d1f\u5e73\u65b9\u60e9\u7f5a\u65b9\u6cd5\u5904\u7406\u975e\u51f8\u79bb\u6563\u661f\u5ea7\u7ea6\u675f\uff0c\u6700\u540e\u5f00\u53d1\u4e86\u7ed3\u5408\u4e3b\u5316\u6700\u5c0f\u5316\u3001\u52a0\u901f\u6295\u5f71\u68af\u5ea6\u548c\u975e\u7cbe\u786e\u52a0\u901f\u8fd1\u7aef\u68af\u5ea6\u65b9\u6cd5\u7684\u8fed\u4ee3\u7b97\u6cd5", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6848\u5b9e\u73b0\u4e86\u8d85\u5206\u8fa8\u7387\u611f\u77e5\u7cbe\u5ea6\uff0c\u901a\u4fe1\u6027\u80fd\u63a5\u8fd1\u5b8c\u7f8e\u4fe1\u9053\u72b6\u6001\u4fe1\u606f\u4e0b\u754c", "conclusion": "\u63d0\u51fa\u7684\u534a\u76f2\u539f\u5b50\u8303\u6570\u53bb\u566a\u65b9\u6848\u6709\u6548\u89e3\u51b3\u4e86Zak-OTFS ISAC\u7cfb\u7edf\u5728\u53cc\u5f25\u6563\u73af\u5883\u4e2d\u7684\u4fe1\u9053\u4f30\u8ba1\u96be\u9898\uff0c\u4e3a\u9ad8\u79fb\u52a8\u6027\u573a\u666f\u4e0b\u7684\u96c6\u6210\u611f\u77e5\u4e0e\u901a\u4fe1\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2601.03735", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.03735", "abs": "https://arxiv.org/abs/2601.03735", "authors": ["Carl Collmann", "Ahmad Nimr", "Gerhard Fettweis"], "title": "Cramer-Rao Bound for Angle of Arrival Estimates in True-Time-Delay Systems", "comment": "5 pages, 7 figures", "summary": "In the context of joint communication and sensing JC&S, the challenge of obtaining accurate parameter estimates is of interest. Parameter estimates, such as the AoA can be utilized for solving the initial access problem, interference mitigation, localization of users or monitoring of the environment and synchronization of MIMO systems. Recently, TTD systems have gained attention for fast beam training during initial access and mitigation of beam squinting. This work derives the CRB for angle estimates in typical TTD systems. Properties of the CRB and the Fisher information are investigated and numerically evaluated. Finally, methods for angle estimation such as ML and established estimators are utilized to solve the angle estimation problem using a uniform linear array.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a8\u5bfc\u4e86\u5178\u578bTTD\u7cfb\u7edf\u4e2d\u89d2\u5ea6\u4f30\u8ba1\u7684\u514b\u62c9\u7f8e-\u7f57\u4e0b\u754c(CRB)\uff0c\u7814\u7a76\u4e86Fisher\u4fe1\u606f\u7684\u7279\u6027\uff0c\u5e76\u5229\u7528ML\u7b49\u4f30\u8ba1\u65b9\u6cd5\u89e3\u51b3ULA\u7684\u89d2\u5ea6\u4f30\u8ba1\u95ee\u9898\u3002", "motivation": "\u5728\u8054\u5408\u901a\u4fe1\u4e0e\u611f\u77e5(JC&S)\u80cc\u666f\u4e0b\uff0c\u83b7\u53d6\u51c6\u786e\u7684\u53c2\u6570\u4f30\u8ba1\uff08\u5982AoA\uff09\u5bf9\u4e8e\u89e3\u51b3\u521d\u59cb\u63a5\u5165\u3001\u5e72\u6270\u6291\u5236\u3001\u7528\u6237\u5b9a\u4f4d\u3001\u73af\u5883\u76d1\u6d4b\u548cMIMO\u7cfb\u7edf\u540c\u6b65\u7b49\u95ee\u9898\u81f3\u5173\u91cd\u8981\u3002TTD\u7cfb\u7edf\u56e0\u5176\u5728\u5feb\u901f\u6ce2\u675f\u8bad\u7ec3\u548c\u6ce2\u675f\u659c\u89c6\u6291\u5236\u65b9\u9762\u7684\u4f18\u52bf\u800c\u53d7\u5230\u5173\u6ce8\u3002", "method": "\u63a8\u5bfc\u4e86\u5178\u578bTTD\u7cfb\u7edf\u4e2d\u89d2\u5ea6\u4f30\u8ba1\u7684\u514b\u62c9\u7f8e-\u7f57\u4e0b\u754c(CRB)\uff0c\u7814\u7a76\u4e86Fisher\u4fe1\u606f\u7684\u7279\u6027\u5e76\u8fdb\u884c\u6570\u503c\u8bc4\u4f30\u3002\u4f7f\u7528\u6700\u5927\u4f3c\u7136(ML)\u548c\u5df2\u6709\u4f30\u8ba1\u65b9\u6cd5\u6765\u89e3\u51b3\u5747\u5300\u7ebf\u6027\u9635\u5217\u7684\u89d2\u5ea6\u4f30\u8ba1\u95ee\u9898\u3002", "result": "\u8bba\u6587\u5f97\u51fa\u4e86TTD\u7cfb\u7edf\u4e2d\u89d2\u5ea6\u4f30\u8ba1\u7684\u7406\u8bba\u6027\u80fd\u754c\u9650\uff08CRB\uff09\uff0c\u5206\u6790\u4e86Fisher\u4fe1\u606f\u7684\u6027\u8d28\uff0c\u5e76\u901a\u8fc7\u6570\u503c\u8bc4\u4f30\u9a8c\u8bc1\u4e86\u7406\u8bba\u7ed3\u679c\u3002\u5c55\u793a\u4e86ML\u7b49\u4f30\u8ba1\u65b9\u6cd5\u5728\u5747\u5300\u7ebf\u6027\u9635\u5217\u4e0a\u7684\u5e94\u7528\u6548\u679c\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3aTTD\u7cfb\u7edf\u4e2d\u7684\u89d2\u5ea6\u4f30\u8ba1\u63d0\u4f9b\u4e86\u7406\u8bba\u6027\u80fd\u754c\u9650\u548c\u5206\u6790\u6846\u67b6\uff0c\u6709\u52a9\u4e8e\u4f18\u5316JC&S\u7cfb\u7edf\u4e2d\u7684\u53c2\u6570\u4f30\u8ba1\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u5feb\u901f\u6ce2\u675f\u8bad\u7ec3\u548c\u6ce2\u675f\u659c\u89c6\u6291\u5236\u65b9\u9762\u5177\u6709\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2601.03745", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.03745", "abs": "https://arxiv.org/abs/2601.03745", "authors": ["Weijia Wang", "Changsheng You", "Xiaodan Shao", "Rui Zhang"], "title": "Two-stage Multi-beam Training for Multiuser Millimeter-Wave Communications", "comment": null, "summary": "In this letter, we study an efficient multi-beam training method for multiuser millimeter-wave communication systems. Unlike the conventional single-beam training method that relies on exhaustive search, multi-beam training design faces a key challenge in balancing the trade-off between beam training overhead and success beam-identification rate, exacerbated by severe inter-beam interference. To tackle this challenge, we propose a new two-stage multi-beam training method with two distinct multi-beam patterns to enable fast and accurate user angle identification. Specifically, in the first stage, the antenna array is divided into sparse subarrays to generate multiple beams (with high array gains), for identifying candidate user angles. In the second stage, the array is redivided into dense subarrays to generate flexibly steered wide beams, for which a cross-validation method is employed to effectively resolve the remaining angular ambiguity in the first stage. Last, numerical results demonstrate that the proposed method significantly improves the success beam-identification rate compared to existing multi-beam training methods, while retaining or even reducing the required beam training overhead.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7528\u4e8e\u6beb\u7c73\u6ce2\u901a\u4fe1\u7cfb\u7edf\u7684\u9ad8\u6548\u591a\u6ce2\u675f\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8bbe\u8ba1\u5e73\u8861\u8bad\u7ec3\u5f00\u9500\u4e0e\u6ce2\u675f\u8bc6\u522b\u6210\u529f\u7387", "motivation": "\u4f20\u7edf\u5355\u6ce2\u675f\u8bad\u7ec3\u65b9\u6cd5\u4f9d\u8d56\u7a77\u4e3e\u641c\u7d22\uff0c\u800c\u591a\u6ce2\u675f\u8bad\u7ec3\u9762\u4e34\u8bad\u7ec3\u5f00\u9500\u4e0e\u6ce2\u675f\u8bc6\u522b\u6210\u529f\u7387\u4e4b\u95f4\u7684\u6743\u8861\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u4e25\u91cd\u6ce2\u675f\u95f4\u5e72\u6270\u7684\u60c5\u51b5\u4e0b", "method": "\u63d0\u51fa\u4e24\u9636\u6bb5\u591a\u6ce2\u675f\u8bad\u7ec3\u65b9\u6cd5\uff1a\u7b2c\u4e00\u9636\u6bb5\u5c06\u5929\u7ebf\u9635\u5217\u5212\u5206\u4e3a\u7a00\u758f\u5b50\u9635\u5217\u751f\u6210\u9ad8\u589e\u76ca\u591a\u6ce2\u675f\u8bc6\u522b\u5019\u9009\u7528\u6237\u89d2\u5ea6\uff1b\u7b2c\u4e8c\u9636\u6bb5\u91cd\u65b0\u5212\u5206\u4e3a\u5bc6\u96c6\u5b50\u9635\u5217\u751f\u6210\u7075\u6d3b\u8f6c\u5411\u7684\u5bbd\u6ce2\u675f\uff0c\u91c7\u7528\u4ea4\u53c9\u9a8c\u8bc1\u65b9\u6cd5\u89e3\u51b3\u7b2c\u4e00\u9636\u6bb5\u5269\u4f59\u7684\u89d2\u6a21\u7cca\u95ee\u9898", "result": "\u6570\u503c\u7ed3\u679c\u8868\u660e\uff0c\u76f8\u6bd4\u73b0\u6709\u591a\u6ce2\u675f\u8bad\u7ec3\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u6ce2\u675f\u8bc6\u522b\u6210\u529f\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u751a\u81f3\u51cf\u5c11\u4e86\u6240\u9700\u7684\u6ce2\u675f\u8bad\u7ec3\u5f00\u9500", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u591a\u6ce2\u675f\u8bad\u7ec3\u4e2d\u8bad\u7ec3\u5f00\u9500\u4e0e\u8bc6\u522b\u6210\u529f\u7387\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\uff0c\u4e3a\u6beb\u7c73\u6ce2\u591a\u7528\u6237\u901a\u4fe1\u7cfb\u7edf\u63d0\u4f9b\u4e86\u9ad8\u6548\u53ef\u9760\u7684\u6ce2\u675f\u8bad\u7ec3\u89e3\u51b3\u65b9\u6848"}}
{"id": "2601.03789", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.03789", "abs": "https://arxiv.org/abs/2601.03789", "authors": ["Jun Jiang", "Xiaolong Ruan", "Shugong Xu"], "title": "CSI-MAE: A Masked Autoencoder-based Channel Foundation Model", "comment": "6 pages", "summary": "Self-Supervised Learning (SSL) has emerged as a key technique in machine learning, tackling challenges such as limited labeled data, high annotation costs, and variable wireless channel conditions. It is essential for developing Channel Foundation Models (CFMs), which extract latent features from channel state information (CSI) and adapt to different wireless settings. Yet, existing CFMs have notable drawbacks: heavy reliance on scenario-specific data hinders generalization, they focus on single/dual tasks, and lack zero-shot learning ability. In this paper, we propose CSI-MAE, a generalized CFM leveraging masked autoencoder for cross-scenario generalization. Trained on 3GPP channel model datasets, it integrates sensing and communication via CSI perception and generation, proven effective across diverse tasks. A lightweight decoder finetuning strategy cuts training costs while maintaining competitive performance. Under this approach, CSI-MAE matches or surpasses supervised models. With full-parameter finetuning, it achieves the state-of-the-art performance. Its exceptional zero-shot transferability also rivals supervised techniques in cross-scenario applications, driving wireless communication innovation.", "AI": {"tldr": "CSI-MAE\uff1a\u57fa\u4e8e\u63a9\u7801\u81ea\u7f16\u7801\u5668\u7684\u901a\u7528\u4fe1\u9053\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7CSI\u611f\u77e5\u548c\u751f\u6210\u6574\u5408\u611f\u77e5\u4e0e\u901a\u4fe1\uff0c\u5728\u8de8\u573a\u666f\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5177\u5907\u96f6\u6837\u672c\u8fc1\u79fb\u80fd\u529b", "motivation": "\u73b0\u6709\u4fe1\u9053\u57fa\u7840\u6a21\u578b\u5b58\u5728\u4e09\u5927\u95ee\u9898\uff1a\u8fc7\u5ea6\u4f9d\u8d56\u573a\u666f\u7279\u5b9a\u6570\u636e\u5bfc\u81f4\u6cdb\u5316\u80fd\u529b\u5dee\u3001\u4e13\u6ce8\u4e8e\u5355/\u53cc\u4efb\u52a1\u3001\u7f3a\u4e4f\u96f6\u6837\u672c\u5b66\u4e60\u80fd\u529b\u3002\u9700\u8981\u5f00\u53d1\u80fd\u591f\u8de8\u573a\u666f\u6cdb\u5316\u7684\u901a\u7528\u6a21\u578b", "method": "\u63d0\u51faCSI-MAE\u6a21\u578b\uff0c\u57fa\u4e8e\u63a9\u7801\u81ea\u7f16\u7801\u5668\u67b6\u6784\uff0c\u57283GPP\u4fe1\u9053\u6a21\u578b\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\uff0c\u6574\u5408CSI\u611f\u77e5\u548c\u751f\u6210\u529f\u80fd\u3002\u91c7\u7528\u8f7b\u91cf\u7ea7\u89e3\u7801\u5668\u5fae\u8c03\u7b56\u7565\u964d\u4f4e\u8bad\u7ec3\u6210\u672c", "result": "CSI-MAE\u5728\u5339\u914d\u6216\u8d85\u8d8a\u76d1\u7763\u6a21\u578b\u6027\u80fd\u7684\u540c\u65f6\uff0c\u5177\u5907\u51fa\u8272\u7684\u96f6\u6837\u672c\u8fc1\u79fb\u80fd\u529b\u3002\u901a\u8fc7\u5168\u53c2\u6570\u5fae\u8c03\u8fbe\u5230SOTA\u6027\u80fd\uff0c\u5728\u8de8\u573a\u666f\u5e94\u7528\u4e2d\u4e0e\u76d1\u7763\u6280\u672f\u76f8\u5ab2\u7f8e", "conclusion": "CSI-MAE\u4f5c\u4e3a\u901a\u7528\u4fe1\u9053\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u5b66\u4e60\u548c\u63a9\u7801\u81ea\u7f16\u7801\u5668\u67b6\u6784\u89e3\u51b3\u4e86\u73b0\u6709\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u63a8\u52a8\u4e86\u65e0\u7ebf\u901a\u4fe1\u9886\u57df\u7684\u521b\u65b0"}}
{"id": "2601.04069", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.04069", "abs": "https://arxiv.org/abs/2601.04069", "authors": ["Lukas Schynol", "Marius Pesavento"], "title": "Hybrid Downlink Beamforming with Outage Constraints under Imperfect CSI using Model-Driven Deep Learning", "comment": null, "summary": "We consider energy-efficient multi-user hybrid downlink beamforming (BF) and power allocation under imperfect channel state information (CSI) and probabilistic outage constraints. In this domain, classical optimization methods resort to computationally costly conic optimization problems. Meanwhile, generic deep network (DN) architectures lack interpretability and require large training data sets to generalize well. In this paper, we therefore propose a lightweight model-aided deep learning architecture based on a greedy selection algorithm for analog beam codewords. The architecture relies on an instance-adaptive augmentation of the signal model to estimate the impact of the CSI error. To learn the DN parameters, we derive a novel and efficient implicit representation of the nested constrained BF problem and prove sufficient conditions for the existence of the corresponding gradient. In the loss function, we utilize an annealing-based approximation of the outage compared to conventional quantile-based loss terms. This approximation adaptively anneals towards the exact probabilistic constraint depending on the current level of quality of service (QoS) violation. Simulations validate that the proposed DN can achieve the nominal outage level under CSI error due to channel estimation and channel compression, while allocating less power than benchmarks. Thereby, a single trained model generalizes to different numbers of users, QoS requirements and levels of CSI quality. We further show that the adaptive annealing-based loss function can accelerate the training and yield a better power-outage trade-off.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u8d2a\u5a6a\u9009\u62e9\u7b97\u6cd5\u7684\u8f7b\u91cf\u7ea7\u6a21\u578b\u8f85\u52a9\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\uff0c\u7528\u4e8e\u4e0d\u5b8c\u7f8eCSI\u4e0b\u7684\u80fd\u6548\u591a\u7528\u6237\u6df7\u5408\u4e0b\u884c\u6ce2\u675f\u6210\u5f62\u548c\u529f\u7387\u5206\u914d\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u9000\u706b\u8fd1\u4f3c\u5b9e\u73b0\u6982\u7387\u4e2d\u65ad\u7ea6\u675f\uff0c\u5728CSI\u8bef\u5dee\u4e0b\u8fbe\u5230\u6807\u79f0\u4e2d\u65ad\u6c34\u5e73\u4e14\u5206\u914d\u66f4\u5c11\u529f\u7387\u3002", "motivation": "\u4f20\u7edf\u4f18\u5316\u65b9\u6cd5\u5728\u8ba1\u7b97\u4e0a\u6210\u672c\u9ad8\u6602\uff0c\u800c\u901a\u7528\u6df1\u5ea6\u7f51\u7edc\u67b6\u6784\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u4e14\u9700\u8981\u5927\u91cf\u8bad\u7ec3\u6570\u636e\u3002\u9700\u8981\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u53ef\u89e3\u91ca\u4e14\u80fd\u5904\u7406\u4e0d\u5b8c\u7f8eCSI\u548c\u6982\u7387\u4e2d\u65ad\u7ea6\u675f\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u8d2a\u5a6a\u9009\u62e9\u7b97\u6cd5\u7684\u6a21\u578b\u8f85\u52a9\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\uff0c\u901a\u8fc7\u5b9e\u4f8b\u81ea\u9002\u5e94\u589e\u5f3a\u4fe1\u53f7\u6a21\u578b\u6765\u4f30\u8ba1CSI\u8bef\u5dee\u5f71\u54cd\u3002\u91c7\u7528\u65b0\u9896\u9ad8\u6548\u7684\u9690\u5f0f\u8868\u793a\u65b9\u6cd5\u5904\u7406\u5d4c\u5957\u7ea6\u675f\u6ce2\u675f\u6210\u5f62\u95ee\u9898\uff0c\u5e76\u4f7f\u7528\u81ea\u9002\u5e94\u9000\u706b\u8fd1\u4f3c\u6982\u7387\u4e2d\u65ad\u7ea6\u675f\u7684\u635f\u5931\u51fd\u6570\u3002", "result": "\u4eff\u771f\u9a8c\u8bc1\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u6df1\u5ea6\u7f51\u7edc\u5728\u4fe1\u9053\u4f30\u8ba1\u548c\u4fe1\u9053\u538b\u7f29\u5bfc\u81f4\u7684CSI\u8bef\u5dee\u4e0b\u80fd\u591f\u8fbe\u5230\u6807\u79f0\u4e2d\u65ad\u6c34\u5e73\uff0c\u540c\u65f6\u6bd4\u57fa\u51c6\u65b9\u6cd5\u5206\u914d\u66f4\u5c11\u529f\u7387\u3002\u5355\u4e2a\u8bad\u7ec3\u6a21\u578b\u80fd\u591f\u6cdb\u5316\u5230\u4e0d\u540c\u7528\u6237\u6570\u3001QoS\u8981\u6c42\u548cCSI\u8d28\u91cf\u6c34\u5e73\u3002", "conclusion": "\u63d0\u51fa\u7684\u8f7b\u91cf\u7ea7\u6a21\u578b\u8f85\u52a9\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\u6709\u6548\u89e3\u51b3\u4e86\u4e0d\u5b8c\u7f8eCSI\u4e0b\u7684\u80fd\u6548\u591a\u7528\u6237\u6df7\u5408\u6ce2\u675f\u6210\u5f62\u95ee\u9898\uff0c\u81ea\u9002\u5e94\u9000\u706b\u635f\u5931\u51fd\u6570\u52a0\u901f\u4e86\u8bad\u7ec3\u5e76\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u529f\u7387-\u4e2d\u65ad\u6743\u8861\uff0c\u5177\u6709\u826f\u597d\u6cdb\u5316\u80fd\u529b\u3002"}}

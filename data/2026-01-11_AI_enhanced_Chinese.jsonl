{"id": "2601.04415", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.04415", "abs": "https://arxiv.org/abs/2601.04415", "authors": ["Charalambos Hadjipanayi", "Maowen Yin", "Alan Bannon", "Ziwei Chen", "Timothy G. Constandinou"], "title": "Towards Radar-Agnostic Gait Analysis Across UWB and FMCW Systems", "comment": null, "summary": "Radar sensing has emerged in recent years as a promising solution for unobtrusive and continuous in-home gait monitoring. This study evaluates whether a unified processing framework can be applied to radar-based spatiotemporal gait analysis independent of radar modality. The framework is validated using collocated impulse-radio ultra-wideband (IR-UWB) and frequency-modulated continuous-wave (FMCW) radars under identical processing settings, without modality-specific tuning, during repeated overground walking trials with 10 healthy participants. A modality-independent approach for automatic walking-segment identification is also introduced to ensure fair and reproducible modality performance assessment. Clinically relevant spatiotemporal gait parameters, including stride time, stride length, walking speed, swing time, and stance time, extracted from each modality were compared against gold-standard motion capture reference estimates. Across all parameters, both radar modalities achieved comparably high mean estimation accuracy in the range of 85-98%, with inter-modality differences remaining below 4.1%, resulting in highly overlapping accuracy distributions. Correlation and Bland-Altman analyses revealed minimal bias, comparable limits of agreement, and strong agreement with reference estimates, while intraclass correlation analysis demonstrated high consistency between radar modalities. These findings indicate that no practically meaningful performance differences arise from radar modality when using a shared processing framework, supporting the feasibility of radar-agnostic gait analysis systems.", "AI": {"tldr": "\u8be5\u7814\u7a76\u9a8c\u8bc1\u4e86\u7edf\u4e00\u5904\u7406\u6846\u67b6\u53ef\u7528\u4e8e\u4e0d\u540c\u96f7\u8fbe\u6a21\u6001\u7684\u6b65\u6001\u5206\u6790\uff0cIR-UWB\u548cFMCW\u96f7\u8fbe\u5728\u76f8\u540c\u8bbe\u7f6e\u4e0b\u5747\u80fd\u8fbe\u523085-98%\u7684\u51c6\u786e\u7387\uff0c\u6a21\u6001\u95f4\u5dee\u5f02\u5c0f\u4e8e4.1%\uff0c\u652f\u6301\u96f7\u8fbe\u65e0\u5173\u7684\u6b65\u6001\u5206\u6790\u7cfb\u7edf\u53ef\u884c\u6027\u3002", "motivation": "\u96f7\u8fbe\u4f20\u611f\u5df2\u6210\u4e3a\u5bb6\u5ead\u6b65\u6001\u76d1\u6d4b\u7684\u6709\u524d\u666f\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u9700\u8981\u9a8c\u8bc1\u662f\u5426\u53ef\u4ee5\u4f7f\u7528\u7edf\u4e00\u5904\u7406\u6846\u67b6\u8fdb\u884c\u96f7\u8fbe\u65f6\u7a7a\u6b65\u6001\u5206\u6790\uff0c\u800c\u4e0d\u4f9d\u8d56\u4e8e\u7279\u5b9a\u96f7\u8fbe\u6a21\u6001\u3002", "method": "\u4f7f\u7528\u5e76\u7f6e\u7684IR-UWB\u548cFMCW\u96f7\u8fbe\uff0c\u5728\u76f8\u540c\u5904\u7406\u8bbe\u7f6e\u4e0b\uff08\u65e0\u6a21\u6001\u7279\u5b9a\u8c03\u4f18\uff09\uff0c\u5bf910\u540d\u5065\u5eb7\u53c2\u4e0e\u8005\u8fdb\u884c\u91cd\u590d\u5730\u9762\u884c\u8d70\u8bd5\u9a8c\u3002\u5f15\u5165\u6a21\u6001\u65e0\u5173\u7684\u81ea\u52a8\u884c\u8d70\u6bb5\u8bc6\u522b\u65b9\u6cd5\uff0c\u63d0\u53d6\u4e34\u5e8a\u76f8\u5173\u7684\u65f6\u7a7a\u6b65\u6001\u53c2\u6570\uff0c\u5e76\u4e0e\u91d1\u6807\u51c6\u8fd0\u52a8\u6355\u6349\u53c2\u8003\u4f30\u8ba1\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u4e24\u79cd\u96f7\u8fbe\u6a21\u6001\u5728\u6240\u6709\u53c2\u6570\u4e0a\u90fd\u5b9e\u73b0\u4e8685-98%\u7684\u9ad8\u5e73\u5747\u4f30\u8ba1\u51c6\u786e\u7387\uff0c\u6a21\u6001\u95f4\u5dee\u5f02\u4fdd\u6301\u57284.1%\u4ee5\u4e0b\uff0c\u51c6\u786e\u7387\u5206\u5e03\u9ad8\u5ea6\u91cd\u53e0\u3002\u76f8\u5173\u6027\u548cBland-Altman\u5206\u6790\u663e\u793a\u504f\u5dee\u6700\u5c0f\uff0c\u4e00\u81f4\u6027\u754c\u9650\u76f8\u5f53\uff0c\u4e0e\u53c2\u8003\u4f30\u8ba1\u6709\u5f3a\u4e00\u81f4\u6027\uff0cICC\u5206\u6790\u663e\u793a\u96f7\u8fbe\u6a21\u6001\u95f4\u9ad8\u5ea6\u4e00\u81f4\u3002", "conclusion": "\u4f7f\u7528\u5171\u4eab\u5904\u7406\u6846\u67b6\u65f6\uff0c\u96f7\u8fbe\u6a21\u6001\u4e4b\u95f4\u6ca1\u6709\u5b9e\u9645\u610f\u4e49\u7684\u6027\u80fd\u5dee\u5f02\uff0c\u652f\u6301\u96f7\u8fbe\u65e0\u5173\u6b65\u6001\u5206\u6790\u7cfb\u7edf\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2601.04478", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.04478", "abs": "https://arxiv.org/abs/2601.04478", "authors": ["Shadeeb Hossain"], "title": "Prediction of Cellular Malignancy Using Electrical Impedance Signatures and Supervised Machine Learning", "comment": null, "summary": "Bioelectrical properties of cells such as relative permittivity, conductivity, and characteristic time constants vary significantly between healthy and malignant cells across different frequencies. These distinctions provide a promising foundation for diagnostic and classification applications. This study systematically reviewed 33 scholarly articles to compile datasets of quantitative bioelectric parameters and evaluated their utility in predictive modeling. Three supervised machine learning algorithms- Random Forest (RF), Support Vector Machine (SVM), and K-Nearest Neighbor (KNN) were implemented and tuned using key hyperparameters to assess classification performance. Model effectiveness was evaluated using accuracy and F1 score as performance metrics. Results demonstrate that Random Forest achieved the highest predictive accuracy of ~ 90% when configured with a maximum depth of 4 and 100 estimators. These findings highlight the potential of integrating bioelectrical property analysis with machine learning for improved diagnostic decision-making. Similarly, for KNN and SVM, the F1 score peaked at approximately 78% and 76.5%, respectively. Future work will explore incorporating additional discriminative features, leveraging stimulated datasets, and optimizing hyperparameter through advanced search strategies. Ultimately, hardware prototype with embedded micro-electrodes and real-time control systems could pave the path for practical diagnostic tools capable of in-situ cell classification.", "AI": {"tldr": "\u8be5\u7814\u7a76\u7cfb\u7edf\u56de\u987e\u4e8633\u7bc7\u5b66\u672f\u6587\u7ae0\uff0c\u6536\u96c6\u7ec6\u80de\u751f\u7269\u7535\u53c2\u6570\u6570\u636e\u96c6\uff0c\u5e76\u8bc4\u4f30\u5176\u5728\u9884\u6d4b\u5efa\u6a21\u4e2d\u7684\u5e94\u7528\u3002\u4f7f\u7528\u968f\u673a\u68ee\u6797\u3001\u652f\u6301\u5411\u91cf\u673a\u548cK\u8fd1\u90bb\u4e09\u79cd\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u8fdb\u884c\u5206\u7c7b\uff0c\u968f\u673a\u68ee\u6797\u5728\u7279\u5b9a\u53c2\u6570\u914d\u7f6e\u4e0b\u8fbe\u5230\u7ea690%\u7684\u6700\u9ad8\u51c6\u786e\u7387\u3002", "motivation": "\u5065\u5eb7\u7ec6\u80de\u548c\u6076\u6027\u7ec6\u80de\u7684\u751f\u7269\u7535\u7279\u6027\uff08\u5982\u76f8\u5bf9\u4ecb\u7535\u5e38\u6570\u3001\u7535\u5bfc\u7387\u3001\u7279\u5f81\u65f6\u95f4\u5e38\u6570\uff09\u5728\u4e0d\u540c\u9891\u7387\u4e0b\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u8fd9\u4e3a\u8bca\u65ad\u548c\u5206\u7c7b\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u57fa\u7840\u3002\u7814\u7a76\u65e8\u5728\u8bc4\u4f30\u8fd9\u4e9b\u751f\u7269\u7535\u53c2\u6570\u5728\u9884\u6d4b\u5efa\u6a21\u4e2d\u7684\u6548\u7528\u3002", "method": "\u7cfb\u7edf\u56de\u987e33\u7bc7\u5b66\u672f\u6587\u7ae0\uff0c\u6536\u96c6\u5b9a\u91cf\u751f\u7269\u7535\u53c2\u6570\u6570\u636e\u96c6\u3002\u4f7f\u7528\u4e09\u79cd\u76d1\u7763\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\uff1a\u968f\u673a\u68ee\u6797(RF)\u3001\u652f\u6301\u5411\u91cf\u673a(SVM)\u548cK\u8fd1\u90bb(KNN)\uff0c\u901a\u8fc7\u5173\u952e\u8d85\u53c2\u6570\u8c03\u4f18\u8bc4\u4f30\u5206\u7c7b\u6027\u80fd\u3002\u4f7f\u7528\u51c6\u786e\u7387\u548cF1\u5206\u6570\u4f5c\u4e3a\u6027\u80fd\u6307\u6807\u3002", "result": "\u968f\u673a\u68ee\u6797\u5728\u6700\u5927\u6df1\u5ea6\u4e3a4\u3001100\u4e2a\u4f30\u8ba1\u5668\u7684\u914d\u7f6e\u4e0b\u8fbe\u5230\u6700\u9ad8\u9884\u6d4b\u51c6\u786e\u7387\u7ea690%\u3002KNN\u548cSVM\u7684F1\u5206\u6570\u5206\u522b\u8fbe\u5230\u7ea678%\u548c76.5%\u3002\u7ed3\u679c\u8868\u660e\u751f\u7269\u7535\u7279\u6027\u5206\u6790\u4e0e\u673a\u5668\u5b66\u4e60\u7ed3\u5408\u5177\u6709\u826f\u597d\u8bca\u65ad\u6f5c\u529b\u3002", "conclusion": "\u7814\u7a76\u8bc1\u660e\u4e86\u6574\u5408\u751f\u7269\u7535\u7279\u6027\u5206\u6790\u4e0e\u673a\u5668\u5b66\u4e60\u53ef\u6539\u5584\u8bca\u65ad\u51b3\u7b56\u3002\u672a\u6765\u5de5\u4f5c\u5c06\u63a2\u7d22\u52a0\u5165\u66f4\u591a\u5224\u522b\u7279\u5f81\u3001\u5229\u7528\u523a\u6fc0\u6570\u636e\u96c6\u3001\u4f18\u5316\u8d85\u53c2\u6570\u641c\u7d22\u7b56\u7565\uff0c\u5e76\u5f00\u53d1\u5e26\u6709\u5d4c\u5165\u5f0f\u5fae\u7535\u6781\u548c\u5b9e\u65f6\u63a7\u5236\u7cfb\u7edf\u7684\u786c\u4ef6\u539f\u578b\uff0c\u5b9e\u73b0\u539f\u4f4d\u7ec6\u80de\u5206\u7c7b\u7684\u5b9e\u7528\u8bca\u65ad\u5de5\u5177\u3002"}}
{"id": "2601.04488", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.04488", "abs": "https://arxiv.org/abs/2601.04488", "authors": ["Yinghui He", "Long Fan", "Lei Xie", "Dusit Niyato", "Chau Yuen", "Jun Luo"], "title": "Invisible Walls: Privacy-Preserving ISAC Empowered by Reconfigurable Intelligent Surfaces", "comment": "This paper has been submitted to IEEE", "summary": "The environmental and target-related information inherently carried in wireless signals, such as channel state information (CSI), has brought increasing attention to integrated sensing and communication (ISAC). However, it also raises pressing concerns about privacy leakage through eavesdropping. While existing efforts have attempted to mitigate this issue, they either fail to account for the needs of legitimate communication and sensing users or rely on hardware with high complexity and cost. To overcome these limitations, we propose PrivISAC, a plug-and-play, low-cost solution that leverages RIS to protect user privacy while preserving ISAC performance. At the core of PrivISAC is a novel strategy in which each RIS row is assigned two distinct beamforming vectors, from which we deliberately construct a limited set of RIS configurations. During operation, exactly one configuration is randomly activated at each time slot to introduce additional perturbations, effectively masking sensitive sensing information from unauthorized eavesdroppers. To jointly ensure privacy protection and communication performance, we design the two vectors such that their responses remain nearly identical in the communication direction, thereby preserving stable, high-throughput transmission, while exhibiting pronounced differences in the sensing direction, which introduces sufficient perturbations to thwart eavesdroppers. Additionally, to enable legitimate sensing under such randomized configurations, we introduce a time-domain masking and demasking method that allows the authorized receiver to associate each CSI sample with its underlying configuration and eliminate configuration-induced discrepancies, thereby recovering valid CSI. We implement PrivISAC on commodity wireless devices and experiment results show that PrivISAC provides strong privacy protection while preserving high-quality legitimate ISAC.", "AI": {"tldr": "PrivISAC\uff1a\u4e00\u79cd\u5229\u7528RIS\u4fdd\u62a4ISAC\u7528\u6237\u9690\u79c1\u7684\u5373\u63d2\u5373\u7528\u4f4e\u6210\u672c\u65b9\u6848\uff0c\u901a\u8fc7\u968f\u673a\u6fc0\u6d3bRIS\u914d\u7f6e\u6765\u6270\u52a8\u7a83\u542c\u8005\uff0c\u540c\u65f6\u4fdd\u6301\u5408\u6cd5\u901a\u4fe1\u548c\u611f\u77e5\u6027\u80fd\u3002", "motivation": "\u65e0\u7ebf\u4fe1\u53f7\u4e2d\u7684CSI\u7b49\u73af\u5883\u4fe1\u606f\u867d\u7136\u652f\u6301ISAC\uff0c\u4f46\u4e5f\u5e26\u6765\u9690\u79c1\u6cc4\u9732\u98ce\u9669\u3002\u73b0\u6709\u65b9\u6848\u8981\u4e48\u5ffd\u89c6\u5408\u6cd5\u7528\u6237\u9700\u6c42\uff0c\u8981\u4e48\u4f9d\u8d56\u9ad8\u590d\u6742\u5ea6\u786c\u4ef6\uff0c\u9700\u8981\u4e00\u79cd\u4f4e\u6210\u672c\u3001\u5b9e\u7528\u7684\u9690\u79c1\u4fdd\u62a4\u65b9\u6848\u3002", "method": "\u4e3aRIS\u6bcf\u884c\u5206\u914d\u4e24\u4e2a\u4e0d\u540c\u6ce2\u675f\u8d4b\u5f62\u5411\u91cf\uff0c\u6784\u5efa\u6709\u9650\u914d\u7f6e\u96c6\u3002\u6bcf\u4e2a\u65f6\u9699\u968f\u673a\u6fc0\u6d3b\u4e00\u4e2a\u914d\u7f6e\uff0c\u5728\u901a\u4fe1\u65b9\u5411\u4fdd\u6301\u7a33\u5b9a\uff0c\u5728\u611f\u77e5\u65b9\u5411\u5f15\u5165\u6270\u52a8\u3002\u540c\u65f6\u8bbe\u8ba1\u65f6\u57df\u63a9\u853d\u4e0e\u89e3\u63a9\u853d\u65b9\u6cd5\uff0c\u8ba9\u6388\u6743\u63a5\u6536\u5668\u80fd\u6d88\u9664\u914d\u7f6e\u5dee\u5f02\uff0c\u6062\u590d\u6709\u6548CSI\u3002", "result": "\u5728\u5546\u7528\u65e0\u7ebf\u8bbe\u5907\u4e0a\u5b9e\u73b0PrivISAC\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6848\u80fd\u63d0\u4f9b\u5f3a\u5927\u7684\u9690\u79c1\u4fdd\u62a4\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u8d28\u91cf\u7684\u5408\u6cd5ISAC\u6027\u80fd\u3002", "conclusion": "PrivISAC\u6210\u529f\u89e3\u51b3\u4e86ISAC\u4e2d\u7684\u9690\u79c1\u4fdd\u62a4\u95ee\u9898\uff0c\u901a\u8fc7RIS\u7684\u5de7\u5999\u914d\u7f6e\u5b9e\u73b0\u4e86\u9690\u79c1\u4fdd\u62a4\u4e0e\u7cfb\u7edf\u6027\u80fd\u7684\u5e73\u8861\uff0c\u4e3a\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2601.04581", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.04581", "abs": "https://arxiv.org/abs/2601.04581", "authors": ["Yi Zhou", "Li Wang", "Hang Su", "Tian Wang"], "title": "Spectral point transformer for significant wave height estimation from sea clutter", "comment": null, "summary": "This paper presents a method for estimating significant wave height (Hs) from sparse S_pectral P_oint using a T_ransformer-based approach (SPT). Based on empirical observations that only a minority of spectral points with strong power contribute to wave energy, the proposed SPT effectively integrates geometric and spectral characteristics of ocean surface waves to estimate Hs through multi-dimensional feature representation. The experiment reveals an intriguing phenomenon: the learned features of SPT align well with physical dispersion relations, where the contribution-score map of selected points is concentrated along dispersion curves. Compared to conventional vision networks that process image sequences and full spectra, SPT demonstrates superior performance in Hs regression while consuming significantly fewer computational resources. On a consumer-grade GPU, SPT completes the training of regression model for 1080 sea clutter image sequences within 4 minutes, showcasing its potential to reduce deployment costs for radar wave-measuring systems. The open-source implementation of SPT will be available at https://github.com/joeyee/spt", "AI": {"tldr": "\u63d0\u51faSPT\u65b9\u6cd5\uff0c\u901a\u8fc7Transformer\u4ece\u7a00\u758f\u8c31\u70b9\u4f30\u8ba1\u6709\u6548\u6ce2\u9ad8\uff0c\u4ec5\u4f7f\u7528\u5c11\u6570\u5f3a\u529f\u7387\u8c31\u70b9\uff0c\u8ba1\u7b97\u6548\u7387\u9ad8\u4e14\u7279\u5f81\u5b66\u4e60\u7b26\u5408\u7269\u7406\u8272\u6563\u5173\u7cfb", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5904\u7406\u56fe\u50cf\u5e8f\u5217\u548c\u5b8c\u6574\u9891\u8c31\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u800c\u5b9e\u9645\u89c2\u6d4b\u8868\u660e\u53ea\u6709\u5c11\u6570\u5f3a\u529f\u7387\u8c31\u70b9\u5bf9\u6ce2\u6d6a\u80fd\u91cf\u6709\u8d21\u732e\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u4e14\u7269\u7406\u53ef\u89e3\u91ca\u7684\u6ce2\u9ad8\u4f30\u8ba1\u65b9\u6cd5", "method": "\u57fa\u4e8eTransformer\u7684SPT\u65b9\u6cd5\uff0c\u6574\u5408\u6d77\u6d0b\u8868\u9762\u6ce2\u7684\u51e0\u4f55\u548c\u9891\u8c31\u7279\u5f81\uff0c\u901a\u8fc7\u591a\u7ef4\u7279\u5f81\u8868\u793a\u4ece\u7a00\u758f\u8c31\u70b9\u4f30\u8ba1\u6709\u6548\u6ce2\u9ad8\uff0c\u4ec5\u9009\u62e9\u5c11\u6570\u5f3a\u529f\u7387\u8d21\u732e\u70b9", "result": "SPT\u5728\u6ce2\u9ad8\u56de\u5f52\u4e2d\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u89c6\u89c9\u7f51\u7edc\uff0c\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\u663e\u8457\u51cf\u5c11\uff1b\u5728\u6d88\u8d39\u7ea7GPU\u4e0a\uff0c1080\u4e2a\u6d77\u6742\u6ce2\u56fe\u50cf\u5e8f\u5217\u7684\u56de\u5f52\u6a21\u578b\u8bad\u7ec3\u4ec5\u97004\u5206\u949f\uff1b\u5b66\u4e60\u7279\u5f81\u4e0e\u7269\u7406\u8272\u6563\u5173\u7cfb\u4e00\u81f4\uff0c\u9009\u62e9\u70b9\u7684\u8d21\u732e\u5206\u6570\u56fe\u6cbf\u8272\u6563\u66f2\u7ebf\u96c6\u4e2d", "conclusion": "SPT\u65b9\u6cd5\u901a\u8fc7\u7a00\u758f\u8c31\u70b9\u6709\u6548\u4f30\u8ba1\u6ce2\u9ad8\uff0c\u8ba1\u7b97\u6548\u7387\u9ad8\u4e14\u5177\u6709\u7269\u7406\u53ef\u89e3\u91ca\u6027\uff0c\u6709\u671b\u964d\u4f4e\u96f7\u8fbe\u6d4b\u6ce2\u7cfb\u7edf\u7684\u90e8\u7f72\u6210\u672c\uff0c\u5df2\u5f00\u6e90\u5b9e\u73b0"}}
{"id": "2601.04221", "categories": ["cs.SD", "eess.AS", "eess.SY"], "pdf": "https://arxiv.org/pdf/2601.04221", "abs": "https://arxiv.org/abs/2601.04221", "authors": ["Midhun T. Augustine"], "title": "Predictive Controlled Music", "comment": "10 pages, 4 figures", "summary": "This paper presents a new approach to algorithmic composition, called predictive controlled music (PCM), which combines model predictive control (MPC) with music generation. PCM uses dynamic models to predict and optimize the music generation process, where musical notes are computed in a manner similar to an MPC problem by optimizing a performance measure. A feedforward neural network-based assessment function is used to evaluate the generated musical score, which serves as the objective function of the PCM optimization problem. Furthermore, a recurrent neural network model is employed to capture the relationships among the variables in the musical notes, and this model is then used to define the constraints in the PCM. Similar to MPC, the proposed PCM computes musical notes in a receding-horizon manner, leading to feedback controlled prediction. Numerical examples are presented to illustrate the PCM generation method.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u4e0e\u97f3\u4e50\u751f\u6210\u7684\u65b0\u65b9\u6cd5PCM\uff0c\u4f7f\u7528\u795e\u7ecf\u7f51\u7edc\u8bc4\u4f30\u51fd\u6570\u548c\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff0c\u4ee5\u6eda\u52a8\u65f6\u57df\u65b9\u5f0f\u751f\u6210\u97f3\u4e50", "motivation": "\u5c06\u63a7\u5236\u7406\u8bba\u4e2d\u7684\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u65b9\u6cd5\u5e94\u7528\u4e8e\u7b97\u6cd5\u4f5c\u66f2\uff0c\u901a\u8fc7\u4f18\u5316\u6027\u80fd\u6307\u6807\u6765\u751f\u6210\u97f3\u4e50\uff0c\u5b9e\u73b0\u66f4\u667a\u80fd\u3001\u53ef\u63a7\u7684\u97f3\u4e50\u521b\u4f5c", "method": "\u7ed3\u5408MPC\u4e0e\u97f3\u4e50\u751f\u6210\uff0c\u4f7f\u7528\u524d\u9988\u795e\u7ecf\u7f51\u7edc\u8bc4\u4f30\u97f3\u4e50\u8d28\u91cf\u4f5c\u4e3a\u76ee\u6807\u51fd\u6570\uff0c\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u5efa\u6a21\u97f3\u7b26\u5173\u7cfb\u4f5c\u4e3a\u7ea6\u675f\uff0c\u91c7\u7528\u6eda\u52a8\u65f6\u57df\u4f18\u5316\u8ba1\u7b97\u97f3\u7b26", "result": "\u63d0\u51fa\u4e86PCM\u6846\u67b6\uff0c\u80fd\u591f\u901a\u8fc7\u4f18\u5316\u8fc7\u7a0b\u751f\u6210\u97f3\u4e50\uff0c\u5e76\u901a\u8fc7\u6570\u503c\u793a\u4f8b\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027", "conclusion": "PCM\u4e3a\u7b97\u6cd5\u4f5c\u66f2\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u63a7\u5236\u7406\u8bba\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u53cd\u9988\u63a7\u5236\u7684\u97f3\u4e50\u9884\u6d4b\u751f\u6210\uff0c\u6269\u5c55\u4e86\u97f3\u4e50\u751f\u6210\u7684\u6280\u672f\u9014\u5f84"}}
{"id": "2601.04459", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2601.04459", "abs": "https://arxiv.org/abs/2601.04459", "authors": ["Da-Hee Yang", "Joon-Hyuk Chang"], "title": "Latent-Level Enhancement with Flow Matching for Robust Automatic Speech Recognition", "comment": "Accepted for publication in IEEE Signal Processing Letters", "summary": "Noise-robust automatic speech recognition (ASR) has been commonly addressed by applying speech enhancement (SE) at the waveform level before recognition. However, speech-level enhancement does not always translate into consistent recognition improvements due to residual distortions and mismatches with the latent space of the ASR encoder. In this letter, we introduce a complementary strategy termed latent-level enhancement, where distorted representations are refined during ASR inference. Specifically, we propose a plug-and-play Flow Matching Refinement module (FM-Refiner) that operates on the output latents of a pretrained CTC-based ASR encoder. Trained to map imperfect latents-either directly from noisy inputs or from enhanced-but-imperfect speech-toward their clean counterparts, the FM-Refiner is applied only at inference, without fine-tuning ASR parameters. Experiments show that FM-Refiner consistently reduces word error rate, both when directly applied to noisy inputs and when combined with conventional SE front-ends. These results demonstrate that latent-level refinement via flow matching provides a lightweight and effective complement to existing SE approaches for robust ASR.", "AI": {"tldr": "\u63d0\u51faFM-Refiner\u6a21\u5757\uff0c\u5728ASR\u7f16\u7801\u5668\u7684\u6f5c\u5728\u7a7a\u95f4\u8fdb\u884c\u6d41\u5339\u914d\u7cbe\u70bc\uff0c\u63d0\u5347\u566a\u58f0\u9c81\u68d2\u6027\uff0c\u65e0\u9700\u5fae\u8c03ASR\u53c2\u6570\u3002", "motivation": "\u4f20\u7edf\u6ce2\u5f62\u7ea7\u8bed\u97f3\u589e\u5f3a\uff08SE\uff09\u5728ASR\u4e2d\u6548\u679c\u6709\u9650\uff0c\u56e0\u4e3a\u6b8b\u7559\u5931\u771f\u548c\u4e0eASR\u7f16\u7801\u5668\u6f5c\u5728\u7a7a\u95f4\u4e0d\u5339\u914d\u3002\u9700\u8981\u5728\u6f5c\u5728\u7a7a\u95f4\u5c42\u9762\u8fdb\u884c\u8865\u5145\u589e\u5f3a\u3002", "method": "\u63d0\u51fa\u5373\u63d2\u5373\u7528\u7684\u6d41\u5339\u914d\u7cbe\u70bc\u6a21\u5757\uff08FM-Refiner\uff09\uff0c\u5728\u9884\u8bad\u7ec3\u7684CTC-based ASR\u7f16\u7801\u5668\u8f93\u51fa\u6f5c\u5728\u8868\u793a\u4e0a\u8fdb\u884c\u64cd\u4f5c\uff0c\u5c06\u4e0d\u5b8c\u7f8e\u7684\u6f5c\u5728\u8868\u793a\u6620\u5c04\u5230\u5e72\u51c0\u7248\u672c\uff0c\u4ec5\u5728\u63a8\u7406\u65f6\u4f7f\u7528\u3002", "result": "FM-Refiner\u80fd\u6301\u7eed\u964d\u4f4e\u8bcd\u9519\u8bef\u7387\uff0c\u65e0\u8bba\u662f\u76f4\u63a5\u5e94\u7528\u4e8e\u566a\u58f0\u8f93\u5165\u8fd8\u662f\u4e0e\u4f20\u7edfSE\u524d\u7aef\u7ed3\u5408\u4f7f\u7528\u3002", "conclusion": "\u901a\u8fc7\u6d41\u5339\u914d\u8fdb\u884c\u6f5c\u5728\u7ea7\u7cbe\u70bc\u4e3a\u73b0\u6709SE\u65b9\u6cd5\u63d0\u4f9b\u4e86\u8f7b\u91cf\u6709\u6548\u7684\u8865\u5145\uff0c\u63d0\u5347ASR\u7684\u566a\u58f0\u9c81\u68d2\u6027\u3002"}}
{"id": "2601.04599", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.04599", "abs": "https://arxiv.org/abs/2601.04599", "authors": ["Hao Sun", "Junting Chen", "Xianghao Yu"], "title": "MIMO Beam Map Reconstruction via Toeplitz-Structured Matrix-Vector Tensor Decomposition", "comment": null, "summary": "As wireless networks progress toward sixthgeneration (6G), understanding the spatial distribution of directional beam coverage becomes increasingly important for beam management and link optimization. Multiple-input multipleoutput (MIMO) beam map provides such spatial awareness, yet accurate construction under sparse measurements remains difficult due to incomplete spatial coverage and strong angular variations. This paper presents a tensor decomposition approach for reconstructing MIMO beam map from limited measurements. By transforming measurements from a Cartesian coordinate system into a polar coordinate system, we uncover a matrix-vector outer-product structure associated with different propagation conditions. Specifically, we mathematically demonstrate that the matrix factor, representing beam-space gain, exhibits an intrinsic Toeplitz structure due to the shift-invariant nature of array responses, and the vector factor captures distance-dependent attenuation. Leveraging these structural priors, we formulate a regularized tensor decomposition problem to jointly reconstruct line-of-sight (LOS), reflection, and obstruction propagation conditions. Simulation results confirm that the proposed method significantly enhances data efficiency, achieving a normalized mean square error (NMSE) reduction of over 20% compared to state-of-the-art baselines, even under sparse sampling regimes.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5f20\u91cf\u5206\u89e3\u7684\u65b9\u6cd5\uff0c\u4ece\u7a00\u758f\u6d4b\u91cf\u4e2d\u91cd\u5efaMIMO\u6ce2\u675f\u56fe\uff0c\u901a\u8fc7\u6781\u5750\u6807\u53d8\u6362\u63ed\u793a\u4f20\u64ad\u6761\u4ef6\u7684\u77e9\u9635-\u5411\u91cf\u5916\u79ef\u7ed3\u6784\uff0c\u5229\u7528Toeplitz\u7ed3\u6784\u5148\u9a8c\u663e\u8457\u63d0\u5347\u6570\u636e\u6548\u7387\u3002", "motivation": "\u968f\u7740\u65e0\u7ebf\u7f51\u7edc\u54116G\u53d1\u5c55\uff0c\u7406\u89e3\u5b9a\u5411\u6ce2\u675f\u8986\u76d6\u7684\u7a7a\u95f4\u5206\u5e03\u5bf9\u4e8e\u6ce2\u675f\u7ba1\u7406\u548c\u94fe\u8def\u4f18\u5316\u81f3\u5173\u91cd\u8981\u3002MIMO\u6ce2\u675f\u56fe\u80fd\u63d0\u4f9b\u8fd9\u79cd\u7a7a\u95f4\u611f\u77e5\uff0c\u4f46\u5728\u7a00\u758f\u6d4b\u91cf\u4e0b\u7531\u4e8e\u7a7a\u95f4\u8986\u76d6\u4e0d\u5b8c\u6574\u548c\u5f3a\u89d2\u5ea6\u53d8\u5316\uff0c\u51c6\u786e\u6784\u5efa\u4ecd\u7136\u56f0\u96be\u3002", "method": "\u5c06\u6d4b\u91cf\u4ece\u7b1b\u5361\u5c14\u5750\u6807\u7cfb\u8f6c\u6362\u5230\u6781\u5750\u6807\u7cfb\uff0c\u63ed\u793a\u4e0d\u540c\u4f20\u64ad\u6761\u4ef6\u4e0b\u7684\u77e9\u9635-\u5411\u91cf\u5916\u79ef\u7ed3\u6784\u3002\u6570\u5b66\u8bc1\u660e\u77e9\u9635\u56e0\u5b50\uff08\u4ee3\u8868\u6ce2\u675f\u7a7a\u95f4\u589e\u76ca\uff09\u7531\u4e8e\u9635\u5217\u54cd\u5e94\u7684\u5e73\u79fb\u4e0d\u53d8\u6027\u5177\u6709\u5185\u5728\u7684Toeplitz\u7ed3\u6784\uff0c\u5411\u91cf\u56e0\u5b50\u6355\u83b7\u8ddd\u79bb\u76f8\u5173\u7684\u8870\u51cf\u3002\u5229\u7528\u8fd9\u4e9b\u7ed3\u6784\u5148\u9a8c\uff0c\u5236\u5b9a\u6b63\u5219\u5316\u5f20\u91cf\u5206\u89e3\u95ee\u9898\uff0c\u8054\u5408\u91cd\u5efaLOS\u3001\u53cd\u5c04\u548c\u906e\u6321\u4f20\u64ad\u6761\u4ef6\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u6570\u636e\u6548\u7387\uff0c\u5373\u4f7f\u5728\u7a00\u758f\u91c7\u6837\u60c5\u51b5\u4e0b\uff0c\u4e0e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u76f8\u6bd4\uff0c\u5f52\u4e00\u5316\u5747\u65b9\u8bef\u5dee\uff08NMSE\uff09\u964d\u4f4e\u4e86\u8d85\u8fc720%\u3002", "conclusion": "\u63d0\u51fa\u7684\u5f20\u91cf\u5206\u89e3\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5229\u7528MIMO\u6ce2\u675f\u56fe\u7684\u7ed3\u6784\u7279\u6027\uff0c\u4ece\u6709\u9650\u6d4b\u91cf\u4e2d\u51c6\u786e\u91cd\u5efa\u6ce2\u675f\u8986\u76d6\uff0c\u4e3a6G\u7f51\u7edc\u4e2d\u7684\u6ce2\u675f\u7ba1\u7406\u548c\u4f18\u5316\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.04222", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.04222", "abs": "https://arxiv.org/abs/2601.04222", "authors": ["Tim Ziemer", "Simon Linke"], "title": "From Imitation to Innovation: The Divergent Paths of Techno in Germany and the USA", "comment": null, "summary": "Many documentaries on early house and techno music exist. Here, protagonists from the scenes describe key elements and events that affected the evolution of the music. In the research community, there is consensus that such descriptions have to be examined critically. Yet, there have not been attempts to validate such statements on the basis of audio analyses. In this study, over 9,000 early house and techno tracks from Germany and the United States of America are analyzed using recording studio features, machine learning and inferential statistics. Three observations can be made: 1.) German and US house/techno music are distinct, 2.) US styles are much more alike, and 3.) scarcely evolved over time compared to German house/techno regarding the recording studio features. These findings are in agreement with documented statements and thus provide an audio-based perspective on why techno became a mass phenomenon in Germany but remained a fringe phenomenon in the USA. Observations like these can help the music industry estimate whether new trends will experience a breakthrough or disappear.", "AI": {"tldr": "\u901a\u8fc7\u5206\u67909000\u591a\u9996\u65e9\u671f\u6d69\u5ba4\u548c\u79d1\u6280\u821e\u66f2\uff0c\u53d1\u73b0\u5fb7\u56fd\u548c\u7f8e\u56fd\u97f3\u4e50\u5728\u5f55\u97f3\u5ba4\u7279\u5f81\u4e0a\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u8fd9\u89e3\u91ca\u4e86\u4e3a\u4ec0\u4e48\u79d1\u6280\u821e\u66f2\u5728\u5fb7\u56fd\u6210\u4e3a\u5927\u4f17\u73b0\u8c61\u800c\u5728\u7f8e\u56fd\u4fdd\u6301\u5c0f\u4f17", "motivation": "\u73b0\u6709\u5173\u4e8e\u65e9\u671f\u6d69\u5ba4\u548c\u79d1\u6280\u821e\u66f2\u7684\u7eaa\u5f55\u7247\u548c\u573a\u666f\u63cf\u8ff0\u7f3a\u4e4f\u97f3\u9891\u5206\u6790\u7684\u9a8c\u8bc1\uff0c\u9700\u8981\u57fa\u4e8e\u97f3\u9891\u6570\u636e\u6765\u68c0\u9a8c\u8fd9\u4e9b\u63cf\u8ff0\u7684\u771f\u5b9e\u6027", "method": "\u5206\u67909000\u591a\u9996\u5fb7\u56fd\u548c\u7f8e\u56fd\u7684\u65e9\u671f\u6d69\u5ba4\u548c\u79d1\u6280\u821e\u66f2\uff0c\u4f7f\u7528\u5f55\u97f3\u5ba4\u7279\u5f81\u3001\u673a\u5668\u5b66\u4e60\u548c\u63a8\u65ad\u7edf\u8ba1\u65b9\u6cd5", "result": "1) \u5fb7\u56fd\u548c\u7f8e\u56fd\u6d69\u5ba4/\u79d1\u6280\u821e\u66f2\u5728\u5f55\u97f3\u5ba4\u7279\u5f81\u4e0a\u660e\u663e\u4e0d\u540c\uff1b2) \u7f8e\u56fd\u98ce\u683c\u66f4\u52a0\u76f8\u4f3c\uff1b3) \u4e0e\u7f8e\u56fd\u76f8\u6bd4\uff0c\u5fb7\u56fd\u6d69\u5ba4/\u79d1\u6280\u821e\u66f2\u968f\u65f6\u95f4\u6f14\u53d8\u66f4\u591a", "conclusion": "\u97f3\u9891\u5206\u6790\u7ed3\u679c\u4e0e\u6587\u732e\u63cf\u8ff0\u4e00\u81f4\uff0c\u4e3a\u79d1\u6280\u821e\u66f2\u5728\u5fb7\u56fd\u6210\u4e3a\u5927\u4f17\u73b0\u8c61\u800c\u5728\u7f8e\u56fd\u4fdd\u6301\u5c0f\u4f17\u63d0\u4f9b\u4e86\u97f3\u9891\u8bc1\u636e\uff0c\u8fd9\u4e9b\u89c2\u5bdf\u53ef\u5e2e\u52a9\u97f3\u4e50\u4ea7\u4e1a\u9884\u6d4b\u65b0\u8d8b\u52bf\u7684\u53d1\u5c55"}}
{"id": "2601.04654", "categories": ["eess.AS", "cs.AI", "cs.SD"], "pdf": "https://arxiv.org/pdf/2601.04654", "abs": "https://arxiv.org/abs/2601.04654", "authors": ["Ryutaro Oshima", "Yuya Hosoda", "Youji Iiguni"], "title": "LLMs-Integrated Automatic Hate Speech Recognition Using Controllable Text Generation Models", "comment": "In Proceedings of the 17th Asia Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC 2025)", "summary": "This paper proposes an automatic speech recognition (ASR) model for hate speech using large language models (LLMs). The proposed method integrates the encoder of the ASR model with the decoder of the LLMs, enabling simultaneous transcription and censorship tasks to prevent the exposure of harmful content. Instruction tuning of the LLM to mask hate-related words with specific tokens requires an annotated hate speech dataset, which is limited. We generate text samples using an LLM with the Chain-of-Thought (CoT) prompting technique guided by cultural context and examples and then convert them into speech samples using a text-to-speech (TTS) system. However, some of them contain non-hate speech samples with hate-related words, which degrades the censorship performance. This paper filters the samples which text classification models correctly label as hate content. By adjusting the threshold for the number of correct answer models, we can control the level of hate in the generated dataset, allowing us to train the LLMs through curriculum learning in a gradual manner. Experimental results show that the proposed method achieves a masking accuracy of 58.6\\% for hate-related words, surpassing previous baselines. We also confirm that the curriculum training contributes to the efficiency of both transcription and censorship tasks.", "AI": {"tldr": "\u63d0\u51fa\u7ed3\u5408ASR\u7f16\u7801\u5668\u548cLLM\u89e3\u7801\u5668\u7684\u6a21\u578b\uff0c\u7528\u4e8e\u540c\u65f6\u8fdb\u884c\u8bed\u97f3\u8f6c\u5f55\u548c\u4ec7\u6068\u5185\u5bb9\u5ba1\u67e5\uff0c\u901a\u8fc7CoT\u63d0\u793a\u751f\u6210\u8bad\u7ec3\u6570\u636e\u5e76\u8fc7\u6ee4\uff0c\u91c7\u7528\u8bfe\u7a0b\u5b66\u4e60\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u4ec7\u6068\u8bed\u97f3\u8bc6\u522b\u6a21\u578b\u9700\u8981\u6807\u6ce8\u6570\u636e\u96c6\uff0c\u4f46\u8fd9\u7c7b\u6570\u636e\u6709\u9650\u3002\u540c\u65f6\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u540c\u65f6\u5904\u7406\u8bed\u97f3\u8f6c\u5f55\u548c\u5185\u5bb9\u5ba1\u67e5\u4efb\u52a1\u3002", "method": "1) \u5c06ASR\u7f16\u7801\u5668\u4e0eLLM\u89e3\u7801\u5668\u96c6\u6210\uff0c\u5b9e\u73b0\u8f6c\u5f55\u548c\u5ba1\u67e5\u5e76\u884c\u5904\u7406\uff1b2) \u4f7f\u7528CoT\u63d0\u793a\u6280\u672f\u751f\u6210\u6587\u672c\u6837\u672c\uff0c\u518d\u901a\u8fc7TTS\u8f6c\u4e3a\u8bed\u97f3\uff1b3) \u7528\u6587\u672c\u5206\u7c7b\u6a21\u578b\u8fc7\u6ee4\u6837\u672c\uff0c\u63a7\u5236\u4ec7\u6068\u5185\u5bb9\u7ea7\u522b\uff1b4) \u91c7\u7528\u8bfe\u7a0b\u5b66\u4e60\u9010\u6b65\u8bad\u7ec3\u6a21\u578b\u3002", "result": "\u4ec7\u6068\u76f8\u5173\u8bcd\u6c47\u7684\u63a9\u7801\u51c6\u786e\u7387\u8fbe\u523058.6%\uff0c\u8d85\u8d8a\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002\u8bfe\u7a0b\u5b66\u4e60\u6709\u6548\u63d0\u5347\u4e86\u8f6c\u5f55\u548c\u5ba1\u67e5\u4efb\u52a1\u7684\u6548\u7387\u3002", "conclusion": "\u63d0\u51fa\u7684\u96c6\u6210\u6a21\u578b\u80fd\u591f\u540c\u65f6\u5904\u7406\u8bed\u97f3\u8f6c\u5f55\u548c\u4ec7\u6068\u5185\u5bb9\u5ba1\u67e5\uff0c\u901a\u8fc7\u6570\u636e\u751f\u6210\u548c\u8fc7\u6ee4\u65b9\u6cd5\u89e3\u51b3\u4e86\u6807\u6ce8\u6570\u636e\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u8bfe\u7a0b\u5b66\u4e60\u7b56\u7565\u8fdb\u4e00\u6b65\u4f18\u5316\u4e86\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2601.04831", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.04831", "abs": "https://arxiv.org/abs/2601.04831", "authors": ["Shay Kreymer", "Amnon Balanov", "Tamir Bendory"], "title": "An Ultra-Fast MLE for Low SNR Multi-Reference Alignment", "comment": null, "summary": "Motivated by single-particle cryo-electron microscopy, multi-reference alignment (MRA) models the task of recovering an unknown signal from multiple noisy observations corrupted by random rotations. The standard approach, expectation-maximization (EM), often becomes computationally prohibitive, particularly in low signal-to-noise ratio (SNR) settings. We introduce an alternative, ultra-fast algorithm for MRA over the special orthogonal group $\\mathrm{SO}(2)$. By performing a Taylor expansion of the log-likelihood in the low-SNR regime, we estimate the signal by sequentially computing data-driven averages of observations. Our method requires only one pass over the data, dramatically reducing computational cost compared to EM. Numerical experiments show that the proposed approach achieves high accuracy in low-SNR environments and provides an excellent initialization for subsequent EM refinement.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7528\u4e8e\u7279\u6b8a\u6b63\u4ea4\u7fa4SO(2)\u4e0a\u591a\u53c2\u8003\u5bf9\u9f50\u7684\u8d85\u5feb\u901f\u7b97\u6cd5\uff0c\u901a\u8fc7\u4f4e\u4fe1\u566a\u6bd4\u4e0b\u7684\u6cf0\u52d2\u5c55\u5f00\uff0c\u5355\u6b21\u6570\u636e\u904d\u5386\u5373\u53ef\u4f30\u8ba1\u4fe1\u53f7\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c", "motivation": "\u5355\u7c92\u5b50\u51b7\u51bb\u7535\u955c\u4e2d\u7684\u591a\u53c2\u8003\u5bf9\u9f50\u95ee\u9898\uff0c\u6807\u51c6\u671f\u671b\u6700\u5927\u5316\u65b9\u6cd5\u5728\u4f4e\u4fe1\u566a\u6bd4\u4e0b\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u7b97\u6cd5", "method": "\u5728\u4f4e\u4fe1\u566a\u6bd4\u6761\u4ef6\u4e0b\u5bf9\u5bf9\u6570\u4f3c\u7136\u8fdb\u884c\u6cf0\u52d2\u5c55\u5f00\uff0c\u901a\u8fc7\u987a\u5e8f\u8ba1\u7b97\u89c2\u6d4b\u6570\u636e\u7684\u6570\u636e\u9a71\u52a8\u5e73\u5747\u503c\u6765\u4f30\u8ba1\u4fe1\u53f7\uff0c\u53ea\u9700\u5355\u6b21\u6570\u636e\u904d\u5386", "result": "\u6570\u503c\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u4f4e\u4fe1\u566a\u6bd4\u73af\u5883\u4e0b\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\uff0c\u4e3a\u540e\u7eedEM\u7ec6\u5316\u63d0\u4f9b\u4f18\u79c0\u521d\u59cb\u5316\uff0c\u8ba1\u7b97\u6210\u672c\u663e\u8457\u4f4e\u4e8eEM", "conclusion": "\u63d0\u51fa\u7684\u8d85\u5feb\u901f\u7b97\u6cd5\u4e3a\u591a\u53c2\u8003\u5bf9\u9f50\u95ee\u9898\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u9002\u7528\u4e8e\u4f4e\u4fe1\u566a\u6bd4\u573a\u666f\uff0c\u53ef\u4f5c\u4e3aEM\u7684\u9884\u5904\u7406\u6b65\u9aa4"}}
{"id": "2601.04227", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.04227", "abs": "https://arxiv.org/abs/2601.04227", "authors": ["Prajwal Chinchmalatpure", "Suyash Chinchmalatpure", "Siddharth Chavan"], "title": "Defense Against Synthetic Speech: Real-Time Detection of RVC Voice Conversion Attacks", "comment": null, "summary": "Generative audio technologies now enable highly realistic voice cloning and real-time voice conversion, increasing the risk of impersonation, fraud, and misinformation in communication channels such as phone and video calls. This study investigates real-time detection of AI-generated speech produced using Retrieval-based Voice Conversion (RVC), evaluated on the DEEP-VOICE dataset, which includes authentic and voice-converted speech samples from multiple well-known speakers. To simulate realistic conditions, deepfake generation is applied to isolated vocal components, followed by the reintroduction of background ambiance to suppress trivial artifacts and emphasize conversion-specific cues. We frame detection as a streaming classification task by dividing audio into one-second segments, extracting time-frequency and cepstral features, and training supervised machine learning models to classify each segment as real or voice-converted. The proposed system enables low-latency inference, supporting both segment-level decisions and call-level aggregation. Experimental results show that short-window acoustic features can reliably capture discriminative patterns associated with RVC speech, even in noisy backgrounds. These findings demonstrate the feasibility of practical, real-time deepfake speech detection and underscore the importance of evaluating under realistic audio mixing conditions for robust deployment.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5b9e\u65f6\u68c0\u6d4bAI\u751f\u6210\u8bed\u97f3\uff08\u7279\u522b\u662f\u57fa\u4e8e\u68c0\u7d22\u7684\u8bed\u97f3\u8f6c\u6362RVC\uff09\u7684\u7cfb\u7edf\uff0c\u901a\u8fc7\u5c06\u97f3\u9891\u5206\u5272\u4e3a1\u79d2\u7247\u6bb5\uff0c\u63d0\u53d6\u65f6\u9891\u548c\u5012\u8c31\u7279\u5f81\uff0c\u8bad\u7ec3\u673a\u5668\u5b66\u4e60\u6a21\u578b\u8fdb\u884c\u5206\u7c7b\uff0c\u5e76\u5728\u5608\u6742\u80cc\u666f\u4e2d\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002", "motivation": "\u751f\u6210\u5f0f\u97f3\u9891\u6280\u672f\uff08\u5982\u8bed\u97f3\u514b\u9686\u548c\u5b9e\u65f6\u8bed\u97f3\u8f6c\u6362\uff09\u5e26\u6765\u4e86\u5192\u5145\u3001\u6b3a\u8bc8\u548c\u865a\u5047\u4fe1\u606f\u7684\u98ce\u9669\uff0c\u7279\u522b\u662f\u5728\u7535\u8bdd\u548c\u89c6\u9891\u901a\u8bdd\u4e2d\u3002\u9700\u8981\u5f00\u53d1\u80fd\u591f\u5728\u771f\u5b9e\u901a\u4fe1\u573a\u666f\u4e2d\u5b9e\u65f6\u68c0\u6d4bAI\u751f\u6210\u8bed\u97f3\u7684\u65b9\u6cd5\u3002", "method": "\u5c06\u68c0\u6d4b\u4efb\u52a1\u6846\u67b6\u5316\u4e3a\u6d41\u5f0f\u5206\u7c7b\uff1a1\uff09\u5c06\u97f3\u9891\u5206\u5272\u4e3a1\u79d2\u7247\u6bb5\uff1b2\uff09\u63d0\u53d6\u65f6\u95f4\u9891\u7387\u548c\u5012\u8c31\u7279\u5f81\uff1b3\uff09\u8bad\u7ec3\u76d1\u7763\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5bf9\u6bcf\u4e2a\u7247\u6bb5\u8fdb\u884c\u5206\u7c7b\uff08\u771f\u5b9e\u6216\u8bed\u97f3\u8f6c\u6362\uff09\uff1b4\uff09\u5728DEEP-VOICE\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u8be5\u6570\u636e\u96c6\u5305\u542b\u771f\u5b9e\u548c\u8bed\u97f3\u8f6c\u6362\u7684\u8bed\u97f3\u6837\u672c\uff1b5\uff09\u6a21\u62df\u771f\u5b9e\u6761\u4ef6\uff1a\u5bf9\u5b64\u7acb\u4eba\u58f0\u8fdb\u884c\u6df1\u5ea6\u4f2a\u9020\u751f\u6210\uff0c\u7136\u540e\u91cd\u65b0\u5f15\u5165\u80cc\u666f\u73af\u5883\u4ee5\u6291\u5236\u7b80\u5355\u4f2a\u5f71\u5e76\u5f3a\u8c03\u8f6c\u6362\u7279\u5b9a\u7ebf\u7d22\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u77ed\u7a97\u53e3\u58f0\u5b66\u7279\u5f81\u80fd\u591f\u53ef\u9760\u5730\u6355\u6349\u4e0eRVC\u8bed\u97f3\u76f8\u5173\u7684\u533a\u5206\u6027\u6a21\u5f0f\uff0c\u5373\u4f7f\u5728\u5608\u6742\u80cc\u666f\u4e2d\u4e5f\u80fd\u6709\u6548\u68c0\u6d4b\u3002\u7cfb\u7edf\u652f\u6301\u4f4e\u5ef6\u8fdf\u63a8\u7406\uff0c\u53ef\u5b9e\u73b0\u7247\u6bb5\u7ea7\u51b3\u7b56\u548c\u901a\u8bdd\u7ea7\u805a\u5408\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u5b9e\u7528\u3001\u5b9e\u65f6\u7684\u6df1\u5ea6\u4f2a\u9020\u8bed\u97f3\u68c0\u6d4b\u7684\u53ef\u884c\u6027\uff0c\u5e76\u5f3a\u8c03\u4e86\u5728\u771f\u5b9e\u97f3\u9891\u6df7\u5408\u6761\u4ef6\u4e0b\u8fdb\u884c\u8bc4\u4f30\u5bf9\u4e8e\u7a33\u5065\u90e8\u7f72\u7684\u91cd\u8981\u6027\u3002\u77ed\u7a97\u53e3\u58f0\u5b66\u7279\u5f81\u80fd\u591f\u6709\u6548\u68c0\u6d4bRVC\u751f\u6210\u7684\u8bed\u97f3\uff0c\u4e3a\u901a\u4fe1\u5b89\u5168\u63d0\u4f9b\u4e86\u6280\u672f\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.04867", "categories": ["eess.AS", "cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2601.04867", "abs": "https://arxiv.org/abs/2601.04867", "authors": ["Alistair Carson", "Alec Wright", "Stefan Bilbao"], "title": "Gradient-based Optimisation of Modulation Effects", "comment": "Submitted to J. Audio Eng. Soc. Dec. 2025", "summary": "Modulation effects such as phasers, flangers and chorus effects are heavily used in conjunction with the electric guitar. Machine learning based emulation of analog modulation units has been investigated in recent years, but most methods have either been limited to one class of effect or suffer from a high computational cost or latency compared to canonical digital implementations. Here, we build on previous work and present a framework for modelling flanger, chorus and phaser effects based on differentiable digital signal processing. The model is trained in the time-frequency domain, but at inference operates in the time-domain, requiring zero latency. We investigate the challenges associated with gradient-based optimisation of such effects, and show that low-frequency weighting of loss functions avoids convergence to local minima when learning delay times. We show that when trained against analog effects units, sound output from the model is in some cases perceptually indistinguishable from the reference, but challenges still remain for effects with long delay times and feedback.", "AI": {"tldr": "\u57fa\u4e8e\u53ef\u5fae\u5206\u6570\u5b57\u4fe1\u53f7\u5904\u7406\u7684\u5409\u4ed6\u8c03\u5236\u6548\u679c\u5efa\u6a21\u6846\u67b6\uff0c\u53ef\u96f6\u5ef6\u8fdf\u6a21\u62df\u9576\u8fb9\u3001\u5408\u5531\u548c\u79fb\u76f8\u6548\u679c", "motivation": "\u73b0\u6709\u673a\u5668\u5b66\u4e60\u6a21\u62df\u8c03\u5236\u6548\u679c\u7684\u65b9\u6cd5\u8981\u4e48\u5c40\u9650\u4e8e\u5355\u4e00\u6548\u679c\u7c7b\u578b\uff0c\u8981\u4e48\u8ba1\u7b97\u6210\u672c\u9ad8\u3001\u5ef6\u8fdf\u5927\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u9ad8\u6548\u901a\u7528\u7684\u5efa\u6a21\u6846\u67b6", "method": "\u91c7\u7528\u53ef\u5fae\u5206\u6570\u5b57\u4fe1\u53f7\u5904\u7406\u6846\u67b6\uff0c\u5728\u65f6\u9891\u57df\u8bad\u7ec3\u4f46\u5728\u65f6\u57df\u63a8\u7406\u5b9e\u73b0\u96f6\u5ef6\u8fdf\uff0c\u901a\u8fc7\u4f4e\u9891\u52a0\u6743\u635f\u5931\u51fd\u6570\u907f\u514d\u5ef6\u8fdf\u65f6\u95f4\u4f18\u5316\u7684\u5c40\u90e8\u6781\u5c0f\u503c", "result": "\u6a21\u578b\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u80fd\u4ea7\u751f\u4e0e\u6a21\u62df\u6548\u679c\u5355\u5143\u5728\u611f\u77e5\u4e0a\u65e0\u6cd5\u533a\u5206\u7684\u97f3\u9891\u8f93\u51fa\uff0c\u4f46\u5bf9\u4e8e\u957f\u5ef6\u8fdf\u65f6\u95f4\u548c\u53cd\u9988\u6548\u679c\u4ecd\u5b58\u5728\u6311\u6218", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u6210\u529f\u5b9e\u73b0\u4e86\u591a\u79cd\u5409\u4ed6\u8c03\u5236\u6548\u679c\u7684\u9ad8\u6548\u6a21\u62df\uff0c\u96f6\u5ef6\u8fdf\u7279\u6027\u4f7f\u5176\u9002\u5408\u5b9e\u65f6\u5e94\u7528\uff0c\u4f46\u957f\u5ef6\u8fdf\u548c\u53cd\u9988\u6548\u679c\u4ecd\u9700\u8fdb\u4e00\u6b65\u6539\u8fdb"}}
{"id": "2601.04844", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.04844", "abs": "https://arxiv.org/abs/2601.04844", "authors": ["Guangyu Zhu", "Xidong Mu", "Li Guo", "Shibiao Xu", "Yuanwei Liu", "Naofal Al-Dhahir"], "title": "SE-EE Tradeoff in Pinching-Antenna Systems: Waveguide Multiplexing or Waveguide Switching?", "comment": null, "summary": "The spectral and energy efficiency (SE-EE) trade-off in pinching-antenna systems (PASS) is investigated in this paper. In particular, two practical operating protocols, namely waveguide multiplexing (WM) and waveguide switching (WS), are considered. A multi-objective optimization problem (MOOP) is formulated to jointly optimize the baseband and pinching beamforming for maximizing the achievable SE and EE, which is then converted into a single-objective problem via the \u03b5-constraint method. For WM, the problem is decomposed within the alternating-optimization framework, where the baseband beamforming is optimized using the successive convex approximation, and the pinching beamforming is updated through the particle swarm optimization. For WS, due to the time-division transmission and interference-free nature, the pinching beamforming in each time slot is first adjusted to maximize the served user channel gain, followed by the baseband power allocation. Simulation results demonstrate that 1) PASS outperforms conventional antennas by mitigating large-scale path losses; 2) WS leads to a higher maximum achievable EE by activating a single RF chain, whereas WM yields a higher SE upper bound by serving all users concurrently; and 3) increasing the number of users substantially enhances SE under WM, whereas WS shows more pronounced benefits in low-signal-to-noise ratio regimes.", "AI": {"tldr": "\u7814\u7a76\u4e86pinching-antenna\u7cfb\u7edf\uff08PASS\uff09\u4e2d\u7684\u9891\u8c31\u6548\u7387\u548c\u80fd\u91cf\u6548\u7387\u6743\u8861\uff0c\u6bd4\u8f83\u4e86\u6ce2\u5bfc\u590d\u7528\u548c\u6ce2\u5bfc\u5207\u6362\u4e24\u79cd\u534f\u8bae\uff0c\u901a\u8fc7\u591a\u76ee\u6807\u4f18\u5316\u6846\u67b6\u5206\u522b\u4f18\u5316\u57fa\u5e26\u548cpinching\u6ce2\u675f\u6210\u5f62\u3002", "motivation": "\u4f20\u7edf\u5929\u7ebf\u7cfb\u7edf\u5728\u5927\u89c4\u6a21\u8def\u5f84\u635f\u8017\u65b9\u9762\u5b58\u5728\u9650\u5236\uff0c\u9700\u8981\u7814\u7a76\u65b0\u578bpinching-antenna\u7cfb\u7edf\u6765\u540c\u65f6\u63d0\u5347\u9891\u8c31\u6548\u7387\u548c\u80fd\u91cf\u6548\u7387\uff0c\u63a2\u7d22\u4e0d\u540c\u64cd\u4f5c\u534f\u8bae\u4e0b\u7684\u6027\u80fd\u6743\u8861\u3002", "method": "\u91c7\u7528\u591a\u76ee\u6807\u4f18\u5316\u95ee\u9898\uff08MOOP\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u03b5-\u7ea6\u675f\u65b9\u6cd5\u8f6c\u5316\u4e3a\u5355\u76ee\u6807\u95ee\u9898\u3002\u5bf9\u4e8e\u6ce2\u5bfc\u590d\u7528\uff08WM\uff09\u534f\u8bae\uff0c\u4f7f\u7528\u4ea4\u66ff\u4f18\u5316\u5206\u89e3\u95ee\u9898\uff0c\u57fa\u5e26\u6ce2\u675f\u6210\u5f62\u91c7\u7528\u9010\u6b21\u51f8\u903c\u8fd1\u4f18\u5316\uff0cpinching\u6ce2\u675f\u6210\u5f62\u4f7f\u7528\u7c92\u5b50\u7fa4\u4f18\u5316\u3002\u5bf9\u4e8e\u6ce2\u5bfc\u5207\u6362\uff08WS\uff09\u534f\u8bae\uff0c\u7531\u4e8e\u65f6\u5206\u4f20\u8f93\u548c\u65e0\u5e72\u6270\u7279\u6027\uff0c\u5148\u4f18\u5316pinching\u6ce2\u675f\u6210\u5f62\u6700\u5927\u5316\u7528\u6237\u4fe1\u9053\u589e\u76ca\uff0c\u518d\u8fdb\u884c\u57fa\u5e26\u529f\u7387\u5206\u914d\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff1a1\uff09PASS\u901a\u8fc7\u51cf\u8f7b\u5927\u89c4\u6a21\u8def\u5f84\u635f\u8017\u4f18\u4e8e\u4f20\u7edf\u5929\u7ebf\uff1b2\uff09WS\u901a\u8fc7\u6fc0\u6d3b\u5355\u4e2aRF\u94fe\u5b9e\u73b0\u66f4\u9ad8\u7684\u6700\u5927\u53ef\u8fbeEE\uff0c\u800cWM\u901a\u8fc7\u540c\u65f6\u670d\u52a1\u6240\u6709\u7528\u6237\u5b9e\u73b0\u66f4\u9ad8\u7684SE\u4e0a\u9650\uff1b3\uff09\u589e\u52a0\u7528\u6237\u6570\u663e\u8457\u63d0\u5347WM\u4e0b\u7684SE\uff0c\u800cWS\u5728\u4f4e\u4fe1\u566a\u6bd4\u533a\u57df\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "PASS\u7cfb\u7edf\u5728SE-EE\u6743\u8861\u65b9\u9762\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0cWM\u548cWS\u534f\u8bae\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u5404\u6709\u4f18\u52bf\uff1aWM\u9002\u5408\u9ad8SE\u9700\u6c42\u573a\u666f\uff0cWS\u9002\u5408\u9ad8EE\u9700\u6c42\u573a\u666f\uff0c\u4e3a\u672a\u6765\u65e0\u7ebf\u901a\u4fe1\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u53c2\u8003\u3002"}}
{"id": "2601.04233", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.04233", "abs": "https://arxiv.org/abs/2601.04233", "authors": ["Zhiyuan Zhao", "Lijian Lin", "Ye Zhu", "Kai Xie", "Yunfei Liu", "Yu Li"], "title": "LEMAS: Large A 150K-Hour Large-scale Extensible Multilingual Audio Suite with Generative Speech Models", "comment": "Demo page: https://lemas-project.github.io/LEMAS-Project", "summary": "We present the LEMAS-Dataset, which, to our knowledge, is currently the largest open-source multilingual speech corpus with word-level timestamps. Covering over 150,000 hours across 10 major languages, LEMAS-Dataset is constructed via a efficient data processing pipeline that ensures high-quality data and annotations. To validate the effectiveness of LEMAS-Dataset across diverse generative paradigms, we train two benchmark models with distinct architectures and task specializations on this dataset. LEMAS-TTS, built upon a non-autoregressive flow-matching framework, leverages the dataset's massive scale and linguistic diversity to achieve robust zero-shot multilingual synthesis. Our proposed accent-adversarial training and CTC loss mitigate cross-lingual accent issues, enhancing synthesis stability. Complementarily, LEMAS-Edit employs an autoregressive decoder-only architecture that formulates speech editing as a masked token infilling task. By exploiting precise word-level alignments to construct training masks and adopting adaptive decoding strategies, it achieves seamless, smooth-boundary speech editing with natural transitions. Experimental results demonstrate that models trained on LEMAS-Dataset deliver high-quality synthesis and editing performance, confirming the dataset's quality. We envision that this richly timestamp-annotated, fine-grained multilingual corpus will drive future advances in prompt-based speech generation systems.", "AI": {"tldr": "LEMAS-Dataset\u662f\u76ee\u524d\u6700\u5927\u7684\u5f00\u6e90\u591a\u8bed\u8a00\u8bed\u97f3\u8bed\u6599\u5e93\uff0c\u5305\u542b\u8bcd\u7ea7\u65f6\u95f4\u6233\uff0c\u8986\u76d610\u79cd\u4e3b\u8981\u8bed\u8a00\u8d85\u8fc715\u4e07\u5c0f\u65f6\u3002\u57fa\u4e8e\u8be5\u6570\u636e\u96c6\u8bad\u7ec3\u4e86\u4e24\u4e2a\u57fa\u51c6\u6a21\u578b\uff1aLEMAS-TTS\u7528\u4e8e\u96f6\u6837\u672c\u591a\u8bed\u8a00\u5408\u6210\uff0cLEMAS-Edit\u7528\u4e8e\u8bed\u97f3\u7f16\u8f91\u3002", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u5927\u89c4\u6a21\u3001\u9ad8\u8d28\u91cf\u3001\u5e26\u6709\u8bcd\u7ea7\u65f6\u95f4\u6233\u7684\u591a\u8bed\u8a00\u8bed\u97f3\u6570\u636e\u96c6\uff0c\u8fd9\u9650\u5236\u4e86\u57fa\u4e8e\u63d0\u793a\u7684\u8bed\u97f3\u751f\u6210\u7cfb\u7edf\u7684\u53d1\u5c55\u3002\u9700\u8981\u6784\u5efa\u4e00\u4e2a\u80fd\u591f\u652f\u6301\u591a\u6837\u5316\u751f\u6210\u8303\u5f0f\u7684\u6570\u636e\u96c6\u3002", "method": "1. \u6784\u5efaLEMAS-Dataset\uff1a\u901a\u8fc7\u9ad8\u6548\u6570\u636e\u5904\u7406\u6d41\u6c34\u7ebf\u521b\u5efa\u5305\u542b\u8bcd\u7ea7\u65f6\u95f4\u6233\u7684\u591a\u8bed\u8a00\u8bed\u97f3\u8bed\u6599\u5e93\uff1b2. \u8bad\u7ec3LEMAS-TTS\uff1a\u57fa\u4e8e\u975e\u81ea\u56de\u5f52\u6d41\u5339\u914d\u6846\u67b6\uff0c\u91c7\u7528\u53e3\u97f3\u5bf9\u6297\u8bad\u7ec3\u548cCTC\u635f\u5931\u7f13\u89e3\u8de8\u8bed\u8a00\u53e3\u97f3\u95ee\u9898\uff1b3. \u8bad\u7ec3LEMAS-Edit\uff1a\u57fa\u4e8e\u81ea\u56de\u5f52\u89e3\u7801\u5668\u67b6\u6784\uff0c\u5c06\u8bed\u97f3\u7f16\u8f91\u5efa\u6a21\u4e3a\u63a9\u7801\u6807\u8bb0\u586b\u5145\u4efb\u52a1\uff0c\u5229\u7528\u8bcd\u7ea7\u5bf9\u9f50\u6784\u5efa\u8bad\u7ec3\u63a9\u7801\u5e76\u91c7\u7528\u81ea\u9002\u5e94\u89e3\u7801\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u57fa\u4e8eLEMAS-Dataset\u8bad\u7ec3\u7684\u6a21\u578b\u80fd\u591f\u5b9e\u73b0\u9ad8\u8d28\u91cf\u7684\u8bed\u97f3\u5408\u6210\u548c\u7f16\u8f91\u6027\u80fd\u3002LEMAS-TTS\u5b9e\u73b0\u4e86\u9c81\u68d2\u7684\u96f6\u6837\u672c\u591a\u8bed\u8a00\u5408\u6210\uff0cLEMAS-Edit\u5b9e\u73b0\u4e86\u65e0\u7f1d\u3001\u5e73\u6ed1\u8fb9\u754c\u7684\u8bed\u97f3\u7f16\u8f91\uff0c\u9a8c\u8bc1\u4e86\u6570\u636e\u96c6\u7684\u8d28\u91cf\u3002", "conclusion": "LEMAS-Dataset\u4f5c\u4e3a\u4e00\u4e2a\u4e30\u5bcc\u65f6\u95f4\u6233\u6807\u6ce8\u7684\u7ec6\u7c92\u5ea6\u591a\u8bed\u8a00\u8bed\u6599\u5e93\uff0c\u5c06\u63a8\u52a8\u57fa\u4e8e\u63d0\u793a\u7684\u8bed\u97f3\u751f\u6210\u7cfb\u7edf\u7684\u672a\u6765\u53d1\u5c55\u3002\u6570\u636e\u96c6\u7684\u8d28\u91cf\u901a\u8fc7\u4e24\u4e2a\u4e0d\u540c\u67b6\u6784\u7684\u57fa\u51c6\u6a21\u578b\u7684\u6709\u6548\u6027\u5f97\u5230\u4e86\u9a8c\u8bc1\u3002"}}
{"id": "2601.04969", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.04969", "abs": "https://arxiv.org/abs/2601.04969", "authors": ["Yichi Zhang", "Yuchen Zhang", "Wenyan Ma", "Lipeng Zhu", "Jianquan Wang", "Wanbin Tang", "Rui Zhang"], "title": "6D Movable Antenna Enhanced Cell-free MIMO: Two-timescale Decentralized Beamforming and Antenna Movement Optimization", "comment": "13 pages, 7 figures, 1 table", "summary": "This paper investigates a six-dimensional movable antenna (6DMA)-aided cell-free multi-user multiple-input multiple-output (MIMO) communication system. In this system, each distributed access point (AP) can flexibly adjust its array orientation and antenna positions to adapt to spatial channel variations and enhance communication performance. However, frequent antenna movements and centralized beamforming based on global instantaneous channel state information (CSI) sharing among APs entail extremely high signal processing delay and system overhead, which is difficult to be practically implemented in high-mobility scenarios with short channel coherence time. To address these practical implementation challenges and improve scalability, a two-timescale decentralized optimization framework is proposed in this paper to jointly design the beamformer, antenna positions, and array orientations. In the short timescale, each AP updates its receive beamformer based on local instantaneous CSI and global statistical CSI. In the long timescale, the central processing unit optimizes the antenna positions and array orientations at all APs based on global statistical CSI to maximize the ergodic sum rate of all users. The resulting optimization problem is non-convex and involves highly coupled variables, thus posing significant challenges for obtaining efficient solutions. To address this problem, a constrained stochastic successive convex approximation algorithm is developed. Numerical results demonstrate that the proposed 6DMA-aided cell-free system with decentralized beamforming significantly outperforms other antenna movement schemes with less flexibility and even achieves a performance comparable to that of the centralized beamforming benchmark.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd6\u7ef4\u53ef\u79fb\u52a8\u5929\u7ebf\u8f85\u52a9\u7684\u65e0\u5c0f\u533aMIMO\u7cfb\u7edf\uff0c\u91c7\u7528\u4e24\u65f6\u95f4\u5c3a\u5ea6\u53bb\u4e2d\u5fc3\u5316\u4f18\u5316\u6846\u67b6\uff0c\u5728\u77ed\u65f6\u95f4\u5c3a\u5ea6\u8fdb\u884c\u672c\u5730\u6ce2\u675f\u6210\u5f62\uff0c\u5728\u957f\u65f6\u95f4\u5c3a\u5ea6\u4f18\u5316\u5929\u7ebf\u4f4d\u7f6e\u548c\u9635\u5217\u65b9\u5411\uff0c\u4ee5\u89e3\u51b3\u4f20\u7edf\u96c6\u4e2d\u5f0f\u65b9\u6cd5\u7684\u9ad8\u5ef6\u8fdf\u548c\u5f00\u9500\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf6DMA\u8f85\u52a9\u7684\u65e0\u5c0f\u533aMIMO\u7cfb\u7edf\u4e2d\uff0c\u9891\u7e41\u7684\u5929\u7ebf\u79fb\u52a8\u548c\u57fa\u4e8e\u5168\u5c40\u77ac\u65f6CSI\u7684\u96c6\u4e2d\u5f0f\u6ce2\u675f\u6210\u5f62\u4f1a\u5bfc\u81f4\u6781\u9ad8\u7684\u4fe1\u53f7\u5904\u7406\u5ef6\u8fdf\u548c\u7cfb\u7edf\u5f00\u9500\uff0c\u96be\u4ee5\u5728\u4fe1\u9053\u76f8\u5e72\u65f6\u95f4\u77ed\u7684\u9ad8\u79fb\u52a8\u6027\u573a\u666f\u4e2d\u5b9e\u9645\u90e8\u7f72\u3002", "method": "\u63d0\u51fa\u4e24\u65f6\u95f4\u5c3a\u5ea6\u53bb\u4e2d\u5fc3\u5316\u4f18\u5316\u6846\u67b6\uff1a1) \u77ed\u65f6\u95f4\u5c3a\u5ea6\uff1a\u6bcf\u4e2a\u63a5\u5165\u70b9\u57fa\u4e8e\u672c\u5730\u77ac\u65f6CSI\u548c\u5168\u5c40\u7edf\u8ba1CSI\u66f4\u65b0\u63a5\u6536\u6ce2\u675f\u6210\u5f62\u5668\uff1b2) \u957f\u65f6\u95f4\u5c3a\u5ea6\uff1a\u4e2d\u592e\u5904\u7406\u5355\u5143\u57fa\u4e8e\u5168\u5c40\u7edf\u8ba1CSI\u4f18\u5316\u6240\u6709\u63a5\u5165\u70b9\u7684\u5929\u7ebf\u4f4d\u7f6e\u548c\u9635\u5217\u65b9\u5411\uff0c\u6700\u5927\u5316\u6240\u6709\u7528\u6237\u7684\u904d\u5386\u548c\u901f\u7387\u3002\u4e3a\u89e3\u51b3\u975e\u51f8\u4f18\u5316\u95ee\u9898\uff0c\u5f00\u53d1\u4e86\u7ea6\u675f\u968f\u673a\u9010\u6b21\u51f8\u903c\u8fd1\u7b97\u6cd5\u3002", "result": "\u6570\u503c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u5177\u6709\u53bb\u4e2d\u5fc3\u5316\u6ce2\u675f\u6210\u5f62\u76846DMA\u8f85\u52a9\u65e0\u5c0f\u533a\u7cfb\u7edf\u663e\u8457\u4f18\u4e8e\u7075\u6d3b\u6027\u8f83\u4f4e\u7684\u5176\u4ed6\u5929\u7ebf\u79fb\u52a8\u65b9\u6848\uff0c\u751a\u81f3\u5b9e\u73b0\u4e86\u4e0e\u96c6\u4e2d\u5f0f\u6ce2\u675f\u6210\u5f62\u57fa\u51c6\u76f8\u5f53\u7684\u6027\u80fd\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u4e24\u65f6\u95f4\u5c3a\u5ea6\u53bb\u4e2d\u5fc3\u5316\u4f18\u5316\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e866DMA\u8f85\u52a9\u65e0\u5c0f\u533aMIMO\u7cfb\u7edf\u7684\u5b9e\u9645\u90e8\u7f72\u6311\u6218\uff0c\u5728\u964d\u4f4e\u7cfb\u7edf\u5f00\u9500\u548c\u5ef6\u8fdf\u7684\u540c\u65f6\u4fdd\u6301\u4e86\u4f18\u5f02\u7684\u901a\u4fe1\u6027\u80fd\uff0c\u4e3a\u9ad8\u79fb\u52a8\u6027\u573a\u666f\u4e0b\u7684\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2601.04236", "categories": ["cs.SD", "cs.AI", "cs.RO", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.04236", "abs": "https://arxiv.org/abs/2601.04236", "authors": ["Yujiao Jiang", "Qingmin Liao", "Zongqing Lu"], "title": "SmoothSync: Dual-Stream Diffusion Transformers for Jitter-Robust Beat-Synchronized Gesture Generation from Quantized Audio", "comment": null, "summary": "Co-speech gesture generation is a critical area of research aimed at synthesizing speech-synchronized human-like gestures. Existing methods often suffer from issues such as rhythmic inconsistency, motion jitter, foot sliding and limited multi-sampling diversity. In this paper, we present SmoothSync, a novel framework that leverages quantized audio tokens in a novel dual-stream Diffusion Transformer (DiT) architecture to synthesis holistic gestures and enhance sampling variation. Specifically, we (1) fuse audio-motion features via complementary transformer streams to achieve superior synchronization, (2) introduce a jitter-suppression loss to improve temporal smoothness, (3) implement probabilistic audio quantization to generate distinct gesture sequences from identical inputs. To reliably evaluate beat synchronization under jitter, we introduce Smooth-BC, a robust variant of the beat consistency metric less sensitive to motion noise. Comprehensive experiments on the BEAT2 and SHOW datasets demonstrate SmoothSync's superiority, outperforming state-of-the-art methods by -30.6% FGD, 10.3% Smooth-BC, and 8.4% Diversity on BEAT2, while reducing jitter and foot sliding by -62.9% and -17.1% respectively. The code will be released to facilitate future research.", "AI": {"tldr": "SmoothSync\u662f\u4e00\u4e2a\u65b0\u7684\u8bed\u97f3\u540c\u6b65\u624b\u52bf\u751f\u6210\u6846\u67b6\uff0c\u4f7f\u7528\u53cc\u6d41\u6269\u6563Transformer\u67b6\u6784\uff0c\u901a\u8fc7\u91cf\u5316\u97f3\u9891token\u751f\u6210\u66f4\u5e73\u6ed1\u3001\u591a\u6837\u5316\u7684\u624b\u52bf\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u8282\u594f\u4e0d\u4e00\u81f4\u3001\u8fd0\u52a8\u6296\u52a8\u548c\u811a\u6ed1\u7b49\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u8bed\u97f3\u540c\u6b65\u624b\u52bf\u751f\u6210\u65b9\u6cd5\u5b58\u5728\u8282\u594f\u4e0d\u4e00\u81f4\u3001\u8fd0\u52a8\u6296\u52a8\u3001\u811a\u6ed1\u548c\u91c7\u6837\u591a\u6837\u6027\u6709\u9650\u7b49\u95ee\u9898\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u9ad8\u8d28\u91cf\u3001\u66f4\u5e73\u6ed1\u3001\u66f4\u591a\u6837\u5316\u7684\u624b\u52bf\u751f\u6210\u65b9\u6cd5\u3002", "method": "\u63d0\u51faSmoothSync\u6846\u67b6\uff1a1\uff09\u4f7f\u7528\u53cc\u6d41\u6269\u6563Transformer\u67b6\u6784\u878d\u5408\u97f3\u9891-\u8fd0\u52a8\u7279\u5f81\u5b9e\u73b0\u66f4\u597d\u7684\u540c\u6b65\uff1b2\uff09\u5f15\u5165\u6296\u52a8\u6291\u5236\u635f\u5931\u63d0\u9ad8\u65f6\u95f4\u5e73\u6ed1\u6027\uff1b3\uff09\u91c7\u7528\u6982\u7387\u97f3\u9891\u91cf\u5316\u4ece\u76f8\u540c\u8f93\u5165\u751f\u6210\u4e0d\u540c\u7684\u624b\u52bf\u5e8f\u5217\uff1b4\uff09\u63d0\u51faSmooth-BC\u6307\u6807\u66f4\u53ef\u9760\u5730\u8bc4\u4f30\u8282\u594f\u540c\u6b65\u6027\u3002", "result": "\u5728BEAT2\u548cSHOW\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSmoothSync\u5728FGD\u6307\u6807\u4e0a\u4f18\u4e8eSOTA\u65b9\u6cd530.6%\uff0cSmooth-BC\u63d0\u9ad810.3%\uff0c\u591a\u6837\u6027\u63d0\u9ad88.4%\uff0c\u540c\u65f6\u6296\u52a8\u51cf\u5c1162.9%\uff0c\u811a\u6ed1\u51cf\u5c1117.1%\u3002", "conclusion": "SmoothSync\u901a\u8fc7\u521b\u65b0\u7684\u53cc\u6d41\u6269\u6563Transformer\u67b6\u6784\u548c\u6296\u52a8\u6291\u5236\u6280\u672f\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8bed\u97f3\u540c\u6b65\u624b\u52bf\u751f\u6210\u7684\u8d28\u91cf\u3001\u5e73\u6ed1\u6027\u548c\u591a\u6837\u6027\uff0c\u4e3a\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u3002"}}
{"id": "2601.05000", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.05000", "abs": "https://arxiv.org/abs/2601.05000", "authors": ["Ronit Sohanpal", "Mindaugus Jarmolovicius", "Jiaqian Yang", "Eric Sillekens", "Romulo Aparecido", "Vitaly Mikhailov", "Jiawei Luo", "David J. DiGiovanni", "Ruben S. Luis", "Hideaki Furukawa", "Robert I. Killey", "Polina Bayvel"], "title": "Ultra-Wideband Transmission Systems From an Energy Perspective: Which Band is Next?", "comment": "Optical Fiber Communications Conference (OFC) 2026", "summary": "Measuring the power efficiency of the state-of-the-art OESCL-band amplifiers, we show that 1000 km OESCL-band systems can achieve 2.98x greater throughput for +48% higher energy-per-bit compared to CL-band transmission only.", "AI": {"tldr": "OESCL\u6ce2\u6bb5\u653e\u5927\u5668\u6bd4CL\u6ce2\u6bb5\u4f20\u8f93\u57281000\u516c\u91cc\u8ddd\u79bb\u4e0a\u53ef\u5b9e\u73b02.98\u500d\u541e\u5410\u91cf\uff0c\u4f46\u80fd\u8017\u589e\u52a048%", "motivation": "\u8bc4\u4f30OESCL\u6ce2\u6bb5\u653e\u5927\u5668\u5728\u957f\u8ddd\u79bb\u4f20\u8f93\u4e2d\u7684\u529f\u7387\u6548\u7387\u548c\u6027\u80fd\u4f18\u52bf\uff0c\u4e3a\u4e0b\u4e00\u4ee3\u5149\u901a\u4fe1\u7cfb\u7edf\u63d0\u4f9b\u53c2\u8003", "method": "\u6d4b\u91cf\u6700\u5148\u8fdbOESCL\u6ce2\u6bb5\u653e\u5927\u5668\u7684\u529f\u7387\u6548\u7387\uff0c\u6bd4\u8f83OESCL\u6ce2\u6bb5\u4e0eCL\u6ce2\u6bb5\u57281000\u516c\u91cc\u4f20\u8f93\u7cfb\u7edf\u4e2d\u7684\u6027\u80fd", "result": "1000\u516c\u91ccOESCL\u6ce2\u6bb5\u7cfb\u7edf\u76f8\u6bd4\u7eafCL\u6ce2\u6bb5\u4f20\u8f93\uff0c\u541e\u5410\u91cf\u63d0\u9ad82.98\u500d\uff0c\u4f46\u6bcf\u6bd4\u7279\u80fd\u8017\u589e\u52a048%", "conclusion": "OESCL\u6ce2\u6bb5\u7cfb\u7edf\u5728\u957f\u8ddd\u79bb\u4f20\u8f93\u4e2d\u5177\u6709\u663e\u8457\u7684\u541e\u5410\u91cf\u4f18\u52bf\uff0c\u4f46\u9700\u8981\u6743\u8861\u80fd\u8017\u589e\u52a0\u7684\u95ee\u9898"}}
{"id": "2601.04343", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.04343", "abs": "https://arxiv.org/abs/2601.04343", "authors": ["Yongyi Zang", "Jiarui Hai", "Wanying Ge", "Qiuqiang Kong", "Zheqi Dai", "Helin Wang", "Yuki Mitsufuji", "Mark D. Plumbley"], "title": "Summary of The Inaugural Music Source Restoration Challenge", "comment": null, "summary": "Music Source Restoration (MSR) aims to recover original, unprocessed instrument stems from professionally mixed and degraded audio, requiring the reversal of both production effects and real-world degradations. We present the inaugural MSR Challenge, which features objective evaluation on studio-produced mixtures using Multi-Mel-SNR, Zimtohrli, and FAD-CLAP, alongside subjective evaluation on real-world degraded recordings. Five teams participated in the challenge. The winning system achieved 4.46 dB Multi-Mel-SNR and 3.47 MOS-Overall, corresponding to relative improvements of 91% and 18% over the second-place system, respectively. Per-stem analysis reveals substantial variation in restoration difficulty across instruments, with bass averaging 4.59 dB across all teams, while percussion averages only 0.29 dB. The dataset, evaluation protocols, and baselines are available at https://msrchallenge.com/.", "AI": {"tldr": "\u9996\u5c4a\u97f3\u4e50\u6e90\u6062\u590d\u6311\u6218\u8d5b\uff1a\u4ece\u4e13\u4e1a\u6df7\u97f3\u548c\u771f\u5b9e\u4e16\u754c\u964d\u8d28\u97f3\u9891\u4e2d\u6062\u590d\u539f\u59cb\u4e50\u5668\u97f3\u8f68\uff0c\u83b7\u80dc\u7cfb\u7edf\u5728\u5ba2\u89c2\u548c\u4e3b\u89c2\u8bc4\u4f30\u4e2d\u5747\u663e\u8457\u4f18\u4e8e\u7b2c\u4e8c\u540d\u3002", "motivation": "\u97f3\u4e50\u6e90\u6062\u590d\uff08MSR\uff09\u65e8\u5728\u4ece\u7ecf\u8fc7\u4e13\u4e1a\u6df7\u97f3\u548c\u964d\u8d28\u7684\u97f3\u9891\u4e2d\u6062\u590d\u539f\u59cb\u3001\u672a\u7ecf\u5904\u7406\u7684\u4e50\u5668\u97f3\u8f68\uff0c\u8fd9\u9700\u8981\u540c\u65f6\u9006\u8f6c\u5236\u4f5c\u6548\u679c\u548c\u771f\u5b9e\u4e16\u754c\u7684\u964d\u8d28\u5f71\u54cd\u3002\u76ee\u524d\u7f3a\u4e4f\u6807\u51c6\u5316\u7684\u8bc4\u4f30\u6846\u67b6\u548c\u57fa\u51c6\u3002", "method": "\u4e3e\u529e\u9996\u5c4aMSR\u6311\u6218\u8d5b\uff0c\u91c7\u7528\u5ba2\u89c2\u8bc4\u4f30\uff08Multi-Mel-SNR\u3001Zimtohrli\u3001FAD-CLAP\uff09\u548c\u4e3b\u89c2\u8bc4\u4f30\uff08MOS-Overall\uff09\u76f8\u7ed3\u5408\u7684\u65b9\u5f0f\u3002\u8bc4\u4f30\u5305\u62ec\u5de5\u4f5c\u5ba4\u5236\u4f5c\u7684\u6df7\u97f3\u548c\u771f\u5b9e\u4e16\u754c\u964d\u8d28\u5f55\u97f3\u3002", "result": "\u4e94\u4e2a\u56e2\u961f\u53c2\u4e0e\u6311\u6218\uff0c\u83b7\u80dc\u7cfb\u7edf\u8fbe\u52304.46 dB Multi-Mel-SNR\u548c3.47 MOS-Overall\uff0c\u5206\u522b\u6bd4\u7b2c\u4e8c\u540d\u7cfb\u7edf\u76f8\u5bf9\u63d0\u534791%\u548c18%\u3002\u4e0d\u540c\u4e50\u5668\u7684\u6062\u590d\u96be\u5ea6\u5dee\u5f02\u663e\u8457\uff1a\u8d1d\u65af\u5e73\u57474.59 dB\uff0c\u800c\u6253\u51fb\u4e50\u4ec50.29 dB\u3002", "conclusion": "MSR\u6311\u6218\u8d5b\u5efa\u7acb\u4e86\u9996\u4e2a\u6807\u51c6\u5316\u8bc4\u4f30\u6846\u67b6\uff0c\u5c55\u793a\u4e86\u97f3\u4e50\u6e90\u6062\u590d\u4efb\u52a1\u7684\u53ef\u884c\u6027\uff0c\u4f46\u4e0d\u540c\u4e50\u5668\u7684\u6062\u590d\u96be\u5ea6\u5dee\u5f02\u5f88\u5927\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u51c6\u548c\u65b9\u5411\u3002"}}
{"id": "2601.05032", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.05032", "abs": "https://arxiv.org/abs/2601.05032", "authors": ["Steven Rivetti", "Gabor Fodor", "Emil Bj\u00f6rnson", "Mikael Skoglund"], "title": "On the Impact of Channel Aging and Doppler-Affected Clutter on OFDM ISAC Systems", "comment": "13 pages, 21 pictures, submitted to IEEE TWC", "summary": "The temporal evolution of the propagation environment plays a central role in integrated sensing and communication (ISAC) systems. A slow-time evolution manifests as channel aging in communication links, while a fast-time one is associated with structured clutter with non-zero Doppler. Nevertheless, the joint impact of these two phenomena on ISAC performance has been largely overlooked. This addresses this research gap in a network utilizing orthogonal frequency division multiplexing waveforms. Here, a base station simultaneously serves multiple user equipment (UE) devices and performs monostatic sensing. Channel aging is captured through an autoregressive model with exponential correlation decay. In contrast, clutter is modeled as a collection of uncorrelated, coherent patches with non-zero Doppler, resulting in a Kronecker-separable covariance structure. We propose an aging-aware channel estimator that uses prior pilot observations to estimate the time-varying UE channels, characterized by a non-isotropic multipath fading structure. The clutter's structure enables a novel low-complexity sensing pipeline: clutter statistics are estimated from raw data and subsequently used to suppress the clutter's action, after which target parameters are extracted through range-angle and range-velocity maps. We evaluate the influence of frame length and pilot history on channel estimation accuracy and demonstrate substantial performance gains over block fading in low-to-moderate mobility regimes. The sensing pipeline is implemented in a clutter-dominated environment, demonstrating that effective clutter suppression can be achieved under practical configurations. Furthermore, our results show that dedicated sensing streams are required, as communication beams provide insufficient range resolution.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u4fe1\u9053\u8001\u5316\u548c\u6742\u6ce2\u5bf9ISAC\u7cfb\u7edf\u7684\u8054\u5408\u5f71\u54cd\uff0c\u63d0\u51fa\u4e86\u8001\u5316\u611f\u77e5\u7684\u4fe1\u9053\u4f30\u8ba1\u5668\u548c\u4f4e\u590d\u6742\u5ea6\u611f\u77e5\u6d41\u6c34\u7ebf\uff0c\u5728\u4f4e\u5230\u4e2d\u7b49\u79fb\u52a8\u6027\u573a\u666f\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5927\u591a\u5ffd\u7565\u4e86\u4fe1\u9053\u8001\u5316\uff08\u6162\u65f6\u6f14\u5316\uff09\u548c\u6742\u6ce2\uff08\u5feb\u65f6\u6f14\u5316\uff09\u5bf9\u96c6\u6210\u611f\u77e5\u4e0e\u901a\u4fe1\uff08ISAC\uff09\u7cfb\u7edf\u7684\u8054\u5408\u5f71\u54cd\u3002\u4fe1\u9053\u8001\u5316\u5bfc\u81f4\u901a\u4fe1\u94fe\u8def\u6027\u80fd\u4e0b\u964d\uff0c\u800c\u5177\u6709\u975e\u96f6\u591a\u666e\u52d2\u7684\u7ed3\u6784\u5316\u6742\u6ce2\u5219\u5f71\u54cd\u611f\u77e5\u6027\u80fd\uff0c\u9700\u8981\u540c\u65f6\u89e3\u51b3\u8fd9\u4e24\u4e2a\u95ee\u9898\u3002", "method": "1. \u4f7f\u7528\u5177\u6709\u6307\u6570\u76f8\u5173\u8870\u51cf\u7684\u81ea\u56de\u5f52\u6a21\u578b\u5efa\u6a21\u4fe1\u9053\u8001\u5316\uff1b2. \u5c06\u6742\u6ce2\u5efa\u6a21\u4e3a\u5177\u6709\u975e\u96f6\u591a\u666e\u52d2\u7684\u975e\u76f8\u5173\u76f8\u5e72\u5757\u96c6\u5408\uff0c\u5f62\u6210Kronecker\u53ef\u5206\u79bb\u534f\u65b9\u5dee\u7ed3\u6784\uff1b3. \u63d0\u51fa\u8001\u5316\u611f\u77e5\u4fe1\u9053\u4f30\u8ba1\u5668\uff0c\u5229\u7528\u5148\u5bfc\u5bfc\u9891\u89c2\u6d4b\u4f30\u8ba1\u65f6\u53d8UE\u4fe1\u9053\uff1b4. \u8bbe\u8ba1\u4f4e\u590d\u6742\u5ea6\u611f\u77e5\u6d41\u6c34\u7ebf\uff1a\u4ece\u539f\u59cb\u6570\u636e\u4f30\u8ba1\u6742\u6ce2\u7edf\u8ba1\u91cf\uff0c\u6291\u5236\u6742\u6ce2\u5f71\u54cd\uff0c\u7136\u540e\u901a\u8fc7\u8ddd\u79bb-\u89d2\u5ea6\u548c\u8ddd\u79bb-\u901f\u5ea6\u56fe\u63d0\u53d6\u76ee\u6807\u53c2\u6570\u3002", "result": "1. \u8bc4\u4f30\u4e86\u5e27\u957f\u5ea6\u548c\u5bfc\u9891\u5386\u53f2\u5bf9\u4fe1\u9053\u4f30\u8ba1\u7cbe\u5ea6\u7684\u5f71\u54cd\uff0c\u5728\u4f4e\u5230\u4e2d\u7b49\u79fb\u52a8\u6027\u573a\u666f\u4e2d\u76f8\u6bd4\u5757\u8870\u843d\u6a21\u578b\u83b7\u5f97\u663e\u8457\u6027\u80fd\u63d0\u5347\uff1b2. \u5728\u6742\u6ce2\u4e3b\u5bfc\u73af\u5883\u4e2d\u5b9e\u73b0\u611f\u77e5\u6d41\u6c34\u7ebf\uff0c\u8bc1\u660e\u5728\u5b9e\u9645\u914d\u7f6e\u4e0b\u53ef\u5b9e\u73b0\u6709\u6548\u6742\u6ce2\u6291\u5236\uff1b3. \u53d1\u73b0\u4e13\u7528\u611f\u77e5\u6d41\u662f\u5fc5\u9700\u7684\uff0c\u56e0\u4e3a\u901a\u4fe1\u6ce2\u675f\u65e0\u6cd5\u63d0\u4f9b\u8db3\u591f\u7684\u8ddd\u79bb\u5206\u8fa8\u7387\u3002", "conclusion": "\u8be5\u7814\u7a76\u586b\u8865\u4e86\u4fe1\u9053\u8001\u5316\u548c\u6742\u6ce2\u5bf9ISAC\u7cfb\u7edf\u8054\u5408\u5f71\u54cd\u7684\u7814\u7a76\u7a7a\u767d\uff0c\u63d0\u51fa\u7684\u8001\u5316\u611f\u77e5\u4fe1\u9053\u4f30\u8ba1\u548c\u4f4e\u590d\u6742\u5ea6\u611f\u77e5\u6d41\u6c34\u7ebf\u5728\u5b9e\u9645\u79fb\u52a8\u6027\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u540c\u65f6\u5f3a\u8c03\u4e86\u4e13\u7528\u611f\u77e5\u6d41\u5bf9\u5b9e\u73b0\u8db3\u591f\u8ddd\u79bb\u5206\u8fa8\u7387\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2601.04564", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2601.04564", "abs": "https://arxiv.org/abs/2601.04564", "authors": ["Dawei Huang", "Yongjie Lv", "Ruijie Xiong", "Chunxiang Jin", "Xiaojiang Peng"], "title": "When Tone and Words Disagree: Towards Robust Speech Emotion Recognition under Acoustic-Semantic Conflict", "comment": null, "summary": "Speech Emotion Recognition (SER) systems often assume congruence between vocal emotion and lexical semantics. However, in real-world interactions, acoustic-semantic conflict is common yet overlooked, where the emotion conveyed by tone contradicts the literal meaning of spoken words. We show that state-of-the-art SER models, including ASR-based, self-supervised learning (SSL) approaches and Audio Language Models (ALMs), suffer performance degradation under such conflicts due to semantic bias or entangled acoustic-semantic representations. To address this, we propose the Fusion Acoustic-Semantic (FAS) framework, which explicitly disentangles acoustic and semantic pathways and bridges them through a lightweight, query-based attention module. To enable systematic evaluation, we introduce the Conflict in Acoustic-Semantic Emotion (CASE), the first dataset dominated by clear and interpretable acoustic-semantic conflicts in varied scenarios. Extensive experiments demonstrate that FAS consistently outperforms existing methods in both in-domain and zero-shot settings. Notably, on the CASE benchmark, conventional SER models fail dramatically, while FAS sets a new SOTA with 59.38% accuracy. Our code and datasets is available at https://github.com/24DavidHuang/FAS.", "AI": {"tldr": "\u63d0\u51faFAS\u6846\u67b6\u89e3\u51b3\u8bed\u97f3\u60c5\u611f\u8bc6\u522b\u4e2d\u58f0\u5b66-\u8bed\u4e49\u51b2\u7a81\u95ee\u9898\uff0c\u5728CASE\u6570\u636e\u96c6\u4e0a\u8fbe\u523059.38%\u51c6\u786e\u7387", "motivation": "\u73b0\u6709\u8bed\u97f3\u60c5\u611f\u8bc6\u522b\u7cfb\u7edf\u5047\u8bbe\u58f0\u5b66\u60c5\u611f\u4e0e\u8bed\u4e49\u5185\u5bb9\u4e00\u81f4\uff0c\u4f46\u73b0\u5b9e\u4e2d\u58f0\u5b66-\u8bed\u4e49\u51b2\u7a81\u5f88\u5e38\u89c1\uff08\u8bed\u8c03\u60c5\u611f\u4e0e\u5b57\u9762\u542b\u4e49\u77db\u76fe\uff09\uff0c\u73b0\u6709\u6a21\u578b\u5bf9\u6b64\u8868\u73b0\u4e0d\u4f73", "method": "\u63d0\u51faFusion Acoustic-Semantic (FAS)\u6846\u67b6\uff0c\u663e\u5f0f\u89e3\u8026\u58f0\u5b66\u548c\u8bed\u4e49\u8def\u5f84\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u57fa\u4e8e\u67e5\u8be2\u7684\u6ce8\u610f\u529b\u6a21\u5757\u8fde\u63a5\u4e24\u8005", "result": "FAS\u5728\u9886\u57df\u5185\u548c\u96f6\u6837\u672c\u8bbe\u7f6e\u4e2d\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5728CASE\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u523059.38%\u51c6\u786e\u7387\uff0c\u800c\u4f20\u7edf\u6a21\u578b\u8868\u73b0\u5f88\u5dee", "conclusion": "FAS\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u58f0\u5b66-\u8bed\u4e49\u51b2\u7a81\u95ee\u9898\uff0c\u5728\u9996\u4e2a\u4ee5\u58f0\u5b66-\u8bed\u4e49\u51b2\u7a81\u4e3a\u4e3b\u7684CASE\u6570\u636e\u96c6\u4e0a\u5efa\u7acb\u4e86\u65b0\u7684SOTA"}}
{"id": "2601.05178", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.05178", "abs": "https://arxiv.org/abs/2601.05178", "authors": ["Ehsan Shourezari", "Ossi Kaltiokallio", "Mehmet C. Ilter", "Jukka Talvitie", "Gonzalo Seco-Granados", "Henk Wymeersch", "Mikko Valkama"], "title": "Multi-band Carrier Phase Positioning toward 6G: Performance Bounds and Efficient Estimators", "comment": "13 pages, 10 figures, under review in IEEE Transactions on Wireless Communications", "summary": "In addition to satellite systems, carrier phase positioning (CPP) is gaining attraction also in terrestrial mobile networks, particularly in 5G New Radio evolution toward 6G. One key challenge is to resolve the integer ambiguity problem, as the carrier phase provides only relative position information. This work introduces and studies a multi-band CPP scenario with intra- and inter-band carrier aggregation (CA) opportunities across FR1, mmWave-FR2, and emerging 6G FR3 bands. Specifically, we derive multi-band CPP performance bounds, showcasing the superiority of multi-band CPP for high-precision localization in current and future mobile networks, while noting also practical imperfections such as clock offsets between the user equipment (UE) and the network as well as mutual clock imperfections between the network nodes. A wide collection of numerical results is provided, covering the impacts of the available carrier bandwidth, number of aggregated carriers, transmit power, and the number of network nodes or base stations. The offered results highlight that only two carriers suffice to substantially facilitate resolving the integer ambiguity problem while also largely enhancing the robustness of positioning against imperfections imposed by the network-side clocks and multi-path propagation. In addition, we also propose a two-stage practical estimator that achieves the derived bounds under all realistic bandwidth and transmit power conditions. Furthermore, we show that with an additional search-based refinement step, the proposed estimator becomes particularly suitable for narrowband Internet of Things applications operating efficiently even under narrow carrier bandwidths. Finally, both the derived bounds and the proposed estimators are extended to scenarios where the bands assigned to each base station are nonuniform or fully disjoint, enhancing the practical deployment flexibility.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u591a\u9891\u6bb5\u8f7d\u6ce2\u76f8\u4f4d\u5b9a\u4f4d(CPP)\u57285G/6G\u79fb\u52a8\u7f51\u7edc\u4e2d\u7684\u5e94\u7528\uff0c\u901a\u8fc7\u591a\u9891\u6bb5\u8f7d\u6ce2\u805a\u5408\u89e3\u51b3\u6574\u6570\u6a21\u7cca\u5ea6\u95ee\u9898\uff0c\u63d0\u51fa\u6027\u80fd\u754c\u9650\u548c\u5b9e\u7528\u4f30\u8ba1\u5668\uff0c\u663e\u8457\u63d0\u5347\u5b9a\u4f4d\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u968f\u77405G\u54116G\u6f14\u8fdb\uff0c\u8f7d\u6ce2\u76f8\u4f4d\u5b9a\u4f4d\u5728\u79fb\u52a8\u7f51\u7edc\u4e2d\u53d7\u5230\u5173\u6ce8\uff0c\u4f46\u9762\u4e34\u6574\u6570\u6a21\u7cca\u5ea6\u95ee\u9898\u7684\u6311\u6218\u3002\u9700\u8981\u7814\u7a76\u591a\u9891\u6bb5CPP\u65b9\u6848\uff0c\u5229\u7528FR1\u3001FR2\u548cFR3\u9891\u6bb5\u7684\u8f7d\u6ce2\u805a\u5408\u673a\u4f1a\uff0c\u89e3\u51b3\u65f6\u949f\u504f\u79fb\u7b49\u5b9e\u9645\u7f3a\u9677\uff0c\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u5b9a\u4f4d\u3002", "method": "\u63a8\u5bfc\u591a\u9891\u6bb5CPP\u6027\u80fd\u754c\u9650\uff0c\u63d0\u51fa\u4e24\u9636\u6bb5\u5b9e\u7528\u4f30\u8ba1\u5668\uff08\u5305\u542b\u641c\u7d22\u4f18\u5316\u6b65\u9aa4\uff09\uff0c\u6269\u5c55\u5230\u57fa\u7ad9\u9891\u6bb5\u975e\u5747\u5300\u6216\u5b8c\u5168\u5206\u79bb\u7684\u573a\u666f\u3002\u5206\u6790\u53ef\u7528\u5e26\u5bbd\u3001\u805a\u5408\u8f7d\u6ce2\u6570\u91cf\u3001\u53d1\u5c04\u529f\u7387\u548c\u57fa\u7ad9\u6570\u91cf\u7b49\u53c2\u6570\u5f71\u54cd\u3002", "result": "\u4ec5\u9700\u4e24\u4e2a\u8f7d\u6ce2\u5373\u53ef\u663e\u8457\u89e3\u51b3\u6574\u6570\u6a21\u7cca\u5ea6\u95ee\u9898\uff0c\u589e\u5f3a\u5bf9\u7f51\u7edc\u4fa7\u65f6\u949f\u7f3a\u9677\u548c\u591a\u5f84\u4f20\u64ad\u7684\u9c81\u68d2\u6027\u3002\u63d0\u51fa\u7684\u4f30\u8ba1\u5668\u5728\u6240\u6709\u5b9e\u9645\u5e26\u5bbd\u548c\u53d1\u5c04\u529f\u7387\u6761\u4ef6\u4e0b\u90fd\u80fd\u8fbe\u5230\u7406\u8bba\u754c\u9650\uff0c\u7279\u522b\u9002\u5408\u7a84\u5e26\u7269\u8054\u7f51\u5e94\u7528\u3002", "conclusion": "\u591a\u9891\u6bb5CPP\u5728\u5f53\u524d\u548c\u672a\u6765\u79fb\u52a8\u7f51\u7edc\u4e2d\u5177\u6709\u4f18\u8d8a\u7684\u9ad8\u7cbe\u5ea6\u5b9a\u4f4d\u6027\u80fd\uff0c\u63d0\u51fa\u7684\u4f30\u8ba1\u5668\u65b9\u6848\u5b9e\u7528\u6027\u5f3a\uff0c\u652f\u6301\u7075\u6d3b\u90e8\u7f72\u914d\u7f6e\uff0c\u4e3a5G/6G\u7f51\u7edc\u5b9a\u4f4d\u6280\u672f\u53d1\u5c55\u63d0\u4f9b\u91cd\u8981\u652f\u6491\u3002"}}
{"id": "2601.04656", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2601.04656", "abs": "https://arxiv.org/abs/2601.04656", "authors": ["Dekun Chen", "Xueyao Zhang", "Yuancheng Wang", "Kenan Dai", "Li Ma", "Zhizheng Wu"], "title": "FlexiVoice: Enabling Flexible Style Control in Zero-Shot TTS with Natural Language Instructions", "comment": null, "summary": "This study proposes FlexiVoice, a text-to-speech (TTS) synthesis system capable of flexible style control with zero-shot voice cloning. The speaking style is controlled by a natural-language instruction and the voice timbre is provided by a speech reference in zero-shot manner. FlexiVoice is built with an LLM core, which takes text as input, and also takes an optional natural language instruction and an optional speech reference to control style and timbre, respectively. FlexiVoice is equipped with a novel Progressive Post-Training (PPT) scheme that progressively unlocks accurate and flexible controllability. In particular, it first employs Direct Preference Optimization (DPO) to enable FlexiVoice to accurately follow both natural language instruction and speech reference simultaneously. It then uses a multi-objective Group Relative Policy Optimization (GRPO) to disentangle style instruction, reference timbre, and textual content. Finally, it adapts instruction GRPO for more advanced instruction following. Experimental results show that FlexiVoice surpasses competing baselines and demonstrates strong capability in decoupling control factors. Human evaluations further confirm its naturalness, controllability, and robustness. Audio samples are available at https://flexi-voice.github.io.", "AI": {"tldr": "FlexiVoice\u662f\u4e00\u4e2a\u57fa\u4e8eLLM\u6838\u5fc3\u7684TTS\u7cfb\u7edf\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u548c\u8bed\u97f3\u53c2\u8003\u5b9e\u73b0\u96f6\u6837\u672c\u8bed\u97f3\u514b\u9686\u548c\u7075\u6d3b\u98ce\u683c\u63a7\u5236\uff0c\u91c7\u7528\u6e10\u8fdb\u5f0f\u540e\u8bad\u7ec3\u65b9\u6848\u63d0\u5347\u63a7\u5236\u7cbe\u5ea6\u548c\u89e3\u8026\u80fd\u529b\u3002", "motivation": "\u73b0\u6709TTS\u7cfb\u7edf\u5728\u96f6\u6837\u672c\u8bed\u97f3\u514b\u9686\u548c\u7075\u6d3b\u98ce\u683c\u63a7\u5236\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u540c\u65f6\u63a7\u5236\u8bf4\u8bdd\u98ce\u683c\u548c\u97f3\u8272\uff0c\u5e76\u5b9e\u73b0\u63a7\u5236\u56e0\u7d20\u7684\u6709\u6548\u89e3\u8026\u3002", "method": "\u57fa\u4e8eLLM\u6838\u5fc3\u6784\u5efa\uff0c\u91c7\u7528\u6e10\u8fdb\u5f0f\u540e\u8bad\u7ec3\u65b9\u6848\uff1a1) \u4f7f\u7528DPO\u4f7f\u7cfb\u7edf\u80fd\u540c\u65f6\u51c6\u786e\u9075\u5faa\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u548c\u8bed\u97f3\u53c2\u8003\uff1b2) \u4f7f\u7528\u591a\u76ee\u6807GRPO\u89e3\u8026\u98ce\u683c\u6307\u4ee4\u3001\u53c2\u8003\u97f3\u8272\u548c\u6587\u672c\u5185\u5bb9\uff1b3) \u4f7f\u7528\u6307\u4ee4GRPO\u8fdb\u884c\u66f4\u9ad8\u7ea7\u7684\u6307\u4ee4\u8ddf\u968f\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660eFlexiVoice\u8d85\u8d8a\u7ade\u4e89\u57fa\u7ebf\uff0c\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u63a7\u5236\u56e0\u7d20\u89e3\u8026\u80fd\u529b\u3002\u4eba\u5de5\u8bc4\u4f30\u8fdb\u4e00\u6b65\u8bc1\u5b9e\u4e86\u5176\u81ea\u7136\u5ea6\u3001\u53ef\u63a7\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "FlexiVoice\u6210\u529f\u5b9e\u73b0\u4e86\u96f6\u6837\u672c\u8bed\u97f3\u514b\u9686\u548c\u7075\u6d3b\u98ce\u683c\u63a7\u5236\uff0c\u901a\u8fc7\u6e10\u8fdb\u5f0f\u540e\u8bad\u7ec3\u65b9\u6848\u6709\u6548\u63d0\u5347\u4e86\u63a7\u5236\u7cbe\u5ea6\u548c\u89e3\u8026\u80fd\u529b\uff0c\u4e3aTTS\u7cfb\u7edf\u63d0\u4f9b\u4e86\u66f4\u5f3a\u5927\u7684\u53ef\u63a7\u6027\u3002"}}
{"id": "2601.04658", "categories": ["cs.SD", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.04658", "abs": "https://arxiv.org/abs/2601.04658", "authors": ["Hyeongkeun Lee", "Jongmin Choi", "KiHyun Nam", "Joon Son Chung"], "title": "LAMB: LLM-based Audio Captioning with Modality Gap Bridging via Cauchy-Schwarz Divergence", "comment": "5 pages, 2 figures;", "summary": "Automated Audio Captioning aims to describe the semantic content of input audio. Recent works have employed large language models (LLMs) as a text decoder to leverage their reasoning capabilities. However, prior approaches that project audio features into the LLM embedding space without considering cross-modal alignment fail to fully utilize these capabilities. To address this, we propose LAMB, an LLM-based audio captioning framework that bridges the modality gap between audio embeddings and the LLM text embedding space. LAMB incorporates a Cross-Modal Aligner that minimizes Cauchy-Schwarz divergence while maximizing mutual information, yielding tighter alignment between audio and text at both global and token levels. We further design a Two-Stream Adapter that extracts semantically enriched audio embeddings, thereby delivering richer information to the Cross-Modal Aligner. Finally, leveraging the aligned audio embeddings, a proposed Token Guide directly computes scores within the LLM text embedding space to steer the output logits of generated captions. Experimental results confirm that our framework strengthens the reasoning capabilities of the LLM decoder, achieving state-of-the-art performance on AudioCaps.", "AI": {"tldr": "LAMB\uff1a\u57fa\u4e8eLLM\u7684\u97f3\u9891\u63cf\u8ff0\u6846\u67b6\uff0c\u901a\u8fc7\u8de8\u6a21\u6001\u5bf9\u9f50\u5668\u7f29\u5c0f\u97f3\u9891\u4e0e\u6587\u672c\u5d4c\u5165\u7a7a\u95f4\u5dee\u8ddd\uff0c\u589e\u5f3aLLM\u63a8\u7406\u80fd\u529b\uff0c\u5728AudioCaps\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5c06\u97f3\u9891\u7279\u5f81\u6295\u5f71\u5230LLM\u5d4c\u5165\u7a7a\u95f4\u65f6\u672a\u8003\u8651\u8de8\u6a21\u6001\u5bf9\u9f50\uff0c\u65e0\u6cd5\u5145\u5206\u5229\u7528LLM\u7684\u63a8\u7406\u80fd\u529b\u3002\u9700\u8981\u89e3\u51b3\u97f3\u9891\u5d4c\u5165\u4e0eLLM\u6587\u672c\u5d4c\u5165\u7a7a\u95f4\u4e4b\u95f4\u7684\u6a21\u6001\u5dee\u8ddd\u95ee\u9898\u3002", "method": "1. \u8de8\u6a21\u6001\u5bf9\u9f50\u5668\uff1a\u6700\u5c0f\u5316\u67ef\u897f-\u65bd\u74e6\u8328\u6563\u5ea6\u540c\u65f6\u6700\u5927\u5316\u4e92\u4fe1\u606f\uff0c\u5b9e\u73b0\u5168\u5c40\u548ctoken\u7ea7\u522b\u7684\u97f3\u9891-\u6587\u672c\u5bf9\u9f50\uff1b2. \u53cc\u6d41\u9002\u914d\u5668\uff1a\u63d0\u53d6\u8bed\u4e49\u4e30\u5bcc\u7684\u97f3\u9891\u5d4c\u5165\uff1b3. \u4ee4\u724c\u5f15\u5bfc\u5668\uff1a\u5728LLM\u6587\u672c\u5d4c\u5165\u7a7a\u95f4\u4e2d\u76f4\u63a5\u8ba1\u7b97\u5206\u6570\u6765\u5f15\u5bfc\u751f\u6210\u63cf\u8ff0\u7684\u5bf9\u6570\u6982\u7387\u3002", "result": "\u5b9e\u9a8c\u8bc1\u5b9e\u6846\u67b6\u589e\u5f3a\u4e86LLM\u89e3\u7801\u5668\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5728AudioCaps\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "LAMB\u901a\u8fc7\u6709\u6548\u7684\u8de8\u6a21\u6001\u5bf9\u9f50\u673a\u5236\uff0c\u6210\u529f\u7f29\u5c0f\u4e86\u97f3\u9891\u4e0e\u6587\u672c\u5d4c\u5165\u7a7a\u95f4\u7684\u5dee\u8ddd\uff0c\u4f7fLLM\u80fd\u591f\u66f4\u597d\u5730\u7406\u89e3\u548c\u63cf\u8ff0\u97f3\u9891\u5185\u5bb9\uff0c\u4e3a\u97f3\u9891\u63cf\u8ff0\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.04744", "categories": ["cs.SD", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.04744", "abs": "https://arxiv.org/abs/2601.04744", "authors": ["Xingyuan Li", "Mengyue Wu"], "title": "Semi-Supervised Diseased Detection from Speech Dialogues with Multi-Level Data Modeling", "comment": null, "summary": "Detecting medical conditions from speech acoustics is fundamentally a weakly-supervised learning problem: a single, often noisy, session-level label must be linked to nuanced patterns within a long, complex audio recording. This task is further hampered by severe data scarcity and the subjective nature of clinical annotations. While semi-supervised learning (SSL) offers a viable path to leverage unlabeled data, existing audio methods often fail to address the core challenge that pathological traits are not uniformly expressed in a patient's speech. We propose a novel, audio-only SSL framework that explicitly models this hierarchy by jointly learning from frame-level, segment-level, and session-level representations within unsegmented clinical dialogues. Our end-to-end approach dynamically aggregates these multi-granularity features and generates high-quality pseudo-labels to efficiently utilize unlabeled data. Extensive experiments show the framework is model-agnostic, robust across languages and conditions, and highly data-efficient-achieving, for instance, 90\\% of fully-supervised performance using only 11 labeled samples. This work provides a principled approach to learning from weak, far-end supervision in medical speech analysis.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u97f3\u9891\u4e13\u7528\u7684\u534a\u76d1\u7763\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u8054\u5408\u5b66\u4e60\u5e27\u7ea7\u3001\u6bb5\u7ea7\u548c\u4f1a\u8bdd\u7ea7\u8868\u793a\uff0c\u89e3\u51b3\u533b\u5b66\u8bed\u97f3\u5206\u6790\u4e2d\u7684\u5f31\u76d1\u7763\u95ee\u9898\uff0c\u5728\u6570\u636e\u7a00\u7f3a\u60c5\u51b5\u4e0b\u5b9e\u73b0\u9ad8\u6548\u5b66\u4e60\u3002", "motivation": "\u533b\u5b66\u8bed\u97f3\u5206\u6790\u9762\u4e34\u6838\u5fc3\u6311\u6218\uff1a\u75c5\u7406\u7279\u5f81\u5728\u60a3\u8005\u8bed\u97f3\u4e2d\u4e0d\u5747\u5300\u8868\u8fbe\uff0c\u4e14\u5b58\u5728\u6570\u636e\u7a00\u7f3a\u3001\u4e34\u5e8a\u6807\u6ce8\u4e3b\u89c2\u6027\u5f3a\u7684\u95ee\u9898\u3002\u73b0\u6709\u97f3\u9891\u65b9\u6cd5\u672a\u80fd\u6709\u6548\u5904\u7406\u8fd9\u79cd\u5c42\u6b21\u7ed3\u6784\uff0c\u9700\u8981\u4e13\u95e8\u9488\u5bf9\u5f31\u76d1\u7763\u5b66\u4e60\u573a\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u7aef\u5230\u7aef\u7684\u97f3\u9891\u534a\u76d1\u7763\u5b66\u4e60\u6846\u67b6\uff0c\u8054\u5408\u5b66\u4e60\u672a\u5206\u5272\u4e34\u5e8a\u5bf9\u8bdd\u4e2d\u7684\u5e27\u7ea7\u3001\u6bb5\u7ea7\u548c\u4f1a\u8bdd\u7ea7\u8868\u793a\u3002\u52a8\u6001\u805a\u5408\u591a\u7c92\u5ea6\u7279\u5f81\u5e76\u751f\u6210\u9ad8\u8d28\u91cf\u4f2a\u6807\u7b7e\uff0c\u9ad8\u6548\u5229\u7528\u672a\u6807\u6ce8\u6570\u636e\u3002\u6846\u67b6\u5177\u6709\u6a21\u578b\u65e0\u5173\u6027\u3002", "result": "\u6846\u67b6\u5728\u8de8\u8bed\u8a00\u548c\u4e0d\u540c\u533b\u7597\u6761\u4ef6\u4e0b\u8868\u73b0\u7a33\u5065\u4e14\u6570\u636e\u9ad8\u6548\uff0c\u4f8b\u5982\u4ec5\u4f7f\u752811\u4e2a\u6807\u6ce8\u6837\u672c\u5c31\u80fd\u8fbe\u5230\u5168\u76d1\u7763\u6027\u80fd\u768490%\u3002\u8bc1\u660e\u80fd\u591f\u6709\u6548\u5904\u7406\u533b\u5b66\u8bed\u97f3\u5206\u6790\u4e2d\u7684\u5f31\u76d1\u7763\u548c\u8fdc\u8ddd\u79bb\u76d1\u7763\u95ee\u9898\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u533b\u5b66\u8bed\u97f3\u5206\u6790\u4e2d\u7684\u5f31\u76d1\u7763\u5b66\u4e60\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u65b9\u6cd5\uff0c\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u8bed\u97f3\u7279\u5f81\u7684\u5c42\u6b21\u7ed3\u6784\uff0c\u5728\u6570\u636e\u7a00\u7f3a\u60c5\u51b5\u4e0b\u5b9e\u73b0\u9ad8\u6548\u5b66\u4e60\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2601.04876", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2601.04876", "abs": "https://arxiv.org/abs/2601.04876", "authors": ["Kaiwen Luo", "Liang Lin", "Yibo Zhang", "Moayad Aloqaily", "Dexian Wang", "Zhenhong Zhou", "Junwei Zhang", "Kun Wang", "Li Sun", "Qingsong Wen"], "title": "ChronosAudio: A Comprehensive Long-Audio Benchmark for Evaluating Audio-Large Language Models", "comment": null, "summary": "Although Audio Large Language Models (ALLMs) have witnessed substantial advancements, their long audio understanding capabilities remain unexplored. A plethora of benchmarks have been proposed for general audio tasks, they predominantly focus on short-form clips, leaving without a consensus on evaluating ALLMs over extended durations. This paper proposes ChronosAudio, the first multi-task benchmark tailored for long-audio understanding in ALLMs. It encompasses six major task categories and comprises 36,000 test instances totaling over 200 hours audio, stratified into short, middle, and long-form categories to comprehensively evaluate length generalization. Extensive experiments on 16 state-of-the-art models using ChronosAudio yield three critical findings: 1.Precipitous Long-Context Collapse: ALLMs exhibit a severe inability to sustain performance, with the transition from short to long contexts triggering a staggering performance degradation of over 90% in specific tasks. 2.Structural Attention Dilution: Performance degradation stems from a fundamental failure in maintaining temporal locality; attention mechanisms suffer from significant diffusion in later sequences. 3.Restorative Ceiling of Mitigation: Current strategies only offer 50% recovery. These findings reveal significant challenges in long-audio, underscoring the urgent need for approaches to achieve robust, document-level audio reasoning.", "AI": {"tldr": "ChronosAudio\u662f\u9996\u4e2a\u9488\u5bf9\u97f3\u9891\u5927\u8bed\u8a00\u6a21\u578b\u7684\u957f\u97f3\u9891\u7406\u89e3\u591a\u4efb\u52a1\u57fa\u51c6\uff0c\u5305\u542b6\u5927\u7c7b\u4efb\u52a1\u30013.6\u4e07\u4e2a\u6d4b\u8bd5\u5b9e\u4f8b\u3001\u8d85\u8fc7200\u5c0f\u65f6\u97f3\u9891\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u5728\u957f\u97f3\u9891\u7406\u89e3\u4e0a\u7684\u4e25\u91cd\u5c40\u9650\u6027\u3002", "motivation": "\u5f53\u524d\u97f3\u9891\u5927\u8bed\u8a00\u6a21\u578b\u5728\u957f\u97f3\u9891\u7406\u89e3\u80fd\u529b\u65b9\u9762\u5c1a\u672a\u88ab\u5145\u5206\u63a2\u7d22\uff0c\u73b0\u6709\u57fa\u51c6\u4e3b\u8981\u5173\u6ce8\u77ed\u97f3\u9891\u7247\u6bb5\uff0c\u7f3a\u4e4f\u8bc4\u4f30\u6a21\u578b\u5728\u957f\u97f3\u9891\u4e0a\u8868\u73b0\u7684\u6807\u51c6\u65b9\u6cd5\u3002", "method": "\u63d0\u51faChronosAudio\u57fa\u51c6\uff0c\u5305\u542b6\u4e2a\u4e3b\u8981\u4efb\u52a1\u7c7b\u522b\uff0c\u97f3\u9891\u65f6\u957f\u5206\u4e3a\u77ed\u3001\u4e2d\u3001\u957f\u4e09\u7c7b\uff0c\u51713.6\u4e07\u4e2a\u6d4b\u8bd5\u5b9e\u4f8b\uff0c\u603b\u8ba1\u8d85\u8fc7200\u5c0f\u65f6\u97f3\u9891\uff0c\u7528\u4e8e\u5168\u9762\u8bc4\u4f30\u957f\u5ea6\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u5bf916\u4e2a\u6700\u5148\u8fdb\u6a21\u578b\u7684\u5b9e\u9a8c\u53d1\u73b0\uff1a1) \u957f\u4e0a\u4e0b\u6587\u5d29\u6e83\u73b0\u8c61\uff1a\u4ece\u77ed\u5230\u957f\u4e0a\u4e0b\u6587\u8f6c\u6362\u65f6\u6027\u80fd\u4e0b\u964d\u8d85\u8fc790%\uff1b2) \u7ed3\u6784\u6ce8\u610f\u529b\u7a00\u91ca\uff1a\u6ce8\u610f\u529b\u673a\u5236\u5728\u540e\u7eed\u5e8f\u5217\u4e2d\u663e\u8457\u6269\u6563\uff1b3) \u7f13\u89e3\u7b56\u7565\u6062\u590d\u4e0a\u9650\uff1a\u5f53\u524d\u65b9\u6cd5\u53ea\u80fd\u6062\u590d50%\u6027\u80fd\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u97f3\u9891\u5927\u8bed\u8a00\u6a21\u578b\u5728\u957f\u97f3\u9891\u7406\u89e3\u4e0a\u7684\u91cd\u5927\u6311\u6218\uff0c\u8feb\u5207\u9700\u8981\u5f00\u53d1\u80fd\u591f\u5b9e\u73b0\u7a33\u5065\u6587\u6863\u7ea7\u97f3\u9891\u63a8\u7406\u7684\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2601.05011", "categories": ["cs.SD", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.05011", "abs": "https://arxiv.org/abs/2601.05011", "authors": ["Karim El Khoury", "Maxime Zanella", "Tiffanie Godelaine", "Christophe De Vleeschouwer", "Benoit Macq"], "title": "Leveraging Prediction Entropy for Automatic Prompt Weighting in Zero-Shot Audio-Language Classification", "comment": null, "summary": "Audio-language models have recently demonstrated strong zero-shot capabilities by leveraging natural-language supervision to classify audio events without labeled training data. Yet, their performance is highly sensitive to the wording of text prompts, with small variations leading to large fluctuations in accuracy. Prior work has mitigated this issue through prompt learning or prompt ensembling. However, these strategies either require annotated data or fail to account for the fact that some prompts may negatively impact performance. In this work, we present an entropy-guided prompt weighting approach that aims to find a robust combination of prompt contributions to maximize prediction confidence. To this end, we formulate a tailored objective function that minimizes prediction entropy to yield new prompt weights, utilizing low-entropy as a proxy for high confidence. Our approach can be applied to individual samples or a batch of audio samples, requiring no additional labels and incurring negligible computational overhead. Experiments on five audio classification datasets covering environmental, urban, and vocal sounds, demonstrate consistent gains compared to classical prompt ensembling methods in a zero-shot setting, with accuracy improvements 5-times larger across the whole benchmark.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u71b5\u5f15\u5bfc\u7684\u63d0\u793a\u8bcd\u52a0\u6743\u65b9\u6cd5\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u9884\u6d4b\u71b5\u6765\u4f18\u5316\u63d0\u793a\u8bcd\u7ec4\u5408\uff0c\u5728\u96f6\u6837\u672c\u97f3\u9891\u5206\u7c7b\u4e2d\u663e\u8457\u63d0\u5347\u6027\u80fd", "motivation": "\u97f3\u9891-\u8bed\u8a00\u6a21\u578b\u5728\u96f6\u6837\u672c\u5206\u7c7b\u4e2d\u6027\u80fd\u5bf9\u6587\u672c\u63d0\u793a\u8bcd\u7684\u63aa\u8f9e\u9ad8\u5ea6\u654f\u611f\uff0c\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u9700\u8981\u6807\u6ce8\u6570\u636e\uff0c\u8981\u4e48\u65e0\u6cd5\u5904\u7406\u67d0\u4e9b\u63d0\u793a\u8bcd\u53ef\u80fd\u5bf9\u6027\u80fd\u4ea7\u751f\u8d1f\u9762\u5f71\u54cd\u7684\u95ee\u9898", "method": "\u63d0\u51fa\u71b5\u5f15\u5bfc\u7684\u63d0\u793a\u8bcd\u52a0\u6743\u65b9\u6cd5\uff0c\u8bbe\u8ba1\u4e13\u95e8\u7684\u76ee\u6807\u51fd\u6570\u6700\u5c0f\u5316\u9884\u6d4b\u71b5\u6765\u83b7\u5f97\u65b0\u7684\u63d0\u793a\u8bcd\u6743\u91cd\uff0c\u5c06\u4f4e\u71b5\u4f5c\u4e3a\u9ad8\u7f6e\u4fe1\u5ea6\u7684\u4ee3\u7406\u6307\u6807", "result": "\u5728\u6db5\u76d6\u73af\u5883\u3001\u57ce\u5e02\u548c\u58f0\u97f3\u7684\u4e94\u79cd\u97f3\u9891\u5206\u7c7b\u6570\u636e\u96c6\u4e0a\uff0c\u76f8\u6bd4\u4f20\u7edf\u63d0\u793a\u8bcd\u96c6\u6210\u65b9\u6cd5\u83b7\u5f97\u4e00\u81f4\u63d0\u5347\uff0c\u5728\u6574\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u51c6\u786e\u7387\u63d0\u5347\u8fbe5\u500d", "conclusion": "\u8be5\u65b9\u6cd5\u65e0\u9700\u989d\u5916\u6807\u7b7e\u4e14\u8ba1\u7b97\u5f00\u9500\u53ef\u5ffd\u7565\uff0c\u80fd\u6709\u6548\u63d0\u5347\u97f3\u9891-\u8bed\u8a00\u6a21\u578b\u5728\u96f6\u6837\u672c\u5206\u7c7b\u4e2d\u7684\u9c81\u68d2\u6027\u548c\u6027\u80fd"}}

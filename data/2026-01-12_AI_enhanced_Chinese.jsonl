{"id": "2601.05329", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.05329", "abs": "https://arxiv.org/abs/2601.05329", "authors": ["Junyang Chen", "Yuhang Jia", "Hui Wang", "Jiaming Zhou", "Yaxin Han", "Mengying Feng", "Yong Qin"], "title": "CosyEdit: Unlocking End-to-End Speech Editing Capability from Zero-Shot Text-to-Speech Models", "comment": null, "summary": "Automatic speech editing aims to modify spoken content based on textual instructions, yet traditional cascade systems suffer from complex preprocessing pipelines and a reliance on explicit external temporal alignment. Addressing these limitations, we propose CosyEdit, an end-to-end speech editing model adapted from CosyVoice through task-specific fine-tuning and an optimized inference procedure, which internalizes speech-text alignment while ensuring high consistency between the speech before and after editing. By fine-tuning on only 250 hours of supervised data from our curated GigaEdit dataset, our 400M-parameter model achieves reliable speech editing performance. Experiments on the RealEdit benchmark indicate that CosyEdit not only outperforms several billion-parameter language model baselines but also matches the performance of state-of-the-art cascade approaches. These results demonstrate that, with task-specific fine-tuning and inference optimization, robust and efficient speech editing capabilities can be unlocked from a zero-shot TTS model, yielding a novel and cost-effective end-to-end solution for high-quality speech editing.", "AI": {"tldr": "CosyEdit\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u8bed\u97f3\u7f16\u8f91\u6a21\u578b\uff0c\u901a\u8fc7\u4efb\u52a1\u7279\u5b9a\u5fae\u8c03\u548c\u4f18\u5316\u63a8\u7406\uff0c\u5728\u4ec5250\u5c0f\u65f6\u76d1\u7763\u6570\u636e\u4e0a\u8bad\u7ec3\uff0c\u6027\u80fd\u4f18\u4e8e\u6570\u5341\u4ebf\u53c2\u6570\u57fa\u7ebf\uff0c\u5339\u914d\u6700\u5148\u8fdb\u7684\u7ea7\u8054\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u7ea7\u8054\u8bed\u97f3\u7f16\u8f91\u7cfb\u7edf\u5b58\u5728\u9884\u5904\u7406\u6d41\u7a0b\u590d\u6742\u3001\u4f9d\u8d56\u663e\u5f0f\u5916\u90e8\u65f6\u95f4\u5bf9\u9f50\u7684\u95ee\u9898\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u3001\u7aef\u5230\u7aef\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u57fa\u4e8eCosyVoice\u6a21\u578b\uff0c\u901a\u8fc7\u4efb\u52a1\u7279\u5b9a\u5fae\u8c03\u548c\u4f18\u5316\u63a8\u7406\u7a0b\u5e8f\uff0c\u5185\u90e8\u5316\u8bed\u97f3-\u6587\u672c\u5bf9\u9f50\uff0c\u786e\u4fdd\u7f16\u8f91\u524d\u540e\u8bed\u97f3\u4e00\u81f4\u6027\u3002\u4f7f\u7528\u81ea\u5efa\u7684GigaEdit\u6570\u636e\u96c6\uff08250\u5c0f\u65f6\u76d1\u7763\u6570\u636e\uff09\u8fdb\u884c\u5fae\u8c03\u3002", "result": "\u5728RealEdit\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c400M\u53c2\u6570\u7684CosyEdit\u4e0d\u4ec5\u4f18\u4e8e\u591a\u4e2a\u6570\u5341\u4ebf\u53c2\u6570\u7684\u8bed\u8a00\u6a21\u578b\u57fa\u7ebf\uff0c\u8fd8\u5339\u914d\u4e86\u6700\u5148\u8fdb\u7684\u7ea7\u8054\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u4efb\u52a1\u7279\u5b9a\u5fae\u8c03\u548c\u63a8\u7406\u4f18\u5316\uff0c\u53ef\u4ee5\u4ece\u96f6\u6837\u672cTTS\u6a21\u578b\u4e2d\u89e3\u9501\u5f3a\u5927\u4e14\u9ad8\u6548\u7684\u8bed\u97f3\u7f16\u8f91\u80fd\u529b\uff0c\u4e3a\u9ad8\u8d28\u91cf\u8bed\u97f3\u7f16\u8f91\u63d0\u4f9b\u65b0\u9896\u4e14\u7ecf\u6d4e\u9ad8\u6548\u7684\u7aef\u5230\u7aef\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.05554", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.05554", "abs": "https://arxiv.org/abs/2601.05554", "authors": ["Chanhee Cho", "Nayeon Kim", "Bugeun Kim"], "title": "SPAM: Style Prompt Adherence Metric for Prompt-based TTS", "comment": null, "summary": "Prompt-based text-to-speech (TTS) aims to generate speech that adheres to fine-grained style cues provided in a text prompt. However, most prior works depend on neither plausible nor faithful measures to evaluate prompt adherence. That is, they cannot ensure whether the evaluation is grounded on the prompt and is similar to a human. Thus, we present a new automatic metric, the Style Prompt Adherence Metric, which explicitly satisfies both plausibility and faithfulness. Inspired by the CLAP, our approach factorizes speech into acoustic attributes and aligns them with the style prompt. Also, we trained the scorer with a supervised contrastive loss, which could provide a clearer distinction between different semantics. We conducted two experiments on two perspectives. The plausibility experiment showed that SPAM achieved a strong correlation with the mean opinion score (MOS). Also, the faithfulness experiment demonstrated that SPAM is successfully grounded to the given style prompt, as it can discriminate different semantics of the prompt. We believe that SPAM can provide a viable automatic solution for evaluating style prompt adherence of synthesized speech.", "AI": {"tldr": "\u63d0\u51faSPAM\u81ea\u52a8\u5ea6\u91cf\u6307\u6807\uff0c\u7528\u4e8e\u8bc4\u4f30\u6587\u672c\u5230\u8bed\u97f3\u5408\u6210\u4e2d\u98ce\u683c\u63d0\u793a\u7684\u5fe0\u5b9e\u5ea6\uff0c\u540c\u65f6\u6ee1\u8db3\u5408\u7406\u6027\u548c\u5fe0\u5b9e\u6027\u8981\u6c42\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u63d0\u793a\u7684\u6587\u672c\u5230\u8bed\u97f3\u7cfb\u7edf\u7f3a\u4e4f\u65e2\u5408\u7406\u53c8\u5fe0\u5b9e\u7684\u8bc4\u4f30\u6307\u6807\uff0c\u65e0\u6cd5\u786e\u4fdd\u8bc4\u4f30\u57fa\u4e8e\u63d0\u793a\u4e14\u7b26\u5408\u4eba\u7c7b\u5224\u65ad\u3002", "method": "\u53d7CLAP\u542f\u53d1\uff0c\u5c06\u8bed\u97f3\u5206\u89e3\u4e3a\u58f0\u5b66\u5c5e\u6027\u5e76\u4e0e\u98ce\u683c\u63d0\u793a\u5bf9\u9f50\uff1b\u4f7f\u7528\u76d1\u7763\u5bf9\u6bd4\u635f\u5931\u8bad\u7ec3\u8bc4\u5206\u5668\uff0c\u4ee5\u66f4\u597d\u533a\u5206\u4e0d\u540c\u8bed\u4e49\u3002", "result": "\u5408\u7406\u6027\u5b9e\u9a8c\u663e\u793aSPAM\u4e0e\u5e73\u5747\u610f\u89c1\u5f97\u5206\u6709\u5f3a\u76f8\u5173\u6027\uff1b\u5fe0\u5b9e\u6027\u5b9e\u9a8c\u8868\u660eSPAM\u80fd\u6210\u529f\u57fa\u4e8e\u7ed9\u5b9a\u98ce\u683c\u63d0\u793a\u8fdb\u884c\u533a\u5206\uff0c\u6709\u6548\u8fa8\u522b\u4e0d\u540c\u8bed\u4e49\u3002", "conclusion": "SPAM\u4e3a\u8bc4\u4f30\u5408\u6210\u8bed\u97f3\u7684\u98ce\u683c\u63d0\u793a\u5fe0\u5b9e\u5ea6\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u81ea\u52a8\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.05564", "categories": ["cs.SD", "cs.CL", "cs.HC", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.05564", "abs": "https://arxiv.org/abs/2601.05564", "authors": ["Zhixian Zhao", "Shuiyuan Wang", "Guojian Li", "Hongfei Xue", "Chengyou Wang", "Shuai Wang", "Longshuai Xiao", "Zihan Zhang", "Hui Bu", "Xin Xu", "Xinsheng Wang", "Hexin Liu", "Eng Siong Chng", "Hung-yi Lee", "Haizhou Li", "Lei Xie"], "title": "The ICASSP 2026 HumDial Challenge: Benchmarking Human-like Spoken Dialogue Systems in the LLM Era", "comment": "Official summary paper for the ICASSP 2026 HumDial Challenge", "summary": "Driven by the rapid advancement of Large Language Models (LLMs), particularly Audio-LLMs and Omni-models, spoken dialogue systems have evolved significantly, progressively narrowing the gap between human-machine and human-human interactions. Achieving truly ``human-like'' communication necessitates a dual capability: emotional intelligence to perceive and resonate with users' emotional states, and robust interaction mechanisms to navigate the dynamic, natural flow of conversation, such as real-time turn-taking. Therefore, we launched the first Human-like Spoken Dialogue Systems Challenge (HumDial) at ICASSP 2026 to benchmark these dual capabilities. Anchored by a sizable dataset derived from authentic human conversations, this initiative establishes a fair evaluation platform across two tracks: (1) Emotional Intelligence, targeting long-term emotion understanding and empathetic generation; and (2) Full-Duplex Interaction, systematically evaluating real-time decision-making under `` listening-while-speaking'' conditions. This paper summarizes the dataset, track configurations, and the final results.", "AI": {"tldr": "ICASSP 2026\u9996\u6b21\u4e3e\u529e\u4eba\u7c7b\u5316\u8bed\u97f3\u5bf9\u8bdd\u7cfb\u7edf\u6311\u6218\u8d5b(HumDial)\uff0c\u65e8\u5728\u8bc4\u4f30\u8bed\u97f3\u5bf9\u8bdd\u7cfb\u7edf\u5728\u60c5\u611f\u667a\u80fd\u548c\u5168\u53cc\u5de5\u4ea4\u4e92\u4e24\u4e2a\u5173\u952e\u7ef4\u5ea6\u7684\u8868\u73b0\uff0c\u57fa\u4e8e\u771f\u5b9e\u4eba\u7c7b\u5bf9\u8bdd\u6570\u636e\u96c6\u5efa\u7acb\u516c\u5e73\u8bc4\u4f30\u5e73\u53f0\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u7279\u522b\u662f\u97f3\u9891LLM\u548c\u5168\u80fd\u6a21\u578b\u7684\u53d1\u5c55\uff0c\u8bed\u97f3\u5bf9\u8bdd\u7cfb\u7edf\u5df2\u663e\u8457\u8fdb\u6b65\uff0c\u4f46\u8981\u5b9e\u73b0\u771f\u6b63\"\u7c7b\u4eba\"\u7684\u4ea4\u6d41\uff0c\u9700\u8981\u53cc\u91cd\u80fd\u529b\uff1a\u60c5\u611f\u667a\u80fd\uff08\u611f\u77e5\u548c\u5171\u9e23\u7528\u6237\u60c5\u611f\u72b6\u6001\uff09\u548c\u5f3a\u5927\u7684\u4ea4\u4e92\u673a\u5236\uff08\u5904\u7406\u52a8\u6001\u81ea\u7136\u7684\u5bf9\u8bdd\u6d41\uff0c\u5982\u5b9e\u65f6\u8bdd\u8f6e\u8f6c\u6362\uff09\u3002", "method": "\u57fa\u4e8e\u771f\u5b9e\u4eba\u7c7b\u5bf9\u8bdd\u6784\u5efa\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u8bbe\u7acb\u4e24\u4e2a\u8bc4\u4f30\u8d5b\u9053\uff1a1) \u60c5\u611f\u667a\u80fd\u8d5b\u9053\uff1a\u4e13\u6ce8\u4e8e\u957f\u671f\u60c5\u611f\u7406\u89e3\u548c\u5171\u60c5\u751f\u6210\uff1b2) \u5168\u53cc\u5de5\u4ea4\u4e92\u8d5b\u9053\uff1a\u7cfb\u7edf\u8bc4\u4f30\"\u8fb9\u542c\u8fb9\u8bf4\"\u6761\u4ef6\u4e0b\u7684\u5b9e\u65f6\u51b3\u7b56\u80fd\u529b\u3002", "result": "\u8bba\u6587\u603b\u7ed3\u4e86\u6570\u636e\u96c6\u3001\u8d5b\u9053\u914d\u7f6e\u548c\u6700\u7ec8\u7ed3\u679c\uff0c\u4e3aICASSP 2026\u9996\u5c4a\u4eba\u7c7b\u5316\u8bed\u97f3\u5bf9\u8bdd\u7cfb\u7edf\u6311\u6218\u8d5b\u5efa\u7acb\u4e86\u5b8c\u6574\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "conclusion": "HumDial\u6311\u6218\u8d5b\u4e3a\u8bc4\u4f30\u8bed\u97f3\u5bf9\u8bdd\u7cfb\u7edf\u7684\u7c7b\u4eba\u4ea4\u6d41\u80fd\u529b\u63d0\u4f9b\u4e86\u9996\u4e2a\u7efc\u5408\u6027\u57fa\u51c6\uff0c\u91cd\u70b9\u5173\u6ce8\u60c5\u611f\u667a\u80fd\u548c\u5168\u53cc\u5de5\u4ea4\u4e92\u8fd9\u4e24\u4e2a\u5173\u952e\u7ef4\u5ea6\uff0c\u63a8\u52a8\u4e86\u8bed\u97f3\u5bf9\u8bdd\u7cfb\u7edf\u5411\u66f4\u81ea\u7136\u3001\u66f4\u4eba\u6027\u5316\u7684\u65b9\u5411\u53d1\u5c55\u3002"}}
{"id": "2601.06006", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2601.06006", "abs": "https://arxiv.org/abs/2601.06006", "authors": ["Bang Zeng", "Beilong Tang", "Wang Xiang", "Ming Li"], "title": "Discriminative-Generative Target Speaker Extraction with Decoder-Only Language Models", "comment": "16 pages,6 figures", "summary": "Target speaker extraction (TSE) aims to recover the speech signal of a desired speaker from a mixed audio recording, given a short enrollment utterance. Most existing TSE approaches are based on discriminative modeling paradigms. Although effective at suppressing interfering speakers, these methods often struggle to produce speech with high perceptual quality and naturalness. To address this limitation, we first propose LauraTSE, a generative TSE model built upon an auto-regressive decoder-only language model. However, purely generative approaches may suffer from hallucinations, content drift, and limited controllability, which may undermine their reliability in complex acoustic scenarios. To overcome these challenges, we further introduce a discriminative-generative TSE framework. In this framework, a discriminative front-end is employed to robustly extract the target speaker's speech, yielding stable and controllable intermediate representations. A generative back-end then operates in the neural audio codec representation space to reconstruct fine-grained speech details and enhance perceptual quality. This two-stage design effectively combines the robustness and controllability of discriminative models with the superior naturalness and quality enhancement capabilities of generative models. Moreover, we systematically investigate collaborative training strategies for the proposed framework, including freezing or fine-tuning the front-end, incorporating an auxiliary SI-SDR loss, and exploring both auto-regressive and non-auto-regressive inference mechanisms. Experimental results demonstrate that the proposed framework achieves a more favorable trade-off among speech quality, intelligibility, and speaker consistency.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7ed3\u5408\u5224\u522b\u5f0f\u548c\u751f\u6210\u5f0f\u6a21\u578b\u7684TSE\u6846\u67b6\uff0c\u524d\u7aef\u5224\u522b\u6a21\u578b\u63d0\u53d6\u76ee\u6807\u8bed\u97f3\uff0c\u540e\u7aef\u751f\u6210\u6a21\u578b\u589e\u5f3a\u97f3\u8d28\uff0c\u5b9e\u73b0\u8d28\u91cf\u3001\u6e05\u6670\u5ea6\u548c\u8bf4\u8bdd\u4eba\u4e00\u81f4\u6027\u7684\u66f4\u597d\u5e73\u8861\u3002", "motivation": "\u73b0\u6709TSE\u65b9\u6cd5\u4e3b\u8981\u57fa\u4e8e\u5224\u522b\u5f0f\u6a21\u578b\uff0c\u867d\u7136\u80fd\u6709\u6548\u6291\u5236\u5e72\u6270\u8bf4\u8bdd\u4eba\uff0c\u4f46\u5728\u8bed\u97f3\u611f\u77e5\u8d28\u91cf\u548c\u81ea\u7136\u5ea6\u65b9\u9762\u8868\u73b0\u4e0d\u8db3\u3002\u7eaf\u751f\u6210\u5f0f\u65b9\u6cd5\u53c8\u5b58\u5728\u5e7b\u89c9\u3001\u5185\u5bb9\u6f02\u79fb\u548c\u53ef\u63a7\u6027\u5dee\u7b49\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u5224\u522b-\u751f\u6210TSE\u6846\u67b6\uff1a\u524d\u7aef\u5224\u522b\u6a21\u578b\u63d0\u53d6\u76ee\u6807\u8bf4\u8bdd\u4eba\u8bed\u97f3\uff0c\u751f\u6210\u7a33\u5b9a\u53ef\u63a7\u7684\u4e2d\u95f4\u8868\u793a\uff1b\u540e\u7aef\u751f\u6210\u6a21\u578b\u5728\u795e\u7ecf\u97f3\u9891\u7f16\u89e3\u7801\u8868\u793a\u7a7a\u95f4\u4e2d\u91cd\u6784\u7ec6\u7c92\u5ea6\u8bed\u97f3\u7ec6\u8282\u3002\u63a2\u7d22\u591a\u79cd\u534f\u4f5c\u8bad\u7ec3\u7b56\u7565\uff0c\u5305\u62ec\u524d\u7aef\u51bb\u7ed3/\u5fae\u8c03\u3001\u8f85\u52a9SI-SDR\u635f\u5931\u3001\u81ea\u56de\u5f52\u548c\u975e\u81ea\u56de\u5f52\u63a8\u7406\u673a\u5236\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u8bed\u97f3\u8d28\u91cf\u3001\u6e05\u6670\u5ea6\u548c\u8bf4\u8bdd\u4eba\u4e00\u81f4\u6027\u65b9\u9762\u5b9e\u73b0\u4e86\u66f4\u4f18\u7684\u6743\u8861\u3002", "conclusion": "\u5224\u522b-\u751f\u6210\u6846\u67b6\u6709\u6548\u7ed3\u5408\u4e86\u5224\u522b\u5f0f\u6a21\u578b\u7684\u9c81\u68d2\u6027\u3001\u53ef\u63a7\u6027\u548c\u751f\u6210\u5f0f\u6a21\u578b\u7684\u81ea\u7136\u5ea6\u3001\u8d28\u91cf\u589e\u5f3a\u80fd\u529b\uff0c\u4e3aTSE\u4efb\u52a1\u63d0\u4f9b\u4e86\u66f4\u4f18\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.05276", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.05276", "abs": "https://arxiv.org/abs/2601.05276", "authors": ["Nicholas R. Rasmussen", "Rodrigue Rizk", "Longwei Wang", "Arun Singh", "KC Santosh"], "title": "Channel Selected Stratified Nested Cross Validation for Clinically Relevant EEG Based Parkinsons Disease Detection", "comment": "Submitted to IEEE Conference -> posting to Arxiv as normal", "summary": "The early detection of Parkinsons disease remains a critical challenge in clinical neuroscience, with electroencephalography offering a noninvasive and scalable pathway toward population level screening. While machine learning has shown promise in this domain, many reported results suffer from methodological flaws, most notably patient level data leakage, inflating performance estimates and limiting clinical translation. To address these modeling pitfalls, we propose a unified evaluation framework grounded in nested cross validation and incorporating three complementary safeguards: (i) patient level stratification to eliminate subject overlap and ensure unbiased generalization, (ii) multi layered windowing to harmonize heterogeneous EEG recordings while preserving temporal dynamics, and (iii) inner loop channel selection to enable principled feature reduction without information leakage. Applied across three independent datasets with a heterogeneous number of channels, a convolutional neural network trained under this framework achieved 80.6% accuracy and demonstrated state of the art performance under held out population block testing, comparable to other methods in the literature. This performance underscores the necessity of nested cross validation as a safeguard against bias and as a principled means of selecting the most relevant information for patient level decisions, providing a reproducible foundation that can extend to other biomedical signal analysis domains.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8e\u5d4c\u5957\u4ea4\u53c9\u9a8c\u8bc1\u7684\u7edf\u4e00\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u5e15\u91d1\u68ee\u75c5\u65e9\u671f\u68c0\u6d4b\u7684EEG\u5206\u6790\uff0c\u901a\u8fc7\u60a3\u8005\u7ea7\u5206\u5c42\u3001\u591a\u5c42\u7a97\u53e3\u5316\u548c\u5185\u90e8\u5faa\u73af\u901a\u9053\u9009\u62e9\u4e09\u4e2a\u4fdd\u969c\u63aa\u65bd\u9632\u6b62\u6570\u636e\u6cc4\u9732\uff0c\u5728\u4e09\u4e2a\u72ec\u7acb\u6570\u636e\u96c6\u4e0a\u8fbe\u523080.6%\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u5e15\u91d1\u68ee\u75c5\u65e9\u671f\u68c0\u6d4b\u662f\u4e34\u5e8a\u795e\u7ecf\u79d1\u5b66\u7684\u5173\u952e\u6311\u6218\uff0cEEG\u63d0\u4f9b\u975e\u4fb5\u5165\u6027\u7b5b\u67e5\u9014\u5f84\u3002\u73b0\u6709\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5b58\u5728\u65b9\u6cd5\u5b66\u7f3a\u9677\uff0c\u7279\u522b\u662f\u60a3\u8005\u7ea7\u6570\u636e\u6cc4\u9732\u95ee\u9898\uff0c\u5bfc\u81f4\u6027\u80fd\u4f30\u8ba1\u81a8\u80c0\u5e76\u9650\u5236\u4e34\u5e8a\u8f6c\u5316\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u5d4c\u5957\u4ea4\u53c9\u9a8c\u8bc1\u7684\u7edf\u4e00\u8bc4\u4f30\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u4fdd\u969c\u63aa\u65bd\uff1a1) \u60a3\u8005\u7ea7\u5206\u5c42\u6d88\u9664\u53d7\u8bd5\u8005\u91cd\u53e0\uff1b2) \u591a\u5c42\u7a97\u53e3\u5316\u534f\u8c03\u5f02\u8d28EEG\u8bb0\u5f55\u540c\u65f6\u4fdd\u7559\u65f6\u95f4\u52a8\u6001\uff1b3) \u5185\u90e8\u5faa\u73af\u901a\u9053\u9009\u62e9\u5b9e\u73b0\u65e0\u4fe1\u606f\u6cc4\u9732\u7684\u7279\u5f81\u964d\u7ef4\u3002\u4f7f\u7528\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u5728\u8be5\u6846\u67b6\u4e0b\u8bad\u7ec3\u3002", "result": "\u5728\u4e09\u4e2a\u5177\u6709\u5f02\u8d28\u901a\u9053\u6570\u7684\u72ec\u7acb\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u6846\u67b6\u4e0b\u7684CNN\u8fbe\u523080.6%\u7684\u51c6\u786e\u7387\uff0c\u5728\u4fdd\u7559\u7fa4\u4f53\u5757\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u4e0e\u6587\u732e\u4e2d\u5176\u4ed6\u65b9\u6cd5\u76f8\u5f53\u3002", "conclusion": "\u5d4c\u5957\u4ea4\u53c9\u9a8c\u8bc1\u662f\u9632\u6b62\u504f\u5dee\u7684\u5fc5\u8981\u4fdd\u969c\uff0c\u4e5f\u662f\u9009\u62e9\u60a3\u8005\u7ea7\u51b3\u7b56\u6700\u76f8\u5173\u4fe1\u606f\u7684\u539f\u5219\u6027\u65b9\u6cd5\u3002\u8be5\u6846\u67b6\u4e3a\u751f\u7269\u533b\u5b66\u4fe1\u53f7\u5206\u6790\u9886\u57df\u63d0\u4f9b\u4e86\u53ef\u590d\u73b0\u7684\u57fa\u7840\uff0c\u53ef\u6269\u5c55\u5230\u5176\u4ed6\u9886\u57df\u3002"}}
{"id": "2601.05323", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.05323", "abs": "https://arxiv.org/abs/2601.05323", "authors": ["Mohammad Ali Vahedifar", "Qi Zhang"], "title": "Discrete Mode Decomposition Meets Shapley Value: Robust Signal Prediction in Tactile Internet", "comment": "This paper has been accepted at IEEE INFOCOM 2026", "summary": "Tactile Internet (TI) requires ultra-low latency and high reliability to ensure stability and transparency in touch-enabled teleoperation. However, variable delays and packet loss present significant challenges to maintaining immersive haptic communication. To address this, we propose a predictive framework that integrates Discrete Mode Decomposition (DMD) with Shapley Mode Value (SMV) for accurate and timely haptic signal prediction. DMD decomposes haptic signals into interpretable intrinsic modes, while SMV evaluates each mode's contribution to prediction accuracy, which is well-aligned with the goal-oriented semantic communication. Integrating SMV with DMD further accelerates inference, enabling efficient communication and smooth teleoperation even under adverse network conditions.\n  Extensive experiments show that DMD+SMV, combined with a Transformer architecture, outperforms baseline methods significantly. It achieves 98.9% accuracy for 1-sample prediction and 92.5% for 100-sample prediction, as well as extremely low inference latency: 0.056 ms and 2 ms, respectively. These results demonstrate that the proposed framework has strong potential to ease the stringent latency and reliability requirements of TI without compromising performance, highlighting its feasibility for real-world deployment in TI systems.", "AI": {"tldr": "\u63d0\u51fa\u7ed3\u5408\u79bb\u6563\u6a21\u5f0f\u5206\u89e3(DMD)\u548cShapley\u6a21\u5f0f\u503c(SMV)\u7684\u9884\u6d4b\u6846\u67b6\uff0c\u7528\u4e8e\u89e6\u89c9\u4e92\u8054\u7f51\u4e2d\u7684\u89e6\u89c9\u4fe1\u53f7\u9884\u6d4b\uff0c\u663e\u8457\u63d0\u5347\u9884\u6d4b\u7cbe\u5ea6\u5e76\u964d\u4f4e\u63a8\u7406\u5ef6\u8fdf\u3002", "motivation": "\u89e6\u89c9\u4e92\u8054\u7f51\u9700\u8981\u8d85\u4f4e\u5ef6\u8fdf\u548c\u9ad8\u53ef\u9760\u6027\uff0c\u4f46\u53ef\u53d8\u5ef6\u8fdf\u548c\u6570\u636e\u5305\u4e22\u5931\u7ed9\u6c89\u6d78\u5f0f\u89e6\u89c9\u901a\u4fe1\u5e26\u6765\u6311\u6218\uff0c\u9700\u8981\u51c6\u786e\u53ca\u65f6\u7684\u89e6\u89c9\u4fe1\u53f7\u9884\u6d4b\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u79bb\u6563\u6a21\u5f0f\u5206\u89e3(DMD)\u5c06\u89e6\u89c9\u4fe1\u53f7\u5206\u89e3\u4e3a\u53ef\u89e3\u91ca\u7684\u5185\u5728\u6a21\u5f0f\uff0c\u7ed3\u5408Shapley\u6a21\u5f0f\u503c(SMV)\u8bc4\u4f30\u5404\u6a21\u5f0f\u5bf9\u9884\u6d4b\u51c6\u786e\u6027\u7684\u8d21\u732e\uff0c\u5e76\u4e0eTransformer\u67b6\u6784\u7ed3\u5408\u3002", "result": "DMD+SMV\u6846\u67b6\u57281\u6837\u672c\u9884\u6d4b\u4e2d\u8fbe\u523098.9%\u51c6\u786e\u7387\uff0c100\u6837\u672c\u9884\u6d4b\u8fbe\u523092.5%\u51c6\u786e\u7387\uff0c\u63a8\u7406\u5ef6\u8fdf\u5206\u522b\u4e3a0.056ms\u548c2ms\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u591f\u7f13\u89e3\u89e6\u89c9\u4e92\u8054\u7f51\u5bf9\u5ef6\u8fdf\u548c\u53ef\u9760\u6027\u7684\u4e25\u683c\u8981\u6c42\uff0c\u540c\u65f6\u4fdd\u6301\u6027\u80fd\uff0c\u5728\u73b0\u5b9e\u89e6\u89c9\u4e92\u8054\u7f51\u7cfb\u7edf\u4e2d\u5177\u6709\u5b9e\u9645\u90e8\u7f72\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2601.05440", "categories": ["eess.SP", "cs.NI"], "pdf": "https://arxiv.org/pdf/2601.05440", "abs": "https://arxiv.org/abs/2601.05440", "authors": ["William Bjorndahl", "Mark O'Hair", "Ben Zoghi", "Joseph Camp"], "title": "SPARK: Sparse Parametric Antenna Representation using Kernels", "comment": "Accepted to IEEE INFOCOM 2026", "summary": "Channel state information (CSI) acquisition and feedback overhead grows with the number of antennas, users, and reported subbands. This growth becomes a bottleneck for many antenna and reconfigurable intelligent surface (RIS) systems as arrays and user densities scale. Practical CSI feedback and beam management rely on codebooks, where beams are selected via indices rather than explicitly transmitting radiation patterns. Hardware-aware operation requires an explicit representation of the measured antenna/RIS response, yet high-fidelity measured patterns are high-dimensional and costly to handle. We present SPARK (Sparse Parametric Antenna Representation using Kernels), a training-free compression model that decomposes patterns into a smooth global base and sparse localized lobes. For 3D patterns, SPARK uses low-order spherical harmonics for global directivity and anisotropic Gaussian kernels for localized features. For RIS 1D azimuth cuts, it uses a Fourier-series base with 1D Gaussians. On patterns from the AERPAW testbed and a public RIS dataset, SPARK achieves up to 2.8$\\times$ and 10.4$\\times$ reductions in reconstruction MSE over baselines, respectively. Simulation shows that amortizing a compact pattern description and reporting sparse path descriptors can produce 12.65% mean uplink goodput gain under a fixed uplink budget. Overall, SPARK turns dense patterns into compact, parametric models for scalable, hardware-aware beam management.", "AI": {"tldr": "SPARK\u662f\u4e00\u79cd\u514d\u8bad\u7ec3\u7684\u538b\u7f29\u6a21\u578b\uff0c\u5c06\u5929\u7ebf/RIS\u8f90\u5c04\u6a21\u5f0f\u5206\u89e3\u4e3a\u5e73\u6ed1\u7684\u5168\u5c40\u57fa\u5e95\u548c\u7a00\u758f\u7684\u5c40\u90e8\u6ce2\u74e3\uff0c\u5b9e\u73b0\u9ad8\u7ef4\u8f90\u5c04\u6a21\u5f0f\u7684\u7d27\u51d1\u53c2\u6570\u5316\u8868\u793a\uff0c\u7528\u4e8e\u53ef\u6269\u5c55\u7684\u6ce2\u675f\u7ba1\u7406\u3002", "motivation": "\u968f\u7740\u5929\u7ebf\u6570\u91cf\u3001\u7528\u6237\u6570\u548c\u62a5\u544a\u5b50\u5e26\u7684\u589e\u52a0\uff0cCSI\u83b7\u53d6\u548c\u53cd\u9988\u5f00\u9500\u6210\u4e3a\u74f6\u9888\u3002\u786c\u4ef6\u611f\u77e5\u64cd\u4f5c\u9700\u8981\u5929\u7ebf/RIS\u54cd\u5e94\u7684\u663e\u5f0f\u8868\u793a\uff0c\u4f46\u9ad8\u4fdd\u771f\u6d4b\u91cf\u6a21\u5f0f\u662f\u9ad8\u7ef4\u4e14\u5904\u7406\u6210\u672c\u9ad8\u7684\u3002", "method": "SPARK\u5c06\u8f90\u5c04\u6a21\u5f0f\u5206\u89e3\u4e3a\u5e73\u6ed1\u5168\u5c40\u57fa\u5e95\u548c\u7a00\u758f\u5c40\u90e8\u6ce2\u74e3\uff1a\u5bf9\u4e8e3D\u6a21\u5f0f\u4f7f\u7528\u4f4e\u9636\u7403\u8c10\u51fd\u6570\u4f5c\u4e3a\u5168\u5c40\u65b9\u5411\u6027\uff0c\u5404\u5411\u5f02\u6027\u9ad8\u65af\u6838\u4f5c\u4e3a\u5c40\u90e8\u7279\u5f81\uff1b\u5bf9\u4e8eRIS 1D\u65b9\u4f4d\u89d2\u5256\u9762\u4f7f\u7528\u5085\u91cc\u53f6\u7ea7\u6570\u57fa\u5e95\u548c1D\u9ad8\u65af\u51fd\u6570\u3002", "result": "\u5728AERPAW\u6d4b\u8bd5\u5e73\u53f0\u548c\u516c\u5171RIS\u6570\u636e\u96c6\u4e0a\uff0cSPARK\u5206\u522b\u5b9e\u73b0\u4e862.8\u500d\u548c10.4\u500d\u7684\u91cd\u5efaMSE\u964d\u4f4e\u3002\u4eff\u771f\u663e\u793a\uff0c\u5728\u56fa\u5b9a\u4e0a\u884c\u94fe\u8def\u9884\u7b97\u4e0b\uff0c\u7d27\u51d1\u6a21\u5f0f\u63cf\u8ff0\u548c\u7a00\u758f\u8def\u5f84\u63cf\u8ff0\u7b26\u53ef\u5e26\u676512.65%\u7684\u5e73\u5747\u4e0a\u884c\u94fe\u8def\u826f\u597d\u541e\u5410\u91cf\u589e\u76ca\u3002", "conclusion": "SPARK\u5c06\u5bc6\u96c6\u8f90\u5c04\u6a21\u5f0f\u8f6c\u6362\u4e3a\u7d27\u51d1\u7684\u53c2\u6570\u5316\u6a21\u578b\uff0c\u4e3a\u53ef\u6269\u5c55\u7684\u786c\u4ef6\u611f\u77e5\u6ce2\u675f\u7ba1\u7406\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.05676", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.05676", "abs": "https://arxiv.org/abs/2601.05676", "authors": ["Guangqi Shi", "Kimitaka Sumi", "Takuya Sakamoto"], "title": "Deformation-Aware Observation Modeling for Radar-Based Human Sensing via 3D Scan-Depth Sequence Fusion", "comment": "10 pages, 8 figures, and 5 tables. This work is going to be submitted to the IEEE for possible publication", "summary": "Non-contact radar-based human sensing is often interpreted using simplified motion assumptions. However, respiration induces non-rigid surface deformation of the human body that impacts electromagnetic wave scattering and can degrade the robustness of measurements. To address this, we propose a surface-deformation-aware observation model for radar-based human sensing that fuses static high-resolution three-dimensional scanner measurements with temporal depth camera data to represent time-varying human surface geometry. Non-rigid registration using the coherent point drift algorithm is employed to align a static template with dynamic depth frames. Frame-wise electromagnetic scattering is subsequently computed using the physical optics approximation, allowing the reconstruction of intermediate-frequency radar signals that emulate radar observations. Validation against experimental radar data demonstrated that the proposed model exhibited greater robustness than a depth-sequence-only model under low-signal-quality conditions involving complex surface dynamics and multiple reflective sites. For two participants, the proposed model achieved higher Pearson correlation coefficients of 0.943 and 0.887 between model-derived and experimentally measured displacement waveforms, compared with 0.868 and 0.796 for the depth-sequence-only model. Furthermore, in a favorable case characterized by a single relatively-stationary reflective site, the proposed method achieved a correlation coefficient of 0.789 between model-derived and experimentally measured in-phase-quadrature magnitude variations. These results suggest that our sensor-fusion-based deformation-aware observation modeling can realistically reproduce radar observations and provide physically grounded insights into the interpretation of radar measurement variations.", "AI": {"tldr": "\u63d0\u51fa\u878d\u5408\u9ad8\u5206\u8fa8\u73873D\u626b\u63cf\u4e0e\u6df1\u5ea6\u76f8\u673a\u6570\u636e\u7684\u8868\u9762\u5f62\u53d8\u611f\u77e5\u96f7\u8fbe\u89c2\u6d4b\u6a21\u578b\uff0c\u901a\u8fc7\u7269\u7406\u5149\u5b66\u8fd1\u4f3c\u8ba1\u7b97\u7535\u78c1\u6563\u5c04\uff0c\u63d0\u5347\u547c\u5438\u76d1\u6d4b\u7684\u9c81\u68d2\u6027", "motivation": "\u4f20\u7edf\u96f7\u8fbe\u4eba\u4f53\u4f20\u611f\u5e38\u57fa\u4e8e\u7b80\u5316\u8fd0\u52a8\u5047\u8bbe\uff0c\u4f46\u547c\u5438\u5f15\u8d77\u7684\u975e\u521a\u6027\u8868\u9762\u5f62\u53d8\u4f1a\u5f71\u54cd\u7535\u78c1\u6ce2\u6563\u5c04\uff0c\u964d\u4f4e\u6d4b\u91cf\u9c81\u68d2\u6027\uff0c\u9700\u8981\u66f4\u7cbe\u786e\u7684\u89c2\u6d4b\u6a21\u578b", "method": "\u878d\u5408\u9759\u6001\u9ad8\u5206\u8fa8\u73873D\u626b\u63cf\u4e0e\u52a8\u6001\u6df1\u5ea6\u76f8\u673a\u6570\u636e\uff0c\u4f7f\u7528\u76f8\u5e72\u70b9\u6f02\u79fb\u7b97\u6cd5\u8fdb\u884c\u975e\u521a\u6027\u914d\u51c6\uff0c\u91c7\u7528\u7269\u7406\u5149\u5b66\u8fd1\u4f3c\u8ba1\u7b97\u7535\u78c1\u6563\u5c04\uff0c\u91cd\u5efa\u4e2d\u9891\u96f7\u8fbe\u4fe1\u53f7", "result": "\u5728\u4f4e\u4fe1\u53f7\u8d28\u91cf\u6761\u4ef6\u4e0b\uff0c\u76f8\u6bd4\u4ec5\u4f7f\u7528\u6df1\u5ea6\u5e8f\u5217\u7684\u6a21\u578b\uff0c\u63d0\u51fa\u7684\u6a21\u578b\u8868\u73b0\u51fa\u66f4\u9ad8\u9c81\u68d2\u6027\uff0c\u4e24\u4f4d\u53c2\u4e0e\u8005\u7684\u4f4d\u79fb\u6ce2\u5f62\u76ae\u5c14\u900a\u76f8\u5173\u7cfb\u6570\u5206\u522b\u8fbe\u52300.943\u548c0.887", "conclusion": "\u4f20\u611f\u5668\u878d\u5408\u7684\u8868\u9762\u5f62\u53d8\u611f\u77e5\u89c2\u6d4b\u6a21\u578b\u80fd\u771f\u5b9e\u518d\u73b0\u96f7\u8fbe\u89c2\u6d4b\uff0c\u4e3a\u96f7\u8fbe\u6d4b\u91cf\u53d8\u5316\u89e3\u91ca\u63d0\u4f9b\u7269\u7406\u57fa\u7840\uff0c\u63d0\u5347\u975e\u63a5\u89e6\u4eba\u4f53\u4f20\u611f\u7684\u53ef\u9760\u6027"}}
{"id": "2601.05920", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.05920", "abs": "https://arxiv.org/abs/2601.05920", "authors": ["Meiwen Men", "Tao Zhou", "Kaifeng Bao", "Zhiyang Guo", "Yongning Qi", "Liu Liu", "Bo Ai"], "title": "A Novel Deep Learning-Based Coarse-to-Fine Frame Synchronization Method for OTFS Systems", "comment": null, "summary": "Orthogonal time frequency space (OTFS) modulation is a robust candidate waveform for future wireless systems, particularly in high-mobility scenarios, as it effectively mitigates the impact of rapidly time-varying channels by mapping symbols in the delay-Doppler (DD) domain. However, accurate frame synchronization in OTFS systems remains a challenge due to the performance limitations of conventional algorithms. To address this, we propose a low-complexity synchronization method based on a coarse-to-fine deep residual network (ResNet) architecture. Unlike traditional approaches relying on high-overhead preamble structures, our method exploits the intrinsic periodic features of OTFS pilots in the delay-time (DT) domain to formulate synchronization as a hierarchical classification problem. Specifically, the proposed architecture employs a two-stage strategy to first narrow the search space and then pinpoint the precise symbol timing offset (STO), thereby significantly reducing computational complexity while maintaining high estimation accuracy. We construct a comprehensive simulation dataset incorporating diverse channel models and randomized STO to validate the method. Extensive simulation results demonstrate that the proposed method achieves robust signal start detection and superior accuracy compared to conventional benchmarks, particularly in low signal-to-noise ratio (SNR) regimes and high-mobility scenarios.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u7c97\u5230\u7ec6\u6df1\u5ea6\u6b8b\u5dee\u7f51\u7edc\u7684OTFS\u7cfb\u7edf\u4f4e\u590d\u6742\u5ea6\u540c\u6b65\u65b9\u6cd5\uff0c\u5229\u7528\u5ef6\u8fdf\u65f6\u95f4\u57df\u4e2dOTFS\u5bfc\u9891\u7684\u5468\u671f\u6027\u7279\u5f81\uff0c\u5c06\u540c\u6b65\u95ee\u9898\u8f6c\u5316\u4e3a\u5c42\u6b21\u5206\u7c7b\u95ee\u9898\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u5e76\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u3002", "motivation": "OTFS\u8c03\u5236\u662f\u672a\u6765\u65e0\u7ebf\u7cfb\u7edf\uff08\u7279\u522b\u662f\u9ad8\u79fb\u52a8\u6027\u573a\u666f\uff09\u7684\u7a33\u5065\u5019\u9009\u6ce2\u5f62\uff0c\u80fd\u6709\u6548\u7f13\u89e3\u5feb\u901f\u65f6\u53d8\u4fe1\u9053\u7684\u5f71\u54cd\u3002\u7136\u800c\uff0c\u4f20\u7edf\u7b97\u6cd5\u6027\u80fd\u6709\u9650\uff0cOTFS\u7cfb\u7edf\u7684\u7cbe\u786e\u5e27\u540c\u6b65\u4ecd\u7136\u662f\u4e00\u4e2a\u6311\u6218\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u7c97\u5230\u7ec6\u6df1\u5ea6\u6b8b\u5dee\u7f51\u7edc\uff08ResNet\uff09\u67b6\u6784\u7684\u4f4e\u590d\u6742\u5ea6\u540c\u6b65\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u5229\u7528OTFS\u5bfc\u9891\u5728\u5ef6\u8fdf\u65f6\u95f4\uff08DT\uff09\u57df\u4e2d\u7684\u56fa\u6709\u5468\u671f\u6027\u7279\u5f81\uff0c\u5c06\u540c\u6b65\u95ee\u9898\u8868\u8ff0\u4e3a\u5c42\u6b21\u5206\u7c7b\u95ee\u9898\u3002\u91c7\u7528\u4e24\u9636\u6bb5\u7b56\u7565\uff1a\u9996\u5148\u7f29\u5c0f\u641c\u7d22\u7a7a\u95f4\uff0c\u7136\u540e\u7cbe\u786e\u5b9a\u4f4d\u7b26\u53f7\u5b9a\u65f6\u504f\u79fb\uff08STO\uff09\u3002", "result": "\u6784\u5efa\u4e86\u5305\u542b\u591a\u6837\u5316\u4fe1\u9053\u6a21\u578b\u548c\u968f\u673aSTO\u7684\u7efc\u5408\u4eff\u771f\u6570\u636e\u96c6\u8fdb\u884c\u9a8c\u8bc1\u3002\u5e7f\u6cdb\u7684\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u7a33\u5065\u7684\u4fe1\u53f7\u8d77\u59cb\u68c0\u6d4b\uff0c\u76f8\u6bd4\u4f20\u7edf\u57fa\u51c6\u65b9\u6cd5\u5177\u6709\u66f4\u9ad8\u7684\u7cbe\u5ea6\uff0c\u7279\u522b\u662f\u5728\u4f4e\u4fe1\u566a\u6bd4\uff08SNR\uff09\u573a\u666f\u548c\u9ad8\u79fb\u52a8\u6027\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u63d0\u51fa\u7684\u57fa\u4e8e\u6df1\u5ea6\u6b8b\u5dee\u7f51\u7edc\u7684\u540c\u6b65\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86OTFS\u7cfb\u7edf\u540c\u6b65\u96be\u9898\uff0c\u5728\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u7684\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u4f30\u8ba1\u7cbe\u5ea6\uff0c\u4e3a\u9ad8\u79fb\u52a8\u6027\u65e0\u7ebf\u901a\u4fe1\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u540c\u6b65\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.05923", "categories": ["eess.SP", "cs.AI", "cs.LG", "eess.IV", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2601.05923", "abs": "https://arxiv.org/abs/2601.05923", "authors": ["E. Middell", "L. Carlton", "S. Moradi", "T. Codina", "T. Fischer", "J. Cutler", "S. Kelley", "J. Behrendt", "T. Dissanayake", "N. Harmening", "M. A. Y\u00fccel", "D. A. Boas", "A. von L\u00fchmann"], "title": "Cedalion Tutorial: A Python-based framework for comprehensive analysis of multimodal fNIRS & DOT from the lab to the everyday world", "comment": "33 pages main manuscript, 180 pages Supplementary Tutorial Notebooks, 12 figures, 6 tables, under review in SPIE Neurophotonics", "summary": "Functional near-infrared spectroscopy (fNIRS) and diffuse optical tomography (DOT) are rapidly evolving toward wearable, multimodal, and data-driven, AI-supported neuroimaging in the everyday world. However, current analytical tools are fragmented across platforms, limiting reproducibility, interoperability, and integration with modern machine learning (ML) workflows. Cedalion is a Python-based open-source framework designed to unify advanced model-based and data-driven analysis of multimodal fNIRS and DOT data within a reproducible, extensible, and community-driven environment. Cedalion integrates forward modelling, photogrammetric optode co-registration, signal processing, GLM Analysis, DOT image reconstruction, and ML-based data-driven methods within a single standardized architecture based on the Python ecosystem. It adheres to SNIRF and BIDS standards, supports cloud-executable Jupyter notebooks, and provides containerized workflows for scalable, fully reproducible analysis pipelines that can be provided alongside original research publications. Cedalion connects established optical-neuroimaging pipelines with ML frameworks such as scikit-learn and PyTorch, enabling seamless multimodal fusion with EEG, MEG, and physiological data. It implements validated algorithms for signal-quality assessment, motion correction, GLM modelling, and DOT reconstruction, complemented by modules for simulation, data augmentation, and multimodal physiology analysis. Automated documentation links each method to its source publication, and continuous-integration testing ensures robustness. This tutorial paper provides seven fully executable notebooks that demonstrate core features. Cedalion offers an open, transparent, and community extensible foundation that supports reproducible, scalable, cloud- and ML-ready fNIRS/DOT workflows for laboratory-based and real-world neuroimaging.", "AI": {"tldr": "Cedalion\u662f\u4e00\u4e2a\u57fa\u4e8ePython\u7684\u5f00\u6e90\u6846\u67b6\uff0c\u7528\u4e8e\u7edf\u4e00fNIRS\u548cDOT\u6570\u636e\u7684\u6a21\u578b\u9a71\u52a8\u548c\u6570\u636e\u9a71\u52a8\u5206\u6790\uff0c\u652f\u6301\u53ef\u91cd\u590d\u3001\u53ef\u6269\u5c55\u7684\u795e\u7ecf\u5f71\u50cf\u5de5\u4f5c\u6d41\u3002", "motivation": "\u5f53\u524dfNIRS\u548cDOT\u5206\u6790\u5de5\u5177\u5206\u6563\u5728\u4e0d\u540c\u5e73\u53f0\uff0c\u9650\u5236\u4e86\u53ef\u91cd\u590d\u6027\u3001\u4e92\u64cd\u4f5c\u6027\u548c\u4e0e\u73b0\u4ee3\u673a\u5668\u5b66\u4e60\u5de5\u4f5c\u6d41\u7684\u96c6\u6210\uff0c\u9700\u8981\u4e00\u4e2a\u7edf\u4e00\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5f00\u53d1\u57fa\u4e8ePython\u7684\u5f00\u6e90\u6846\u67b6\uff0c\u96c6\u6210\u524d\u5411\u5efa\u6a21\u3001\u5149\u6781\u914d\u51c6\u3001\u4fe1\u53f7\u5904\u7406\u3001GLM\u5206\u6790\u3001DOT\u56fe\u50cf\u91cd\u5efa\u548c\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u9075\u5faaSNIRF\u548cBIDS\u6807\u51c6\uff0c\u652f\u6301\u5bb9\u5668\u5316\u5de5\u4f5c\u6d41\u3002", "result": "\u521b\u5efa\u4e86Cedalion\u6846\u67b6\uff0c\u63d0\u4f9b\u4e03\u4e2a\u53ef\u6267\u884c\u7b14\u8bb0\u672c\u6f14\u793a\u6838\u5fc3\u529f\u80fd\uff0c\u5b9e\u73b0\u4e86\u53ef\u91cd\u590d\u3001\u53ef\u6269\u5c55\u3001\u4e91\u5c31\u7eea\u548cML\u5c31\u7eea\u7684fNIRS/DOT\u5de5\u4f5c\u6d41\u3002", "conclusion": "Cedalion\u4e3a\u5b9e\u9a8c\u5ba4\u548c\u771f\u5b9e\u4e16\u754c\u795e\u7ecf\u5f71\u50cf\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f00\u653e\u3001\u900f\u660e\u3001\u793e\u533a\u53ef\u6269\u5c55\u7684\u57fa\u7840\uff0c\u652f\u6301\u53ef\u91cd\u590d\u3001\u53ef\u6269\u5c55\u7684fNIRS/DOT\u5206\u6790\u5de5\u4f5c\u6d41\u3002"}}
{"id": "2601.05998", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.05998", "abs": "https://arxiv.org/abs/2601.05998", "authors": ["Caroline Jane Spindel", "Edward Knightly"], "title": "Curving Beam Reflections: Model and Experimental Validation", "comment": "Accepted to IEEE INFOCOM 2026", "summary": "Curving beams are a promising new method for bypassing obstacles in future millimeter-wave to sub-terahertz (sub-THz) networks but lack a general predictive model for their reflections from arbitrary surfaces. We show that, unfortunately, attempting to \"mirror\" the incident beam trajectory across the normal of the reflector, as in ray optics, fails in general. Thus, we introduce the first geometric framework capable of modeling the reflections of arbitrary convex sub-THz curving beams from general reflectors with experimental verification. Rather than \"mirroring\" the trajectory, we decompose the beam into a family of tangents and demonstrate that this process is equivalent to the Legendre transform. This approach allows us to accurately account for reflectors of any shape, size, and position while preserving the underlying physics of wave propagation. Our model is validated through finite element method simulations and over-the-air experiments, demonstrating millimeter-scale accuracy in predicting reflections. Our model provides a foundation for future curving beam communication and sensing systems, enabling the design of reflected curved links and curving radar paths.", "AI": {"tldr": "\u63d0\u51fa\u9996\u4e2a\u51e0\u4f55\u6846\u67b6\uff0c\u7528\u4e8e\u5efa\u6a21\u4efb\u610f\u51f8\u5f62\u4e9a\u592a\u8d6b\u5179\u5f2f\u66f2\u5149\u675f\u5728\u4efb\u610f\u53cd\u5c04\u9762\u4e0a\u7684\u53cd\u5c04\uff0c\u901a\u8fc7\u52d2\u8ba9\u5fb7\u53d8\u6362\u800c\u975e\u4f20\u7edf\u5149\u7ebf\u955c\u50cf\u65b9\u6cd5\uff0c\u5b9e\u73b0\u6beb\u7c73\u7ea7\u7cbe\u5ea6\u7684\u9884\u6d4b\u3002", "motivation": "\u5f2f\u66f2\u5149\u675f\u662f\u672a\u6765\u6beb\u7c73\u6ce2\u5230\u4e9a\u592a\u8d6b\u5179\u7f51\u7edc\u4e2d\u7ed5\u8fc7\u969c\u788d\u7269\u7684\u6709\u524d\u666f\u65b9\u6cd5\uff0c\u4f46\u7f3a\u4e4f\u901a\u7528\u7684\u9884\u6d4b\u6a21\u578b\u6765\u63cf\u8ff0\u5176\u5728\u4efb\u610f\u8868\u9762\u4e0a\u7684\u53cd\u5c04\u884c\u4e3a\u3002\u4f20\u7edf\u7684\u5149\u7ebf\u5149\u5b66\"\u955c\u50cf\"\u65b9\u6cd5\u5728\u4e00\u822c\u60c5\u51b5\u4e0b\u4f1a\u5931\u6548\u3002", "method": "\u5c06\u5149\u675f\u5206\u89e3\u4e3a\u4e00\u7cfb\u5217\u5207\u7ebf\u65cf\uff0c\u8bc1\u660e\u8be5\u8fc7\u7a0b\u7b49\u4ef7\u4e8e\u52d2\u8ba9\u5fb7\u53d8\u6362\u3002\u8fd9\u79cd\u65b9\u6cd5\u80fd\u591f\u51c6\u786e\u8003\u8651\u4efb\u610f\u5f62\u72b6\u3001\u5927\u5c0f\u548c\u4f4d\u7f6e\u7684\u53cd\u5c04\u9762\uff0c\u540c\u65f6\u4fdd\u7559\u6ce2\u4f20\u64ad\u7684\u5e95\u5c42\u7269\u7406\u7279\u6027\u3002\u901a\u8fc7\u6709\u9650\u5143\u65b9\u6cd5\u6a21\u62df\u548c\u7a7a\u4e2d\u5b9e\u9a8c\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u6a21\u578b\u9a8c\u8bc1\u663e\u793a\u5728\u9884\u6d4b\u53cd\u5c04\u65b9\u9762\u8fbe\u5230\u6beb\u7c73\u7ea7\u7cbe\u5ea6\uff0c\u4e3a\u672a\u6765\u5f2f\u66f2\u5149\u675f\u901a\u4fe1\u548c\u4f20\u611f\u7cfb\u7edf\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u80fd\u591f\u8bbe\u8ba1\u53cd\u5c04\u5f2f\u66f2\u94fe\u8def\u548c\u5f2f\u66f2\u96f7\u8fbe\u8def\u5f84\u3002", "conclusion": "\u63d0\u51fa\u7684\u51e0\u4f55\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u5f2f\u66f2\u5149\u675f\u5728\u4efb\u610f\u53cd\u5c04\u9762\u4e0a\u53cd\u5c04\u7684\u5efa\u6a21\u95ee\u9898\uff0c\u4e3a\u4e9a\u592a\u8d6b\u5179\u9891\u6bb5\u7684\u5f2f\u66f2\u5149\u675f\u5e94\u7528\u63d0\u4f9b\u4e86\u5173\u952e\u7684\u7406\u8bba\u548c\u5b9e\u9a8c\u57fa\u7840\u3002"}}
{"id": "2601.06012", "categories": ["eess.SP", "stat.AP"], "pdf": "https://arxiv.org/pdf/2601.06012", "abs": "https://arxiv.org/abs/2601.06012", "authors": ["Helena Calatrava", "Daniel Medina", "Pau Closas"], "title": "Cooperative Differential GNSS Positioning: Estimators and Bounds", "comment": "The manuscript comprises a 13-page main paper and a 6-page supplementary appendix providing extended derivations and matrix expansions. The main body includes 5 figures and 5 tables", "summary": "In Differential GNSS (DGNSS) positioning, differencing measurements between a user and a reference station suppresses common-mode errors but also introduces reference-station noise, which fundamentally limits accuracy. This limitation is minor for high-grade stations but becomes significant when using reference infrastructure of mixed quality. This paper investigates how large-scale user cooperation can mitigate the impact of reference-station noise in conventional (non-cooperative) DGNSS systems. We develop a unified estimation framework for cooperative DGNSS (C-DGNSS) and cooperative real-time kinematic (C-RTK) positioning, and derive parameterized expressions for their Fisher information matrices as functions of network size, satellite geometry, and reference-station noise. This formulation enables theoretical analysis of estimation performance, identifying regimes where cooperation asymptotically restores the accuracy of DGNSS with an ideal (noise-free) reference. Simulations validate these theoretical findings.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5927\u89c4\u6a21\u7528\u6237\u5408\u4f5c\u5982\u4f55\u7f13\u89e3\u4f20\u7edfDGNSS\u7cfb\u7edf\u4e2d\u53c2\u8003\u7ad9\u566a\u58f0\u7684\u5f71\u54cd\uff0c\u63d0\u51fa\u4e86\u5408\u4f5cDGNSS\u548c\u5408\u4f5cRTK\u7684\u7edf\u4e00\u4f30\u8ba1\u6846\u67b6\uff0c\u5e76\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u4eff\u771f\u9a8c\u8bc1\u4e86\u5408\u4f5c\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u53ef\u4ee5\u6062\u590d\u7406\u60f3\u65e0\u566a\u58f0\u53c2\u8003\u7ad9\u7684\u7cbe\u5ea6\u3002", "motivation": "\u5728\u5dee\u5206GNSS\u5b9a\u4f4d\u4e2d\uff0c\u7528\u6237\u4e0e\u53c2\u8003\u7ad9\u4e4b\u95f4\u7684\u5dee\u5206\u6d4b\u91cf\u867d\u7136\u6291\u5236\u4e86\u5171\u6a21\u8bef\u5dee\uff0c\u4f46\u5f15\u5165\u4e86\u53c2\u8003\u7ad9\u566a\u58f0\uff0c\u8fd9\u6210\u4e3a\u5b9a\u4f4d\u7cbe\u5ea6\u7684\u6839\u672c\u9650\u5236\u3002\u5f53\u4f7f\u7528\u8d28\u91cf\u53c2\u5dee\u4e0d\u9f50\u7684\u53c2\u8003\u57fa\u7840\u8bbe\u65bd\u65f6\uff0c\u8fd9\u4e2a\u95ee\u9898\u5c24\u4e3a\u663e\u8457\u3002\u56e0\u6b64\u9700\u8981\u7814\u7a76\u5982\u4f55\u901a\u8fc7\u7528\u6237\u5408\u4f5c\u6765\u7f13\u89e3\u53c2\u8003\u7ad9\u566a\u58f0\u7684\u5f71\u54cd\u3002", "method": "\u63d0\u51fa\u4e86\u5408\u4f5cDGNSS\uff08C-DGNSS\uff09\u548c\u5408\u4f5c\u5b9e\u65f6\u52a8\u6001\uff08C-RTK\uff09\u5b9a\u4f4d\u7684\u7edf\u4e00\u4f30\u8ba1\u6846\u67b6\uff0c\u63a8\u5bfc\u4e86\u5176Fisher\u4fe1\u606f\u77e9\u9635\u7684\u53c2\u6570\u5316\u8868\u8fbe\u5f0f\uff0c\u8be5\u8868\u8fbe\u5f0f\u662f\u7f51\u7edc\u89c4\u6a21\u3001\u536b\u661f\u51e0\u4f55\u6784\u578b\u548c\u53c2\u8003\u7ad9\u566a\u58f0\u7684\u51fd\u6570\u3002\u901a\u8fc7\u8fd9\u79cd\u5f62\u5f0f\u5316\u8868\u8fbe\uff0c\u80fd\u591f\u4ece\u7406\u8bba\u4e0a\u5206\u6790\u4f30\u8ba1\u6027\u80fd\u3002", "result": "\u7406\u8bba\u5206\u6790\u786e\u5b9a\u4e86\u5408\u4f5c\u80fd\u591f\u6e10\u8fdb\u6062\u590d\u7406\u60f3\uff08\u65e0\u566a\u58f0\uff09\u53c2\u8003\u7ad9DGNSS\u7cbe\u5ea6\u7684\u6761\u4ef6\u3002\u4eff\u771f\u7ed3\u679c\u9a8c\u8bc1\u4e86\u8fd9\u4e9b\u7406\u8bba\u53d1\u73b0\uff0c\u8868\u660e\u5728\u7279\u5b9a\u7f51\u7edc\u89c4\u6a21\u548c\u51e0\u4f55\u6761\u4ef6\u4e0b\uff0c\u7528\u6237\u5408\u4f5c\u53ef\u4ee5\u6709\u6548\u7f13\u89e3\u53c2\u8003\u7ad9\u566a\u58f0\u7684\u5f71\u54cd\u3002", "conclusion": "\u5927\u89c4\u6a21\u7528\u6237\u5408\u4f5c\u80fd\u591f\u6709\u6548\u7f13\u89e3\u4f20\u7edfDGNSS\u7cfb\u7edf\u4e2d\u53c2\u8003\u7ad9\u566a\u58f0\u7684\u9650\u5236\uff0c\u7279\u522b\u662f\u5728\u4f7f\u7528\u8d28\u91cf\u53c2\u5dee\u4e0d\u9f50\u7684\u53c2\u8003\u57fa\u7840\u8bbe\u65bd\u65f6\u3002\u63d0\u51fa\u7684\u7edf\u4e00\u6846\u67b6\u4e3a\u5206\u6790\u5408\u4f5c\u5b9a\u4f4d\u6027\u80fd\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u5e76\u8bc6\u522b\u4e86\u5408\u4f5c\u80fd\u591f\u6062\u590d\u7406\u60f3\u53c2\u8003\u7ad9\u7cbe\u5ea6\u7684\u6761\u4ef6\u3002"}}

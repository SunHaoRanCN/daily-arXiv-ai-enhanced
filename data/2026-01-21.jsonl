{"id": "2601.11713", "categories": ["eess.SP", "cs.AI", "cs.LG", "cs.NI"], "pdf": "https://arxiv.org/pdf/2601.11713", "abs": "https://arxiv.org/abs/2601.11713", "authors": ["Rodney Martinez Alonso", "Cel Thys", "Cedric Dehos", "Yuneisy Esthela Garcia Guzman", "Sofie Pollin"], "title": "Inter-Cell Interference Rejection Based on Ultrawideband Walsh-Domain Wireless Autoencoding", "comment": "This preprint was submitted to The 2026 EuCNC & 6G Summit", "summary": "This paper proposes a novel technique for rejecting partial-in-band inter-cell interference (ICI) in ultrawideband communication systems. We present the design of an end-to-end wireless autoencoder architecture that jointly optimizes the transmitter and receiver encoding/decoding in the Walsh domain to mitigate interference from coexisting narrower-band 5G base stations. By exploiting the orthogonality and self-inverse properties of Walsh functions, the system distributes and learns to encode bit-words across parallel Walsh branches. Through analytical modeling and simulation, we characterize how 5G CPOFDM interference maps into the Walsh domain and identify optimal ratios of transmission frequencies and sampling rate where the end-to-end autoencoder achieves the highest rejection. Experimental results show that the proposed autoencoder achieves up to 12 dB of ICI rejection while maintaining a low block error rate (BLER) for the same baseline channel noise, i.e., baseline Signal-to-Noise-Ratio (SNR) without the interference."}
{"id": "2601.11720", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.11720", "abs": "https://arxiv.org/abs/2601.11720", "authors": ["Hasan M. Boudi", "Taissir Y. Elganimi"], "title": "Sparsity Realization in User-Side Multilayer RIS", "comment": null, "summary": "User-side reconfigurable intelligent surface (US-RIS)-aided communication has recently emerged as a promising solution to overcome the high hardware cost and physical size limitations of large-scale user side antenna arrays. This letter proposes, for the first time, a framework that realizes sparsity in multilayer US-RIS using two strategies, namely element-wise sparsity and geometric sparsity. The element-wise approach distributes a limited number of active elements irregularly across multiple layers, thereby exploiting additional spatial degrees of freedom and boosting the achievable rate. For further performance enhancement, a novel foldable RIS architecture leveraging geometric sparsity is proposed, achieving additional gains by optimizing the folding topology of its multilayer structure. Simulation results show that the proposed sparse architectures provide consistently higher achievable rates than existing designs."}
{"id": "2601.11734", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.11734", "abs": "https://arxiv.org/abs/2601.11734", "authors": ["Hao Guo", "Ruoyu Sun", "Amir Hossein Fahim Raouf", "Rahil Gandotra", "Jiayu Mao", "Mark Poletti"], "title": "LarS-Net: A Large-Scale Framework for Network-Level Spectrum Sensing", "comment": "7 pages, 5 figures, this paper is under review at an IEEE conference", "summary": "As the demand of wireless communication continues to rise, the radio spectrum (a finite resource) requires increasingly efficient utilization. This trend is driving the evolution from static, stand-alone spectrum allocation toward spectrum sharing and dynamic spectrum sharing. A critical element of this transition is spectrum sensing, which facilitates informed decision-making in shared environments. Previous studies on spectrum sensing and cognitive radio have been largely limited to individual sensors or small sensor groups. In this work, a large-scale spectrum sensing network (LarS-Net) is designed in a cost-effective manner. Spectrum sensors are either co-located with base stations (BSs) to share the tower, backhaul, and power infrastructure, or integrated directly into BSs as a new feature leveraging active BS antenna systems. As an example incumbent system, fixed service microwave link operating in the lower-7 GHz band is investigated. This band is a primary candidate for 6G, being considered by the WRC-23, ITU, and FCC. Based on Monte Carlo simulations, we determine the minimum subset of BSs equipped with sensing capability to guarantee a target incumbent detection probability. The simulations account for various sensor antenna configurations, propagation channel models, and duty cycles for both incumbent transmissions and sensing operations. Building on this framework, we introduce three network-level sensing performance metrics: Emission Detection Probability (EDP), Temporal Detection Probability (TDP), and Temporal Mis-detection Probability (TMP), which jointly capture spatial coverage, temporal detectability, and multi-node diversity effects. Using these metrics, we analyze the impact of LarS-Net inter-site distance, noise uncertainty, and sensing duty-cycle on large-scale sensing performance."}
{"id": "2601.11741", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.11741", "abs": "https://arxiv.org/abs/2601.11741", "authors": ["Oliver Kirkpatrick", "Santiago Ozafrain", "Christopher Gilliam", "Beth Jelfs"], "title": "MIMO Array Calibration in Non-stationary Channels with Residual Surfaces and Slepian Spherical Harmonics", "comment": "5 pages, 3 figures, 1 table", "summary": "The fundamental mechanism driving MIMO beamforming is the relative phases of signals departing the transmit array and arriving at the receive array. If a propagation channel affects all transmitted signals equally, the relative phases are a function of the directions of departure and arrival, as well as the transmit and receive hardware. In a non-stationary channel, the amplitudes and phases of arriving signals may vary significantly over time, making it infeasible to directly measure the influence of hardware. In this paper, we present a calibration method for achieving indirect measurement and compensation of hardware influences in non-stationary channels. Our method characterizes the patterns of array elements relative to a reference element and estimates these relative patterns, termed residual surfaces, using a Slepian spherical harmonic basis. Using simulations, we demonstrate that our calibration method achieves beamforming gains that closely match theoretical optimums. Our results also show a reduction in the error in estimating the target direction, lower side lobes, and improve null-steering capabilities."}
{"id": "2601.11768", "categories": ["eess.AS", "cs.AI", "cs.LG", "cs.SD", "eess.SP"], "pdf": "https://arxiv.org/pdf/2601.11768", "abs": "https://arxiv.org/abs/2601.11768", "authors": ["Venkat Suprabath Bitra", "Homayoon Beigi"], "title": "Lightweight Self-Supervised Detection of Fundamental Frequency and Accurate Probability of Voicing in Monophonic Music", "comment": "12 pages, 6 figures, 3 tables, and an appendix, Accepted for publication at ICPRAM 2026 in Marbella, Spain, on March 2, 2026", "summary": "Reliable fundamental frequency (F 0) and voicing estimation is essential for neural synthesis, yet many pitch extractors depend on large labeled corpora and degrade under realistic recording artifacts. We propose a lightweight, fully self-supervised framework for joint F 0 estimation and voicing inference, designed for rapid single-instrument training from limited audio. Using transposition-equivariant learning on CQT features, we introduce an EM-style iterative reweighting scheme that uses Shift Cross-Entropy (SCE) consistency as a reliability signal to suppress uninformative noisy/unvoiced frames. The resulting weights provide confidence scores that enable pseudo-labeling for a separate lightweight voicing classifier without manual annotations. Trained on MedleyDB and evaluated on MDB-stem-synth ground truth, our method achieves competitive cross-corpus performance (RPA 95.84, RCA 96.24) and demonstrates cross-instrument generalization."}
{"id": "2601.12203", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2601.12203", "abs": "https://arxiv.org/abs/2601.12203", "authors": ["Antonella M. C. Torrisi", "Inês Nolasco", "Paola Sgadò", "Elisabetta Versace", "Emmanouil Benetos"], "title": "Embryonic Exposure to VPA Influences Chick Vocalisations: A Computational Study", "comment": "Main text (approx. 23 pages including references) with extensive Supplementary Material ( 20 pages) and multiple figures", "summary": "In young animals like poultry chicks (Gallus gallus), vocalisations convey information about affective and behavioural states. Traditional approaches to vocalisation analysis, relying on manual annotation and predefined categories, introduce biases, limit scalability, and fail to capture the full complexity of vocal repertoires. We introduce a computational framework for the automated detection, acoustic feature extraction, and unsupervised learning of chick vocalisations. Applying this framework to a dataset of newly hatched chicks, we identified two primary vocal clusters. We then tested our computational framework on an independent dataset of chicks exposed during embryonic development to vehicle or Valproic Acid (VPA), a compound that disrupts neural development and is linked to autistic-like symptoms. Clustering analysis on the experimental dataset confirmed two primary vocal clusters and revealed systematic differences between groups. VPA-exposed chicks showed an altered repertoire, with a relative increase in softer calls. VPA differentially affected call clusters, modulating temporal, frequency, and energy domain features. Overall, VPA-exposed chicks produced vocalisations with shorter duration, reduced pitch variability, and modified energy profiles, with the strongest alterations observed in louder calls. This study provides a computational framework for analysing animal vocalisations, advancing knowledge of early-life communication in typical and atypical vocal development."}
{"id": "2601.11742", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.11742", "abs": "https://arxiv.org/abs/2601.11742", "authors": ["Jiayu Mao", "Ruoyu Sun", "Mark Poletti", "Rahil Gandotra", "Hao Guo", "Aylin Yener"], "title": "AI-Driven Spectrum Occupancy Prediction Using Real-World Spectrum Measurements", "comment": "8 pages, 7 figures. This paper is under review at an IEEE conference", "summary": "Spectrum occupancy prediction is a critical enabler for real-time and proactive dynamic spectrum sharing (DSS), as it can provide short-term channel availability information to support more efficient spectrum access decisions in wireless communication systems. Instead of relying on open-source datasets or simulated data, commonly used in the literature, this paper investigates short-horizon spectrum occupancy prediction using mid-band, 24X7 real-world spectrum measurement data collected in the United States. We construct a multi-band channel occupancy dataset through analyzing 61 days of empirical data and formulate a next-minute channel occupancy prediction task across all frequency channels. This study focuses on AI-driven prediction methods, including Random Forest, Extreme Gradient Boosting (XGBoost), and a Long Short-Term Memory (LSTM) network, and compares their performance against a conventional Markov chain-based statistical baseline. Numerical results show that learning-based methods outperform the statistical baseline on dynamic channels, particularly under fixed false-alarm constraints. These results demonstrate the effectiveness of AI-driven spectrum occupancy prediction, indicating that lightweight learning models can effectively support future deployment-oriented DSS systems."}
{"id": "2601.12142", "categories": ["eess.AS", "cs.MM", "cs.RO"], "pdf": "https://arxiv.org/pdf/2601.12142", "abs": "https://arxiv.org/abs/2601.12142", "authors": ["Ziang Guo", "Feng Yang", "Xuefeng Zhang", "Jiaqi Guo", "Kun Zhao", "Peng Lu", "Zufeng Zhang", "Sifa Zheng"], "title": "Listen, Look, Drive: Coupling Audio Instructions for User-aware VLA-based Autonomous Driving", "comment": "Accepted by IV", "summary": "Vision Language Action (VLA) models promise an open-vocabulary interface that can translate perceptual ambiguity into semantically grounded driving decisions, yet they still treat language as a static prior fixed at inference time. As a result, the model must infer continuously shifting objectives from pixels alone, yielding delayed or overly conservative maneuvers. We argue that effective VLAs for autonomous driving need an online channel in which users can influence driving with specific intentions. To this end, we present EchoVLA, a user-aware VLA that couples camera streams with in situ audio instructions. We augment the nuScenes dataset with temporally aligned, intent-specific speech commands generated by converting ego-motion descriptions into synthetic audios. Further, we compose emotional speech-trajectory pairs into a multimodal Chain-of-Thought (CoT) for fine-tuning a Multimodal Large Model (MLM) based on Qwen2.5-Omni. Specifically, we synthesize the audio-augmented dataset with different emotion types paired with corresponding driving behaviors, leveraging the emotional cues embedded in tone, pitch, and speech tempo to reflect varying user states, such as urgent or hesitant intentions, thus enabling our EchoVLA to interpret not only the semantic content but also the emotional context of audio commands for more nuanced and emotionally adaptive driving behavior. In open-loop benchmarks, our approach reduces the average L2 error by $59.4\\%$ and the collision rate by $74.4\\%$ compared to the baseline of vision-only perception. More experiments on nuScenes dataset validate that EchoVLA not only steers the trajectory through audio instructions, but also modulates driving behavior in response to the emotions detected in the user's speech."}
{"id": "2601.12205", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.12205", "abs": "https://arxiv.org/abs/2601.12205", "authors": ["Shih-Heng Wang", "Jiatong Shi", "Jinchuan Tian", "Haibin Wu", "Shinji Watanabe"], "title": "Do Neural Codecs Generalize? A Controlled Study Across Unseen Languages and Non-Speech Tasks", "comment": null, "summary": "This paper investigates three crucial yet underexplored aspects of the generalization capabilities of neural audio codecs (NACs): (i) whether NACs can generalize to unseen languages during pre-training, (ii) whether speech-only pre-trained NACs can effectively generalize to non-speech applications such as environmental sounds, music, and animal vocalizations, and (iii) whether incorporating non-speech data during pre-training can improve performance on both speech and non-speech tasks. Existing studies typically rely on off-the-shelf NACs for comparison, which limits insight due to variations in implementation. In this work, we train NACs from scratch using strictly controlled configurations and carefully curated pre-training data to enable fair comparisons. We conduct a comprehensive evaluation of NAC performance on both signal reconstruction quality and downstream applications using 11 metrics. Our results show that NACs can generalize to unseen languages during pre-training, speech-only pre-trained NACs exhibit degraded performance on non-speech tasks, and incorporating non-speech data during pre-training improves performance on non-speech tasks while maintaining comparable performance on speech tasks."}
{"id": "2601.11748", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.11748", "abs": "https://arxiv.org/abs/2601.11748", "authors": ["Rahil Gandotra", "Ruoyu Sun", "Mark Poletti", "Jiayu Mao", "Hao Guo"], "title": "Automated Spectrum Sensing and Analysis Framework", "comment": "This paper is under review at an IEEE conference", "summary": "Spectrum sensing and analysis is crucial for a variety of reasons, including regulatory compliance, interference detection and mitigation, and spectrum resource planning and optimization. Effective, real-time spectrum analysis remains a challenge, stemming from the need to analyse an increasingly complex and dynamic environment with limited resources. The vast amount of data generated from sensing the spectrum at multiple sites requires sophisticated data analysis and processing techniques, which can be technically demanding and expensive. This paper presents a novel, holistic framework developed and deployed at multiple locations across the USA for spectrum analysis and describes the different parts of the end-to-end pipeline. The details of each of the modules of the pipeline, data collection and pre-processing at remote locations, transfer to a centralized location, post-processing analysis, visualization, and long-term storage, are reported. The motivation behind this work is to develop a robust spectrum analysis framework that can help gain greater insights into the spectrum usage across the country and augment additional use cases such as dynamic spectrum sharing."}
{"id": "2601.12153", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2601.12153", "abs": "https://arxiv.org/abs/2601.12153", "authors": ["Arthur N. dos Santos", "Bruno S. Masiero"], "title": "A Survey on 30+ Years of Automatic Singing Assessment and Singing Information Processing", "comment": null, "summary": "Automatic Singing Assessment and Singing Information Processing have evolved over the past three decades to support singing pedagogy, performance analysis, and vocal training. While the first approach objectively evaluates a singer's performance through computational metrics ranging from real-time visual feedback and acoustical biofeedback to sophisticated pitch tracking and spectral analysis, the latter method compares a predictor vocal signal with a target reference to capture nuanced data embedded in the singing voice. Notable advancements include the development of interactive systems that have significantly improved real-time visual feedback, and the integration of machine learning and deep neural network architectures that enhance the precision of vocal signal processing. This survey critically examines the literature to map the historical evolution of these technologies, while identifying and discussing key gaps. The analysis reveals persistent challenges, such as the lack of standardized evaluation frameworks, difficulties in reliably separating vocal signals from various noise sources, and the underutilization of advanced digital signal processing and artificial intelligence methodologies for capturing artistic expressivity. By detailing these limitations and the corresponding technological advances, this review demonstrates how addressing these issues can bridge the gap between objective computational assessments and subjective human-like evaluations of singing performance, ultimately enhancing both the technical accuracy and pedagogical relevance of automated singing evaluation systems."}
{"id": "2601.12222", "categories": ["cs.SD", "cs.MM", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.12222", "abs": "https://arxiv.org/abs/2601.12222", "authors": ["Yishan Lv", "Jing Luo", "Boyuan Ju", "Yang Zhang", "Xinda Wu", "Bo Yuan", "Xinyu Yang"], "title": "Song Aesthetics Evaluation with Multi-Stem Attention and Hierarchical Uncertainty Modeling", "comment": null, "summary": "Music generative artificial intelligence (AI) is rapidly expanding music content, necessitating automated song aesthetics evaluation. However, existing studies largely focus on speech, audio or singing quality, leaving song aesthetics underexplored. Moreover, conventional approaches often predict a precise Mean Opinion Score (MOS) value directly, which struggles to capture the nuances of human perception in song aesthetics evaluation. This paper proposes a song-oriented aesthetics evaluation framework, featuring two novel modules: 1) Multi-Stem Attention Fusion (MSAF) builds bidirectional cross-attention between mixture-vocal and mixture-accompaniment pairs, fusing them to capture complex musical features; 2) Hierarchical Granularity-Aware Interval Aggregation (HiGIA) learns multi-granularity score probability distributions, aggregates them into a score interval, and applies a regression within the interval to produce the final score. We evaluated on two datasets of full-length songs: SongEval dataset (AI-generated) and an internal aesthetics dataset (human-created), and compared with two state-of-the-art (SOTA) models. Results show that the proposed method achieves stronger performance for multi-dimensional song aesthetics evaluation."}
{"id": "2601.11844", "categories": ["eess.SP", "cs.IT"], "pdf": "https://arxiv.org/pdf/2601.11844", "abs": "https://arxiv.org/abs/2601.11844", "authors": ["Yue Bi", "Michèle Wigger"], "title": "Necessity of Cooperative Transmissions for Wireless MapReduce", "comment": null, "summary": "The paper presents an improved upper bound (achievability result) on the optimal tradeoff between Normalized Delivery Time (NDT) and computation load for distributed computing MapReduce systems in certain ranges of the parameters. The upper bound is based on interference alignment combined with zero-forcing. The paper further provides a lower bound (converse) on the optimal NDT-computation tradeoff that can be achieved when IVAs are partitioned into sub-IVAs, and these sub-IVAs are then transmitted (in an arbitrary form) by a single node, without cooperation among nodes. For appropriate linear functions (e.g., XORs), such non-cooperative schemes can achieve some of the best NDT-computation tradeoff points so far obtained in the literature. However, as our lower bound shows, any non-cooperative scheme achieves a worse NDT-computation tradeoff than our new proposed scheme for certain parameters, thus proving the necessity of cooperative schemes like zero-forcing to attain the optimal NDT-computation tradeoff."}
{"id": "2601.12248", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2601.12248", "abs": "https://arxiv.org/abs/2601.12248", "authors": ["Chun-Yi Kuan", "Hung-yi Lee"], "title": "AQUA-Bench: Beyond Finding Answers to Knowing When There Are None in Audio Question Answering", "comment": "Accepted to ICASSP 2026. Project Website: https://kuan2jiu99.github.io/AQUA-Bench-demo/", "summary": "Recent advances in audio-aware large language models have shown strong performance on audio question answering. However, existing benchmarks mainly cover answerable questions and overlook the challenge of unanswerable ones, where no reliable answer can be inferred from the audio. Such cases are common in real-world settings, where questions may be misleading, ill-posed, or incompatible with the information. To address this gap, we present AQUA-Bench, a benchmark for Audio Question Unanswerability Assessment. It systematically evaluates three scenarios: Absent Answer Detection (the correct option is missing), Incompatible Answer Set Detection (choices are categorically mismatched with the question), and Incompatible Audio Question Detection (the question is irrelevant or lacks sufficient grounding in the audio). By assessing these cases, AQUA-Bench offers a rigorous measure of model reliability and promotes the development of audio-language systems that are more robust and trustworthy. Our experiments suggest that while models excel on standard answerable tasks, they often face notable challenges with unanswerable ones, pointing to a blind spot in current audio-language understanding."}
{"id": "2601.12254", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.12254", "abs": "https://arxiv.org/abs/2601.12254", "authors": ["Kazuki Yamauchi", "Masato Murata", "Shogo Seki"], "title": "Confidence-based Filtering for Speech Dataset Curation with Generative Speech Enhancement Using Discrete Tokens", "comment": "Accepted for ICASSP 2026", "summary": "Generative speech enhancement (GSE) models show great promise in producing high-quality clean speech from noisy inputs, enabling applications such as curating noisy text-to-speech (TTS) datasets into high-quality ones. However, GSE models are prone to hallucination errors, such as phoneme omissions and speaker inconsistency, which conventional error filtering based on non-intrusive speech quality metrics often fails to detect. To address this issue, we propose a non-intrusive method for filtering hallucination errors from discrete token-based GSE models. Our method leverages the log-probabilities of generated tokens as confidence scores to detect potential errors. Experimental results show that the confidence scores strongly correlate with a suite of intrusive SE metrics, and that our method effectively identifies hallucination errors missed by conventional filtering methods. Furthermore, we demonstrate the practical utility of our method: curating an in-the-wild TTS dataset with our confidence-based filtering improves the performance of subsequently trained TTS models."}
{"id": "2601.11869", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.11869", "abs": "https://arxiv.org/abs/2601.11869", "authors": ["Zekun Hong", "Shinya Sugiura", "Chao Xu", "Lajos Hanzo"], "title": "Delay-Doppler-Domain Channel Estimation and Reduced-Complexity Detection of Faster-than-Nyquist Signaling Aided OTFS", "comment": "16 pages, 18 figures, 3 tables", "summary": "We conceive a novel channel estimation and data detection scheme for OTFS-modulated faster-than-Nyquist (FTN) transmission over doubly selective fading channels, aiming for enhancing the spectral efficiency and Doppler resilience. The delay-Doppler (DD) domain's input-output relationship of OTFS-FTN signaling is derived by employing a root-raised cosine (RRC) shaping filter. More specifically, we design our DD-domain channel estimator for FTN-based pilot transmission, where the pilot symbol interval is lower than that defined by the classic Nyquist criterion. Moreover, we propose a reduced-complexity linear minimum mean square error equalizer, supporting noise whitening, where the FTN-induced inter-symbol interference (ISI) matrix is approximated by a sparse one. Our performance results demonstrate that the proposed OTFS-FTN scheme is capable of enhancing the achievable information rate, while attaining a comparable BER performance to both that of its Nyquist-based OTFS counterpart and to other FTN transmission schemes, which employ the same RRC shaping filter."}
{"id": "2601.12345", "categories": ["eess.AS", "cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2601.12345", "abs": "https://arxiv.org/abs/2601.12345", "authors": ["Jakob Kienegger", "Timo Gerkmann"], "title": "Adaptive Rotary Steering with Joint Autoregression for Robust Extraction of Closely Moving Speakers in Dynamic Scenarios", "comment": "Accepted at IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP) 2026", "summary": "Latest advances in deep spatial filtering for Ambisonics demonstrate strong performance in stationary multi-speaker scenarios by rotating the sound field toward a target speaker prior to multi-channel enhancement. For applicability in dynamic acoustic conditions with moving speakers, we propose to automate this rotary steering using an interleaved tracking algorithm conditioned on the target's initial direction. However, for nearby or crossing speakers, robust tracking becomes difficult and spatial cues less effective for enhancement. By incorporating the processed recording as additional guide into both algorithms, our novel joint autoregressive framework leverages temporal-spectral correlations of speech to resolve spatially challenging speaker constellations. Consequently, our proposed method significantly improves tracking and enhancement of closely spaced speakers, consistently outperforming comparable non-autoregressive methods on a synthetic dataset. Real-world recordings complement these findings in complex scenarios with multiple speaker crossings and varying speaker-to-array distances."}
{"id": "2601.12289", "categories": ["cs.SD", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.12289", "abs": "https://arxiv.org/abs/2601.12289", "authors": ["Haowei Lou", "Hye-young Paik", "Wen Hu", "Lina Yao"], "title": "ParaMETA: Towards Learning Disentangled Paralinguistic Speaking Styles Representations from Speech", "comment": "9 pages, 7 figures, Accepted to AAAI-26 (Main Technical Track)", "summary": "Learning representative embeddings for different types of speaking styles, such as emotion, age, and gender, is critical for both recognition tasks (e.g., cognitive computing and human-computer interaction) and generative tasks (e.g., style-controllable speech generation). In this work, we introduce ParaMETA, a unified and flexible framework for learning and controlling speaking styles directly from speech. Unlike existing methods that rely on single-task models or cross-modal alignment, ParaMETA learns disentangled, task-specific embeddings by projecting speech into dedicated subspaces for each type of style. This design reduces inter-task interference, mitigates negative transfer, and allows a single model to handle multiple paralinguistic tasks such as emotion, gender, age, and language classification. Beyond recognition, ParaMETA enables fine-grained style control in Text-To-Speech (TTS) generative models. It supports both speech- and text-based prompting and allows users to modify one speaking styles while preserving others. Extensive experiments demonstrate that ParaMETA outperforms strong baselines in classification accuracy and generates more natural and expressive speech, while maintaining a lightweight and efficient model suitable for real-world applications."}
{"id": "2601.11878", "categories": ["eess.SP", "cs.CV", "cs.LG", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2601.11878", "abs": "https://arxiv.org/abs/2601.11878", "authors": ["Xi Peng"], "title": "Accelerated MR Elastography Using Learned Neural Network Representation", "comment": null, "summary": "To develop a deep-learning method for achieving fast high-resolution MR elastography from highly undersampled data without the need of high-quality training dataset. We first framed the deep neural network representation as a nonlinear extension of the linear subspace model, then used it to represent and reconstruct MRE image repetitions from undersampled k-space data. The network weights were learned using a multi-level k-space consistent loss in a self-supervised manner. To further enhance reconstruction quality, phase-contrast specific magnitude and phase priors were incorporated, including the similarity of anatomical structures and smoothness of wave-induced harmonic displacement. Experiments were conducted using both 3D gradient-echo spiral and multi-slice spin-echo spiral MRE datasets. Compared to the conventional linear subspace-based approaches, the nonlinear network representation method was able to produce superior image reconstruction with suppressed noise and artifacts from a single in-plane spiral arm per MRE repetition (e.g., total R=10), yielding comparable stiffness estimation to the fully sampled data. This work demonstrated the feasibility of using deep network representations to model and reconstruct MRE images from highly-undersampled data, a nonlinear extension of the subspace-based approaches."}
{"id": "2601.12354", "categories": ["eess.AS", "cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2601.12354", "abs": "https://arxiv.org/abs/2601.12354", "authors": ["Sina Khanagha", "Bunlong Lay", "Timo Gerkmann"], "title": "Bone-conduction Guided Multimodal Speech Enhancement with Conditional Diffusion Models", "comment": "Accepted to IEEE ICASSP 2026", "summary": "Single-channel speech enhancement models face significant performance degradation in extremely noisy environments. While prior work has shown that complementary bone-conducted speech can guide enhancement, effective integration of this noise-immune modality remains a challenge. This paper introduces a novel multimodal speech enhancement framework that integrates bone-conduction sensors with air-conducted microphones using a conditional diffusion model. Our proposed model significantly outperforms previously established multimodal techniques and a powerful diffusion-based single-modal baseline across a wide range of acoustic conditions."}
{"id": "2601.12314", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2601.12314", "abs": "https://arxiv.org/abs/2601.12314", "authors": ["Yiwen Zhang", "Hui Zhang", "Fanqin Meng"], "title": "A Similarity Network for Correlating Musical Structure to Military Strategy", "comment": "This paper was completed in 2024", "summary": "Music perception, a multi-sensory process based on the synesthesia effect, is an essential component of music aesthetic education. Understanding music structure helps both perception and aesthetic education. Music structure incorporates a range of information, the coordination of which forms the melody, just as different military actions cooperate to produce a military strategy. However, there are a few ways for assessing music perception from the perspectives of system operation and information management. In this paper, we explore the similarities between music structure and military strategy while creating the Music Clips Correlation Network (MCCN) based on Mel-frequency Cepstral Coefficients (MFCCs). The inspiration comes from the comparison between a concert conductor's musical score and a military war commander's sand table exercise. Specifically, we create MCCNs for various kinds of war movie soundtracks, then relate military tactics (Sun Tzu's Art of War, etc.) and political institutions to military operations networks. Our primary findings suggest a few similarities, implying that music perception and aesthetic education can be approached from a military strategy and management perspective through this interdisciplinary research. Similarly, we can discover similarities between the art of military scheming and the art of musical structure based on network analysis in order to facilitate the understanding of the relationship between technology and art."}
{"id": "2601.11894", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.11894", "abs": "https://arxiv.org/abs/2601.11894", "authors": ["Haotian Liu", "Zhiqing Wei", "Yucong Du", "Jiachen Wei", "Xingwang Li", "Zhiyong Feng"], "title": "Beyond Target-Level: ISAC-Enabled Event-Level Sensing for Behavioral Intention Prediction", "comment": "5 pages, 5 figures, 1 table, and 15 references. Reviewed by IEEE WCL", "summary": "Integrated Sensing and Communication (ISAC) holds great promise for enabling event-level sensing, such as behavioral intention prediction (BIP) in autonomous driving, particularly under non-line-of-sight (NLoS) or adverse weather conditions where conventional sensors degrade. However, as a key instance of event-level sensing, ISAC-based BIP remains unexplored. To address this gap, we propose an ISAC-enabled BIP framework and validate its feasibility and effectiveness through extensive simulations. Our framework achieves robust performance in safety-critical scenarios, improving the F1-score by 11.4% over sensor-based baselines in adverse weather, thereby demonstrating ISAC's potential for intelligent event-level sensing."}
{"id": "2601.12436", "categories": ["eess.AS", "cs.AI", "cs.LG", "cs.MM", "cs.SD"], "pdf": "https://arxiv.org/pdf/2601.12436", "abs": "https://arxiv.org/abs/2601.12436", "authors": ["Linzhi Wu", "Xingyu Zhang", "Hao Yuan", "Yakun Zhang", "Changyan Zheng", "Liang Xie", "Tiejun Liu", "Erwei Yin"], "title": "Purification Before Fusion: Toward Mask-Free Speech Enhancement for Robust Audio-Visual Speech Recognition", "comment": "Accepted by ICASSP2026", "summary": "Audio-visual speech recognition (AVSR) typically improves recognition accuracy in noisy environments by integrating noise-immune visual cues with audio signals. Nevertheless, high-noise audio inputs are prone to introducing adverse interference into the feature fusion process. To mitigate this, recent AVSR methods often adopt mask-based strategies to filter audio noise during feature interaction and fusion, yet such methods risk discarding semantically relevant information alongside noise. In this work, we propose an end-to-end noise-robust AVSR framework coupled with speech enhancement, eliminating the need for explicit noise mask generation. This framework leverages a Conformer-based bottleneck fusion module to implicitly refine noisy audio features with video assistance. By reducing modality redundancy and enhancing inter-modal interactions, our method preserves speech semantic integrity to achieve robust recognition performance. Experimental evaluations on the public LRS3 benchmark suggest that our method outperforms prior advanced mask-based baselines under noisy conditions."}
{"id": "2601.12480", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.12480", "abs": "https://arxiv.org/abs/2601.12480", "authors": ["Hanchen Pei", "Shujie Liu", "Yanqing Liu", "Jianwei Yu", "Yuanhang Qian", "Gongping Huang", "Sheng Zhao", "Yan Lu"], "title": "A Unified Neural Codec Language Model for Selective Editable Text to Speech Generation", "comment": null, "summary": "Neural codec language models achieve impressive zero-shot Text-to-Speech (TTS) by fully imitating the acoustic characteristics of a short speech prompt, including timbre, prosody, and paralinguistic information. However, such holistic imitation limits their ability to isolate and control individual attributes. In this paper, we present a unified codec language model SpeechEdit that extends zero-shot TTS with a selective control mechanism. By default, SpeechEdit reproduces the complete acoustic profile inferred from the speech prompt, but it selectively overrides only the attributes specified by explicit control instructions. To enable controllable modeling, SpeechEdit is trained on our newly constructed LibriEdit dataset, which provides delta (difference-aware) training pairs derived from LibriHeavy. Experimental results show that our approach maintains naturalness and robustness while offering flexible and localized control over desired attributes. Audio samples are available at https://speech-editing.github.io/speech-editing/."}
{"id": "2601.11938", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.11938", "abs": "https://arxiv.org/abs/2601.11938", "authors": ["Sebastian Ratto", "Huy Trinh", "Ahmed N. Sayed", "Abdelrahman Elbadrawy", "Arien Sligar", "George Shaker"], "title": "Radar-Based Fall Detection for Assisted Living: A Digital-Twin Representation Case Study", "comment": "6 pages, 8 figures", "summary": "Obtaining data on high-impact falls from older adults is ethically difficult, yet these rare events cause many fall-related health problems. As a result, most radar-based fall detectors are trained on staged falls from young volunteers, and representation choices are rarely tested against the radar signals from dangerous falls. This paper uses a frequency-modulated continuous-wave (FMCW) radar digital twin as a single simulated room testbed to study how representation choice affects fall/non-fall discrimination. From the same simulated range-Doppler sequence, Doppler-time spectrograms, three-channel per-receiver spectrogram stacks, and time-pooled range-Doppler maps (RDMs) are derived and fed to an identical compact CNN under matched training on a balanced fall/non-fall dataset. In this twin, temporal spectrograms reach 98-99% test accuracy with similar precision and recall for both classes, while static RDMs reach 89.4% and show more variable training despite using the same backbone. A qualitative comparison between synthetic and measured fall spectrograms suggests that the twin captures gross Doppler-time structure, but amplitude histograms reveal differences in the distributions of amplitude values consistent with receiver processing not modeled in the twin. Because the twin omits noise and hardware impairments and is only qualitatively compared to a single measured example, these results provide representation-level guidance under controlled synthetic conditions rather than ready-to-use clinical performance in real settings."}
{"id": "2601.12485", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2601.12485", "abs": "https://arxiv.org/abs/2601.12485", "authors": ["Kang Chen", "Xianrui Wang", "Yichen Yang", "Andreas Brendel", "Gongping Huang", "Zbyněk Koldovský", "Jingdong Chen", "Jacob Benesty", "Shoji Makino"], "title": "Robust Online Overdetermined Independent Vector Analysis Based on Bilinear Decomposition", "comment": null, "summary": "Online blind source separation is essential for both speech communication and human-machine interaction. Among existing approaches, overdetermined independent vector analysis (OverIVA) delivers strong performance by exploiting the statistical independence of source signals and the orthogonality between source and noise subspaces. However, when applied to large microphone arrays, the number of parameters grows rapidly, which can degrade online estimation accuracy. To overcome this challenge, we propose decomposing each long separation filter into a bilinear form of two shorter filters, thereby reducing the number of parameters. Because the two filters are closely coupled, we design an alternating iterative projection algorithm to update them in turn. Simulation results show that, with far fewer parameters, the proposed method achieves improved performance and robustness."}
{"id": "2601.12494", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.12494", "abs": "https://arxiv.org/abs/2601.12494", "authors": ["Hunzalah Hassan Bhatti", "Firoj Alam", "Shammur Absar Chowdhury"], "title": "Harmonizing the Arabic Audio Space with Data Scheduling", "comment": "Foundation Models, Large Language Models, Native, Speech Models, Arabic", "summary": "Audio large language models (LLMs) enable unified speech understanding and generation, yet their adaptation to linguistically complex, dialect-rich settings remains underexplored. This paper presents the first systematic study of multi-task instruction tuning for an Arabic-centric audio LLM, covering a hierarchy of generative tasks (ASR, speech summarization) and discriminative tasks (dialect and emotion identification). To support this study, we introduce AraMega-SSum, a novel dataset for Arabic speech summarization. We fine-tune Qwen2.5-Omni (7B) and propose Task-Progressive Curriculum (TPC) along with Aligner-Based Diverse Sampling (ADS), a strategy that constructs information-dense batches by selecting task- and label-balanced examples. Our results reveal a critical efficiency, robustness trade-off: while ADS accelerates initial convergence and boosts paralinguistic F1-scores, its inherent gradient volatility can destabilize generative decoding under prolonged training. Furthermore, while the TPC stabilizes core acoustic mapping, it often induces negative transfer in downstream tasks. We demonstrate that a Hybrid TPC+ADS Strategy provides an optimal training ``recipe'', first establishing a robust representative foundation before employing diversity-aware refinement to capture fine-grained nuances. These findings offer practical guidance for the efficient adaptation of Omni-models in complex, low-resource multimodal environments."}
{"id": "2601.11971", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.11971", "abs": "https://arxiv.org/abs/2601.11971", "authors": ["Duc Viet Nguyen", "Haiquan Zhao", "Jinhui Hu", "Xiaoli Li"], "title": "Robust distributed extended Kalman filter based on adaptive multi-kernel mixture maximum correntropy for non-Gaussian systems", "comment": "15 pages, 16 figures,", "summary": "As one of the most advanced variants in the correntropy family, the multi-kernel correntropy criterion demonstrates superior accuracy in handling non-Gaussian noise, particularly with multimodal distributions. However, current approaches suffer from key limitations-namely, reliance on a single type of sensitive Gaussian kernel and the manual selection of free parameters. To address these issues and further boost robustness, this paper introduces the concept of multi-kernel mixture correntropy (MKMC), along with its key properties. MKMC employs a flexible kernel function composed of a mixture of two Students t-Cauchy functions with adjustable (non-zero) means. Building on this criterion within multi-sensor networks, we propose a robust distributed extended Kalman filter-AMKMMC-RDEKF based on adaptive multi-kernel mixture maximum correntropy. To reduce communication overhead, a consensus averaging strategy is incorporated. Furthermore, an adaptive mechanism is introduced to mitigate the impact of manually tuned free parameters. At the same time, the computational complexity and convergence ability of the proposed algorithm are analyzed. The effectiveness of the proposed algorithm is validated through challenging scenarios involving power system and land vehicle state estimation."}
{"id": "2601.12594", "categories": ["eess.AS", "cs.AI", "cs.SD"], "pdf": "https://arxiv.org/pdf/2601.12594", "abs": "https://arxiv.org/abs/2601.12594", "authors": ["Xinhao Mei", "Gael Le Lan", "Haohe Liu", "Zhaoheng Ni", "Varun Nagaraja", "Yang Liu", "Yangyang Shi", "Vikas Chandra"], "title": "SLAP: Scalable Language-Audio Pretraining with Variable-Duration Audio and Multi-Objective Training", "comment": "Accepted to ICASSP 2026", "summary": "Contrastive language-audio pretraining (CLAP) has achieved notable success in learning semantically rich audio representations and is widely adopted for various audio-related tasks. However, current CLAP models face several key limitations. First, they are typically trained on relatively small datasets, often comprising a few million audio samples. Second, existing CLAP models are restricted to short and fixed duration, which constrains their usage in real-world scenarios with variable-duration audio. Third, the standard contrastive training objective operates on global representations, which may hinder the learning of dense, fine-grained audio features. To address these challenges, we introduce Scalable Language-Audio Pretraining (SLAP), which scales language-audio pretraining to 109 million audio-text pairs with variable audio durations and incorporates multiple training objectives. SLAP unifies contrastive loss with additional self-supervised and captioning losses in a single-stage training, facilitating the learning of richer dense audio representations. The proposed SLAP model achieves new state-of-the-art performance on audio-text retrieval and zero-shot audio classification tasks, demonstrating its effectiveness across diverse benchmarks."}
{"id": "2601.12591", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.12591", "abs": "https://arxiv.org/abs/2601.12591", "authors": ["Xin Jing", "Jiadong Wang", "Andreas Triantafyllopoulos", "Maurice Gerczuk", "Shahin Amiriparian", "Jun Luo", "Björn Schuller"], "title": "SmoothCLAP: Soft-Target Enhanced Contrastive Language\\--Audio Pretraining for Affective Computing", "comment": "5 pages, accepted by ICASSP 2026", "summary": "The ambiguity of human emotions poses several challenges for machine learning models, as they often overlap and lack clear delineating boundaries. Contrastive language-audio pretraining (CLAP) has emerged as a key technique for generalisable emotion recognition. However, as conventional CLAP enforces a strict one-to-one alignment between paired audio-text samples, it overlooks intra-modal similarity and treats all non-matching pairs as equally negative. This conflicts with the fuzzy boundaries between different emotions. To address this limitation, we propose SmoothCLAP, which introduces softened targets derived from intra-modal similarity and paralinguistic features. By combining these softened targets with conventional contrastive supervision, SmoothCLAP learns embeddings that respect graded emotional relationships, while retaining the same inference pipeline as CLAP. Experiments on eight affective computing tasks across English and German demonstrate that SmoothCLAP is consistently achieving superior performance. Our results highlight that leveraging soft supervision is a promising strategy for building emotion-aware audio-text models."}
{"id": "2601.12110", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.12110", "abs": "https://arxiv.org/abs/2601.12110", "authors": ["David. Casillas-Pérez", "Daniel. Merino-Pérez", "Silvia. Jiménez-Fernández", "J. Antonio. Portilla-Figueras", "Sancho. Salcedo-Sanz"], "title": "Extended Weighted ABG: A Robust Non-Linear ABG-Based Approach for Optimal Combination of ABG Path-Loss Propagation Models", "comment": null, "summary": "This paper proposes a robust non-linear generalized path-loss propagation model, the Extended Weighted ABG (EWABG), which efficiently allows generating a path-loss propagation model by combining several available path-loss datasets (from measurements campaigns) and other previously proposed state-of-the-art 5G path-loss propagation models. The EWABG model works by integrating individual path-loss models into one single model in the least-squares sense, allowing to extend knowledge from frequencies and distances covered by path-loss datasets or path-loss propagation models. The proposed EWABG model is the first non-linear extension of the common ABG-based approach, which surpasses the non-uniformity problem between the low and high 5G frequencies (as most measurements campaigns have taken place in low frequencies). The EWABG also addresses the problem of removing outlier measurements, a step not included in previous propagation path-loss models. In this case, we have compared the most recent techniques for avoiding outliers, and we have adopted the Theil-Sen method, due to its strong robustness demonstrated in the experiments carried out. In addition, the proposed model specifically considers non-linear attenuation by atmospheric gases, in order to improve its estimations. The good performance of the proposed EWABG model has been tested and compared against recent 5G propagation path-loss models including the ABG and WABG models. The exhaustive experimentation carried out includes the 5G non-line-of-sight environment in different 5G scenarios, UMiSC, UMiOS and UMa. The proposed EWABG obtains the best accuracy, specially in noisy environments with outliers, reporting negligible increment error rates (with respect to the non-outliers situation), lower than 1%, compared to the ABG and WABG."}
{"id": "2601.12700", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2601.12700", "abs": "https://arxiv.org/abs/2601.12700", "authors": ["Haolin Chen"], "title": "Improving Audio Question Answering with Variational Inference", "comment": "ICASSP 2026", "summary": "Variational inference (VI) provides a principled framework for estimating posterior distributions over model parameters, enabling explicit modeling of weight uncertainty during optimization. By capturing this uncertainty, VI improves the reliability of predictions, yielding better calibrated outputs. In this work, we investigate the benefits of VI for challenging multimodal understanding and reasoning by applying the Improved Variational Online Newton (IVON), a recent VI optimizer, to fine-tuning a multimodal large language model on audio question answering tasks. Our results show that VI not only enhances predictive accuracy but also significantly improves calibration, reducing the model's overconfidence. These advances further support risk-sensitive applications such as selective prediction, where reliable confidence estimates are crucial."}
{"id": "2601.12600", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.12600", "abs": "https://arxiv.org/abs/2601.12600", "authors": ["Pu Wang", "Shinji Watanabe", "Hugo Van hamme"], "title": "SSVD-O: Parameter-Efficient Fine-Tuning with Structured SVD for Speech Recognition", "comment": "Accepted by IEEE ICASSP 2026", "summary": "Parameter-efficient fine-tuning (PEFT) is a scalable approach for adapting large speech foundation models to new domains. While methods such as LoRA and its state-of-the-art variants reduce adaptation costs, they typically allocate parameters uniformly across model subspaces, which limits their efficiency and scalability in speech applications. Building on our prior work, this paper introduces SSVD-Outer (SSVD-O), an extension of the structured SVD-guided (SSVD) fine-tuning method. SSVD-O combines input acoustic feature space-associated inner transformations with output semantic feature space-associated outer transformations to enable scalable and balanced adaptation. We conduct the first systematic analysis of parameter budget allocation across model subspaces in PEFT for automatic speech recognition (ASR), and investigate the trade-off between learning and forgetting under constrained resources. SSVD-O is benchmarked against LoRA, DoRA, PiSSA, and SSVD on domain-shifted ASR tasks, including child speech and regional accents, across model scales from 0.1B to 2B within the ESPnet framework. Experimental results show that SSVD-O consistently narrows the performance gap to full fine-tuning while improving generalization and mitigating catastrophic forgetting."}
{"id": "2601.12171", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.12171", "abs": "https://arxiv.org/abs/2601.12171", "authors": ["Jeffrey W. Utley", "Gregery T. Buzzard", "Charles A. Bouman", "Matthew R. Kemnetz"], "title": "Boiling flow estimation for aero-optic phase screen generation", "comment": "Submitted to the Unconventional Imaging, Sensing, and Adaptive Optics special session of Optical Engineering", "summary": "Aero-optic effects due to turbulence can reduce the effectiveness of transmitting light waves to a distant target. Methods to compensate for turbulence typically rely on realistic turbulence data, which can be generated by i) experiment, ii) high-fidelity CFD, iii) low-fidelity CFD, and iv) autoregressive methods. However, each of these methods has significant drawbacks, including monetary and/or computational expense, limited quantity, inaccurate statistics, and overall complexity. In contrast, the boiling flow algorithm is a simple, computationally efficient model that can generate atmospheric phase screen data with only a handful of parameters. However, boiling flow has not been widely used in aero-optic applications, at least in part because some of these parameters, such as r0, are not clearly defined for aero-optic data. In this paper, we demonstrate a method to use the boiling flow algorithm to generate arbitrary length synthetic data to match the statistics of measured aero-optic data. Importantly, we modify the standard boiling flow method to generate anisotropic phase screens. While this model does not fully capture all statistics, it can be used to generate data that matches the temporal power spectrum or the anisotropic 2D structure function, with the ability to trade fidelity to one for fidelity to the other."}
{"id": "2601.12757", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2601.12757", "abs": "https://arxiv.org/abs/2601.12757", "authors": ["Hui-Peng Du", "Yang Ai", "Xiao-Hang Jiang", "Rui-Chen Zheng", "Zhen-Hua Ling"], "title": "CodeSep: Low-Bitrate Codec-Driven Speech Separation with Base-Token Disentanglement and Auxiliary-Token Serial Prediction", "comment": "Accepted by ICASSP 2026", "summary": "This paper targets a new scenario that integrates speech separation with speech compression, aiming to disentangle multiple speakers while producing discrete representations for efficient transmission or storage, with applications in online meetings and dialogue archiving. To address this scenario, we propose CodeSep, a codec-driven model that jointly performs speech separation and low-bitrate compression. CodeSep comprises a residual vector quantizer (RVQ)-based plain neural speech codec, a base-token disentanglement (BTD) module, and parallel auxiliary-token serial prediction (ATSP) modules. The BTD module disentangles mixed-speech mel-spectrograms into base tokens for each speaker, which are then refined by ATSP modules to serially predict auxiliary tokens, and finally, all tokens are decoded to reconstruct separated waveforms through the codec decoder. During training, the codec's RVQ provides supervision with permutation-invariant and teacher-forcing-based cross-entropy losses. As only base tokens are transmitted or stored, CodeSep achieves low-bitrate compression. Experimental results show that CodeSep attains satisfactory separation performance at only 1 kbps compared with baseline methods."}
{"id": "2601.12660", "categories": ["cs.SD", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.12660", "abs": "https://arxiv.org/abs/2601.12660", "authors": ["Maab Elrashid", "Anthony Deschênes", "Cem Subakan", "Mirco Ravanelli", "Rémi Georges", "Michael Morin"], "title": "Toward Faithful Explanations in Acoustic Anomaly Detection", "comment": "Accepted at the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP) 2026. Code: https://github.com/Maab-Nimir/Faithful-Explanations-in-Acoustic-Anomaly-Detection", "summary": "Interpretability is essential for user trust in real-world anomaly detection applications. However, deep learning models, despite their strong performance, often lack transparency. In this work, we study the interpretability of autoencoder-based models for audio anomaly detection, by comparing a standard autoencoder (AE) with a mask autoencoder (MAE) in terms of detection performance and interpretability. We applied several attribution methods, including error maps, saliency maps, SmoothGrad, Integrated Gradients, GradSHAP, and Grad-CAM. Although MAE shows a slightly lower detection, it consistently provides more faithful and temporally precise explanations, suggesting a better alignment with true anomalies. To assess the relevance of the regions highlighted by the explanation method, we propose a perturbation-based faithfulness metric that replaces them with their reconstructions to simulate normal input. Our findings, based on experiments in a real industrial scenario, highlight the importance of incorporating interpretability into anomaly detection pipelines and show that masked training improves explanation quality without compromising performance."}
{"id": "2601.12278", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.12278", "abs": "https://arxiv.org/abs/2601.12278", "authors": ["Yingquan Li", "Jiajie Xu", "Bodhibrata Mukhopadhyay", "Mohamed-Slim Alouini"], "title": "Low-Complexity RSS-based Underwater Localization with Unknown Transmit Power", "comment": null, "summary": "Underwater wireless sensor networks (UWSNs) have received significant attention due to their various applications, with underwater target localization playing a vital role in enhancing network performance. Given the challenges and high costs associated with UWSN deployments, Received Signal Strength (RSS)-based localization offers a viable solution due to its minimal hardware requirements and cost-effectiveness. In this paper, we assign distance-based weights to RSS measurements, providing higher reliability to closer anchor nodes. Using the weighted RSS measurements and generalized trust region subproblem (GTRS), we propose the GTRS-based localization technique with Unknown Transmit Power (GUTP), which can be solved by a simple bisection method. Unlike conventional localization methods that require prior knowledge of the target node's transmit power, GUTP jointly estimates both the location and transmit power of the target node, broadening its practical use. Additionally, we derive the Cramer-Rao lower bounds (CRLBs) for RSS-based underwater localization with known and unknown transmit power, respectively. Extensive simulations demonstrate that GUTP achieves enhanced accuracy and significantly lower computational complexity in estimating the target node's location and transmit power compared to existing semidefinite programming (SDP)-based techniques."}
{"id": "2601.12769", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2601.12769", "abs": "https://arxiv.org/abs/2601.12769", "authors": ["Fuyuan Feng", "Wenbin Zhang", "Yu Gao", "Longting Xu", "Xiaofeng Mou", "Yi Xu"], "title": "Adaptive Speaker Embedding Self-Augmentation for Personal Voice Activity Detection with Short Enrollment Speech", "comment": "Accepted by ICASSP 2026", "summary": "Personal Voice Activity Detection (PVAD) is crucial for identifying target speaker segments in the mixture, yet its performance heavily depends on the quality of speaker embeddings. A key practical limitation is the short enrollment speech--such as a wake-up word--which provides limited cues. This paper proposes a novel adaptive speaker embedding self-augmentation strategy that enhances PVAD performance by augmenting the original enrollment embeddings through additive fusion of keyframe embeddings extracted from mixed speech. Furthermore, we introduce a long-term adaptation strategy to iteratively refine embeddings during detection, mitigating speaker temporal variability. Experiments show significant gains in recall, precision, and F1-score under short enrollment conditions, matching full-length enrollment performance after five iterative updates. The source code is available at https://anonymous.4open.science/r/ASE-PVAD-E5D6 ."}
{"id": "2601.12752", "categories": ["cs.SD", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.12752", "abs": "https://arxiv.org/abs/2601.12752", "authors": ["Naqcho Ali Mehdi", "Mohammad Adeel", "Aizaz Ali Larik"], "title": "SoundPlot: An Open-Source Framework for Birdsong Acoustic Analysis and Neural Synthesis with Interactive 3D Visualization", "comment": null, "summary": "We present SoundPlot, an open-source framework for analyzing avian vocalizations through acoustic feature extraction, dimensionality reduction, and neural audio synthesis. The system transforms audio signals into a multi-dimensional acoustic feature space, enabling real-time visualization of temporal dynamics in 3D using web-based interactive graphics. Our framework implements a complete analysis-synthesis pipeline that extracts spectral features (centroid, bandwidth, contrast), pitch contours via probabilistic YIN (pYIN), and mel-frequency cepstral coefficients (MFCCs), mapping them to a unified timbre space for visualization. Audio reconstruction employs the Griffin-Lim phase estimation algorithm applied to mel spectrograms. The accompanying Three.js-based interface provides dual-viewport visualization comparing original and synthesized audio trajectories with independent playback controls. We demonstrate the framework's capabilities through comprehensive waveform analysis, spectrogram comparisons, and feature space evaluation using Principal Component Analysis (PCA). Quantitative evaluation shows mel spectrogram correlation scores exceeding 0.92, indicating high-fidelity preservation of perceptual acoustic structure. SoundPlot is released under the MIT License to facilitate research in bioacoustics, audio signal processing, and computational ethology."}
{"id": "2601.12281", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.12281", "abs": "https://arxiv.org/abs/2601.12281", "authors": ["Lingyi Zhu", "Zhongxiang Wei", "Fan Liu", "Jianjun Wu", "Xiao-Wei Tang", "Christos Masouros", "Shanpu Shen"], "title": "Overcoming BS Down-Tilt for Air-Ground ISAC Coverage: Antenna Design, Beamforming and User Scheduling", "comment": null, "summary": "Integrated sensing and communication holds great promise for low-altitude economy applications. However, conventional downtilted base stations primarily provide sectorized forward lobes for ground services, failing to sense air targets due to backward blind zones. In this paper, a novel antenna structure is proposed to enable air-ground beam steering, facilitating simultaneous full-space sensing and communication (S&C). Specifically, instead of inserting a reflector behind the antenna array for backlobe mitigation, an omni-steering plate is introduced to collaborate with the active array for omnidirectional beamforming. Building on this hardware innovation, sum S&C mutual information (MI) is maximized, jointly optimizing user scheduling, passive coefficients of the omni-steering plate, and beamforming of the active array. The problem is decomposed into two subproblems: one for optimizing passive coefficients via Riemannian gradient on the manifold, and the other for optimizing user scheduling and active array beamforming. Exploiting relationships among S&C MI, data decoding MMSE, and parameter estimation MMSE, the original subproblem is equivalently transformed into a sum weighted MMSE problem, rigorously established via the Lagrangian and first-order optimality conditions. Simulations show that the proposed algorithm outperforms baselines in sum-MI and MSE, while providing 360 sensing coverage. Beampattern analysis further demonstrates effective user scheduling and accurate target alignment."}
{"id": "2601.12950", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2601.12950", "abs": "https://arxiv.org/abs/2601.12950", "authors": ["Zining Liang", "Runbang Wang", "Xuzhou Ye", "Qiuqiang Kong"], "title": "ImmersiveFlow: Stereo-to-7.1.4 spatial audio generation with flow matching", "comment": "5 pages, 3 figures, 2 tables", "summary": "Immersive spatial audio has become increasingly critical for applications ranging from AR/VR to home entertainment and automotive sound systems. However, existing generative methods remain constrained to low-dimensional formats such as binaural audio and First-Order Ambisonics (FOA). Binaural rendering is inherently limited to headphone playback, while FOA suffers from spatial aliasing and insufficient resolution for high-frequency. To overcome these limitations, we introduce ImmersiveFlow, the first end-to-end generative framework that directly synthesizes discrete 7.1.4 format spatial audio from stereo input. ImmersiveFlow leverages Flow Matching to learn trajectories from stereo inputs to multichannel spatial features within a pretrained VAE latent space. At inference, the Flow Matching model predicted latent features are decoded by the VAE and converted into the final 7.1.4 waveform. Comprehensive objective and subjective evaluations demonstrate that our method produces perceptually rich sound fields and enhanced externalization, significantly outperforming traditional upmixing techniques. Code implementations and audio samples are provided at: https://github.com/violet-audio/ImmersiveFlow."}
{"id": "2601.12802", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.12802", "abs": "https://arxiv.org/abs/2601.12802", "authors": ["Jihoo Jung", "Ji-Hoon Kim", "Doyeop Kwak", "Junwon Lee", "Juhan Nam", "Joon Son Chung"], "title": "UNMIXX: Untangling Highly Correlated Singing Voices Mixtures", "comment": "Accepted by ICASSP 2026", "summary": "We introduce UNMIXX, a novel framework for multiple singing voices separation (MSVS). While related to speech separation, MSVS faces unique challenges: data scarcity and the highly correlated nature of singing voices mixture. To address these issues, we propose UNMIXX with three key components: (1) musically informed mixing strategy to construct highly correlated, music-like mixtures, (2) cross-source attention that drives representations of two singers apart via reverse attention, and (3) magnitude penalty loss penalizing erroneously assigned interfering energy. UNMIXX not only addresses data scarcity by simulating realistic training data, but also excels at separating highly correlated mixtures through cross-source interactions at both the architectural and loss levels. Our extensive experiments demonstrate that UNMIXX greatly enhances performance, with SDRi gains exceeding 2.2 dB over prior work."}
{"id": "2601.12403", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.12403", "abs": "https://arxiv.org/abs/2601.12403", "authors": ["Shu Cai", "Ya-Feng Liu", "Jun Zhan", "Qi Zhang"], "title": "RIS-Enhanced Information-Decoupled Symbiotic Radio Over Broadcasting Signals", "comment": "Accepted to ICASSP 2026. ©2026 IEEE. Personal use of this material is permitted", "summary": "This paper studies a reconfigurable intelligent surface (RIS)-enhanced decoupled symbiotic radio (SR) system in which a primary transmitter delivers common data to multiple primary receivers (PRs), while a RIS-based backscatter device sends secondary data to a backscatter receiver (BRx). Unlike conventional SR, the BRx performs energy detection and never decodes the primary signal, thereby removing ambiguity and preventing exposure of the primary payload to unintended receivers. In this paper, we formulate the problem as the minimization of the transmit power subject to a common broadcast rate constraint across all PRs and a bit error rate (BER) constraint at the BRx. The problem is nonconvex due to the unit-modulus RIS constraint and coupled quadratic forms. Leveraging a rate-balanced reformulation and a monotonic BER ratio characterization, we develop a low-complexity penalty-based block coordinate descent algorithm with closed-form updates. Numerical results show fast convergence of the proposed algorithm and reduced power consumption of the considered RIS-enhanced information-decoupled SR system over conventional SR baselines."}
{"id": "2601.13055", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2601.13055", "abs": "https://arxiv.org/abs/2601.13055", "authors": ["Leyan Yang", "Ronghui Hu", "Yang Xu", "Jing Lu"], "title": "VoCodec: An Efficient Lightweight Low-Bitrate Speech Codec", "comment": null, "summary": "Recent advancements in end-to-end neural speech codecs enable compressing audio at extremely low bitrates while maintaining high-fidelity reconstruction. Meanwhile, low computational complexity and low latency are crucial for real-time communication. In this paper, we propose VoCodec, a speech codec model featuring a computational complexity of only 349.29M multiply-accumulate operations per second (MACs/s) and a latency of 30 ms. With the competitive vocoder Vocos as its backbone, the proposed model ranked fourth on Track 1 in the 2025 LRAC Challenge and achieved the highest subjective evaluation score (MUSHRA) on the clean speech test set. Additionally, we cascade a lightweight neural network at the front end to extend its capability of speech enhancement. Experimental results demonstrate that the two systems achieve competitive performance across multiple evaluation metrics. Speech samples can be found at https://acceleration123.github.io/."}
{"id": "2601.12961", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2601.12961", "abs": "https://arxiv.org/abs/2601.12961", "authors": ["Shangxuan Luo", "Joshua Reiss"], "title": "Supervised Learning for Game Music Segmentation", "comment": null, "summary": "At present, neural network-based models, including transformers, struggle to generate memorable and readily comprehensible music from unified and repetitive musical material due to a lack of understanding of musical structure. Consequently, these models are rarely employed by the games industry. It is hypothesised by many scholars that the modelling of musical structure may inform models at a higher level, thereby enhancing the quality of music generation. The aim of this study is to explore the performance of supervised learning methods in the task of structural segmentation, which is the initial step in music structure modelling. An audio game music dataset with 309 structural annotations was created to train the proposed method, which combines convolutional neural networks and recurrent neural networks, achieving performance comparable to the state-of-the-art unsupervised learning methods with fewer training resources."}
{"id": "2601.12433", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.12433", "abs": "https://arxiv.org/abs/2601.12433", "authors": ["Amanda Nyholm", "Yessica Arellano", "Jinyu Liu", "Damian Krakowiak", "Pierluigi Salvo Rossi"], "title": "Temporal Data and Short-Time Averages Improve Multiphase Mass Flow Metering", "comment": "9 pages, 6 figures", "summary": "Reliable flow measurements are essential in many industries, but current instruments often fail to accurately estimate multiphase flows, which are frequently encountered in real-world operations. Combining machine learning (ML) algorithms with accurate single-phase flowmeters has therefore received extensive research attention in recent years. The Coriolis mass flowmeter is a widely used single-phase meter that provides direct mass flow measurements, which ML models can be trained to correct, thereby reducing measurement errors in multiphase conditions. This paper demonstrates that preserving temporal information significantly improves model performance in such scenarios. We compare a multilayer perceptron, a windowed multilayer perceptron, and a convolutional neural network (CNN) on three-phase air-water-oil flow data from 342 experiments. Whereas prior work typically compresses each experiment into a single averaged sample, we instead compute short-time averages from within each experiment and train models that preserve temporal information at several downsampling intervals. The CNN performed best at 0.25 Hz with approximately 95 % of relative errors below 13 %, a normalized root mean squared error of 0.03, and a mean absolute percentage error of approximately 4.3 %, clearly outperforming the best single-averaged model and demonstrating that short-time averaging within individual experiments is preferable. Results are consistent across multiple data splits and random seeds, demonstrating robustness."}
{"id": "2601.13107", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2601.13107", "abs": "https://arxiv.org/abs/2601.13107", "authors": ["Carlos Franzreb", "Arnab Das", "Tim Polzehl", "Sebastian Möller"], "title": "Content Leakage in LibriSpeech and Its Impact on the Privacy Evaluation of Speaker Anonymization", "comment": "Accepted to ICASSP 2026", "summary": "Speaker anonymization aims to conceal a speaker's identity, without considering the linguistic content. In this study, we reveal a weakness of Librispeech, the dataset that is commonly used to evaluate anonymizers: the books read by the Librispeech speakers are so distinct, that speakers can be identified by their vocabularies. Even perfect anonymizers cannot prevent this identity leakage. The EdAcc dataset is better in this regard: only a few speakers can be identified through their vocabularies, encouraging the attacker to look elsewhere for the identities of the anonymized speakers. EdAcc also comprises spontaneous speech and more diverse speakers, complementing Librispeech and giving more insights into how anonymizers work."}
{"id": "2601.12966", "categories": ["cs.SD", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.12966", "abs": "https://arxiv.org/abs/2601.12966", "authors": ["Seymanur Akti", "Alexander Waibel"], "title": "Lombard Speech Synthesis for Any Voice with Controllable Style Embeddings", "comment": null, "summary": "The Lombard effect plays a key role in natural communication, particularly in noisy environments or when addressing hearing-impaired listeners. We present a controllable text-to-speech (TTS) system capable of synthesizing Lombard speech for any speaker without requiring explicit Lombard data during training. Our approach leverages style embeddings learned from a large, prosodically diverse dataset and analyzes their correlation with Lombard attributes using principal component analysis (PCA). By shifting the relevant PCA components, we manipulate the style embeddings and incorporate them into our TTS model to generate speech at desired Lombard levels. Evaluations demonstrate that our method preserves naturalness and speaker identity, enhances intelligibility under noise, and provides fine-grained control over prosody, offering a robust solution for controllable Lombard TTS for any speaker."}
{"id": "2601.12482", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.12482", "abs": "https://arxiv.org/abs/2601.12482", "authors": ["Minhua Ding", "Prathapasinghe Dharmawansa", "Italo Atzeni", "Antti Tölli"], "title": "The Effect of Noise Correlation on MMSE Channel Estimation in One-Bit Quantized Systems", "comment": null, "summary": "This paper analyzes the impact of spatially correlated additive noise on the minimum mean-square error (MMSE) estimation of multiple-input multiple-output (MIMO) channels from one-bit quantized observations. Although additive noise can be correlated in practical scenarios, e.g., due to jamming, clutter, or other external disturbances, the effect of such correlation on the MMSE channel estimator in this setting remains unexplored in prior work. Against this backdrop, we derive a novel analytical expression for the general MIMO MMSE channel estimator, which is inherently nonlinear in one-bit observations, and accommodates arbitrary channel and noise correlation structures. To further characterize the impact of noise correlation, we subsequently specialize the general MMSE expression to certain tractable multi antenna configurations in which both the channel and the noise assume single-parameter constant correlation structures. Our analyses reveal nontrivial, noise-correlation-induced scenarios in which the estimator remains linear despite non-zero channel and noise correlation parameters. Moreover, the results indicate that, at low-to-medium signal-to-noise ratio, noise correlation improves the MMSE performance when channels are uncorrelated, but degrades performance when channels are strongly correlated."}
{"id": "2601.13140", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2601.13140", "abs": "https://arxiv.org/abs/2601.13140", "authors": ["Renana Opochinsky", "Sharon Gannot"], "title": "AMDM-SE: Attention-based Multichannel Diffusion Model for Speech Enhancement", "comment": null, "summary": "Diffusion models have recently achieved impressive results in reconstructing images from noisy inputs, and similar ideas have been applied to speech enhancement by treating time-frequency representations as images. With the ubiquity of multi-microphone devices, we extend state-of-the-art diffusion-based methods to exploit multichannel inputs for improved performance. Multichannel diffusion-based enhancement remains in its infancy, with prior work making limited use of advanced mechanisms such as attention for spatial modeling - a gap addressed in this paper. We propose AMDM-SE, an Attention-based Multichannel Diffusion Model for Speech Enhancement, designed specifically for noise reduction. AMDM-SE leverages spatial inter-channel information through a novel cross-channel time-frequency attention block, enabling faithful reconstruction of fine-grained signal details within a generative diffusion framework. On the CHiME-3 benchmark, AMDM-SE outperforms both a single-channel diffusion baseline and a multichannel model without attention, as well as a strong DNN-based predictive method. Simulated-data experiments further underscore the importance of the proposed multichannel attention mechanism. Overall, our results show that incorporating targeted multichannel attention into diffusion models substantially improves noise reduction. While multichannel diffusion-based speech enhancement is still an emerging field, our work contributes a new and complementary approach to the growing body of research in this direction."}
{"id": "2601.13198", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2601.13198", "abs": "https://arxiv.org/abs/2601.13198", "authors": ["Yang Wang", "Yiqi Liu", "Chenghao Xiao", "Chenghua Lin"], "title": "The Achilles' Heel of Angular Margins: A Chebyshev Polynomial Fix for Speaker Verification", "comment": "Accepted for presentation at ICASSP 2026", "summary": "Angular margin losses, such as AAM-Softmax, have become the de facto in speaker and face verification. Their success hinges on directly manipulating the angle between features and class prototypes. However, this manipulation relies on the arccos function to recover the angle, introducing a significant yet overlooked source of training instability. The derivative of arccos explodes at its boundaries, causing gradient peaks during optimisation. Furthermore, the formulation fails to generate a sufficiently sharp gradient for hard-to-classify examples. We address these issues by proposing ChebyAAM, a loss that replaces the arccos operation with its Chebyshev polynomial approximation. This substitution eliminates gradient explosion and applies a stronger corrective signal to hard examples, leading to more effective optimisation. Experiments on three benchmarks (VoxCeleb, SITW, and CN-Celeb) demonstrate that our method resolves the instability and consistently improves performance. Our work suggests that approximating angular operations, rather than calculating them explicitly, offers a more robust path for designing future metric learning losses. Code is available at https://github.com/ExtraOrdinaryLab/vibe."}
{"id": "2601.12562", "categories": ["eess.SP", "eess.SY"], "pdf": "https://arxiv.org/pdf/2601.12562", "abs": "https://arxiv.org/abs/2601.12562", "authors": ["Maaz Qureshi", "Mohammad Omid Bagheri", "Abdelrahman Elbadrawy", "William Melek", "George Shaker"], "title": "Automated Angular Received-Power Characterization of Embedded mmWave Transmitters Using Geometry-Calibrated Spatial Sampling", "comment": null, "summary": "This paper presents an automated measurement methodology for angular received-power characterization of embedded millimeter-wave transmitters using geometry-calibrated spatial sampling. Characterization of integrated mmWave transmitters remains challenging due to limited angular coverage and alignment variability in conventional probe-station techniques, as well as the impracticality of anechoic-chamber testing for platform-mounted active modules. To address these challenges, we introduce RAPTAR, an autonomous measurement system for angular received-power acquisition under realistic installation constraints. A collaborative robot executes geometry-calibrated, collision-aware hemispherical trajectories while carrying a calibrated receive probe, enabling controlled and repeatable spatial positioning around a fixed device under test. A spectrum-analyzer-based receiver chain acquires amplitude-only received power as a function of angle and distance following quasi-static pose stabilization. The proposed framework enables repeatable angular received-power mapping and power-domain comparison against idealized free-space references derived from full-wave simulation. Experimental results for a 60-GHz radar module demonstrate a mean absolute received-power error below 2 dB relative to simulation-derived references and a 36.5 % reduction in error compared to manual probe-station measurements, attributed primarily to reduced alignment variability and consistent spatial sampling. The proposed method eliminates the need for coherent field measurements and near-field transformations, enabling practical power-domain characterization of embedded mmWave modules. It is well suited for angular validation in real-world platforms where conventional anechoic measurements are impractical."}
{"id": "2601.13409", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2601.13409", "abs": "https://arxiv.org/abs/2601.13409", "authors": ["Bo Ren", "Ruchao Fan", "Yelong Shen", "Weizhu Chen", "Jinyu Li"], "title": "RLBR: Reinforcement Learning with Biasing Rewards for Contextual Speech Large Language Models", "comment": "Accepted to the 2026 IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP 2026)", "summary": "Speech large language models (LLMs) have driven significant progress in end-to-end speech understanding and recognition, yet they continue to struggle with accurately recognizing rare words and domain-specific terminology. This paper presents a novel fine-tuning method, Reinforcement Learning with Biasing Rewards (RLBR), which employs a specialized biasing words preferred reward to explicitly emphasize biasing words in the reward calculation. In addition, we introduce reference-aware mechanisms that extend the reinforcement learning algorithm with reference transcription to strengthen the potential trajectory exploration space. Experiments on the LibriSpeech corpus across various biasing list sizes demonstrate that RLBR delivers substantial performance improvements over a strong supervised fine-tuning (SFT) baseline and consistently outperforms several recently published methods. The proposed approach achieves excellent performance on the LibriSpeech test-clean and test-other sets, reaching Biasing Word Error Rates (BWERs) of 0.59% / 2.11%, 1.09% / 3.24%, and 1.36% / 4.04% for biasing list sizes of 100, 500, and 1000, respectively, without compromising the overall WERs."}
{"id": "2601.13513", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.13513", "abs": "https://arxiv.org/abs/2601.13513", "authors": ["Noriyuki Tonami", "Wataru Kohno", "Yoshiyuki Yajima", "Sakiko Mishima", "Yumi Arai", "Reishi Kondo", "Tomoyuki Hino"], "title": "Event Classification by Physics-informed Inpainting for Distributed Multichannel Acoustic Sensor with Partially Degraded Channels", "comment": "Accepted to ICASSP 2026", "summary": "Distributed multichannel acoustic sensing (DMAS) enables large-scale sound event classification (SEC), but performance drops when many channels are degraded and when sensor layouts at test time differ from training layouts. We propose a learning-free, physics-informed inpainting frontend based on reverse time migration (RTM). In this approach, observed multichannel spectrograms are first back-propagated on a 3D grid using an analytic Green's function to form a scene-consistent image, and then forward-projected to reconstruct inpainted signals before log-mel feature extraction and Transformer-based classification. We evaluate the method on ESC-50 with 50 sensors and three layouts (circular, linear, right-angle), where per-channel SNRs are sampled from -30 to 0 dB. Compared with an AST baseline, scaling-sparsemax channel selection, and channel-swap augmentation, the proposed RTM frontend achieves the best or competitive accuracy across all layouts, improving accuracy by 13.1 points on the right-angle layout (from 9.7% to 22.8%). Correlation analyses show that spatial weights align more strongly with SNR than with channel--source distance, and that higher SNR--weight correlation corresponds to higher SEC accuracy. These results demonstrate that a reconstruct-then-project, physics-based preprocessing effectively complements learning-only methods for DMAS under layout-open configurations and severe channel degradation."}
{"id": "2601.12629", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.12629", "abs": "https://arxiv.org/abs/2601.12629", "authors": ["Mohammad Omid Bagheri", "Justin Chow", "Josh Visser", "Veronica Leong", "George Shaker"], "title": "Millimeter-Wave Multi-Radar Tracking System Enabled by a Modified GRIN Luneburg Lens for Real-Time Healthcare Monitoring", "comment": null, "summary": "Multi-beam radar sensing systems are emerging as powerful tools for non-contact motion tracking and vital-sign monitoring in healthcare environments. This paper presents the design and experimental validation of a synchronized millimeter-wave multi-radar tracking system enhanced by a modified spherical gradient-index (GRIN) Luneburg lens. Five commercial FMCW radar modules operating in the 58--63 GHz band are arranged in a semi-circular configuration around the lens, whose tailored refractive-index profile accommodates bistatic radar modules with co-located transmit (TX) and receive (RX) antennas. The resulting architecture generates multiple fixed high-gain beams with improved angular resolution and minimal mutual interference. Each radar operates independently but is temporally synchronized through a centralized Python-based acquisition framework to enable parallel data collection and low-latency motion tracking. A 10-cm-diameter 3D-printed prototype demonstrates a measured gain enhancement of approximately 12 dB for each module, corresponding to a substantial improvement in detection range. Full-wave simulations and measurements confirm effective non-contact, privacy-preserving short-range human-motion detection across five 28-degree sectors, providing 140-degree total angular coverage. Fall-detection experiments further validate reliable wide-angle performance and continuous spatial tracking. The proposed system offers a compact, low-cost, and scalable platform for millimeter-wave sensing in ambient healthcare and smart-environment applications."}
{"id": "2601.13531", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2601.13531", "abs": "https://arxiv.org/abs/2601.13531", "authors": ["Chenda Li", "Wei Wang", "Marvin Sach", "Wangyou Zhang", "Kohei Saijo", "Samuele Cornell", "Yihui Fu", "Zhaoheng Ni", "Tim Fingscheidt", "Shinji Watanabe", "Yanmin Qian"], "title": "ICASSP 2026 URGENT Speech Enhancement Challenge", "comment": "The overview paper of the ICASSP 2026 URGENT Speech Enhancement Challenge", "summary": "The ICASSP 2026 URGENT Challenge advances the series by focusing on universal speech enhancement (SE) systems that handle diverse distortions, domains, and input conditions. This overview paper details the challenge's motivation, task definitions, datasets, baseline systems, evaluation protocols, and results. The challenge is divided into two complementary tracks. Track 1 focuses on universal speech enhancement, while Track 2 introduces speech quality assessment for enhanced speech. The challenge attracted over 80 team registrations, with 29 submitting valid entries, demonstrating significant community interest in robust SE technologies."}
{"id": "2601.13539", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2601.13539", "abs": "https://arxiv.org/abs/2601.13539", "authors": ["Fei Yang", "Xuanfan Ni", "Renyi Yang", "Jiahui Geng", "Qing Li", "Chenyang Lyu", "Yichao Du", "Longyue Wang", "Weihua Luo", "Kaifu Zhang"], "title": "LongSpeech: A Scalable Benchmark for Transcription, Translation and Understanding in Long Speech", "comment": "ICASSP 2026", "summary": "Recent advances in audio-language models have demonstrated remarkable success on short, segment-level speech tasks. However, real-world applications such as meeting transcription, spoken document understanding, and conversational analysis require robust models capable of processing and reasoning over long-form audio. In this work, we present LongSpeech, a large-scale and scalable benchmark specifically designed to evaluate and advance the capabilities of speech models on long-duration audio. LongSpeech comprises over 100,000 speech segments, each approximately 10 minutes long, with rich annotations for ASR, speech translation, summarization, language detection, speaker counting, content separation, and question answering. We introduce a reproducible pipeline for constructing long-form speech benchmarks from diverse sources, enabling future extensions. Our initial experiments with state-of-the-art models reveal significant performance gaps, with models often specializing in one task at the expense of others and struggling with higher-level reasoning. These findings underscore the challenging nature of our benchmark. Our benchmark will be made publicly available to the research community."}
{"id": "2601.12659", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.12659", "abs": "https://arxiv.org/abs/2601.12659", "authors": ["Ruiqi Wang", "Essra M. Ghoura", "Omar Alhussein", "Yuzhi Yang", "Yuhang Sheng", "Jing Ren", "Shizhong Xu", "Sami Muhaidat"], "title": "Two-Layer Reinforcement Learning-Assisted Joint Beamforming and Trajectory Optimization for Multi-UAV Downlink Communications", "comment": null, "summary": "Unmanned aerial vehicles (UAVs) are pivotal for future 6G non-terrestrial networks, yet their high mobility creates a complex coupled optimization problem for beamforming and trajectory design. Existing numerical methods suffer from prohibitive latency, while standard deep learning often ignores dynamic interference topology, limiting scalability. To address these issues, this paper proposes a hierarchically decoupled framework synergizing graph neural networks (GNNs) with multi-agent reinforcement learning. Specifically, on the short timescale, we develop a topology-aware GNN beamformer by incorporating GraphNorm. By modeling the dynamic UAV-user association as a time-varying heterogeneous graph, this method explicitly extracts interference patterns to achieve sub-millisecond inference. On the long timescale, trajectory planning is modeled as a decentralized partially observable Markov decision process and solved via the multi-agent proximal policy optimization algorithm under the centralized training with decentralized execution paradigm, facilitating cooperative behaviors. Extensive simulation results demonstrate that the proposed framework significantly outperforms state-of-the-art optimization heuristics and deep learning baselines in terms of system sum rate, convergence speed, and generalization capability."}
{"id": "2601.13629", "categories": ["eess.AS", "eess.SP"], "pdf": "https://arxiv.org/pdf/2601.13629", "abs": "https://arxiv.org/abs/2601.13629", "authors": ["Ziqian Wang", "Xianjun Xia", "Chuanzeng Huang", "Lei Xie"], "title": "S$^2$Voice: Style-Aware Autoregressive Modeling with Enhanced Conditioning for Singing Style Conversion", "comment": "accepted to ICASSP 2026", "summary": "We present S$^2$Voice, the winning system of the Singing Voice Conversion Challenge (SVCC) 2025 for both the in-domain and zero-shot singing style conversion tracks. Built on the strong two-stage Vevo baseline, S$^2$Voice advances style control and robustness through several contributions. First, we integrate style embeddings into the autoregressive large language model (AR LLM) via a FiLM-style layer-norm conditioning and a style-aware cross-attention for enhanced fine-grained style modeling. Second, we introduce a global speaker embedding into the flow-matching transformer to improve timbre similarity. Third, we curate a large, high-quality singing corpus via an automated pipeline for web harvesting, vocal separation, and transcript refinement. Finally, we employ a multi-stage training strategy combining supervised fine-tuning (SFT) and direct preference optimization (DPO). Subjective listening tests confirm our system's superior performance: leading in style similarity and singer similarity for Task 1, and across naturalness, style similarity, and singer similarity for Task 2. Ablation studies demonstrate the effectiveness of our contributions in enhancing style fidelity, timbre preservation, and generalization. Audio samples are available~\\footnote{https://honee-w.github.io/SVC-Challenge-Demo/}."}
{"id": "2601.13647", "categories": ["cs.SD", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.13647", "abs": "https://arxiv.org/abs/2601.13647", "authors": ["Yumin Kim", "Seonghyeon Go"], "title": "Fusion Segment Transformer: Bi-Directional Attention Guided Fusion Network for AI-Generated Music Detection", "comment": null, "summary": "With the rise of generative AI technology, anyone can now easily create and deploy AI-generated music, which has heightened the need for technical solutions to address copyright and ownership issues. While existing works mainly focused on short-audio, the challenge of full-audio detection, which requires modeling long-term structure and context, remains insufficiently explored. To address this, we propose an improved version of the Segment Transformer, termed the Fusion Segment Transformer. As in our previous work, we extract content embeddings from short music segments using diverse feature extractors. Furthermore, we enhance the architecture for full-audio AI-generated music detection by introducing a Gated Fusion Layer that effectively integrates content and structural information, enabling the capture of long-term context. Experiments on the SONICS and AIME datasets show that our approach outperforms the previous model and recent baselines, achieving state-of-the-art results in AI-generated music detection."}
{"id": "2601.12663", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.12663", "abs": "https://arxiv.org/abs/2601.12663", "authors": ["Yan-Chen Chen", "Wei-Yu Chiu", "Qun-Yu Wang", "Jing-Wei Chen", "Hao-Ting Zhao"], "title": "Energy-Efficient Prediction in Textile Manufacturing: Enhancing Accuracy and Data Efficiency With Ensemble Deep Transfer Learning", "comment": "26 pages, 11 figures", "summary": "Traditional textile factories consume substantial energy, making energy-efficient production optimization crucial for sustainability and cost reduction. Meanwhile, deep neural networks (DNNs), which are effective for factory output prediction and operational optimization, require extensive historical data, posing challenges due to high sensor deployment and data collection costs. To address this, we propose Ensemble Deep Transfer Learning (EDTL), a novel framework that enhances prediction accuracy and data efficiency by integrating transfer learning with an ensemble strategy and a feature alignment layer. EDTL pretrains DNN models on data-rich production lines (source domain) and adapts them to data-limited lines (target domain), reducing dependency on large datasets. Experiments on real-world textile factory datasets show that EDTL improves prediction accuracy by 5.66% and enhances model robustness by 3.96% compared to conventional DNNs, particularly in data-limited scenarios (20%-40% data availability). This research contributes to energy-efficient textile manufacturing by enabling accurate predictions with fewer data requirements, providing a scalable and cost-effective solution for smart production systems."}
{"id": "2601.13849", "categories": ["eess.AS", "cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2601.13849", "abs": "https://arxiv.org/abs/2601.13849", "authors": ["Ziyi Yang", "Li Rao", "Zhengding Luo", "Dongyuan Shi", "Qirui Huang", "Woon-Seng Gan"], "title": "Co-Initialization of Control Filter and Secondary Path via Meta-Learning for Active Noise Control", "comment": null, "summary": "Active noise control (ANC) must adapt quickly when the acoustic environment changes, yet early performance is largely dictated by initialization. We address this with a Model-Agnostic Meta-Learning (MAML) co-initialization that jointly sets the control filter and the secondary-path model for FxLMS-based ANC while keeping the runtime algorithm unchanged. The initializer is pre-trained on a small set of measured paths using short two-phase inner loops that mimic identification followed by residual-noise reduction, and is applied by simply setting the learned initial coefficients. In an online secondary path modeling FxLMS testbed, it yields lower early-stage error, shorter time-to-target, reduced auxiliary-noise energy, and faster recovery after path changes than a baseline without re-initialization. The method provides a simple fast start for feedforward ANC under environment changes, requiring a small set of paths to pre-train."}
{"id": "2601.13679", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2601.13679", "abs": "https://arxiv.org/abs/2601.13679", "authors": ["Sangwon Park", "Dongjun Kim", "Sung-Hoon Byun", "Sangwook Park"], "title": "Ultra-Lightweight Network for Ship-Radiated Sound Classification on Embedded Deployment", "comment": "This manuscript is under review at IEEE Geoscience and Remote Sensing Letters", "summary": "This letter presents ShuffleFAC, a lightweight acoustic model for ship-radiated sound classification in resource-constrained maritime monitoring systems. ShuffleFAC integrates Frequency-Aware convolution into an efficiency-oriented backbone using separable convolution, point-wise group convolution, and channel shuffle, enabling frequency-sensitive feature extraction with low computational cost. Experiments on the DeepShip dataset show that ShuffleFAC achieves competitive performance with substantially reduced complexity. In particular, ShuffleFAC ($γ=16$) attains a macro F1-score of 71.45 $\\pm$ 1.18% using 39K parameters and 3.06M MACs, and achieves an inference latency of 6.05 $\\pm$ 0.95ms on a Raspberry Pi. Compared with MicroNet0, it improves macro F1-score by 1.82 % while reducing model size by 9.7x and latency by 2.5x. These results indicate that ShuffleFAC is suitable for real-time embedded UATR."}
{"id": "2601.12708", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.12708", "abs": "https://arxiv.org/abs/2601.12708", "authors": ["Yuxi Zhao", "Vicente Casares-Giner", "Vicent Pla", "Luis Guijarro", "Iztok Humar", "Yi Zhong", "Xiaohu Ge"], "title": "Energy-Based Cell Association in Nonuniform Renewable Energy-Powered Cellular Networks: Analysis and Optimization of Carbon Efficiency", "comment": null, "summary": "The increasing global push for carbon reduction highlights the importance of integrating renewable energy into the supply chain of cellular networks. However, due to the stochastic nature of renewable energy generation and the uneven load distribution across base stations, the utilization rate of renewable energy remains low. To address these challenges, this paper investigates the trade-off between carbon emissions and downlink throughput in cellular networks, offering insights into optimizing both network performance and sustainability. The renewable energy state of base station batteries and the number of occupied channels are modeled as a quasi-birth-death process. We construct models for the probability of channel blocking, average successful transmission probability for users, downlink throughput, carbon emissions, and carbon efficiency based on stochastic geometry. Based on these analyses, an energy-based cell association scheme is proposed to optimize the carbon efficiency of cellular networks. The results show that, compared to the closest cell association scheme, the energy-based cell association scheme is capable of reducing the carbon emissions of the network by 13.0% and improving the carbon efficiency by 11.3%."}
{"id": "2601.13910", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2601.13910", "abs": "https://arxiv.org/abs/2601.13910", "authors": ["Changhao Pan", "Dongyu Yao", "Yu Zhang", "Wenxiang Guo", "Jingyu Lu", "Zhiyuan Zhu", "Zhou Zhao"], "title": "Synthetic Singers: A Review of Deep-Learning-based Singing Voice Synthesis Approaches", "comment": "Accepetd by IJCNLP-AACL 2025(Oral)", "summary": "Recent advances in singing voice synthesis (SVS) have attracted substantial attention from both academia and industry. With the advent of large language models and novel generative paradigms, producing controllable, high-fidelity singing voices has become an attainable goal. Yet the field still lacks a comprehensive survey that systematically analyzes deep-learning-based singing voice synthesis systems and their enabling technologies. To address the aforementioned issue, this survey first categorizes existing systems by task type and then organizes current architectures into two major paradigms: cascaded and end-to-end approaches. Moreover, we provide an in-depth analysis of core technologies, covering singing modeling and control techniques. Finally, we review relevant datasets, annotation tools, and evaluation benchmarks that support training and assessment. In appendix, we introduce training strategies and further discussion of SVS. This survey provides an up-to-date review of the literature on SVS models, which would be a useful reference for both researchers and engineers. Related materials are available at https://github.com/David-Pigeon/SyntheticSingers."}
{"id": "2601.13700", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2601.13700", "abs": "https://arxiv.org/abs/2601.13700", "authors": ["Jianing Yang", "Wataru Nakata", "Yuki Saito", "Hiroshi Saruwatari"], "title": "DistilMOS: Layer-Wise Self-Distillation For Self-Supervised Learning Model-Based MOS Prediction", "comment": "Accepted to ICASSP 2026", "summary": "With the advancement of self-supervised learning (SSL), fine-tuning pretrained SSL models for mean opinion score (MOS) prediction has achieved state-of-the-art performance. However, during fine-tuning, these SSL-based MOS prediction models often suffer from catastrophic forgetting of the pretrained knowledge and tend to overfit the training set, resulting in poor generalization performance. In this study, we propose DistilMOS, a novel method that learns to predict not only MOS but also token IDs obtained by clustering the hidden representations of each layer in the pretrained SSL model. These layer-wise token targets serve as self-distillation signals that enables the MOS prediction model to extract rich internal knowledge from SSL models, enhancing both prediction accuracy and generalization capability. Experimental evaluations demonstrate that our method significantly outperforms standard SSL-based MOS prediction models on both in-domain and out-of-domain evaluations, verifying the effectiveness and practicality of the proposed method."}
{"id": "2601.12725", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.12725", "abs": "https://arxiv.org/abs/2601.12725", "authors": ["Chaedam Son", "Si-Hyeon Lee"], "title": "Robust Beamforming and Time Allocation for Time-Division Cell-Free Near-Field ISAC", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "In this paper, we propose a time-division near-field integrated sensing and communication (ISAC) framework for cell-free multiple-input multiple-output (MIMO), where sensing and downlink communication are separated in time. During the sensing phase, user locations are estimated and used to construct location-aware channels, which are then exploited in the subsequent communication phase. By explicitly modeling the coupling between sensing-induced localization errors and channel-estimation errors, we capture the tradeoff between sensing accuracy and communication throughput. Based on this model, we jointly optimize the time-allocation ratio, sensing covariance matrix, and robust downlink beamforming under imperfect channel state information (CSI). The resulting non-convex problem is addressed via a semidefinite programming (SDP)-based reformulation within an alternating-optimization framework. To further reduce computational complexity, we also propose two low-complexity suboptimal designs: an error-ignorant scheme and a maximum ratio transmission (MRT)-based scheme. Simulation results show that the proposed scheme significantly improves localization accuracy over far-field and monostatic setups, thereby reducing channel estimation errors and ultimately enhancing the achievable rate. Moreover, the error-ignorant scheme performs well under stringent sensing requirements, whereas the MRT-based scheme remains robust over a wide range of sensing requirements by adapting the time-allocation ratio, albeit with some beamforming loss."}
{"id": "2601.13948", "categories": ["eess.AS", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.13948", "abs": "https://arxiv.org/abs/2601.13948", "authors": ["Nikita Kuzmin", "Songting Liu", "Kong Aik Lee", "Eng Siong Chng"], "title": "Stream-Voice-Anon: Enhancing Utility of Real-Time Speaker Anonymization via Neural Audio Codec and Language Models", "comment": "Accepted by ICASSP2026", "summary": "Protecting speaker identity is crucial for online voice applications, yet streaming speaker anonymization (SA) remains underexplored. Recent research has demonstrated that neural audio codec (NAC) provides superior speaker feature disentanglement and linguistic fidelity. NAC can also be used with causal language models (LM) to enhance linguistic fidelity and prompt control for streaming tasks. However, existing NAC-based online LM systems are designed for voice conversion (VC) rather than anonymization, lacking the techniques required for privacy protection. Building on these advances, we present Stream-Voice-Anon, which adapts modern causal LM-based NAC architectures specifically for streaming SA by integrating anonymization techniques. Our anonymization approach incorporates pseudo-speaker representation sampling, a speaker embedding mixing and diverse prompt selection strategies for LM conditioning that leverage the disentanglement properties of quantized content codes to prevent speaker information leakage. Additionally, we compare dynamic and fixed delay configurations to explore latency-privacy trade-offs in real-time scenarios. Under the VoicePrivacy 2024 Challenge protocol, Stream-Voice-Anon achieves substantial improvements in intelligibility (up to 46% relative WER reduction) and emotion preservation (up to 28% UAR relative) compared to the previous state-of-the-art streaming method DarkStream while maintaining comparable latency (180ms vs 200ms) and privacy protection against lazy-informed attackers, though showing 15% relative degradation against semi-informed attackers."}
{"id": "2601.13704", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.13704", "abs": "https://arxiv.org/abs/2601.13704", "authors": ["Esteban Gómez", "Tom Bäckström"], "title": "Performance and Complexity Trade-off Optimization of Speech Models During Training", "comment": null, "summary": "In speech machine learning, neural network models are typically designed by choosing an architecture with fixed layer sizes and structure. These models are then trained to maximize performance on metrics aligned with the task's objective. While the overall architecture is usually guided by prior knowledge of the task, the sizes of individual layers are often chosen heuristically. However, this approach does not guarantee an optimal trade-off between performance and computational complexity; consequently, post hoc methods such as weight quantization or model pruning are typically employed to reduce computational cost. This occurs because stochastic gradient descent (SGD) methods can only optimize differentiable functions, while factors influencing computational complexity, such as layer sizes and floating-point operations per second (FLOP/s), are non-differentiable and require modifying the model structure during training. We propose a reparameterization technique based on feature noise injection that enables joint optimization of performance and computational complexity during training using SGD-based methods. Unlike traditional pruning methods, our approach allows the model size to be dynamically optimized for a target performance-complexity trade-off, without relying on heuristic criteria to select which weights or structures to remove. We demonstrate the effectiveness of our method through three case studies, including a synthetic example and two practical real-world applications: voice activity detection and audio anti-spoofing. The code related to our work is publicly available to encourage further research."}
{"id": "2601.12788", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.12788", "abs": "https://arxiv.org/abs/2601.12788", "authors": ["Kaihe Wang", "Ran Yang", "Lipeng Zhu", "Rongyan Xi", "Yue Xiu", "Zhongpei Zhang"], "title": "Movable Antenna Enhanced MIMO Communications with Spatial Modulation", "comment": null, "summary": "Movable antenna (MA) has demonstrated great potential in enhancing wireless communication performance. In this paper, we investigate an MA-enabled multiple-input multiple-output (MIMO) communication system with spatial modulation (SM), which improves communication performance by utilizing flexible MA placement while reducing the cost of RF chains. To this end, we propose a joint transceiver design framework aimed at minimizing the bit error rate (BER) based on the maximum minimum distance (MMD) criterion. To address the intractable problem, we develop an efficient iterative algorithm based on alternating optimization (AO) and successive convex approximation (SCA) techniques. Simulation results demonstrate that the proposed algorithm achieves rapid convergence performance and significantly outperforms the existing benchmark schemes."}
{"id": "2601.13999", "categories": ["eess.AS", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.13999", "abs": "https://arxiv.org/abs/2601.13999", "authors": ["Youngmoon Jung", "Joon-Young Yang", "Ju-ho Kim", "Jaeyoung Roh", "Chang Woo Han", "Hoon-Young Cho"], "title": "DAME: Duration-Aware Matryoshka Embedding for Duration-Robust Speaker Verification", "comment": "5 pages, 2 figures, Accepted at ICASSP 2026", "summary": "Short-utterance speaker verification remains challenging due to limited speaker-discriminative cues in short speech segments. While existing methods focus on enhancing speaker encoders, the embedding learning strategy still forces a single fixed-dimensional representation reused for utterances of any length, leaving capacity misaligned with the information available at different durations. We propose Duration-Aware Matryoshka Embedding (DAME), a model-agnostic framework that builds a nested hierarchy of sub-embeddings aligned to utterance durations: lower-dimensional representations capture compact speaker traits from short utterances, while higher dimensions encode richer details from longer speech. DAME supports both training from scratch and fine-tuning, and serves as a direct alternative to conventional large-margin fine-tuning, consistently improving performance across durations. On the VoxCeleb1-O/E/H and VOiCES evaluation sets, DAME consistently reduces the equal error rate on 1-s and other short-duration trials, while maintaining full-length performance with no additional inference cost. These gains generalize across various speaker encoder architectures under both general training and fine-tuning setups."}
{"id": "2601.13758", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2601.13758", "abs": "https://arxiv.org/abs/2601.13758", "authors": ["Lingling Dai", "Andong Li", "Cheng Chi", "Yifan Liang", "Xiaodong Li", "Chengshi Zheng"], "title": "GOMPSNR: Reflourish the Signal-to-Noise Ratio Metric for Audio Generation Tasks", "comment": "Accepted by AAAI 2026", "summary": "In the field of audio generation, signal-to-noise ratio (SNR) has long served as an objective metric for evaluating audio quality. Nevertheless, recent studies have shown that SNR and its variants are not always highly correlated with human perception, prompting us to raise the questions: Why does SNR fail in measuring audio quality? And how to improve its reliability as an objective metric? In this paper, we identify the inadequate measurement of phase distance as a pivotal factor and propose to reformulate SNR with specially designed phase-distance terms, yielding an improved metric named GOMPSNR. We further extend the newly proposed formulation to derive two novel categories of loss function, corresponding to magnitude-guided phase refinement and joint magnitude-phase optimization, respectively. Besides, extensive experiments are conducted for an optimal combination of different loss functions. Experimental results on advanced neural vocoders demonstrate that our proposed GOMPSNR exhibits more reliable error measurement than SNR. Meanwhile, our proposed loss functions yield substantial improvements in model performance, and our wellchosen combination of different loss functions further optimizes the overall model capability."}
{"id": "2601.12827", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.12827", "abs": "https://arxiv.org/abs/2601.12827", "authors": ["Haotian Wang", "Dan Wang", "Xiaodong Xu", "Chuan Huang", "Hao Chen", "Nan Ma"], "title": "Integrated Sensing and Semantic Communication with Adaptive Source-Channel Coding", "comment": null, "summary": "Semantic communication has emerged as a new paradigm to facilitate the performance of integrated sensing and communication systems in 6G. However, most of the existing works mainly focus on sensing data compression to reduce the subsequent communication overheads, without considering the integrated transmission framework for both the SemCom and sensing tasks. This paper proposes an adaptive source-channel coding and beamforming design framework for integrated sensing and SemCom systems by jointly optimizing the coding rate for SemCom task and the transmit beamforming for both the SemCom and sensing tasks. Specifically, an end-to-end semantic distortion function is approximated by deriving an upper bound composing of source and channel coding induced components, and then a hybrid Cramér-Rao bound (HCRB) is also derived for target position under imperfect time synchronization. To facilitate the joint optimization, a distortion minimization problem is formulated by considering the HCRB threshold, channel uses, and power budget. Subsequently, an alternative optimization algorithm composed of successive convex approximation and fractional programming is proposed to address this problem by decoupling it into two subproblems for coding rate and beamforming designs, respectively. Simulation results demonstrate that our proposed scheme outperforms the conventional deep joint source-channel coding -water filling-zero forcing benchmark."}
{"id": "2601.14012", "categories": ["eess.AS", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.14012", "abs": "https://arxiv.org/abs/2601.14012", "authors": ["Youngmoon Jung", "Myunghun Jung", "Joon-Young Yang", "Yong-Hyeok Lee", "Jaeyoung Roh", "Hoon-Young Cho"], "title": "MATE: Matryoshka Audio-Text Embeddings for Open-Vocabulary Keyword Spotting", "comment": "5 pages, 1 figure, Accepted at ICASSP 2026", "summary": "Open-vocabulary keyword spotting (KWS) with text-based enrollment has emerged as a flexible alternative to fixed-phrase triggers. Prior utterance-level matching methods, from an embedding-learning standpoint, learn embeddings at a single fixed dimensionality. We depart from this design and propose Matryoshka Audio-Text Embeddings (MATE), a dual-encoder framework that encodes multiple embedding granularities within a single vector via nested sub-embeddings (\"prefixes\"). Specifically, we introduce a PCA-guided prefix alignment: PCA-compressed versions of the full text embedding for each prefix size serve as teacher targets to align both audio and text prefixes. This alignment concentrates salient keyword cues in lower-dimensional prefixes, while higher dimensions add detail. MATE is trained with standard deep metric learning objectives for audio-text KWS, and is loss-agnostic. To our knowledge, this is the first application of matryoshka-style embeddings to KWS, achieving state-of-the-art results on WSJ and LibriPhrase without any inference overhead."}
{"id": "2601.13847", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2601.13847", "abs": "https://arxiv.org/abs/2601.13847", "authors": ["Jinhua Zhang", "Zhenqi Jia", "Rui Liu"], "title": "Emotion and Acoustics Should Agree: Cross-Level Inconsistency Analysis for Audio Deepfake Detection", "comment": "Accepted by ICASSP 2026", "summary": "Audio Deepfake Detection (ADD) aims to detect spoof speech from bonafide speech. Most prior studies assume that stronger correlations within or across acoustic and emotional features imply authenticity, and thus focus on enhancing or measuring such correlations. However, existing methods often treat acoustic and emotional features in isolation or rely on correlation metrics, which overlook subtle desynchronization between them and smooth out abrupt discontinuities. To address these issues, we propose EAI-ADD, which treats cross level emotion acoustic inconsistency as the primary detection signal. We first project emotional and acoustic representations into a comparable space. Then we progressively integrate frame level and utterance level emotion features with acoustic features to capture cross level emotion acoustic inconsistencies across different temporal granularities. Experimental results on the ASVspoof 2019LA and 2021LA datasets demonstrate that the proposed EAI-ADD outperforms baselines, providing a more effective solution for audio anti spoofing detection."}
{"id": "2601.12867", "categories": ["eess.SP", "cs.IT"], "pdf": "https://arxiv.org/pdf/2601.12867", "abs": "https://arxiv.org/abs/2601.12867", "authors": ["Zixiang Han", "Hanning Wang", "Shiwen Tang", "Yujie Zhang"], "title": "Angular Sensing by Highly Reconfigurable Pixel Antennas with Joint Radiating Aperture and Feeding Ports Reconfiguration", "comment": null, "summary": "Angular sensing capability is realized using highly reconfigurable pixel antenna (HRPA) with joint radiating aperture and feeding ports reconfiguration. Pixel antennas represent a general class of reconfigurable antenna designs in which the radiating surface, regardless of its shape or size, is divided into sub-wavelength elements called pixels. Each pixel is connected to its neighboring elements through radio frequency switches. By controlling pixel connections, the pixel antenna topology can be flexibly adjusted so that the resulting radiation pattern can be reconfigured. However, conventional pixel antennas have only a single, fixed-position feeding port, which is not efficient for angular sensing. Therefore, in this work, we further extend the reconfigurability of pixel antennas by introducing the HRPA, which enables both geometry control of the pixel antenna and switching of its feeding ports. The model of the proposed HRPA, including both circuit and radiation parameters, is derived. A codebook is then defined, consisting of pixel connection states and feeding port positions for each sensing area. Based on this codebook, an efficient optimization approach is developed to minimize the Cram\\acute{\\mathrm{\\mathbf{e}}}r-Rao lower bound (CRLB) and obtain the optimal HRPA geometries for angular sensing within a given area. Numerical results show that the HRPA reduces the angle estimation error by more than 50% across the full three-dimensional sphere when compared with a conventional uniform planar array of the same size. This demonstrates the effectiveness of the proposed approach and highlights the potential of HRPA for integrated sensing and communication systems."}
{"id": "2601.11027", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.11027", "abs": "https://arxiv.org/abs/2601.11027", "authors": ["Chengyou Wang", "Mingchen Shao", "Jingbin Hu", "Zeyu Zhu", "Hongfei Xue", "Bingshen Mu", "Xin Xu", "Xingyi Duan", "Binbin Zhang", "Pengcheng Zhu", "Chuang Ding", "Xiaojun Zhang", "Hui Bu", "Lei Xie"], "title": "WenetSpeech-Wu: Datasets, Benchmarks, and Models for a Unified Chinese Wu Dialect Speech Processing Ecosystem", "comment": null, "summary": "Speech processing for low-resource dialects remains a fundamental challenge in developing inclusive and robust speech technologies. Despite its linguistic significance and large speaker population, the Wu dialect of Chinese has long been hindered by the lack of large-scale speech data, standardized evaluation benchmarks, and publicly available models. In this work, we present WenetSpeech-Wu, the first large-scale, multi-dimensionally annotated open-source speech corpus for the Wu dialect, comprising approximately 8,000 hours of diverse speech data. Building upon this dataset, we introduce WenetSpeech-Wu-Bench, the first standardized and publicly accessible benchmark for systematic evaluation of Wu dialect speech processing, covering automatic speech recognition (ASR), Wu-to-Mandarin translation, speaker attribute prediction, speech emotion recognition, text-to-speech (TTS) synthesis, and instruction-following TTS (instruct TTS). Furthermore, we release a suite of strong open-source models trained on WenetSpeech-Wu, establishing competitive performance across multiple tasks and empirically validating the effectiveness of the proposed dataset. Together, these contributions lay the foundation for a comprehensive Wu dialect speech processing ecosystem, and we open-source proposed datasets, benchmarks, and models to support future research on dialectal speech intelligence."}
{"id": "2601.13931", "categories": ["cs.SD", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.13931", "abs": "https://arxiv.org/abs/2601.13931", "authors": ["Yannis Vasilakis", "Rachel Bittner", "Johan Pauwels"], "title": "Towards Effective Negation Modeling in Joint Audio-Text Models for Music", "comment": "Accepted at IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP) 2026", "summary": "Joint audio-text models are widely used for music retrieval, yet they struggle with semantic phenomena such as negation. Negation is fundamental for distinguishing the absence (or presence) of musical elements (e.g., \"with vocals\" vs. \"without vocals\"), but current systems fail to represent this reliably. In this work, we investigate and mitigate this limitation by training CLAP models from scratch on the Million Song Dataset with LP-MusicCaps-MSD captions. We introduce negation through text augmentation and a dissimilarity-based contrastive loss, designed to explicitly separate original and negated captions in the joint embedding space. To evaluate progress, we propose two protocols that frame negation modeling as retrieval and binary classification tasks. Experiments demonstrate that both methods, individually and combined, improve negation handling while largely preserving retrieval performance."}
{"id": "2601.12924", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.12924", "abs": "https://arxiv.org/abs/2601.12924", "authors": ["Ruopeng Xu", "Songling Zhang", "Zhaohui Yang", "Mingzhe Chen", "Zhaoyang Zhang", "Kai-Kit Wong"], "title": "Fluid Antenna Relay (FAR)-assisted Communication with Hybrid Relaying Scheme Selection", "comment": null, "summary": "In this paper, we investigate a fluid antenna relay (FAR)-assisted communication system with hybrid relaying scheme selection. By leveraging statistical channel state information (CSI) and distribution characteristics of fluid antenna system (FAS), we approximate the outage probability (OP) with different relaying schemes utilizing a Gaussian copula-based method. Each relay node follows the OP-minimized principle to choose the forwarding schemes. To reduce self-interference and avoid multi-user interference, half-duplex relays and frequency division multiple access schemes are considered, respectively. On this basis, we formulate a sum-rate maximization problem to mitigate the rate loss introduced by the half-duplex mode. To solve this problem, we first transform the original nonconvex problem into a power control optimization problem by obtaining the closed form of bandwidth allocation and substituting it into the original problem. Then, we solve the power control optimization problem with a low complexity method. Simulation results verify the effectiveness of our proposed algorithm to improve the sum rate of the system."}
{"id": "2601.12205", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.12205", "abs": "https://arxiv.org/abs/2601.12205", "authors": ["Shih-Heng Wang", "Jiatong Shi", "Jinchuan Tian", "Haibin Wu", "Shinji Watanabe"], "title": "Do Neural Codecs Generalize? A Controlled Study Across Unseen Languages and Non-Speech Tasks", "comment": null, "summary": "This paper investigates three crucial yet underexplored aspects of the generalization capabilities of neural audio codecs (NACs): (i) whether NACs can generalize to unseen languages during pre-training, (ii) whether speech-only pre-trained NACs can effectively generalize to non-speech applications such as environmental sounds, music, and animal vocalizations, and (iii) whether incorporating non-speech data during pre-training can improve performance on both speech and non-speech tasks. Existing studies typically rely on off-the-shelf NACs for comparison, which limits insight due to variations in implementation. In this work, we train NACs from scratch using strictly controlled configurations and carefully curated pre-training data to enable fair comparisons. We conduct a comprehensive evaluation of NAC performance on both signal reconstruction quality and downstream applications using 11 metrics. Our results show that NACs can generalize to unseen languages during pre-training, speech-only pre-trained NACs exhibit degraded performance on non-speech tasks, and incorporating non-speech data during pre-training improves performance on non-speech tasks while maintaining comparable performance on speech tasks."}
{"id": "2601.14157", "categories": ["cs.SD", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.14157", "abs": "https://arxiv.org/abs/2601.14157", "authors": ["Bruno Sienkiewicz", "Łukasz Neumann", "Mateusz Modrzejewski"], "title": "ConceptCaps -- a Distilled Concept Dataset for Interpretability in Music Models", "comment": null, "summary": "Concept-based interpretability methods like TCAV require clean, well-separated positive and negative examples for each concept. Existing music datasets lack this structure: tags are sparse, noisy, or ill-defined. We introduce ConceptCaps, a dataset of 23k music-caption-audio triplets with explicit labels from a 200-attribute taxonomy. Our pipeline separates semantic modeling from text generation: a VAE learns plausible attribute co-occurrence patterns, a fine-tuned LLM converts attribute lists into professional descriptions, and MusicGen synthesizes corresponding audio. This separation improves coherence and controllability over end-to-end approaches. We validate the dataset through audio-text alignment (CLAP), linguistic quality metrics (BERTScore, MAUVE), and TCAV analysis confirming that concept probes recover musically meaningful patterns. Dataset and code are available online."}
{"id": "2601.12963", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.12963", "abs": "https://arxiv.org/abs/2601.12963", "authors": ["Mauro Marchese", "Musa Furkan Keskin", "Pietro Savazzi", "Henk Wymeersch"], "title": "Monostatic ISAC Without Full Buffers: Revisiting Spatial Trade-Offs Under Bursty Traffic", "comment": "Presented at 6th IEEE International Symposium on Joint Communication and Sensing (JC&S) 2026", "summary": "This work investigates the spatial trade-offs arising from the design of the transmit beamformer in a monostatic integrated sensing and communication (ISAC) base station (BS) under bursty traffic, a crucial aspect necessitated by the integration of communication and sensing functionalities in next-generation wireless systems. In this setting, the BS does not always have data available for transmission. This study compares different ISAC policies and reveals the presence of multiple effects influencing ISAC performance: signal-to-noise ratio (SNR) boosting of data-aided strategies compared to pilot-based ones, saturation of the probability of detection in data-aided strategies due to the non-full-buffer assumption, and, finally, directional masking of sensing targets due to the relative position between target and user. Simulation results demonstrate varying impact of these effects on ISAC trade-offs under different operating conditions, thus guiding the design of efficient ISAC transmission strategies."}
{"id": "2601.12222", "categories": ["cs.SD", "cs.MM", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.12222", "abs": "https://arxiv.org/abs/2601.12222", "authors": ["Yishan Lv", "Jing Luo", "Boyuan Ju", "Yang Zhang", "Xinda Wu", "Bo Yuan", "Xinyu Yang"], "title": "Song Aesthetics Evaluation with Multi-Stem Attention and Hierarchical Uncertainty Modeling", "comment": null, "summary": "Music generative artificial intelligence (AI) is rapidly expanding music content, necessitating automated song aesthetics evaluation. However, existing studies largely focus on speech, audio or singing quality, leaving song aesthetics underexplored. Moreover, conventional approaches often predict a precise Mean Opinion Score (MOS) value directly, which struggles to capture the nuances of human perception in song aesthetics evaluation. This paper proposes a song-oriented aesthetics evaluation framework, featuring two novel modules: 1) Multi-Stem Attention Fusion (MSAF) builds bidirectional cross-attention between mixture-vocal and mixture-accompaniment pairs, fusing them to capture complex musical features; 2) Hierarchical Granularity-Aware Interval Aggregation (HiGIA) learns multi-granularity score probability distributions, aggregates them into a score interval, and applies a regression within the interval to produce the final score. We evaluated on two datasets of full-length songs: SongEval dataset (AI-generated) and an internal aesthetics dataset (human-created), and compared with two state-of-the-art (SOTA) models. Results show that the proposed method achieves stronger performance for multi-dimensional song aesthetics evaluation."}
{"id": "2601.14227", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2601.14227", "abs": "https://arxiv.org/abs/2601.14227", "authors": ["Theodore Aptekarev", "Vladimir Sokolovsky", "Gregory Furman"], "title": "Transformer Architectures for Respiratory Sound Analysis and Multimodal Diagnosis", "comment": "7 pages, 4 figures", "summary": "Respiratory sound analysis is a crucial tool for screening asthma and other pulmonary pathologies, yet traditional auscultation remains subjective and experience-dependent. Our prior research established a CNN baseline using DenseNet201, which demonstrated high sensitivity in classifying respiratory sounds. In this work, we (i) adapt the Audio Spectrogram Transformer (AST) for respiratory sound analysis and (ii) evaluate a multimodal Vision-Language Model (VLM) that integrates spectrograms with structured patient metadata.\n  AST is initialized from publicly available weights and fine-tuned on a medical dataset containing hundreds of recordings per diagnosis. The VLM experiment uses a compact Moondream-type model that processes spectrogram images alongside a structured text prompt (sex, age, recording site) to output a JSON-formatted diagnosis. Results indicate that AST achieves approximately 97% accuracy with an F1-score around 97% and ROC AUC of 0.98 for asthma detection, significantly outperforming both the internal CNN baseline and typical external benchmarks. The VLM reaches 86-87% accuracy, performing comparably to the CNN baseline while demonstrating the capability to integrate clinical context into the inference process. These results confirm the effectiveness of self-attention for acoustic screening and highlight the potential of multimodal architectures for holistic diagnostic tools."}
{"id": "2601.12970", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.12970", "abs": "https://arxiv.org/abs/2601.12970", "authors": ["Mauro Marchese", "Musa Furkan Keskin", "Henk Wymeersch", "Pietro Savazzi"], "title": "6G OFDM Communications with High Mobility Transceivers and Scatterers via Angle-Domain Processing and Deep Learning", "comment": "Accepted for presentation at IEEE International Conference on Communications (ICC) 2026", "summary": "High-mobility communications, which are crucial for next-generation wireless systems, cause the orthogonal frequency division multiplexing (OFDM) waveform to suffer from strong intercarrier interference (ICI) due to the Doppler effect. In this work, we propose a novel receiver architecture for OFDM that leverages the angular domain to separate multipaths. A block-type pilot is sent to estimate direction-of-arrivals (DoAs), propagation delays, and channel gains of the multipaths. Subsequently, a decision-directed (DD) approach is employed to estimate and iteratively refine the Dopplers. Two different approaches are investigated to provide initial Doppler estimates: an error vector magnitude (EVM)-based method and a deep learning (DL)-based method. Simulation results reveal that the DL-based approach allows for constant bit error rate (BER) performance up to the maximum 6G speed of 1000 km/h."}
{"id": "2601.12254", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.12254", "abs": "https://arxiv.org/abs/2601.12254", "authors": ["Kazuki Yamauchi", "Masato Murata", "Shogo Seki"], "title": "Confidence-based Filtering for Speech Dataset Curation with Generative Speech Enhancement Using Discrete Tokens", "comment": "Accepted for ICASSP 2026", "summary": "Generative speech enhancement (GSE) models show great promise in producing high-quality clean speech from noisy inputs, enabling applications such as curating noisy text-to-speech (TTS) datasets into high-quality ones. However, GSE models are prone to hallucination errors, such as phoneme omissions and speaker inconsistency, which conventional error filtering based on non-intrusive speech quality metrics often fails to detect. To address this issue, we propose a non-intrusive method for filtering hallucination errors from discrete token-based GSE models. Our method leverages the log-probabilities of generated tokens as confidence scores to detect potential errors. Experimental results show that the confidence scores strongly correlate with a suite of intrusive SE metrics, and that our method effectively identifies hallucination errors missed by conventional filtering methods. Furthermore, we demonstrate the practical utility of our method: curating an in-the-wild TTS dataset with our confidence-based filtering improves the performance of subsequently trained TTS models."}
{"id": "2601.11768", "categories": ["eess.AS", "cs.AI", "cs.LG", "cs.SD", "eess.SP"], "pdf": "https://arxiv.org/pdf/2601.11768", "abs": "https://arxiv.org/abs/2601.11768", "authors": ["Venkat Suprabath Bitra", "Homayoon Beigi"], "title": "Lightweight Self-Supervised Detection of Fundamental Frequency and Accurate Probability of Voicing in Monophonic Music", "comment": "12 pages, 6 figures, 3 tables, and an appendix, Accepted for publication at ICPRAM 2026 in Marbella, Spain, on March 2, 2026", "summary": "Reliable fundamental frequency (F 0) and voicing estimation is essential for neural synthesis, yet many pitch extractors depend on large labeled corpora and degrade under realistic recording artifacts. We propose a lightweight, fully self-supervised framework for joint F 0 estimation and voicing inference, designed for rapid single-instrument training from limited audio. Using transposition-equivariant learning on CQT features, we introduce an EM-style iterative reweighting scheme that uses Shift Cross-Entropy (SCE) consistency as a reliability signal to suppress uninformative noisy/unvoiced frames. The resulting weights provide confidence scores that enable pseudo-labeling for a separate lightweight voicing classifier without manual annotations. Trained on MedleyDB and evaluated on MDB-stem-synth ground truth, our method achieves competitive cross-corpus performance (RPA 95.84, RCA 96.24) and demonstrates cross-instrument generalization."}
{"id": "2601.12982", "categories": ["eess.SP", "cs.NI"], "pdf": "https://arxiv.org/pdf/2601.12982", "abs": "https://arxiv.org/abs/2601.12982", "authors": ["Alexandros I. Papadopoulos", "Maria Anna Pistela", "Dimitrios Tyrovolas", "Antonios Lalas", "Konstantinos Votis", "Sotiris Ioannidis", "George K. Karagiannidis", "Christos Liaskos"], "title": "Physics-Aware RIS Codebook Compilation for Near-Field Beam Focusing under Mutual Coupling and Specular Reflections", "comment": "Accepted for presentation in IEEE International Conference on Communications (IEEE ICC 2026)", "summary": "Next-generation wireless networks are envisioned to achieve reliable, low-latency connectivity within environments characterized by strong multipath and severe channel variability. Programmable wireless environments (PWEs) address this challenge by enabling deterministic control of electromagnetic (EM) propagation through software-defined reconfigurable intelligent surfaces (RISs). However, effectively configuring RISs in real time remains a major bottleneck, particularly under near-field conditions where mutual coupling and specular reflections alter the intended response. To overcome this limitation, this paper introduces MATCH, a physics-based codebook compilation algorithm that explicitly accounts for the EM coupling among RIS unit cells and the reflective interactions with surrounding structures, ensuring that the resulting codebooks remain consistent with the physical characteristics of the environment. Finally, MATCH is evaluated under a full-wave simulation framework incorporating mutual coupling and secondary reflections, demonstrating its ability to concentrate scattered energy within the focal region, confirming that physics-consistent, codebook-based optimization constitutes an effective approach for practical and efficient RIS configuration."}
{"id": "2601.12289", "categories": ["cs.SD", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.12289", "abs": "https://arxiv.org/abs/2601.12289", "authors": ["Haowei Lou", "Hye-young Paik", "Wen Hu", "Lina Yao"], "title": "ParaMETA: Towards Learning Disentangled Paralinguistic Speaking Styles Representations from Speech", "comment": "9 pages, 7 figures, Accepted to AAAI-26 (Main Technical Track)", "summary": "Learning representative embeddings for different types of speaking styles, such as emotion, age, and gender, is critical for both recognition tasks (e.g., cognitive computing and human-computer interaction) and generative tasks (e.g., style-controllable speech generation). In this work, we introduce ParaMETA, a unified and flexible framework for learning and controlling speaking styles directly from speech. Unlike existing methods that rely on single-task models or cross-modal alignment, ParaMETA learns disentangled, task-specific embeddings by projecting speech into dedicated subspaces for each type of style. This design reduces inter-task interference, mitigates negative transfer, and allows a single model to handle multiple paralinguistic tasks such as emotion, gender, age, and language classification. Beyond recognition, ParaMETA enables fine-grained style control in Text-To-Speech (TTS) generative models. It supports both speech- and text-based prompting and allows users to modify one speaking styles while preserving others. Extensive experiments demonstrate that ParaMETA outperforms strong baselines in classification accuracy and generates more natural and expressive speech, while maintaining a lightweight and efficient model suitable for real-world applications."}
{"id": "2601.12153", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2601.12153", "abs": "https://arxiv.org/abs/2601.12153", "authors": ["Arthur N. dos Santos", "Bruno S. Masiero"], "title": "A Survey on 30+ Years of Automatic Singing Assessment and Singing Information Processing", "comment": null, "summary": "Automatic Singing Assessment and Singing Information Processing have evolved over the past three decades to support singing pedagogy, performance analysis, and vocal training. While the first approach objectively evaluates a singer's performance through computational metrics ranging from real-time visual feedback and acoustical biofeedback to sophisticated pitch tracking and spectral analysis, the latter method compares a predictor vocal signal with a target reference to capture nuanced data embedded in the singing voice. Notable advancements include the development of interactive systems that have significantly improved real-time visual feedback, and the integration of machine learning and deep neural network architectures that enhance the precision of vocal signal processing. This survey critically examines the literature to map the historical evolution of these technologies, while identifying and discussing key gaps. The analysis reveals persistent challenges, such as the lack of standardized evaluation frameworks, difficulties in reliably separating vocal signals from various noise sources, and the underutilization of advanced digital signal processing and artificial intelligence methodologies for capturing artistic expressivity. By detailing these limitations and the corresponding technological advances, this review demonstrates how addressing these issues can bridge the gap between objective computational assessments and subjective human-like evaluations of singing performance, ultimately enhancing both the technical accuracy and pedagogical relevance of automated singing evaluation systems."}
{"id": "2601.13001", "categories": ["eess.SP", "cs.IT"], "pdf": "https://arxiv.org/pdf/2601.13001", "abs": "https://arxiv.org/abs/2601.13001", "authors": ["Wenrui Yu", "Jaron Skovsted Gundersen", "Richard Heusdens", "Qiongxiu Li"], "title": "When Is Distributed Nonlinear Aggregation Private? Optimality and Information-Theoretical Bounds", "comment": null, "summary": "Nonlinear aggregation is central to modern distributed systems, yet its privacy behavior is far less understood than that of linear aggregation. Unlike linear aggregation where mature mechanisms can often suppress information leakage, nonlinear operators impose inherent structural limits on what privacy guarantees are theoretically achievable when the aggregate must be computed exactly. This paper develops a unified information-theoretic framework to characterize privacy leakage in distributed nonlinear aggregation under a joint adversary that combines passive (honest-but-curious) corruption and eavesdropping over communication channels.\n  We cover two broad classes of nonlinear aggregates: order-based operators (maximum/minimum and top-$K$) and robust aggregation (median/quantiles and trimmed mean). We first derive fundamental lower bounds on leakage that hold without sacrificing accuracy, thereby identifying the minimum unavoidable information revealed by the computation and the transcript. We then propose simple yet effective privacy-preserving distributed algorithms, and show that with appropriate randomized initialization and parameter choices, our proposed approaches can attach the derived optimal bounds for the considered operators. Extensive experiments validate the tightness of the bounds and demonstrate that network topology and key algorithmic parameters (including the stepsize) govern the observed leakage in line with the theoretical analysis, yielding actionable guidelines for privacy-preserving nonlinear aggregation."}
{"id": "2601.12480", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.12480", "abs": "https://arxiv.org/abs/2601.12480", "authors": ["Hanchen Pei", "Shujie Liu", "Yanqing Liu", "Jianwei Yu", "Yuanhang Qian", "Gongping Huang", "Sheng Zhao", "Yan Lu"], "title": "A Unified Neural Codec Language Model for Selective Editable Text to Speech Generation", "comment": null, "summary": "Neural codec language models achieve impressive zero-shot Text-to-Speech (TTS) by fully imitating the acoustic characteristics of a short speech prompt, including timbre, prosody, and paralinguistic information. However, such holistic imitation limits their ability to isolate and control individual attributes. In this paper, we present a unified codec language model SpeechEdit that extends zero-shot TTS with a selective control mechanism. By default, SpeechEdit reproduces the complete acoustic profile inferred from the speech prompt, but it selectively overrides only the attributes specified by explicit control instructions. To enable controllable modeling, SpeechEdit is trained on our newly constructed LibriEdit dataset, which provides delta (difference-aware) training pairs derived from LibriHeavy. Experimental results show that our approach maintains naturalness and robustness while offering flexible and localized control over desired attributes. Audio samples are available at https://speech-editing.github.io/speech-editing/."}
{"id": "2601.12248", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2601.12248", "abs": "https://arxiv.org/abs/2601.12248", "authors": ["Chun-Yi Kuan", "Hung-yi Lee"], "title": "AQUA-Bench: Beyond Finding Answers to Knowing When There Are None in Audio Question Answering", "comment": "Accepted to ICASSP 2026. Project Website: https://kuan2jiu99.github.io/AQUA-Bench-demo/", "summary": "Recent advances in audio-aware large language models have shown strong performance on audio question answering. However, existing benchmarks mainly cover answerable questions and overlook the challenge of unanswerable ones, where no reliable answer can be inferred from the audio. Such cases are common in real-world settings, where questions may be misleading, ill-posed, or incompatible with the information. To address this gap, we present AQUA-Bench, a benchmark for Audio Question Unanswerability Assessment. It systematically evaluates three scenarios: Absent Answer Detection (the correct option is missing), Incompatible Answer Set Detection (choices are categorically mismatched with the question), and Incompatible Audio Question Detection (the question is irrelevant or lacks sufficient grounding in the audio). By assessing these cases, AQUA-Bench offers a rigorous measure of model reliability and promotes the development of audio-language systems that are more robust and trustworthy. Our experiments suggest that while models excel on standard answerable tasks, they often face notable challenges with unanswerable ones, pointing to a blind spot in current audio-language understanding."}
{"id": "2601.13065", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.13065", "abs": "https://arxiv.org/abs/2601.13065", "authors": ["Davide Bergamasco", "Federico Clazzer", "Paolo Casari"], "title": "OTFS-IDMA: An Unsourced Multiple Access Scheme for Doubly-Dispersive Channels", "comment": "6 pages, 5 figures", "summary": "We present an unsourced multiple access (UMAC) scheme tailored to high-mobility wireless channels. The proposed construction is based on orthogonal time frequency space (OTFS) modulation and sparse interleaver division multiple access (IDMA) in the delay-Doppler (DD) domain. The receiver runs a compressive-sensing joint activity-detection and channel estimation process followed by a single-user decoder which harnesses multipath diversity via the maximal-ratio combining (MRC) principle. Numerical results show the potential of DD-based uncoordinated schemes in the presence of double selectivity, while remarking the design tradeoffs and remaining challenges introduced by the proposed design."}
{"id": "2601.12494", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.12494", "abs": "https://arxiv.org/abs/2601.12494", "authors": ["Hunzalah Hassan Bhatti", "Firoj Alam", "Shammur Absar Chowdhury"], "title": "Harmonizing the Arabic Audio Space with Data Scheduling", "comment": "Foundation Models, Large Language Models, Native, Speech Models, Arabic", "summary": "Audio large language models (LLMs) enable unified speech understanding and generation, yet their adaptation to linguistically complex, dialect-rich settings remains underexplored. This paper presents the first systematic study of multi-task instruction tuning for an Arabic-centric audio LLM, covering a hierarchy of generative tasks (ASR, speech summarization) and discriminative tasks (dialect and emotion identification). To support this study, we introduce AraMega-SSum, a novel dataset for Arabic speech summarization. We fine-tune Qwen2.5-Omni (7B) and propose Task-Progressive Curriculum (TPC) along with Aligner-Based Diverse Sampling (ADS), a strategy that constructs information-dense batches by selecting task- and label-balanced examples. Our results reveal a critical efficiency, robustness trade-off: while ADS accelerates initial convergence and boosts paralinguistic F1-scores, its inherent gradient volatility can destabilize generative decoding under prolonged training. Furthermore, while the TPC stabilizes core acoustic mapping, it often induces negative transfer in downstream tasks. We demonstrate that a Hybrid TPC+ADS Strategy provides an optimal training ``recipe'', first establishing a robust representative foundation before employing diversity-aware refinement to capture fine-grained nuances. These findings offer practical guidance for the efficient adaptation of Omni-models in complex, low-resource multimodal environments."}
{"id": "2601.12345", "categories": ["eess.AS", "cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2601.12345", "abs": "https://arxiv.org/abs/2601.12345", "authors": ["Jakob Kienegger", "Timo Gerkmann"], "title": "Adaptive Rotary Steering with Joint Autoregression for Robust Extraction of Closely Moving Speakers in Dynamic Scenarios", "comment": "Accepted at IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP) 2026", "summary": "Latest advances in deep spatial filtering for Ambisonics demonstrate strong performance in stationary multi-speaker scenarios by rotating the sound field toward a target speaker prior to multi-channel enhancement. For applicability in dynamic acoustic conditions with moving speakers, we propose to automate this rotary steering using an interleaved tracking algorithm conditioned on the target's initial direction. However, for nearby or crossing speakers, robust tracking becomes difficult and spatial cues less effective for enhancement. By incorporating the processed recording as additional guide into both algorithms, our novel joint autoregressive framework leverages temporal-spectral correlations of speech to resolve spatially challenging speaker constellations. Consequently, our proposed method significantly improves tracking and enhancement of closely spaced speakers, consistently outperforming comparable non-autoregressive methods on a synthetic dataset. Real-world recordings complement these findings in complex scenarios with multiple speaker crossings and varying speaker-to-array distances."}
{"id": "2601.13157", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.13157", "abs": "https://arxiv.org/abs/2601.13157", "authors": ["Hang Zou", "Bohao Wang", "Yu Tian", "Lina Bariah", "Chongwen Huang", "Samson Lasaulce", "Mérouane Debbah"], "title": "Seeing Radio: From Zero RF Priors to Explainable Modulation Recognition with Vision Language Models", "comment": null, "summary": "The rise of vision language models (VLMs) paves a new path for radio frequency (RF) perception. Rather than designing task-specific neural receivers, we ask if VLMs can learn to recognize modulations when RF waveforms are expressed as images. In this work, we find that they can. In specific, in this paper, we introduce a practical pipeline for converting complex IQ streams into visually interpretable inputs, hence, enabling general-purpose VLMs to classify modulation schemes without changing their underlying design. Building on this, we construct an RF visual question answering (VQA) benchmark framework that covers 57 classes across major families of analog/digital modulations with three complementary image modes, namely, (i) short \\emph{time-series} IQ segments represented as real/imaginary traces, (ii) magnitude-only \\emph{spectrograms}, and (iii) \\emph{joint} representations that pair spectrograms with a synchronized time-series waveforms. We design uniform zero-shot and few-shot prompts for both class-level and family-level evaluations. Our finetuned VLMs with these images achieve competitive accuracy of $90\\%$ compared to $10\\%$ of the base models. Furthermore, the fine-tuned VLMs show robust performance under noise and demonstrate high generalization performance to unseen modulation types, without relying on RF-domain priors or specialized architectures. The obtained results show that combining RF-to-image conversion with promptable VLMs provides a scalable and practical foundation for RF-aware AI systems in future 6G networks."}
{"id": "2601.12591", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.12591", "abs": "https://arxiv.org/abs/2601.12591", "authors": ["Xin Jing", "Jiadong Wang", "Andreas Triantafyllopoulos", "Maurice Gerczuk", "Shahin Amiriparian", "Jun Luo", "Björn Schuller"], "title": "SmoothCLAP: Soft-Target Enhanced Contrastive Language\\--Audio Pretraining for Affective Computing", "comment": "5 pages, accepted by ICASSP 2026", "summary": "The ambiguity of human emotions poses several challenges for machine learning models, as they often overlap and lack clear delineating boundaries. Contrastive language-audio pretraining (CLAP) has emerged as a key technique for generalisable emotion recognition. However, as conventional CLAP enforces a strict one-to-one alignment between paired audio-text samples, it overlooks intra-modal similarity and treats all non-matching pairs as equally negative. This conflicts with the fuzzy boundaries between different emotions. To address this limitation, we propose SmoothCLAP, which introduces softened targets derived from intra-modal similarity and paralinguistic features. By combining these softened targets with conventional contrastive supervision, SmoothCLAP learns embeddings that respect graded emotional relationships, while retaining the same inference pipeline as CLAP. Experiments on eight affective computing tasks across English and German demonstrate that SmoothCLAP is consistently achieving superior performance. Our results highlight that leveraging soft supervision is a promising strategy for building emotion-aware audio-text models."}
{"id": "2601.12354", "categories": ["eess.AS", "cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2601.12354", "abs": "https://arxiv.org/abs/2601.12354", "authors": ["Sina Khanagha", "Bunlong Lay", "Timo Gerkmann"], "title": "Bone-conduction Guided Multimodal Speech Enhancement with Conditional Diffusion Models", "comment": "Accepted to IEEE ICASSP 2026", "summary": "Single-channel speech enhancement models face significant performance degradation in extremely noisy environments. While prior work has shown that complementary bone-conducted speech can guide enhancement, effective integration of this noise-immune modality remains a challenge. This paper introduces a novel multimodal speech enhancement framework that integrates bone-conduction sensors with air-conducted microphones using a conditional diffusion model. Our proposed model significantly outperforms previously established multimodal techniques and a powerful diffusion-based single-modal baseline across a wide range of acoustic conditions."}
{"id": "2601.13201", "categories": ["eess.SP", "cs.ET", "cs.IT"], "pdf": "https://arxiv.org/pdf/2601.13201", "abs": "https://arxiv.org/abs/2601.13201", "authors": ["Konstantinos D. Katsanos", "George C. Alexandropoulos"], "title": "Decentralized Cooperative Beamforming for BDRIS-Assisted Cell-Free MIMO OFDM Systems", "comment": "13 pages, 6 figures, submitted to an IEEE Transactions journal", "summary": "In this paper, a wideband cell-free multi-stream multi-user Multiple-Input Multiple-Output (MIMO) Orthogonal Frequency Division Multiplexing (OFDM) system is considered operating within a smart wireless environment enabled by multiple Beyond Diagonal Reconfigurable Intelligent Surfaces (BDRISs). A novel decentralized active and passive beamforming framework, robust to imperfect channel state availability and with minimal cooperation among the system's multiple Base Stations (BSs) for deciding the final configurations of the shared BDRISs, is proposed, which aims to substantially reduce the overhead inherent in centralized solutions necessitating a central processing unit of high computational power. By considering a Dynamic Group-Connected (DGC) BDRIS architecture with frequency-selective responses per unit element, we formulate the system's sum-rate maximization problem with respect to the tunable capacitances and permutation matrices of the BDRISs as well as the precoding matrices of the BSs, which is solved via successive concave approximation and alternating projections as well as consensus-based updates for the BDRISs' design. Through extensive simulation results, it is showcased that the proposed robust decentralized cooperative approach with diverse BDRIS architectures outperforms non-cooperation benchmarks. It is also demonstrated that the considered DGC BDRIS architecture is able to provide sum-rate performance gains sufficiently close to the more complex fully-connected BDRIS structure."}
{"id": "2601.12600", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.12600", "abs": "https://arxiv.org/abs/2601.12600", "authors": ["Pu Wang", "Shinji Watanabe", "Hugo Van hamme"], "title": "SSVD-O: Parameter-Efficient Fine-Tuning with Structured SVD for Speech Recognition", "comment": "Accepted by IEEE ICASSP 2026", "summary": "Parameter-efficient fine-tuning (PEFT) is a scalable approach for adapting large speech foundation models to new domains. While methods such as LoRA and its state-of-the-art variants reduce adaptation costs, they typically allocate parameters uniformly across model subspaces, which limits their efficiency and scalability in speech applications. Building on our prior work, this paper introduces SSVD-Outer (SSVD-O), an extension of the structured SVD-guided (SSVD) fine-tuning method. SSVD-O combines input acoustic feature space-associated inner transformations with output semantic feature space-associated outer transformations to enable scalable and balanced adaptation. We conduct the first systematic analysis of parameter budget allocation across model subspaces in PEFT for automatic speech recognition (ASR), and investigate the trade-off between learning and forgetting under constrained resources. SSVD-O is benchmarked against LoRA, DoRA, PiSSA, and SSVD on domain-shifted ASR tasks, including child speech and regional accents, across model scales from 0.1B to 2B within the ESPnet framework. Experimental results show that SSVD-O consistently narrows the performance gap to full fine-tuning while improving generalization and mitigating catastrophic forgetting."}
{"id": "2601.12436", "categories": ["eess.AS", "cs.AI", "cs.LG", "cs.MM", "cs.SD"], "pdf": "https://arxiv.org/pdf/2601.12436", "abs": "https://arxiv.org/abs/2601.12436", "authors": ["Linzhi Wu", "Xingyu Zhang", "Hao Yuan", "Yakun Zhang", "Changyan Zheng", "Liang Xie", "Tiejun Liu", "Erwei Yin"], "title": "Purification Before Fusion: Toward Mask-Free Speech Enhancement for Robust Audio-Visual Speech Recognition", "comment": "Accepted by ICASSP2026", "summary": "Audio-visual speech recognition (AVSR) typically improves recognition accuracy in noisy environments by integrating noise-immune visual cues with audio signals. Nevertheless, high-noise audio inputs are prone to introducing adverse interference into the feature fusion process. To mitigate this, recent AVSR methods often adopt mask-based strategies to filter audio noise during feature interaction and fusion, yet such methods risk discarding semantically relevant information alongside noise. In this work, we propose an end-to-end noise-robust AVSR framework coupled with speech enhancement, eliminating the need for explicit noise mask generation. This framework leverages a Conformer-based bottleneck fusion module to implicitly refine noisy audio features with video assistance. By reducing modality redundancy and enhancing inter-modal interactions, our method preserves speech semantic integrity to achieve robust recognition performance. Experimental evaluations on the public LRS3 benchmark suggest that our method outperforms prior advanced mask-based baselines under noisy conditions."}
{"id": "2601.13204", "categories": ["eess.SP", "cs.IT", "eess.IV"], "pdf": "https://arxiv.org/pdf/2601.13204", "abs": "https://arxiv.org/abs/2601.13204", "authors": ["Yanfeng Zhang", "Xi'an Fan", "Jinkai Zheng", "Xiaoye Jing", "Weiwei Yang", "Xu Zhu"], "title": "Hierarchical Sparse Vector Transmission for Ultra Reliable and Low Latency Communications", "comment": null, "summary": "Sparse vector transmission (SVT) is a promising candidate technology for achieving ultra-reliable low-latency communication (URLLC). In this paper, a hierarchical SVT scheme is proposed for multi-user URLLC scenarios. The hierarchical SVT scheme partitions the transmitted bits into common and private parts. The common information is conveyed by the indices of non-zero sections in a sparse vector, while each user's private information is embedded into non-zero blocks with specific block lengths. At the receiver, the common bits are first recovered from the detected non-zero sections, followed by user-specific private bits decoding based on the corresponding non-zero block indices. Simulation results show the proposed scheme outperforms state-of-the-art SVT schemes in terms of block error rate."}
{"id": "2601.12660", "categories": ["cs.SD", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.12660", "abs": "https://arxiv.org/abs/2601.12660", "authors": ["Maab Elrashid", "Anthony Deschênes", "Cem Subakan", "Mirco Ravanelli", "Rémi Georges", "Michael Morin"], "title": "Toward Faithful Explanations in Acoustic Anomaly Detection", "comment": "Accepted at the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP) 2026. Code: https://github.com/Maab-Nimir/Faithful-Explanations-in-Acoustic-Anomaly-Detection", "summary": "Interpretability is essential for user trust in real-world anomaly detection applications. However, deep learning models, despite their strong performance, often lack transparency. In this work, we study the interpretability of autoencoder-based models for audio anomaly detection, by comparing a standard autoencoder (AE) with a mask autoencoder (MAE) in terms of detection performance and interpretability. We applied several attribution methods, including error maps, saliency maps, SmoothGrad, Integrated Gradients, GradSHAP, and Grad-CAM. Although MAE shows a slightly lower detection, it consistently provides more faithful and temporally precise explanations, suggesting a better alignment with true anomalies. To assess the relevance of the regions highlighted by the explanation method, we propose a perturbation-based faithfulness metric that replaces them with their reconstructions to simulate normal input. Our findings, based on experiments in a real industrial scenario, highlight the importance of incorporating interpretability into anomaly detection pipelines and show that masked training improves explanation quality without compromising performance."}
{"id": "2601.12485", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2601.12485", "abs": "https://arxiv.org/abs/2601.12485", "authors": ["Kang Chen", "Xianrui Wang", "Yichen Yang", "Andreas Brendel", "Gongping Huang", "Zbyněk Koldovský", "Jingdong Chen", "Jacob Benesty", "Shoji Makino"], "title": "Robust Online Overdetermined Independent Vector Analysis Based on Bilinear Decomposition", "comment": null, "summary": "Online blind source separation is essential for both speech communication and human-machine interaction. Among existing approaches, overdetermined independent vector analysis (OverIVA) delivers strong performance by exploiting the statistical independence of source signals and the orthogonality between source and noise subspaces. However, when applied to large microphone arrays, the number of parameters grows rapidly, which can degrade online estimation accuracy. To overcome this challenge, we propose decomposing each long separation filter into a bilinear form of two shorter filters, thereby reducing the number of parameters. Because the two filters are closely coupled, we design an alternating iterative projection algorithm to update them in turn. Simulation results show that, with far fewer parameters, the proposed method achieves improved performance and robustness."}
{"id": "2601.13205", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.13205", "abs": "https://arxiv.org/abs/2601.13205", "authors": ["Kadyrzhan Tortayev", "Oliver Falkenberg Damborg", "Jònas À Hàlvmørk Joensen", "Jonas Pedesk", "Yifa Li", "Fengchun Zhang", "Zeliang An", "Yubo Wang", "Ming Shen"], "title": "Co-Channel Interference Mitigation Using Deep Learning for Drone-Based Large-Scale Antenna Measurements", "comment": null, "summary": "Unmanned aerial vehicles (UAVs) enable efficient in-situ radiation characterization of large-aperture antennas directly in their deployment environments. In such measurements, a continuous-wave (CW) probe tone is commonly transmitted to characterize the antenna response. However, active co-channel emissions from neighboring antennas often introduce severe in-band interference, where classical FFT-based estimators fail to accurately estimate the CW tone amplitude when the signal-to-interference ratios (SIR) falls below -10 dB. This paper proposes a lightweight deep convolutional neural network (DC-CNN) that estimates the amplitude of the CW tone. The model is trained and evaluated on real 5~GHz measurement bursts spanning an effective SIR range of --33.3 dB to +46.7 dB. Despite its compact size (<20k parameters), the proposed DC-CNN achieves a mean absolute error (MAE) of 7% over the full range, with <1 dB error for SIR >= -30 dB. This robustness and efficiency make DC-CNN suitable for deployment on embedded UAV platforms for interference-resilient antenna pattern characterization."}
{"id": "2601.12802", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.12802", "abs": "https://arxiv.org/abs/2601.12802", "authors": ["Jihoo Jung", "Ji-Hoon Kim", "Doyeop Kwak", "Junwon Lee", "Juhan Nam", "Joon Son Chung"], "title": "UNMIXX: Untangling Highly Correlated Singing Voices Mixtures", "comment": "Accepted by ICASSP 2026", "summary": "We introduce UNMIXX, a novel framework for multiple singing voices separation (MSVS). While related to speech separation, MSVS faces unique challenges: data scarcity and the highly correlated nature of singing voices mixture. To address these issues, we propose UNMIXX with three key components: (1) musically informed mixing strategy to construct highly correlated, music-like mixtures, (2) cross-source attention that drives representations of two singers apart via reverse attention, and (3) magnitude penalty loss penalizing erroneously assigned interfering energy. UNMIXX not only addresses data scarcity by simulating realistic training data, but also excels at separating highly correlated mixtures through cross-source interactions at both the architectural and loss levels. Our extensive experiments demonstrate that UNMIXX greatly enhances performance, with SDRi gains exceeding 2.2 dB over prior work."}
{"id": "2601.12594", "categories": ["eess.AS", "cs.AI", "cs.SD"], "pdf": "https://arxiv.org/pdf/2601.12594", "abs": "https://arxiv.org/abs/2601.12594", "authors": ["Xinhao Mei", "Gael Le Lan", "Haohe Liu", "Zhaoheng Ni", "Varun Nagaraja", "Yang Liu", "Yangyang Shi", "Vikas Chandra"], "title": "SLAP: Scalable Language-Audio Pretraining with Variable-Duration Audio and Multi-Objective Training", "comment": "Accepted to ICASSP 2026", "summary": "Contrastive language-audio pretraining (CLAP) has achieved notable success in learning semantically rich audio representations and is widely adopted for various audio-related tasks. However, current CLAP models face several key limitations. First, they are typically trained on relatively small datasets, often comprising a few million audio samples. Second, existing CLAP models are restricted to short and fixed duration, which constrains their usage in real-world scenarios with variable-duration audio. Third, the standard contrastive training objective operates on global representations, which may hinder the learning of dense, fine-grained audio features. To address these challenges, we introduce Scalable Language-Audio Pretraining (SLAP), which scales language-audio pretraining to 109 million audio-text pairs with variable audio durations and incorporates multiple training objectives. SLAP unifies contrastive loss with additional self-supervised and captioning losses in a single-stage training, facilitating the learning of richer dense audio representations. The proposed SLAP model achieves new state-of-the-art performance on audio-text retrieval and zero-shot audio classification tasks, demonstrating its effectiveness across diverse benchmarks."}
{"id": "2601.13289", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.13289", "abs": "https://arxiv.org/abs/2601.13289", "authors": ["Ruhul Amin Khalil", "Asiya Jehangir", "Hanane Lamaazi", "Sadaf Rubab", "Nasir Saeed"], "title": "Semantic Communication in Underwater IoT Networks for Meaning-Driven Connectivity", "comment": "25 pages, 3 figures and 8 tables", "summary": "The Internet of Underwater Things (IoUT) is revolutionizing marine sensing and environmental monitoring, as well as subaquatic exploration, which are enabled by interconnected and intelligent subsystems. Nevertheless, underwater communication is constrained by narrow bandwidth, high latency, and strict energy constraints, which are the source of efficiency problems in traditional data-centric networks. To tackle these problematic issues, this work provides a survey of recent advances in Semantic Communication (SC) for IoUT, a novel communication paradigm that seeks to harness not raw symbol information but rather its meaning and/or contextual significance. In this paper, we investigate the emerging advanced AI-powered frameworks, including large language models (LLMs), diffusion-based generative encoders, and federated learning (FL), that bridge semantic compression with context-aware prioritization and robust information reconstruction over noisy underwater channels. Hybrid acoustic-optical-RF architectures and edge-intelligent semantic encoders are also considered enablers of sustainable, adaptive operations. Examples in underwater archaeology, marine ecology, and autonomous underwater vehicles (AUVs) coordination are provided as a relief to illustrate the merits of meaning-driven connectivity. The paper concludes with some recommendations, including semantic representations standardization, cross-domain interpolation, and privacy-support schemes. These issues must be addressed in the future before trustworthy SC-enabled IoUT systems can be developed for underwater communication."}
{"id": "2601.13513", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.13513", "abs": "https://arxiv.org/abs/2601.13513", "authors": ["Noriyuki Tonami", "Wataru Kohno", "Yoshiyuki Yajima", "Sakiko Mishima", "Yumi Arai", "Reishi Kondo", "Tomoyuki Hino"], "title": "Event Classification by Physics-informed Inpainting for Distributed Multichannel Acoustic Sensor with Partially Degraded Channels", "comment": "Accepted to ICASSP 2026", "summary": "Distributed multichannel acoustic sensing (DMAS) enables large-scale sound event classification (SEC), but performance drops when many channels are degraded and when sensor layouts at test time differ from training layouts. We propose a learning-free, physics-informed inpainting frontend based on reverse time migration (RTM). In this approach, observed multichannel spectrograms are first back-propagated on a 3D grid using an analytic Green's function to form a scene-consistent image, and then forward-projected to reconstruct inpainted signals before log-mel feature extraction and Transformer-based classification. We evaluate the method on ESC-50 with 50 sensors and three layouts (circular, linear, right-angle), where per-channel SNRs are sampled from -30 to 0 dB. Compared with an AST baseline, scaling-sparsemax channel selection, and channel-swap augmentation, the proposed RTM frontend achieves the best or competitive accuracy across all layouts, improving accuracy by 13.1 points on the right-angle layout (from 9.7% to 22.8%). Correlation analyses show that spatial weights align more strongly with SNR than with channel--source distance, and that higher SNR--weight correlation corresponds to higher SEC accuracy. These results demonstrate that a reconstruct-then-project, physics-based preprocessing effectively complements learning-only methods for DMAS under layout-open configurations and severe channel degradation."}
{"id": "2601.12700", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2601.12700", "abs": "https://arxiv.org/abs/2601.12700", "authors": ["Haolin Chen"], "title": "Improving Audio Question Answering with Variational Inference", "comment": "ICASSP 2026", "summary": "Variational inference (VI) provides a principled framework for estimating posterior distributions over model parameters, enabling explicit modeling of weight uncertainty during optimization. By capturing this uncertainty, VI improves the reliability of predictions, yielding better calibrated outputs. In this work, we investigate the benefits of VI for challenging multimodal understanding and reasoning by applying the Improved Variational Online Newton (IVON), a recent VI optimizer, to fine-tuning a multimodal large language model on audio question answering tasks. Our results show that VI not only enhances predictive accuracy but also significantly improves calibration, reducing the model's overconfidence. These advances further support risk-sensitive applications such as selective prediction, where reliable confidence estimates are crucial."}
{"id": "2601.13418", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.13418", "abs": "https://arxiv.org/abs/2601.13418", "authors": ["Sambrama Hegde", "Venkata Srirama Rohit Kantheti", "Liang C Chu", "Erik Blasch", "Shih-Chun Lin"], "title": "Autonomous Self-Healing UAV Swarms for Robust 6G Non-Terrestrial Networks", "comment": null, "summary": "Recent years have seen an increased interest in the use of Non-terrestrial networks (NTNs), especially the unmanned aerial vehicles (UAVs) to provide cost-effective global connectivity in next-generation wireless networks. We introduce a resilient, adaptive, self-healing network design (RASHND) to optimize signal quality under dynamic interference and adversarial conditions. RASHND leverages inter-node communication and an intelligent algorithm selection process, incorporating combining techniques like distributed-Maximal Ratio Combining (d-MRC), distributed-Linear Minimum Mean Squared Error Estimation(d-LMMSE), and Selection Combining (SC). These algorithms are selected to improve performance by adapting to changing network conditions. To evaluate the effectiveness of the proposed RASHND solutions, a software-defined radio (SDR)-based hardware testbed afforded initial testing and evaluations. Additionally, we present results from UAV tests conducted on the AERPAW testbed to validate our solutions in real-world scenarios. The results demonstrate that RASHND significantly enhances the reliability and interference resilience of UAV networks, making them well-suited for critical communications."}
{"id": "2601.13704", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.13704", "abs": "https://arxiv.org/abs/2601.13704", "authors": ["Esteban Gómez", "Tom Bäckström"], "title": "Performance and Complexity Trade-off Optimization of Speech Models During Training", "comment": null, "summary": "In speech machine learning, neural network models are typically designed by choosing an architecture with fixed layer sizes and structure. These models are then trained to maximize performance on metrics aligned with the task's objective. While the overall architecture is usually guided by prior knowledge of the task, the sizes of individual layers are often chosen heuristically. However, this approach does not guarantee an optimal trade-off between performance and computational complexity; consequently, post hoc methods such as weight quantization or model pruning are typically employed to reduce computational cost. This occurs because stochastic gradient descent (SGD) methods can only optimize differentiable functions, while factors influencing computational complexity, such as layer sizes and floating-point operations per second (FLOP/s), are non-differentiable and require modifying the model structure during training. We propose a reparameterization technique based on feature noise injection that enables joint optimization of performance and computational complexity during training using SGD-based methods. Unlike traditional pruning methods, our approach allows the model size to be dynamically optimized for a target performance-complexity trade-off, without relying on heuristic criteria to select which weights or structures to remove. We demonstrate the effectiveness of our method through three case studies, including a synthetic example and two practical real-world applications: voice activity detection and audio anti-spoofing. The code related to our work is publicly available to encourage further research."}
{"id": "2601.13107", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2601.13107", "abs": "https://arxiv.org/abs/2601.13107", "authors": ["Carlos Franzreb", "Arnab Das", "Tim Polzehl", "Sebastian Möller"], "title": "Content Leakage in LibriSpeech and Its Impact on the Privacy Evaluation of Speaker Anonymization", "comment": "Accepted to ICASSP 2026", "summary": "Speaker anonymization aims to conceal a speaker's identity, without considering the linguistic content. In this study, we reveal a weakness of Librispeech, the dataset that is commonly used to evaluate anonymizers: the books read by the Librispeech speakers are so distinct, that speakers can be identified by their vocabularies. Even perfect anonymizers cannot prevent this identity leakage. The EdAcc dataset is better in this regard: only a few speakers can be identified through their vocabularies, encouraging the attacker to look elsewhere for the identities of the anonymized speakers. EdAcc also comprises spontaneous speech and more diverse speakers, complementing Librispeech and giving more insights into how anonymizers work."}
{"id": "2601.13470", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.13470", "abs": "https://arxiv.org/abs/2601.13470", "authors": ["Gabriel Avanzi Ubiali", "José Carlos Marinello Filho", "Taufik Abrão"], "title": "Joint Subarray Selection, User Scheduling, and Pilot Assignment for XL-MIMO", "comment": "31 pages, 5 figures, full paper", "summary": "Extra-large scale MIMO (XL-MIMO) is a key technology for meeting sixth-generation (6G) requirements for high-rate connectivity and uniform quality of service (QoS); however, its deployment is challenged by the prohibitive complexity of resource management based on instantaneous channel state information (CSI). To address this intractability, this work derives novel closed-form deterministic signal-to-interference-plus-noise ratio (SINR) expressions for both centralized and distributed uplink operations. Valid for Rician fading channels with minimum mean square error (MMSE) receive combining and MMSE channel estimation, these expressions depend exclusively on long-term channel statistics, providing a tractable alternative to computationally expensive instantaneous CSI-driven optimization. Building on these results, we develop statistical-CSI-based algorithms for joint subarray selection, users scheduling, and pilot assignment, leveraging the derived SINR approximations to maximize the minimum spectral efficiency (SE) among scheduled users while preserving computational tractability. The proposed framework exploits the spatial sparsity of user equipment (UE) visibility regions (VRs) to enable more aggressive pilot reuse than is possible in conventional massive MIMO. Numerical results validate the high accuracy of the derived SINR approximations and demonstrate that the proposed algorithms significantly enhance fairness and throughput in crowded scenarios."}
{"id": "2601.13531", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2601.13531", "abs": "https://arxiv.org/abs/2601.13531", "authors": ["Chenda Li", "Wei Wang", "Marvin Sach", "Wangyou Zhang", "Kohei Saijo", "Samuele Cornell", "Yihui Fu", "Zhaoheng Ni", "Tim Fingscheidt", "Shinji Watanabe", "Yanmin Qian"], "title": "ICASSP 2026 URGENT Speech Enhancement Challenge", "comment": "The overview paper of the ICASSP 2026 URGENT Speech Enhancement Challenge", "summary": "The ICASSP 2026 URGENT Challenge advances the series by focusing on universal speech enhancement (SE) systems that handle diverse distortions, domains, and input conditions. This overview paper details the challenge's motivation, task definitions, datasets, baseline systems, evaluation protocols, and results. The challenge is divided into two complementary tracks. Track 1 focuses on universal speech enhancement, while Track 2 introduces speech quality assessment for enhanced speech. The challenge attracted over 80 team registrations, with 29 submitting valid entries, demonstrating significant community interest in robust SE technologies."}
{"id": "2601.13549", "categories": ["eess.SP", "cs.IT"], "pdf": "https://arxiv.org/pdf/2601.13549", "abs": "https://arxiv.org/abs/2601.13549", "authors": ["Chao Zhou", "Changsheng You", "Cong Zhou", "Chengwen Xing", "Jianhua Zhang"], "title": "Near-field Physical Layer Security: Robust Beamforming under Location Uncertainty", "comment": "13 pages, 11 figures, submitted to IEEE for possible publication", "summary": "In this paper, we study robust beamforming design for near-field physical-layer-security (PLS) systems, where a base station (BS) equipped with an extremely large-scale array (XL-array) serves multiple near-field legitimate users (Bobs) in the presence of multiple near-field eavesdroppers (Eves). Unlike existing works that mostly assume perfect channel state information (CSI) or location information of Eves, we consider a more practical and challenging scenario, where the locations of Bobs are perfectly known, while only imperfect location information of Eves is available at the BS. We first formulate a robust optimization problem to maximize the sum-rate of Bobs while guaranteeing a worst-case limit on the eavesdropping rate under location uncertainty. By transforming Cartesian position errors into the polar domain, we reveal an important near-field angular-error amplification effect: for the same location error, the closer the Eve, the larger the angle error, severely degrading the performance of conventional robust beamforming methods based on imperfect channel state information. To address this issue, we first establish the conditions for which the first-order Taylor approximation of the near-field channel steering vector under location uncertainty is largely accurate. Then, we propose a two-stage robust beamforming method, which first partitions the uncertainty region into multiple fan-shaped sub-regions, followed by the second stage to formulate and solve a refined linear-matrix-inequality (LMI)-based robust beamforming optimization problem. In addition, the proposed method is further extended to scenarios with multiple Bobs and multiple Eves. Finally, numerical results validate that the proposed method achieves a superior trade-off between rate performance and secrecy robustness, hence significantly outperforming existing benchmarks under Eve location uncertainty."}
{"id": "2601.13593", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.13593", "abs": "https://arxiv.org/abs/2601.13593", "authors": ["Aswin Jose", "Roeland P. J. E. Decorte", "Laurent Locquet"], "title": "Instant Preliminary Cardiac Analysis from Smartphone Auscultation: A Real-World Canine Heart Sound Dataset and Evaluation", "comment": null, "summary": "This study presents a real-world canine heart sound dataset and evaluates SoNUS version 3.2.x, a machine learning algorithm for preliminary cardiac analysis using smartphone microphone recordings. More than one hundred recordings were collected from dogs across four continents, with thirty eight recordings annotated by board certified veterinary cardiologists for quantitative evaluation. SoNUS version 3.2.x employs a multi-stage fallback architecture with quality-aware filtering to ensure reliable output under variable recording conditions. The primary sixty second model achieved mean and median heart rate accuracies of ninety one point six three percent and ninety four point nine five percent, while a fast model optimized for thirty to forty second recordings achieved mean and median accuracies of eighty eight point eight six percent and ninety two point nine eight percent. These results demonstrate the feasibility of extracting clinically relevant cardiac information from opportunistic smartphone recordings, supporting scalable preliminary assessment and telehealth applications in veterinary cardiology."}
{"id": "2601.13635", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.13635", "abs": "https://arxiv.org/abs/2601.13635", "authors": ["Emin Akpinar", "Emir Aslandogan", "Burak Ahmet Ozden", "Haci Ilhan", "Erdogan Aydin"], "title": "Deep Learning-Enabled Signal Detection for MIMO-OTFS-Based 6G and Future Wireless Networks", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Orthogonal time frequency space (OTFS) modulation stands out as a promising waveform for sixth generation (6G) and beyond wireless communication systems, offering superior performance over conventional methods, particularly in high-mobility scenarios and dispersive channel conditions. Recent research has demonstrated that the reduced computational complexity of deep learning (DL)-based signal detection (SD) methods constitutes a compelling alternative to conventional techniques. In this study, low-complexity DL-based SD methods are proposed for a multiple-input multiple-output (MIMO)-OTFS system and examined under Nakagami-$m$ channel conditions. The symbols obtained from the receiver antennas are combined using maximum ratio combining (MRC) and detected with the help of a DL-based detector implemented with multi-layer perceptron (MLP), convolutional neural network (CNN), and residual network (ResNet). Complexity analysis reveals that the MLP architecture offers significantly lower computational complexity compared to CNN, ResNet, and classical methods such as maximum likelihood detection (MLD). Furthermore, numerical analyses have shown that the proposed DL-based detectors, despite their low complexity, achieve comparable bit error rate (BER) performance to that of a high-performance MLD under various system conditions."}
{"id": "2601.13680", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.13680", "abs": "https://arxiv.org/abs/2601.13680", "authors": ["Shama Siddiqui", "Anwar Ahmed Khan", "Nicola Marchetti"], "title": "TSN-IoT: A Two-Stage NOMA-Enabled Framework for Prioritized Traffic Handling in Dense IoT Networks", "comment": null, "summary": "With the growing applications of the Internet of Things (IoT), a major challenge is to ensure continuous connectivity while providing prioritized access. In dense IoT scenarios, synchronization may be disrupted either by the movement of nodes away from base stations or by the unavailability of reliable Global Navigation Satellite System (GNSS) signals, which can be affected by physical obstructions, multipath fading, or environmental interference, such as such as walls, buildings, moving objects, or electromagnetic noise from surrounding devices. In such contexts, distributed synchronization through Non-Orthogonal Multiple Access (NOMA) offers a promising solution, as it enables simultaneous transmission to multiple users with different power levels, supporting efficient synchronization while minimizing the signaling overhead. Moreover, NOMA also plays a vital role for dynamic priority management in dense and heterogeneous IoT environments. In this article, we proposed a Two-Stage NOMA-Enabled Framework \"TSN-IoT\" that integrates the mechanisms of conventional Precision Time Protocol (PTP) based synchronization, distributed synchronization and data transmission. The framework is designed as a four-tier architecture that facilitates prioritized data delivery from sensor nodes to the central base station. We demonstrated the performance of \"TSN-IoT\" through a healthcare use case, where intermittent connectivity and varying data priority levels present key challenges for reliable communication. Synchronization speed and end-to-end delay were evaluated through a series of simulations implemented in Python. Results show that, compared to priority-based Orthogonal Frequency Division Multiple Access (OFDMA), TSN-IoT achieves significantly better performance by offering improved synchronization opportunities and enabling parallel transmissions over the same sub-carrier."}
{"id": "2601.13827", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.13827", "abs": "https://arxiv.org/abs/2601.13827", "authors": ["Yongqiang Zhang", "Qurrat-Ul-Ain Nadeem"], "title": "Channel Estimation in MIMO Systems Using Flow Matching Models", "comment": "6 pages, 3 figures, accepted by IEEE International Conference on Communications (ICC) 2026", "summary": "Multiple-input multiple-output (MIMO) systems require efficient and accurate channel estimation with low pilot overhead to unlock their full potential for high spectral and energy efficiency. While deep generative models have emerged as a powerful foundation for the channel estimation task, the existing approaches using diffusion-based and score-based models suffer from high computational runtime due to their stochastic and many-step iterative sampling. In this paper, we introduce a flow matching-based channel estimator to overcome this limitation. The proposed channel estimator is based on a deep neural network trained to learn the velocity field of wireless channels, which we then integrate into a plug-and-play proximal gradient descent (PnP-PGD) framework. Simulation results reveal that our formulated approach consistently outperforms existing state-of-the-art (SOTA) generative model-based estimators, achieves up to 49 times faster inference at test time, and reduces up to 20 times peak graphics processing unit (GPU) memory usage. Our code and models are publicly available to support reproducible research."}
{"id": "2601.13877", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.13877", "abs": "https://arxiv.org/abs/2601.13877", "authors": ["Ignacio Santamaria", "Mohammad Soleymani", "Eduard Jorswieck", "Jesus Gutierrez", "Carlos Beltran"], "title": "Riemannian optimization on the manifold of unitary and symmetric matrices with application to BD-RIS-assisted systems", "comment": "5 pages, 2 figures", "summary": "In this paper, we rigorously characterize for the first time the manifold of unitary and symmetric matrices, deriving its tangent space and its geodesics. The resulting parameterization of the geodesics (through a real and symmetric matrix) allows us to derive a new Riemannian manifold optimization (MO) algorithm whose most remarkable feature is that it does not need to set any adaptation parameter. We apply the proposed MO algorithm to maximize the achievable rate in a multiple-input multiple-output (MIMO) system assisted by a beyond-diagonal reconfigurable intelligent surface (BD-RIS), illustrating the method's performance through simulations. The MO algorithm achieves a significant reduction in computational cost compared to previous alternatives based on Takagi decomposition, while retaining global convergence to a stationary point of the cost function."}
{"id": "2601.13934", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.13934", "abs": "https://arxiv.org/abs/2601.13934", "authors": ["Phuong Nam Tran", "Nhan Thanh Nguyen", "Hien Quoc Ngo", "Markku Juntti"], "title": "Deep Reinforcement Learning-Based Dynamic Resource Allocation in Cell-Free Massive MIMO", "comment": null, "summary": "In this paper, we consider power allocation and antenna activation of cell-free massive multiple-input multiple-output (CFmMIMO) systems. We first derive closed-form expressions for the system spectral efficiency (SE) and energy efficiency (EE) as functions of the power allocation coefficients and the number of active antennas at the access points (APs). Then, we aim to enhance the EE through jointly optimizing antenna activation and power control. This task leads to a non-convex and mixed-integer design problem with high-dimensional design variables. To address this, we propose a novel DRL-based framework, in which the agent learns to map large-scale fading coefficients to AP activation ratio, antenna coefficient, and power coefficient. These coefficients are then employed to determine the number of active antennas per AP and the power factors assigned to users based on closed-form expressions. By optimizing these parameters instead of directly controlling antenna selection and power allocation, the proposed method transforms the intractable optimization into a low-dimensional learning task. Our extensive simulations demonstrate the efficiency and scalability of the proposed scheme. Specifically, in a CFmMIMO system with 40 APs and 20 users, it achieves a 50% EE improvement and 3350 times run time reduction compared to the conventional sequential convex approximation method."}
{"id": "2601.13962", "categories": ["eess.SP", "eess.SY", "q-bio.NC", "stat.ME"], "pdf": "https://arxiv.org/pdf/2601.13962", "abs": "https://arxiv.org/abs/2601.13962", "authors": ["Eike Osmers", "Dorothea Kolossa"], "title": "Optimal Calibration of the endpoint-corrected Hilbert Transform", "comment": null, "summary": "Accurate, low-latency estimates of the instantaneous phase of oscillations are essential for closed-loop sensing and actuation, including (but not limited to) phase-locked neurostimulation and other real-time applications. The endpoint-corrected Hilbert transform (ecHT) reduces boundary artefacts of the Hilbert transform by applying a causal narrow-band filter to the analytic spectrum. This improves the phase estimate at the most recent sample. Despite its widespread empirical use, the systematic endpoint distortions of ecHT have lacked a principled, closed-form analysis. In this study, we derive the ecHT endpoint operator analytically and demonstrate that its output can be decomposed into a desired positive-frequency term (a deterministic complex gain that induces a calibratable amplitude/phase bias) and a residual leakage term setting an irreducible variance floor. This yields (i) an explicit characterisation and bounds for endpoint phase/amplitude error, (ii) a mean-squared-error-optimal scalar calibration (c-ecHT), and (iii) practical design rules relating window length, bandwidth/order, and centre-frequency mismatch to residual bias via an endpoint group delay. The resulting calibrated ecHT achieves near-zero mean phase error and remains computationally compatible with real-time pipelines. Code and analyses are provided at https://github.com/eosmers/cecHT."}
{"id": "2601.13997", "categories": ["eess.SP", "cs.IT"], "pdf": "https://arxiv.org/pdf/2601.13997", "abs": "https://arxiv.org/abs/2601.13997", "authors": ["Xuehan Wang", "Jinhong Yuan", "Jintao Wang", "Kehan Huang"], "title": "Achieving Full Multipath Diversity by Random Constellation Rotation: a Theoretical Perspective", "comment": "10 pages, 5 figures. This paper has been accepted for publication in IEEE TSP", "summary": "Diversity is an essential concept associated with communication reliability in multipath channels since it determines the slope of bit error rate performance in the medium to high signal-to-noise ratio regions. However, most of the existing analytical frameworks were developed for specific modulation schemes while the efficient validation of full multipath diversity for general modulation schemes remains an open problem. To fill this research gap, we propose to utilize random constellation rotation to ease the conditions for full-diversity modulation designs. For linearly precoded cyclic-prefix orthogonal frequency division multiplexing (OFDM) systems, we prove that maximum multipath diversity can be attained as long as the spread matrix does not have zero entries, which is a sufficient but easily satisfied condition. Furthermore, we derive the sufficient and necessary condition for general modulation schemes, whose verification can be divided into validation tasks for each column of the modulation matrix. Based on the proposed conditions, maximum diversity order can be attained with the probability of 1 by enabling a randomly generated rotation pattern for both time and doubly dispersive channels. The theoretical analysis in this paper also demonstrates that the diversity evaluation can be concentrated on the pairwise error probability when the number of error symbols is one, which reduces the complexity of diversity-driven design and performance analysis for novel modulation schemes significantly in both time and doubly dispersive channels. Finally, numerical results for various modulation schemes confirm that the theoretical analysis holds in both time and doubly dispersive channels. Furthermore, when employing practical detectors, the random constellation rotation technique consistently enhance the transmission reliability for both coded and uncoded systems."}
{"id": "2601.14080", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.14080", "abs": "https://arxiv.org/abs/2601.14080", "authors": ["Alexander Ihlow", "Marius Schmidt", "Carsten Andrich", "Reiner S. Thomä"], "title": "Background Subtraction with Drift Correction for Bistatic Radar Reflectivity Measurements", "comment": "20th European Conference on Antennas and Propagation (EuCAP 2026)", "summary": "Fundamental research on bistatic radar reflectivity is highly relevant, e.g., to the upcoming mobile communication standard 6G, which includes integrated sensing and communication (ISAC). We introduce a model for correcting instrumentation drift during bistatic radar measurements in anechoic chambers. Usually, background subtraction is applied with the goal to yield the target reflection signal as best as possible while coherently subtracting all signals which were present in both the foreground and background measurement. However, even slight incoherences between the foreground and background measurement process deteriorate the result. We analyze these effects in real measurements in the frequency range 2-18 GHz, taken with the Bistatic Radar (BIRA) measurement facility at TU Ilmenau. Applying our proposed drift correction model, we demonstrate up to 40 dB improvement for the removal of direct line-of-sight antenna crosstalk over the state of the art."}
{"id": "2601.14220", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.14220", "abs": "https://arxiv.org/abs/2601.14220", "authors": ["Wenyi Yan", "Zeyuan Li", "Lu Gan", "Honqing Liu", "Guoquan Li"], "title": "Bit-Efficient Quantisation for Two-Channel Modulo-Sampling Systems", "comment": null, "summary": "Two-channel modulo analog-to-digital converters (ADCs) enable high-dynamic-range signal sensing at the Nyquist rate per channel, but existing designs quantise both channel outputs independently, incurring redundant bitrate costs. This paper proposes a bit-efficient quantisation scheme that exploits the integer-valued structure of inter-channel differences, transmitting one quantised channel output together with a compact difference index. We prove that this approach requires only 1-2 bits per signal sample overhead relative to conventional ADCs, despite operating with a much smaller per-channel dynamic range. Simulations confirm the theoretical error bounds and bitrate analysis, while hardware experiments demonstrate substantial bitrate savings compared with existing modulo sampling schemes, while maintaining comparable reconstruction accuracy. These results highlight a practical path towards high-resolution, bandwidth-efficient modulo ADCs for bitrate-constrained systems."}
{"id": "2601.14233", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.14233", "abs": "https://arxiv.org/abs/2601.14233", "authors": ["Yekta Demirci", "Guillaume Mantelet", "Stephane Martel", "Jean-Francois Frigon", "Gunes Karabulut Kurt"], "title": "Burst Aware Forecasting of User Traffic Demand in LEO Satellite Networks", "comment": "Accepted by IEEE International Conference on Communications (ICC) 2026", "summary": "In Low Earth Orbit (LEO) satellite networks, Beam Hopping (BH) technology enables the efficient utilization of limited radio resources by adapting to varying user demands and link conditions. Effective BH planning requires prior knowledge of upcoming traffic at the time of scheduling, making forecasting an important sub-task. Forecasting becomes particularly critical under heavy load conditions where an unexpected demand burst combined with link degradation may cause buffer overflows and packet loss. To address this challenge, we propose a burst aware forecasting solution. This challenge may arise in a wide range of wireless networks; therefore, the proposed solution is broadly applicable to settings characterized by bursty traffic patterns where accurate demand forecasting is essential. Our approach introduces three key enhancements to a transformer architecture: (i) a distance from the last burst embedding to capture burst proximity, (ii) two additional linear layers in the decoder to forecast both upcoming bursts and their relative impact, and (iii) use of an asymmetric cost function during model training to better capture burst dynamics. Empirical evaluations in an Earth-fixed cell under high-traffic demand scenario demonstrate that the proposed model reduces prediction error by up to 94% at a one-step horizon and maintains the ability to accurately capture bursts even near the end of longer prediction horizons following Mean Square Error (MSE) metric."}
{"id": "2601.14244", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.14244", "abs": "https://arxiv.org/abs/2601.14244", "authors": ["Qing Zhang", "Adham Sakhnini", "Robbert Beerten", "Haoqiu Xiong", "Zhuangzhuang Cui", "Yang Miao", "Sofie Pollin"], "title": "Robust Localization in OFDM-Based Massive MIMO through Phase Offset Calibration", "comment": "Accepted to IEEE International Symposium on Joint Communications & Sensing (JC&S) 2026; recipient of the Best Student Paper Award", "summary": "Accurate localization in Orthogonal Frequency Division Multiplexing (OFDM)-based massive Multiple-Input Multiple-Output (MIMO) systems depends critically on phase coherence across subcarriers and antennas. However, practical systems suffer from frequency-dependent and (spatial) antenna-dependent phase offsets, degrading localization accuracy. This paper analytically studies the impact of phase incoherence on localization performance under a static User Equipment (UE) and Line-of-Sight (LoS) scenario. We use two complementary tools. First, we derive the Cramér-Rao Lower Bound (CRLB) to quantify the theoretical limits under phase offsets. Then, we develop a Spatial Ambiguity Function (SAF)-based model to characterize ambiguity patterns. Simulation results reveal that spatial phase offsets severely degrade localization performance, while frequency phase offsets have a minor effect in the considered system configuration. To address this, we propose a robust Channel State Information (CSI) calibration framework and validate it using real-world measurements from a practical massive MIMO testbed. The experimental results confirm that the proposed calibration framework significantly improves the localization Root Mean Squared Error (RMSE) from 5 m to 1.2 cm, aligning well with the theoretical predictions."}
{"id": "2601.11768", "categories": ["eess.AS", "cs.AI", "cs.LG", "cs.SD", "eess.SP"], "pdf": "https://arxiv.org/pdf/2601.11768", "abs": "https://arxiv.org/abs/2601.11768", "authors": ["Venkat Suprabath Bitra", "Homayoon Beigi"], "title": "Lightweight Self-Supervised Detection of Fundamental Frequency and Accurate Probability of Voicing in Monophonic Music", "comment": "12 pages, 6 figures, 3 tables, and an appendix, Accepted for publication at ICPRAM 2026 in Marbella, Spain, on March 2, 2026", "summary": "Reliable fundamental frequency (F 0) and voicing estimation is essential for neural synthesis, yet many pitch extractors depend on large labeled corpora and degrade under realistic recording artifacts. We propose a lightweight, fully self-supervised framework for joint F 0 estimation and voicing inference, designed for rapid single-instrument training from limited audio. Using transposition-equivariant learning on CQT features, we introduce an EM-style iterative reweighting scheme that uses Shift Cross-Entropy (SCE) consistency as a reliability signal to suppress uninformative noisy/unvoiced frames. The resulting weights provide confidence scores that enable pseudo-labeling for a separate lightweight voicing classifier without manual annotations. Trained on MedleyDB and evaluated on MDB-stem-synth ground truth, our method achieves competitive cross-corpus performance (RPA 95.84, RCA 96.24) and demonstrates cross-instrument generalization."}
{"id": "2601.13629", "categories": ["eess.AS", "eess.SP"], "pdf": "https://arxiv.org/pdf/2601.13629", "abs": "https://arxiv.org/abs/2601.13629", "authors": ["Ziqian Wang", "Xianjun Xia", "Chuanzeng Huang", "Lei Xie"], "title": "S$^2$Voice: Style-Aware Autoregressive Modeling with Enhanced Conditioning for Singing Style Conversion", "comment": "accepted to ICASSP 2026", "summary": "We present S$^2$Voice, the winning system of the Singing Voice Conversion Challenge (SVCC) 2025 for both the in-domain and zero-shot singing style conversion tracks. Built on the strong two-stage Vevo baseline, S$^2$Voice advances style control and robustness through several contributions. First, we integrate style embeddings into the autoregressive large language model (AR LLM) via a FiLM-style layer-norm conditioning and a style-aware cross-attention for enhanced fine-grained style modeling. Second, we introduce a global speaker embedding into the flow-matching transformer to improve timbre similarity. Third, we curate a large, high-quality singing corpus via an automated pipeline for web harvesting, vocal separation, and transcript refinement. Finally, we employ a multi-stage training strategy combining supervised fine-tuning (SFT) and direct preference optimization (DPO). Subjective listening tests confirm our system's superior performance: leading in style similarity and singer similarity for Task 1, and across naturalness, style similarity, and singer similarity for Task 2. Ablation studies demonstrate the effectiveness of our contributions in enhancing style fidelity, timbre preservation, and generalization. Audio samples are available~\\footnote{https://honee-w.github.io/SVC-Challenge-Demo/}."}
{"id": "2601.13849", "categories": ["eess.AS", "cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2601.13849", "abs": "https://arxiv.org/abs/2601.13849", "authors": ["Ziyi Yang", "Li Rao", "Zhengding Luo", "Dongyuan Shi", "Qirui Huang", "Woon-Seng Gan"], "title": "Co-Initialization of Control Filter and Secondary Path via Meta-Learning for Active Noise Control", "comment": null, "summary": "Active noise control (ANC) must adapt quickly when the acoustic environment changes, yet early performance is largely dictated by initialization. We address this with a Model-Agnostic Meta-Learning (MAML) co-initialization that jointly sets the control filter and the secondary-path model for FxLMS-based ANC while keeping the runtime algorithm unchanged. The initializer is pre-trained on a small set of measured paths using short two-phase inner loops that mimic identification followed by residual-noise reduction, and is applied by simply setting the learned initial coefficients. In an online secondary path modeling FxLMS testbed, it yields lower early-stage error, shorter time-to-target, reduced auxiliary-noise energy, and faster recovery after path changes than a baseline without re-initialization. The method provides a simple fast start for feedforward ANC under environment changes, requiring a small set of paths to pre-train."}

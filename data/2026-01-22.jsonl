{"id": "2601.14648", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.14648", "abs": "https://arxiv.org/abs/2601.14648", "authors": ["Qingji Jiang", "Jing jin", "Qixing Wang", "Yuanyuan Tang", "Yang Cao", "Bin Kuang", "Jing Dong", "Siying Lv", "Dongming Wang", "Yongming Huang", "Jiangzhou Wang", "Xiaohu You"], "title": "Experimental Performance of Bidirectional Phase Coherent Transmission and Sensing for mmWave Cell-free Massive MIMO Systems with Reciprocity Calibration", "comment": null, "summary": "Phase synchronization among distributed transmission reception points (TRPs) is a prerequisite for enabling coherent joint transmission and high-precision sensing in millimeter wave (mmWave) cell-free massive multiple-input and multiple-output (MIMO) systems. This paper proposes a bidirectional calibration scheme and a calibration coefficient estimation method for phase synchronization, and presents a calibration coefficient phase tracking method using unilateral uplink/downlink channel state information (CSI). Furthermore, this paper introduces the use of reciprocity calibration to eliminate non-ideal factors in sensing and leverages sensing results to achieve calibration coefficient phase tracking in dynamic scenarios, thus enabling bidirectional empowerment of both communication and sensing. Simulation results demonstrate that the proposed method can effectively implement reciprocal calibration with lower overhead, enabling coherent collaborative transmission, and resolving non-ideal factors to acquire lower sensing error in sensing applications. Experimental results show that, in the mmWave band, over-the-air (OTA) bidirectional calibration enables coherent collaborative transmission for both collaborative TRPs and collaborative user equipments (UEs), achieving beamforming gain and long-time coherent sensing capabilities."}
{"id": "2601.14759", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.14759", "abs": "https://arxiv.org/abs/2601.14759", "authors": ["Syed Luqman Shah", "Nurul Huda Mahmood", "Italo Atzeni"], "title": "Improved GPR-Based CSI Acquisition via Spatial-Correlation Kernel", "comment": "Submiited for possible publication in IEEE", "summary": "Accurate channel estimation with low pilot overhead and computational complexity is key to efficiently utilizing multi-antenna wireless systems. Motivated by the evolution from purely statistical descriptions toward physics- and geometry-aware propagation models, this work focuses on incorporating channel information into a Gaussian process regression (GPR) framework for improving the channel estimation accuracy. In this work, we propose a GPR-based channel estimation framework along with a novel Spatial-correlation (SC) kernel that explicitly captures the channel's second-order statistics. We derive a closed-form expression of the proposed SC-based GPR estimator and prove that its posterior mean is optimal in terms of minimum mean-square error (MMSE) under the same second-order statistics, without requiring the underlying channel distribution to be Gaussian. Our analysis reveals that, with up to 50% pilot overhead reduction, the proposed method achieves the lowest normalized mean-square error, the highest empirical 95% credible-interval coverage, and superior preservation of spectral efficiency compared to benchmark estimators, while maintaining lower computational complexity than the conventional MMSE estimator."}
{"id": "2601.14783", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.14783", "abs": "https://arxiv.org/abs/2601.14783", "authors": ["Zhiqing Wei", "Yucong Du", "Zhiyong Feng", "Haotian Liu", "Yanpeng Cui", "Tao Zhang", "Ying Zhou", "Huici Wu"], "title": "Integrated Sensing, Communication and Control enabled Agile UAV Swarm", "comment": null, "summary": "Uncrewed aerial vehicle (UAV) swarms are pivotal in the applications such as disaster relief, aerial base station (BS) and logistics transportation. These scenarios require the capabilities in accurate sensing, efficient communication and flexible control for real-time and reliable task execution. However, sensing, communication and control are studied independently in traditional research, which limits the overall performance of UAV swarms. To overcome this disadvantage, we propose a deeply coupled scheme of integrated sensing, communication and control (ISCC) for UAV swarms, which is a systemic paradigm that transcends traditional isolated designs of sensing, communication and control by establishing a tightly-coupled closed-loop through the co-optimization of sensing, communication and control. In this article, we firstly analyze the requirements of scenarios and key performance metrics. Subsequently, the enabling technologies are proposed, including communication-and-control-enhanced sensing, sensing-and-control-enhanced communication, and sensing-and-communication-enhanced control. Simulation results validate the performance of the proposed ISCC framework, demonstrating its application potential in the future."}
{"id": "2601.14820", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.14820", "abs": "https://arxiv.org/abs/2601.14820", "authors": ["Maria A van Agthoven", "Marek Polák", "Jan Fiala", "Claude Nelcy Ounounou", "Petr Halada", "Michael Palasser", "Anne Briot-Dietsch", "Alan Kádek", "Kathrin Breuker", "Petr Novák", "Carlos Afonso", "Marc-André Delsuc"], "title": "Absorption mode broadband 2D MS for proteomics and metabolomics", "comment": null, "summary": "Two-dimensional mass spectrometry (2D MS) is a method for tandem mass spectrometry that enables the correlation between precursor and fragment ions without the need for ion isolation. On a Fourier transform ion cyclotron resonance mass spectrometer, the phase correction functions for absorption mode data processing were found to be linear in the precursor ion dimension and quadratic in the fragment ion dimension. Absorption mode data processing on limited data sets has previously shown improvements in signal-to-noise ratio and resolving power by a factor of 2. Here, we have expanded absorption mode data processing to 2D mass spectra regardless of size and frequency range. We have applied absorption mode 2D MS to top-down analysis of variously oxidized ubiquitin proteoforms generated by fast photochemical oxidation of proteins (FPOP) and to an extract of ergot alkaloids. We show that absorption mode data processing significantly improves both the signal-to-noise ratio and the resolving power of the 2D mass spectrum compared to standard magnitude mode in terms of sequence coverage in top-down proteomics, as well as the accuracy of precursor-fragment correlation in metabolomics."}
{"id": "2601.14356", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2601.14356", "abs": "https://arxiv.org/abs/2601.14356", "authors": ["Carlos Hernandez-Olivan", "Hendrik Vincent Koops", "Hao Hao Tan", "Elio Quinton"], "title": "Single-step Controllable Music Bandwidth Extension With Flow Matching", "comment": "Accepted at the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) 2026", "summary": "Audio restoration consists in inverting degradations of a digital audio signal to recover what would have been the pristine quality signal before the degradation occurred. This is valuable in contexts such as archives of music recordings, particularly those of precious historical value, for which a clean version may have been lost or simply does not exist. Recent work applied generative models to audio restoration, showing promising improvement over previous methods, and opening the door to the ability to perform restoration operations that were not possible before. However, making these models finely controllable remains a challenge. In this paper, we propose an extension of FLowHigh and introduce the Dynamic Spectral Contour (DSC) as a control signal for bandwidth extension via classifier-free guidance. Our experiments show competitive model performance, and indicate that DSC is a promising feature to support fine-grained conditioning."}
{"id": "2601.14516", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2601.14516", "abs": "https://arxiv.org/abs/2601.14516", "authors": ["Saba Tabatabaee", "Carol Espy-Wilson"], "title": "Towards noise-robust speech inversion through multi-task learning with speech enhancement", "comment": "Accepted for presentation at ICASSP 2026", "summary": "Recent studies demonstrate the effectiveness of Self Supervised Learning (SSL) speech representations for Speech Inversion (SI). However, applying SI in real-world scenarios remains challenging due to the pervasive presence of background noise. We propose a unified framework that integrates Speech Enhancement (SE) and SI models through shared SSL-based speech representations. In this framework, the SSL model is trained not only to support the SE module in suppressing noise but also to produce representations that are more informative for the SI task, allowing both modules to benefit from joint training. At a Signal-to-Noise Ratio of -5 db, our method for the SI task achieves relative improvements over the baseline of 80.95% under babble noise and 38.98% under non-babble noise, as measured by the average Pearson product-moment correlation across all estimated parameters."}
{"id": "2601.14868", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.14868", "abs": "https://arxiv.org/abs/2601.14868", "authors": ["Ran Yang", "Ning Wei", "Zheng Dong", "Lin Zhang", "Wanting Lyu", "Yue Xiu", "Ahmad Bazzi", "Chadi Assi"], "title": "Movable Antenna Empowered Covert Dual-Functional Radar-Communication", "comment": null, "summary": "Movable antenna (MA) has emerged as a promising technology to flexibly reconfigure wireless channels by adjusting antenna placement. In this paper, we study a secured dual-functional radar-communication (DFRC) system aided by movable antennas. To enhance the communication security, we aim to maximize the achievable sum rate by jointly optimizing the transmitter beamforming vectors, receiving filter, and antenna placement, subject to radar signal-to-noise ratio (SINR) and transmission covertness constraints. We consider multiple Willies operating in both non-colluding and colluding modes. For noncolluding Willies, we first employ a Lagrangian dual transformation procedure to reformulate the challenging optimization problem into a more tractable form. Subsequently, we develop an efficient block coordinate descent (BCD) algorithm that integrates semidefinite relaxation (SDR), projected gradient descent (PGD), Dinkelbach transformation, and successive convex approximation (SCA) techniques to tackle the resulting problem. For colluding Willies, we first derive the minimum detection error probability (DEP) by characterizing the optimal detection statistic, which is proven to follow the generalized Erlang distribution. Then, we develop a minimum mean square error (MMSE)-based algorithm to address the colluding detection problem. We further provide a comprehensive complexity analysis on the unified design framework. Simulation results demonstrate that the proposed method can significantly improve the covert sum rate, and achieve a superior balance between communication and radar performance compared with existing benchmark schemes."}
{"id": "2601.14472", "categories": ["cs.SD", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.14472", "abs": "https://arxiv.org/abs/2601.14472", "authors": ["Mohammed Salah Al-Radhi", "Riad Larbi", "Mátyás Bartalis", "Géza Németh"], "title": "Prosody-Guided Harmonic Attention for Phase-Coherent Neural Vocoding in the Complex Spectrum", "comment": "5 pages, 2 figures, 1 table. Accepted for presentation at ICASSP 2026", "summary": "Neural vocoders are central to speech synthesis; despite their success, most still suffer from limited prosody modeling and inaccurate phase reconstruction. We propose a vocoder that introduces prosody-guided harmonic attention to enhance voiced segment encoding and directly predicts complex spectral components for waveform synthesis via inverse STFT. Unlike mel-spectrogram-based approaches, our design jointly models magnitude and phase, ensuring phase coherence and improved pitch fidelity. To further align with perceptual quality, we adopt a multi-objective training strategy that integrates adversarial, spectral, and phase-aware losses. Experiments on benchmark datasets demonstrate consistent gains over HiFi-GAN and AutoVocoder: F0 RMSE reduced by 22 percent, voiced/unvoiced error lowered by 18 percent, and MOS scores improved by 0.15. These results show that prosody-guided attention combined with direct complex spectrum modeling yields more natural, pitch-accurate, and robust synthetic speech, setting a strong foundation for expressive neural vocoding."}
{"id": "2601.14620", "categories": ["eess.AS", "cs.AI", "cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2601.14620", "abs": "https://arxiv.org/abs/2601.14620", "authors": ["Wenda Zhang", "Hongyu Jin", "Siyi Wang", "Zhiqiang Wei", "Ting Dang"], "title": "Scaling Ambiguity: Augmenting Human Annotation in Speech Emotion Recognition with Audio-Language Models", "comment": "Accepted by ICASSP 2026", "summary": "Speech Emotion Recognition models typically use single categorical labels, overlooking the inherent ambiguity of human emotions. Ambiguous Emotion Recognition addresses this by representing emotions as probability distributions, but progress is limited by unreliable ground-truth distributions inferred from sparse human annotations. This paper explores whether Large Audio-Language Models (ALMs) can mitigate the annotation bottleneck by generating high-quality synthetic annotations. We introduce a framework leveraging ALMs to create Synthetic Perceptual Proxies, augmenting human annotations to improve ground-truth distribution reliability. We validate these proxies through statistical analysis of their alignment with human distributions and evaluate their impact by fine-tuning ALMs with the augmented emotion distributions. Furthermore, to address class imbalance and enable unbiased evaluation, we propose DiME-Aug, a Distribution-aware Multimodal Emotion Augmentation strategy. Experiments on IEMOCAP and MSP-Podcast show that synthetic annotations enhance emotion distribution, especially in low-ambiguity regions where annotation agreement is high. However, benefits diminish for highly ambiguous emotions with greater human disagreement. This work provides the first evidence that ALMs could address annotation scarcity in ambiguous emotion recognition, but highlights the need for more advanced prompting or generation strategies to handle highly ambiguous cases."}
{"id": "2601.14881", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.14881", "abs": "https://arxiv.org/abs/2601.14881", "authors": ["Lucas Giroto", "Ândrei Camponogara", "Yueheng Li", "Jiayi Chen", "Lukas Sigg", "Thomas Zwick", "Benjamin Nuss"], "title": "Analysis of Sensing in OFDM-based ISAC under the Influence of Sampling Jitter", "comment": null, "summary": "To enable integrated sensing and communication (ISAC) in cellular networks, a wide range of additional requirements and challenges are either imposed or become more critical. One such impairment is sampling jitter (SJ), which arises due to imperfections in the sampling instants of the clocks of digital-to-analog converters (DACs) and analog-to-digital converters (ADCs). While SJ is already well studied for communication systems based on orthogonal frequency-division multiplexing (OFDM), which is expected to be the waveform of choice for most sixth-generation (6G) scenarios where ISAC could be possible, the implications of SJ on the OFDM-based radar sensing must still be thoroughly analyzed. Considering that phase-locked loop (PLL)-based oscillators are used to derive sampling clocks, which leads to colored SJ, i.e., SJ with non-flat power spectral density, this article analyzes the resulting distortion of the adopted digital constellation modulation and sensing performance in OFDM-based ISAC for both baseband (BB) and bandpass (BP) sampling strategies and different oversampling factors. For BB sampling, it is seen that SJ induces intercarrier interference (ICI), while for BP sampling, it causes carrier phase error and more severe ICI due to a phase noise-like effect at the digital intermediate frequency. Obtained results for a single-input single-output OFDM-based ISAC system with various OFDM signal parameterizations demonstrate that SJ-induced degradation becomes non-negligible for both BB and BP sampling only for root mean square (RMS) SJ values above 10^-11 s at both DAC and ADC, which corresponds to 0.5*10^-2 times the considered critical sampling period without oversampling. Based on the achieved results, it can be concluded that state-of-the-art hardware enables sufficient communication and sensing robustness against SJ, as RMS SJ values in the femtosecond range can be achieved."}
{"id": "2601.14684", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2601.14684", "abs": "https://arxiv.org/abs/2601.14684", "authors": ["Kanami Imamura", "Tomohiko Nakamura", "Kohei Yatabe", "Hiroshi Saruwatari"], "title": "Dissecting Performance Degradation in Audio Source Separation under Sampling Frequency Mismatch", "comment": "Accepted for ICASSP 2026", "summary": "Audio processing methods based on deep neural networks are typically trained at a single sampling frequency (SF). To handle untrained SFs, signal resampling is commonly employed, but it can degrade performance, particularly when the input SF is lower than the trained SF. This paper investigates the causes of this degradation through two hypotheses: (i) the lack of high-frequency components introduced by up-sampling, and (ii) the greater importance of their presence than their precise representation. To examine these hypotheses, we compare conventional resampling with three alternatives: post-resampling noise addition, which adds Gaussian noise to the resampled signal; noisy-kernel resampling, which perturbs the kernel with Gaussian noise to enrich high-frequency components; and trainable-kernel resampling, which adapts the interpolation kernel through training. Experiments on music source separation show that noisy-kernel and trainable-kernel resampling alleviate the degradation observed with conventional resampling. We further demonstrate that noisy-kernel resampling is effective across diverse models, highlighting it as a simple yet practical option."}
{"id": "2601.14699", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2601.14699", "abs": "https://arxiv.org/abs/2601.14699", "authors": ["Ju-ho Kim", "Youngmoon Jung", "Joon-Young Yang", "Jaeyoung Roh", "Chang Woo Han", "Hoon-Young Cho"], "title": "Triage knowledge distillation for speaker verification", "comment": "5 pages, 2 figures, Accepted at ICASSP 2026", "summary": "Deploying speaker verification on resource-constrained devices remains challenging due to the computational cost of high-capacity models; knowledge distillation (KD) offers a remedy. Classical KD entangles target confidence with non-target structure in a Kullback-Leibler term, limiting the transfer of relational information. Decoupled KD separates these signals into target and non-target terms, yet treats non-targets uniformly and remains vulnerable to the long tail of low-probability classes in large-class settings. We introduce Triage KD (TRKD), a distillation scheme that operationalizes assess-prioritize-focus. TRKD introduces a cumulative-probability cutoff $τ$ to assess per-example difficulty and partition the teacher posterior into three groups: the target class, a high-probability non-target confusion-set, and a background-set. To prioritize informative signals, TRKD distills the confusion-set conditional distribution and discards the background. Concurrently, it transfers a three-mass (target/confusion/background) that capture sample difficulty and inter-class confusion. Finally, TRKD focuses learning via a curriculum on $τ$: training begins with a larger $τ$ to convey broad non-target context, then $τ$ is progressively decreased to shrink the confusion-set, concentrating supervision on the most confusable classes. In extensive experiments on VoxCeleb1 with both homogeneous and heterogeneous teacher-student pairs, TRKD was consistently superior to recent KD variants and attained the lowest EER across all protocols."}
{"id": "2601.14953", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.14953", "abs": "https://arxiv.org/abs/2601.14953", "authors": ["Advaith Arun", "Shiv Shankar", "Dhivagar Baskaran", "Klutto Milleth", "Bhaskar Ramamurthi"], "title": "Deep Learning assisted Port-Cycling based Channel Sounding for Precoder Estimation in Massive MIMO Arrays", "comment": "6 pages, 9 figures", "summary": "Future wireless systems are expected to employ a substantially larger number of transmit ports for channel state information (CSI) estimation compared to current specifications. Although scaling ports improves spectral efficiency, it also increases the resource overhead to transmit reference signals across the time-frequency grid, ultimately reducing achievable data throughput. In this work, we propose an deep learning (DL)-based CSI reconstruction framework that serves as an enabler for reliable CSI acquisition in future 6G systems. The proposed solution involves designing a port-cycling mechanism that sequentially sounds different portions of CSI ports across time, thereby lowering the overhead while preserving channel observability. The proposed CSI Adaptive Network (CsiAdaNet) model exploits the resulting sparse measurements and captures both spatial and temporal correlations to accurately reconstruct the full-port CSI. The simulation results show that our method achieves overhead reduction while maintaining high CSI reconstruction accuracy."}
{"id": "2601.14744", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.14744", "abs": "https://arxiv.org/abs/2601.14744", "authors": ["Hongfu Liu", "Zhouying Cui", "Xiangming Gu", "Ye Wang"], "title": "Unlocking Large Audio-Language Models for Interactive Language Learning", "comment": "Accepted to the Findings of EACL 2026", "summary": "Achieving pronunciation proficiency in a second language (L2) remains a challenge, despite the development of Computer-Assisted Pronunciation Training (CAPT) systems. Traditional CAPT systems often provide unintuitive feedback that lacks actionable guidance, limiting its effectiveness. Recent advancements in audio-language models (ALMs) offer the potential to enhance these systems by providing more user-friendly feedback. In this work, we investigate ALMs for chat-based pronunciation training by introducing L2-Arctic-plus, an English dataset with detailed error explanations and actionable suggestions for improvement. We benchmark cascaded ASR+LLMs and existing ALMs on this dataset, specifically in detecting mispronunciation and generating actionable feedback. To improve the performance, we further propose to instruction-tune ALMs on L2-Arctic-plus. Experimental results demonstrate that our instruction-tuned models significantly outperform existing baselines on mispronunciation detection and suggestion generation in terms of both objective and human evaluation, highlighting the value of the proposed dataset."}
{"id": "2601.14721", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2601.14721", "abs": "https://arxiv.org/abs/2601.14721", "authors": ["Ruixing Ren", "Junhui Zhao", "Xiaoke Sun", "Qiuping Li"], "title": "NLP-Based Review for Toxic Comment Detection Tailored to the Chinese Cyberspace", "comment": "20 pages, 6 figures. This review focuses on toxic comment detection in Chinese cyberspace", "summary": "With the in-depth integration of mobile Internet and widespread adoption of social platforms, user-generated content in the Chinese cyberspace has witnessed explosive growth. Among this content, the proliferation of toxic comments poses severe challenges to individual mental health, community atmosphere and social trust. Owing to the strong context dependence, cultural specificity and rapid evolution of Chinese cyber language, toxic expressions are often conveyed through complex forms such as homophones and metaphors, imposing notable limitations on traditional detection methods. To address this issue, this review focuses on the core topic of natural language processing based toxic comment detection in the Chinese cyberspace, systematically collating and critically analyzing the research progress and key challenges in this field. This review first defines the connotation and characteristics of Chinese toxic comments, and analyzes the platform ecology and transmission mechanisms they rely on. It then comprehensively reviews the construction methods and limitations of existing public datasets, and proposes a novel fine-grained and scalable framework for toxic comment definition and classification, along with corresponding data annotation and quality assessment strategies. We systematically summarize the evolutionary path of detection models from traditional methods to deep learning, with special emphasis on the importance of interpretability in model design. Finally, we thoroughly discuss the open challenges faced by current research and provide forward-looking suggestions for future research directions."}
{"id": "2601.15004", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.15004", "abs": "https://arxiv.org/abs/2601.15004", "authors": ["Nipun Agarwal"], "title": "Alternative Shapes of Modulation Schemes Detailed Exposition and Simulation Methodology", "comment": null, "summary": "Modulation constellation design is a core challenge in digital communications, especially under stringent demands on spectral efficiency, robustness, and energy consumption. Classical schemes like PSK and QAM, while analytically tractable, often lose optimality under realistic channels and nonlinear hardware constraints. This paper provides a unified study of constellation design from geometric, probabilistic, optimization, and machine learning perspectives, focusing on symbol error rate (SER), fading robustness, peak-to-average power ratio (PAPR), and energy efficiency. We evaluate classical, lattice-based, asymmetric, probabilistically shaped, Golden Angle, heuristic-optimized, and machine learning assisted constellations under AWGN and Rayleigh fading via large-scale Monte Carlo simulations. Incorporating PAPR-aware and power amplifier models reveals that SER-optimal designs are not always energy-optimal; small SER trade-offs can yield substantial energy savings. Machine learning approaches offer flexible joint optimization of reliability, robustness, and energy efficiency by embedding channel and hardware constraints into the learning objective."}
{"id": "2601.14786", "categories": ["cs.SD", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.14786", "abs": "https://arxiv.org/abs/2601.14786", "authors": ["Wei-Jaw Lee", "Fang-Chih Hsieh", "Xuanjun Chen", "Fang-Duo Tsai", "Yi-Hsuan Yang"], "title": "Training-Efficient Text-to-Music Generation with State-Space Modeling", "comment": "9 pages, 3 figures. This is a preprint of a paper submitted to IEEE/ACM TASLP", "summary": "Recent advances in text-to-music generation (TTM) have yielded high-quality results, but often at the cost of extensive compute and the use of large proprietary internal data. To improve the affordability and openness of TTM training, an open-source generative model backbone that is more training- and data-efficient is needed. In this paper, we constrain the number of trainable parameters in the generative model to match that of the MusicGen-small benchmark (with about 300M parameters), and replace its Transformer backbone with the emerging class of state-space models (SSMs). Specifically, we explore different SSM variants for sequence modeling, and compare a single-stage SSM-based design with a decomposable two-stage SSM/diffusion hybrid design. All proposed models are trained from scratch on a purely public dataset comprising 457 hours of CC-licensed music, ensuring full openness. Our experimental findings are three-fold. First, we show that SSMs exhibit superior training efficiency compared to the Transformer counterpart. Second, despite using only 9% of the FLOPs and 2% of the training data size compared to the MusicGen-small benchmark, our model achieves competitive performance in both objective metrics and subjective listening tests based on MusicCaps captions. Finally, our scaling-down experiment demonstrates that SSMs can maintain competitive performance relative to the Transformer baseline even at the same training budget (measured in iterations), when the model size is reduced to four times smaller. To facilitate the democratization of TTM research, the processed captions, model checkpoints, and source code are available on GitHub via the project page: https://lonian6.github.io/ssmttm/."}
{"id": "2601.14728", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2601.14728", "abs": "https://arxiv.org/abs/2601.14728", "authors": ["Chun-Yi Kuan", "Kai-Wei Chang", "Hung-yi Lee"], "title": "AQAScore: Evaluating Semantic Alignment in Text-to-Audio Generation via Audio Question Answering", "comment": "Manuscript in progress", "summary": "Although text-to-audio generation has made remarkable progress in realism and diversity, the development of evaluation metrics has not kept pace. Widely-adopted approaches, typically based on embedding similarity like CLAPScore, effectively measure general relevance but remain limited in fine-grained semantic alignment and compositional reasoning. To address this, we introduce AQAScore, a backbone-agnostic evaluation framework that leverages the reasoning capabilities of audio-aware large language models (ALLMs). AQAScore reformulates assessment as a probabilistic semantic verification task; rather than relying on open-ended text generation, it estimates alignment by computing the exact log-probability of a \"Yes\" answer to targeted semantic queries. We evaluate AQAScore across multiple benchmarks, including human-rated relevance, pairwise comparison, and compositional reasoning tasks. Experimental results show that AQAScore consistently achieves higher correlation with human judgments than similarity-based metrics and generative prompting baselines, showing its effectiveness in capturing subtle semantic inconsistencies and scaling with the capability of underlying ALLMs."}
{"id": "2601.15024", "categories": ["eess.SP", "stat.CO"], "pdf": "https://arxiv.org/pdf/2601.15024", "abs": "https://arxiv.org/abs/2601.15024", "authors": ["Nipun Agarwal"], "title": "Physical Layer Security in Massive MIMO: Challenges and Open Research Directions Against Passive Eavesdroppers", "comment": null, "summary": "Massive Multiple-Input Multiple-Output (MIMO) has become a crucial enabling technology for 5G and beyond, providing previously unheard-of increases in energy and spectrum efficiency. It is still difficult to guarantee secure communication in these systems, particularly when it comes to passive eavesdroppers whose base station is unaware of their channel state information. By taking advantage of the inherent randomness of wireless channels, Physical Layer Security (PLS) offers a promising paradigm; however, its efficacy in massive MIMO is heavily reliant on resource allocation and transmission strategies. In this work, the performance of secure transmission schemes, such as Maximum Ratio Transmission (MRT), Zero-Forcing (ZF), and Artificial Noise (AN)-aided beamforming, is examined when passive eavesdroppers are present. This work will use extensive Monte Carlo simulations to assess important performance metrics such as energy efficiency, secrecy outage probability, and secrecy sum rate under different system parameters (e.g., number of antennas, Signal-to-Noise Ratio (SNR), power allocation). The results aim to provide comparative insight into the strengths and limitations of different PLS strategies and to highlight open research directions to design scalable, energy-efficient, and robust secure transmission techniques in future 6G networks."}
{"id": "2601.14850", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2601.14850", "abs": "https://arxiv.org/abs/2601.14850", "authors": ["Viola Negroni", "Luca Cuccovillo", "Paolo Bestagini", "Patrick Aichroth", "Stefano Tubaro"], "title": "Multi-Tast Transformer for Explainable Speech Deepfake Detection via Formant Modeling", "comment": "Accepted @ IEEE ICASSP 2026", "summary": "In this work, we introduce a multi-task transformer for speech deepfake detection, capable of predicting formant trajectories and voicing patterns over time, ultimately classifying speech as real or fake, and highlighting whether its decisions rely more on voiced or unvoiced regions. Building on a prior speaker-formant transformer architecture, we streamline the model with an improved input segmentation strategy, redesign the decoding process, and integrate built-in explainability. Compared to the baseline, our model requires fewer parameters, trains faster, and provides better interpretability, without sacrificing prediction performance."}
{"id": "2601.14751", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2601.14751", "abs": "https://arxiv.org/abs/2601.14751", "authors": ["Steven Vander Eeckt", "Hugo Van hamme"], "title": "Inverse-Hessian Regularization for Continual Learning in ASR", "comment": "Accepted for presentation at ICASSP 2026", "summary": "Catastrophic forgetting remains a major challenge for continual learning (CL) in automatic speech recognition (ASR), where models must adapt to new domains without losing performance on previously learned conditions. Several CL methods have been proposed for ASR, and, recently, weight averaging - where models are averaged in a merging step after fine-tuning - has proven effective as a simple memory-free strategy. However, it is heuristic in nature and ignores the underlying loss landscapes of the tasks, hindering adaptability. In this work, we propose Inverse Hessian Regularization (IHR), a memory-free approach for CL in ASR that incorporates curvature information into the merging step. After fine-tuning on a new task, the adaptation is adjusted through a Kronecker-factored inverse Hessian approximation of the previous task, ensuring that the model moves primarily in directions less harmful to past performance, while keeping the method lightweight. We evaluate IHR on two CL benchmarks and show that it significantly outperforms state-of-the-art baselines, reducing forgetting while improving adaptability. Ablation studies and analyses further confirm its effectiveness."}
{"id": "2601.15097", "categories": ["eess.SP", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.15097", "abs": "https://arxiv.org/abs/2601.15097", "authors": ["Johanna Wilroth", "Oskar Keding", "Martin A. Skoglund", "Maria Sandsten", "Martin Enqvist", "Emina Alickovic"], "title": "Neural Tracking of Sustained Attention, Attention Switching, and Natural Conversation in Audiovisual Environments using Mobile EEG", "comment": "Submitted to European Journal of Neuroscience", "summary": "Everyday communication is dynamic and multisensory, often involving shifting attention, overlapping speech and visual cues. Yet, most neural attention tracking studies are still limited to highly controlled lab settings, using clean, often audio-only stimuli and requiring sustained attention to a single talker. This work addresses that gap by introducing a novel dataset from 24 normal-hearing participants. We used a mobile electroencephalography (EEG) system (44 scalp electrodes and 20 cEEGrid electrodes) in an audiovisual (AV) paradigm with three conditions: sustained attention to a single talker in a two-talker environment, attention switching between two talkers, and unscripted two-talker conversations with a competing single talker. Analysis included temporal response functions (TRFs) modeling, optimal lag analysis, selective attention classification with decision windows ranging from 1.1s to 35s, and comparisons of TRFs for attention to AV conversations versus side audio-only talkers. Key findings show significant differences in the attention-related P2-peak between attended and ignored speech across conditions for scalp EEG. No significant change in performance between switching and sustained attention suggests robustness for attention switches. Optimal lag analysis revealed narrower peak for conversation compared to single-talker AV stimuli, reflecting the additional complexity of multi-talker processing. Classification of selective attention was consistently above chance (55-70% accuracy) for scalp EEG, while cEEGrid data yielded lower correlations, highlighting the need for further methodological improvements. These results demonstrate that mobile EEG can reliably track selective attention in dynamic, multisensory listening scenarios and provide guidance for designing future AV paradigms and real-world attention tracking applications."}
{"id": "2601.14931", "categories": ["cs.SD", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.14931", "abs": "https://arxiv.org/abs/2601.14931", "authors": ["Nouhoum Coulibaly", "Ousmane Ly", "Michael Leventhal", "Ousmane Goro"], "title": "Generative Artificial Intelligence, Musical Heritage and the Construction of Peace Narratives: A Case Study in Mali", "comment": "12 pages, 2 figures", "summary": "This study explores the capacity of generative artificial intelligence (Gen AI) to contribute to the construction of peace narratives and the revitalization of musical heritage in Mali. The study has been made in a political and social context where inter-community tensions and social fractures motivate a search for new symbolic frameworks for reconciliation. The study empirically explores three questions: (1) how Gen AI can be used as a tool for musical creation rooted in national languages and traditions; (2) to what extent Gen AI systems enable a balanced hybridization between technological innovation and cultural authenticity; and (3) how AI-assisted musical co-creation can strengthen social cohesion and cultural sovereignty. The experimental results suggest that Gen AI, embedded in a culturally conscious participatory framework, can act as a catalyst for symbolic diplomacy, amplifying local voices instead of standardizing them. However, challenges persist regarding the availability of linguistic corpora, algorithmic censorship, and the ethics of generating compositions derived from copyrighted sources."}
{"id": "2601.14770", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2601.14770", "abs": "https://arxiv.org/abs/2601.14770", "authors": ["Tobias Raichle", "Erfan Amini", "Bin Yang"], "title": "Test-Time Adaptation For Speech Enhancement Via Mask Polarization", "comment": "Accepted at ICASSP 2026", "summary": "Adapting speech enhancement (SE) models to unseen environments is crucial for practical deployments, yet test-time adaptation (TTA) for SE remains largely under-explored due to a lack of understanding of how SE models degrade under domain shifts. We observe that mask-based SE models lose confidence under domain shifts, with predicted masks becoming flattened and losing decisive speech preservation and noise suppression. Based on this insight, we propose mask polarization (MPol), a lightweight TTA method that restores mask bimodality through distribution comparison using the Wasserstein distance. MPol requires no additional parameters beyond the trained model, making it suitable for resource-constrained edge deployments. Experimental results across diverse domain shifts and architectures demonstrate that MPol achieves very consistent gains that are competitive with significantly more complex approaches."}
{"id": "2601.15126", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.15126", "abs": "https://arxiv.org/abs/2601.15126", "authors": ["Robin Rajamäki", "Visa Koivunen"], "title": "Sparse Sensor Arrays for Active Sensing: Models, Configurations and Applications", "comment": null, "summary": "This chapter focuses on active sensing using sparse arrays. In active sensing applications, such as radar, sonar, wireless communications, and medical ultrasound, a collection of sensors probes the environment by emitting self-generated energy. A key benefit of such active multi-sensor arrays is their ability to focus and steer energy in desired directions by beamforming on transmit. Sparse sensor arrays offer several advantages over conventional uniform arrays, including improved resolution using fewer physical sensors and the capability to identify more scatterers than sensors. This is facilitated by the effective transmit-receive virtual array known as the sum co-array, which can have many more virtual sensors than the number of physical transmit or receive sensors. Herein, we focus on the design of low-redundancy sparse array configurations and on employing transmit-receive (Tx-Rx) beamforming using sparse arrays. We discuss the optimal, but computationally intractable Minimum-redundancy array, and a scalable symmetric array framework, which extends many well-known passive sparse array geometries to the active case. We also examine mitigating side lobes arising from spatial undersampling by a synthetic beamforming method known as image addition. We briefly present approaches for finding the physical beamforming weights synthesizing a desired Tx-Rx beampattern, and consider related spatio-temporal trade-offs. We conclude by discussing selected applications of sparse arrays in active sensing."}
{"id": "2601.14960", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.14960", "abs": "https://arxiv.org/abs/2601.14960", "authors": ["Florian Grötschla", "Arunasish Sen", "Alessandro Lombardi", "Guillermo Cámbara", "Andreas Schwarz"], "title": "VCNAC: A Variable-Channel Neural Audio Codec for Mono, Stereo, and Surround Sound", "comment": "Submitted to EUSIPCO 2026", "summary": "We present VCNAC, a variable channel neural audio codec. Our approach features a single encoder and decoder parametrization that enables native inference for different channel setups, from mono speech to cinematic 5.1 channel surround audio. Channel compatibility objectives ensure that multi-channel content maintains perceptual quality when decoded to fewer channels. The shared representation enables training of generative language models on a single set of codebooks while supporting inference-time scalability across modalities and channel configurations. Evaluation using objective spatial audio metrics and subjective listening tests demonstrates that our unified approach maintains high reconstruction quality across mono, stereo, and surround audio configurations."}
{"id": "2601.14925", "categories": ["eess.AS", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.14925", "abs": "https://arxiv.org/abs/2601.14925", "authors": ["Nicolás Arrieta Larraza", "Niels de Koeijer"], "title": "Fast-ULCNet: A fast and ultra low complexity network for single-channel speech enhancement", "comment": "©2026 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works", "summary": "Single-channel speech enhancement algorithms are often used in resource-constrained embedded devices, where low latency and low complexity designs gain more importance. In recent years, researchers have proposed a wide variety of novel solutions to this problem. In particular, a recent deep learning model named ULCNet is among the state-of-the-art approaches in this domain. This paper proposes an adaptation of ULCNet, by replacing its GRU layers with FastGRNNs, to reduce both computational latency and complexity. Furthermore, this paper shows empirical evidence on the performance decay of FastGRNNs in long audio signals during inference due to internal state drifting, and proposes a novel approach based on a trainable complementary filter to mitigate it. The resulting model, Fast-ULCNet, performs on par with the state-of-the-art original ULCNet architecture on a speech enhancement task, while reducing its model size by more than half and decreasing its latency by 34% on average."}
{"id": "2601.15145", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.15145", "abs": "https://arxiv.org/abs/2601.15145", "authors": ["Victoria Palhares", "Artjom Grudnitsky", "Silvio Mandelli"], "title": "Weather Estimation for Integrated Sensing and Communication", "comment": "This work has been submitted to IEEE for possible publication", "summary": "One of the key features of sixth generation (6G) mobile communications will be integrated sensing and communication (ISAC). While the main goal of ISAC in standardization efforts is to detect objects, the byproducts of radar operations can be used to enable new services in 6G, such as weather sensing. Even though weather radars are the most prominent technology for weather detection and monitoring, they are expensive and usually neglect areas in close vicinity. To this end, we propose reusing the dense deployment of 6G base stations for weather sensing purposes by detecting and estimating weather conditions. We implement both a classifier and a regressor as a convolutional neural network trained across measurements with varying precipitation rates and wind speeds. We implement our approach in an ISAC proof-of-concept, and conduct a multi-week experiment campaign. Experimental results show that we are able to jointly and accurately classify weather conditions with accuracies of 99.38% and 98.99% for precipitation rate and wind speed, respectively. For estimation, we obtain errors of 1.2 mm/h and 1.5 km/h, for precipitation rate and wind speed, respectively. These findings indicate that weather sensing services can be reliably deployed in 6G ISAC networks, broadening their service portfolio and boosting their market value."}
{"id": "2601.15083", "categories": ["cs.SD", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.15083", "abs": "https://arxiv.org/abs/2601.15083", "authors": ["Muntakimur Rahaman", "Md Mahmudul Hoque", "Md Mehedi Hassain"], "title": "Bangla Music Genre Classification Using Bidirectional LSTMS", "comment": null, "summary": "Bangla music is enrich in its own music cultures. Now a days music genre classification is very significant because of the exponential increase in available music, both in digital and physical formats. It is necessary to index them accordingly to facilitate improved retrieval. Automatically classifying Bangla music by genre is essential for efficiently locating specific pieces within a vast and diverse music library. Prevailing methods for genre classification predominantly employ conventional machine learning or deep learning approaches. This work introduces a novel music dataset comprising ten distinct genres of Bangla music. For the task of audio classification, we utilize a recurrent neural network (RNN) architecture. Specifically, a Long Short-Term Memory (LSTM) network is implemented to train the model and perform the classification. Feature extraction represents a foundational stage in audio data processing. This study utilizes Mel-Frequency Cepstral Coefficients (MFCCs) to transform raw audio waveforms into a compact and representative set of features. The proposed framework facilitates music genre classification by leveraging these extracted features. Experimental results demonstrate a classification accuracy of 78%, indicating the system's strong potential to enhance and streamline the organization of Bangla music genres."}
{"id": "2601.14744", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.14744", "abs": "https://arxiv.org/abs/2601.14744", "authors": ["Hongfu Liu", "Zhouying Cui", "Xiangming Gu", "Ye Wang"], "title": "Unlocking Large Audio-Language Models for Interactive Language Learning", "comment": "Accepted to the Findings of EACL 2026", "summary": "Achieving pronunciation proficiency in a second language (L2) remains a challenge, despite the development of Computer-Assisted Pronunciation Training (CAPT) systems. Traditional CAPT systems often provide unintuitive feedback that lacks actionable guidance, limiting its effectiveness. Recent advancements in audio-language models (ALMs) offer the potential to enhance these systems by providing more user-friendly feedback. In this work, we investigate ALMs for chat-based pronunciation training by introducing L2-Arctic-plus, an English dataset with detailed error explanations and actionable suggestions for improvement. We benchmark cascaded ASR+LLMs and existing ALMs on this dataset, specifically in detecting mispronunciation and generating actionable feedback. To improve the performance, we further propose to instruction-tune ALMs on L2-Arctic-plus. Experimental results demonstrate that our instruction-tuned models significantly outperform existing baselines on mispronunciation detection and suggestion generation in terms of both objective and human evaluation, highlighting the value of the proposed dataset."}
{"id": "2601.15118", "categories": ["cs.SD", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.15118", "abs": "https://arxiv.org/abs/2601.15118", "authors": ["Gokul Karthik Kumar", "Ludovick Lepauloux", "Hakim Hacid"], "title": "WavLink: Compact Audio--Text Embeddings with a Global Whisper Token", "comment": "Accepted at ICASSP 2026", "summary": "Whisper has become the de-facto encoder for extracting general-purpose audio features in large audio-language models, where a 30-second clip is typically represented by 1500 frame features projected into an LLM. In contrast, audio-text embedding models like CLAP-based models have largely relied on alternative audio encoders (e.g., HTS-AT, PaSST), and have not leveraged Whisper effectively. We present WavLink, a compact audio-text embedding model that augments Whisper encoder with a learnable global token, trained jointly with a text encoder. Through a systematic study of design choices, including pretrained text encoders, loss functions, training modes, and data mixtures, we identify configurations that yield state-of-the-art retrieval performance. Our two-stage training recipe across three model sizes, combined with Matryoshka-style supervision, improves scalability, enabling 8x smaller embeddings with minimal performance drop. WavLink also demonstrates competitive performance on AIR-Bench with MCQs and zero-shot classification."}
{"id": "2601.14960", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.14960", "abs": "https://arxiv.org/abs/2601.14960", "authors": ["Florian Grötschla", "Arunasish Sen", "Alessandro Lombardi", "Guillermo Cámbara", "Andreas Schwarz"], "title": "VCNAC: A Variable-Channel Neural Audio Codec for Mono, Stereo, and Surround Sound", "comment": "Submitted to EUSIPCO 2026", "summary": "We present VCNAC, a variable channel neural audio codec. Our approach features a single encoder and decoder parametrization that enables native inference for different channel setups, from mono speech to cinematic 5.1 channel surround audio. Channel compatibility objectives ensure that multi-channel content maintains perceptual quality when decoded to fewer channels. The shared representation enables training of generative language models on a single set of codebooks while supporting inference-time scalability across modalities and channel configurations. Evaluation using objective spatial audio metrics and subjective listening tests demonstrates that our unified approach maintains high reconstruction quality across mono, stereo, and surround audio configurations."}
{"id": "2601.15240", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.15240", "abs": "https://arxiv.org/abs/2601.15240", "authors": ["Lin Zhang", "Johan Rohdin", "Xin Wang", "Junyi Peng", "Tianchi Liu", "You Zhang", "Hieu-Thi Luong", "Shuai Wang", "Chengdong Liang", "Anna Silnova", "Nicholas Evans"], "title": "WeDefense: A Toolkit to Defend Against Fake Audio", "comment": "This is an ongoing work. v1 corresponds to the version completed by June 4, 2025 and previously submitted to ASRU 2025", "summary": "The advances in generative AI have enabled the creation of synthetic audio which is perceptually indistinguishable from real, genuine audio. Although this stellar progress enables many positive applications, it also raises risks of misuse, such as for impersonation, disinformation and fraud. Despite a growing number of open-source fake audio detection codes released through numerous challenges and initiatives, most are tailored to specific competitions, datasets or models. A standardized and unified toolkit that supports the fair benchmarking and comparison of competing solutions with not just common databases, protocols, metrics, but also a shared codebase, is missing. To address this, we propose WeDefense, the first open-source toolkit to support both fake audio detection and localization. Beyond model training, WeDefense emphasizes critical yet often overlooked components: flexible input and augmentation, calibration, score fusion, standardized evaluation metrics, and analysis tools for deeper understanding and interpretation. The toolkit is publicly available at https://github.com/zlin0/wedefense with interactive demos for fake audio detection and localization."}
{"id": "2601.15097", "categories": ["eess.SP", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.15097", "abs": "https://arxiv.org/abs/2601.15097", "authors": ["Johanna Wilroth", "Oskar Keding", "Martin A. Skoglund", "Maria Sandsten", "Martin Enqvist", "Emina Alickovic"], "title": "Neural Tracking of Sustained Attention, Attention Switching, and Natural Conversation in Audiovisual Environments using Mobile EEG", "comment": "Submitted to European Journal of Neuroscience", "summary": "Everyday communication is dynamic and multisensory, often involving shifting attention, overlapping speech and visual cues. Yet, most neural attention tracking studies are still limited to highly controlled lab settings, using clean, often audio-only stimuli and requiring sustained attention to a single talker. This work addresses that gap by introducing a novel dataset from 24 normal-hearing participants. We used a mobile electroencephalography (EEG) system (44 scalp electrodes and 20 cEEGrid electrodes) in an audiovisual (AV) paradigm with three conditions: sustained attention to a single talker in a two-talker environment, attention switching between two talkers, and unscripted two-talker conversations with a competing single talker. Analysis included temporal response functions (TRFs) modeling, optimal lag analysis, selective attention classification with decision windows ranging from 1.1s to 35s, and comparisons of TRFs for attention to AV conversations versus side audio-only talkers. Key findings show significant differences in the attention-related P2-peak between attended and ignored speech across conditions for scalp EEG. No significant change in performance between switching and sustained attention suggests robustness for attention switches. Optimal lag analysis revealed narrower peak for conversation compared to single-talker AV stimuli, reflecting the additional complexity of multi-talker processing. Classification of selective attention was consistently above chance (55-70% accuracy) for scalp EEG, while cEEGrid data yielded lower correlations, highlighting the need for further methodological improvements. These results demonstrate that mobile EEG can reliably track selective attention in dynamic, multisensory listening scenarios and provide guidance for designing future AV paradigms and real-world attention tracking applications."}
{"id": "2601.13910", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2601.13910", "abs": "https://arxiv.org/abs/2601.13910", "authors": ["Changhao Pan", "Dongyu Yao", "Yu Zhang", "Wenxiang Guo", "Jingyu Lu", "Zhiyuan Zhu", "Zhou Zhao"], "title": "Synthetic Singers: A Review of Deep-Learning-based Singing Voice Synthesis Approaches", "comment": "Accepetd by IJCNLP-AACL 2025(Oral)", "summary": "Recent advances in singing voice synthesis (SVS) have attracted substantial attention from both academia and industry. With the advent of large language models and novel generative paradigms, producing controllable, high-fidelity singing voices has become an attainable goal. Yet the field still lacks a comprehensive survey that systematically analyzes deep-learning-based singing voice synthesis systems and their enabling technologies. To address the aforementioned issue, this survey first categorizes existing systems by task type and then organizes current architectures into two major paradigms: cascaded and end-to-end approaches. Moreover, we provide an in-depth analysis of core technologies, covering singing modeling and control techniques. Finally, we review relevant datasets, annotation tools, and evaluation benchmarks that support training and assessment. In appendix, we introduce training strategies and further discussion of SVS. This survey provides an up-to-date review of the literature on SVS models, which would be a useful reference for both researchers and engineers. Related materials are available at https://github.com/David-Pigeon/SyntheticSingers."}
{"id": "2601.15240", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.15240", "abs": "https://arxiv.org/abs/2601.15240", "authors": ["Lin Zhang", "Johan Rohdin", "Xin Wang", "Junyi Peng", "Tianchi Liu", "You Zhang", "Hieu-Thi Luong", "Shuai Wang", "Chengdong Liang", "Anna Silnova", "Nicholas Evans"], "title": "WeDefense: A Toolkit to Defend Against Fake Audio", "comment": "This is an ongoing work. v1 corresponds to the version completed by June 4, 2025 and previously submitted to ASRU 2025", "summary": "The advances in generative AI have enabled the creation of synthetic audio which is perceptually indistinguishable from real, genuine audio. Although this stellar progress enables many positive applications, it also raises risks of misuse, such as for impersonation, disinformation and fraud. Despite a growing number of open-source fake audio detection codes released through numerous challenges and initiatives, most are tailored to specific competitions, datasets or models. A standardized and unified toolkit that supports the fair benchmarking and comparison of competing solutions with not just common databases, protocols, metrics, but also a shared codebase, is missing. To address this, we propose WeDefense, the first open-source toolkit to support both fake audio detection and localization. Beyond model training, WeDefense emphasizes critical yet often overlooked components: flexible input and augmentation, calibration, score fusion, standardized evaluation metrics, and analysis tools for deeper understanding and interpretation. The toolkit is publicly available at https://github.com/zlin0/wedefense with interactive demos for fake audio detection and localization."}
{"id": "2601.14516", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2601.14516", "abs": "https://arxiv.org/abs/2601.14516", "authors": ["Saba Tabatabaee", "Carol Espy-Wilson"], "title": "Towards noise-robust speech inversion through multi-task learning with speech enhancement", "comment": "Accepted for presentation at ICASSP 2026", "summary": "Recent studies demonstrate the effectiveness of Self Supervised Learning (SSL) speech representations for Speech Inversion (SI). However, applying SI in real-world scenarios remains challenging due to the pervasive presence of background noise. We propose a unified framework that integrates Speech Enhancement (SE) and SI models through shared SSL-based speech representations. In this framework, the SSL model is trained not only to support the SE module in suppressing noise but also to produce representations that are more informative for the SI task, allowing both modules to benefit from joint training. At a Signal-to-Noise Ratio of -5 db, our method for the SI task achieves relative improvements over the baseline of 80.95% under babble noise and 38.98% under non-babble noise, as measured by the average Pearson product-moment correlation across all estimated parameters."}
{"id": "2601.14620", "categories": ["eess.AS", "cs.AI", "cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2601.14620", "abs": "https://arxiv.org/abs/2601.14620", "authors": ["Wenda Zhang", "Hongyu Jin", "Siyi Wang", "Zhiqiang Wei", "Ting Dang"], "title": "Scaling Ambiguity: Augmenting Human Annotation in Speech Emotion Recognition with Audio-Language Models", "comment": "Accepted by ICASSP 2026", "summary": "Speech Emotion Recognition models typically use single categorical labels, overlooking the inherent ambiguity of human emotions. Ambiguous Emotion Recognition addresses this by representing emotions as probability distributions, but progress is limited by unreliable ground-truth distributions inferred from sparse human annotations. This paper explores whether Large Audio-Language Models (ALMs) can mitigate the annotation bottleneck by generating high-quality synthetic annotations. We introduce a framework leveraging ALMs to create Synthetic Perceptual Proxies, augmenting human annotations to improve ground-truth distribution reliability. We validate these proxies through statistical analysis of their alignment with human distributions and evaluate their impact by fine-tuning ALMs with the augmented emotion distributions. Furthermore, to address class imbalance and enable unbiased evaluation, we propose DiME-Aug, a Distribution-aware Multimodal Emotion Augmentation strategy. Experiments on IEMOCAP and MSP-Podcast show that synthetic annotations enhance emotion distribution, especially in low-ambiguity regions where annotation agreement is high. However, benefits diminish for highly ambiguous emotions with greater human disagreement. This work provides the first evidence that ALMs could address annotation scarcity in ambiguous emotion recognition, but highlights the need for more advanced prompting or generation strategies to handle highly ambiguous cases."}
{"id": "2601.14728", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2601.14728", "abs": "https://arxiv.org/abs/2601.14728", "authors": ["Chun-Yi Kuan", "Kai-Wei Chang", "Hung-yi Lee"], "title": "AQAScore: Evaluating Semantic Alignment in Text-to-Audio Generation via Audio Question Answering", "comment": "Manuscript in progress", "summary": "Although text-to-audio generation has made remarkable progress in realism and diversity, the development of evaluation metrics has not kept pace. Widely-adopted approaches, typically based on embedding similarity like CLAPScore, effectively measure general relevance but remain limited in fine-grained semantic alignment and compositional reasoning. To address this, we introduce AQAScore, a backbone-agnostic evaluation framework that leverages the reasoning capabilities of audio-aware large language models (ALLMs). AQAScore reformulates assessment as a probabilistic semantic verification task; rather than relying on open-ended text generation, it estimates alignment by computing the exact log-probability of a \"Yes\" answer to targeted semantic queries. We evaluate AQAScore across multiple benchmarks, including human-rated relevance, pairwise comparison, and compositional reasoning tasks. Experimental results show that AQAScore consistently achieves higher correlation with human judgments than similarity-based metrics and generative prompting baselines, showing its effectiveness in capturing subtle semantic inconsistencies and scaling with the capability of underlying ALLMs."}
{"id": "2601.15097", "categories": ["eess.SP", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.15097", "abs": "https://arxiv.org/abs/2601.15097", "authors": ["Johanna Wilroth", "Oskar Keding", "Martin A. Skoglund", "Maria Sandsten", "Martin Enqvist", "Emina Alickovic"], "title": "Neural Tracking of Sustained Attention, Attention Switching, and Natural Conversation in Audiovisual Environments using Mobile EEG", "comment": "Submitted to European Journal of Neuroscience", "summary": "Everyday communication is dynamic and multisensory, often involving shifting attention, overlapping speech and visual cues. Yet, most neural attention tracking studies are still limited to highly controlled lab settings, using clean, often audio-only stimuli and requiring sustained attention to a single talker. This work addresses that gap by introducing a novel dataset from 24 normal-hearing participants. We used a mobile electroencephalography (EEG) system (44 scalp electrodes and 20 cEEGrid electrodes) in an audiovisual (AV) paradigm with three conditions: sustained attention to a single talker in a two-talker environment, attention switching between two talkers, and unscripted two-talker conversations with a competing single talker. Analysis included temporal response functions (TRFs) modeling, optimal lag analysis, selective attention classification with decision windows ranging from 1.1s to 35s, and comparisons of TRFs for attention to AV conversations versus side audio-only talkers. Key findings show significant differences in the attention-related P2-peak between attended and ignored speech across conditions for scalp EEG. No significant change in performance between switching and sustained attention suggests robustness for attention switches. Optimal lag analysis revealed narrower peak for conversation compared to single-talker AV stimuli, reflecting the additional complexity of multi-talker processing. Classification of selective attention was consistently above chance (55-70% accuracy) for scalp EEG, while cEEGrid data yielded lower correlations, highlighting the need for further methodological improvements. These results demonstrate that mobile EEG can reliably track selective attention in dynamic, multisensory listening scenarios and provide guidance for designing future AV paradigms and real-world attention tracking applications."}

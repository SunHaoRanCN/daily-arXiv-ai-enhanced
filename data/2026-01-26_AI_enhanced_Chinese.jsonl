{"id": "2601.16225", "categories": ["eess.AS", "cs.AI", "cs.SD"], "pdf": "https://arxiv.org/pdf/2601.16225", "abs": "https://arxiv.org/abs/2601.16225", "authors": ["Zhuoyue Gao", "Xiaohui Wang", "Xiaocui Yang", "Wen Zhang", "Daling Wang", "Shi Feng", "Yifei Zhang"], "title": "ES4R: Speech Encoding Based on Prepositive Affective Modeling for Empathetic Response Generation", "comment": null, "summary": "Empathetic speech dialogue requires not only understanding linguistic content but also perceiving rich paralinguistic information such as prosody, tone, and emotional intensity for affective understandings. Existing speech-to-speech large language models either rely on ASR transcription or use encoders to extract latent representations, often weakening affective information and contextual coherence in multi-turn dialogues. To address this, we propose \\textbf{ES4R}, a framework for speech-based empathetic response generation. Our core innovation lies in explicitly modeling structured affective context before speech encoding, rather than relying on implicit learning by the encoder or explicit emotion supervision. Specifically, we introduce a dual-level attention mechanism to capture turn-level affective states and dialogue-level affective dynamics. The resulting affective representations are then integrated with textual semantics through speech-guided cross-modal attention to generate empathetic responses. For speech output, we employ energy-based strategy selection and style fusion to achieve empathetic speech synthesis. ES4R consistently outperforms strong baselines in both automatic and human evaluations and remains robust across different LLM backbones.", "AI": {"tldr": "ES4R\u662f\u4e00\u4e2a\u7528\u4e8e\u8bed\u97f3\u5171\u60c5\u5bf9\u8bdd\u751f\u6210\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u7ed3\u6784\u5316\u60c5\u611f\u4e0a\u4e0b\u6587\uff0c\u7ed3\u5408\u53cc\u7ea7\u6ce8\u610f\u529b\u673a\u5236\u548c\u8bed\u97f3\u5f15\u5bfc\u7684\u8de8\u6a21\u6001\u6ce8\u610f\u529b\uff0c\u5728\u8bed\u97f3\u5bf9\u8bdd\u4e2d\u5b9e\u73b0\u66f4\u597d\u7684\u5171\u60c5\u54cd\u5e94\u3002", "motivation": "\u73b0\u6709\u8bed\u97f3\u5bf9\u8bdd\u5927\u6a21\u578b\u4f9d\u8d56ASR\u8f6c\u5f55\u6216\u7f16\u7801\u5668\u63d0\u53d6\u6f5c\u5728\u8868\u793a\uff0c\u5f80\u5f80\u524a\u5f31\u4e86\u60c5\u611f\u4fe1\u606f\u548c\u591a\u8f6e\u5bf9\u8bdd\u7684\u4e0a\u4e0b\u6587\u8fde\u8d2f\u6027\uff0c\u65e0\u6cd5\u6709\u6548\u611f\u77e5\u97f5\u5f8b\u3001\u8bed\u8c03\u7b49\u526f\u8bed\u8a00\u4fe1\u606f\u3002", "method": "\u63d0\u51faES4R\u6846\u67b6\uff0c\u6838\u5fc3\u521b\u65b0\u662f\u5728\u8bed\u97f3\u7f16\u7801\u524d\u663e\u5f0f\u5efa\u6a21\u7ed3\u6784\u5316\u60c5\u611f\u4e0a\u4e0b\u6587\u3002\u91c7\u7528\u53cc\u7ea7\u6ce8\u610f\u529b\u673a\u5236\u6355\u6349\u8f6e\u6b21\u7ea7\u60c5\u611f\u72b6\u6001\u548c\u5bf9\u8bdd\u7ea7\u60c5\u611f\u52a8\u6001\uff0c\u901a\u8fc7\u8bed\u97f3\u5f15\u5bfc\u7684\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u5c06\u60c5\u611f\u8868\u793a\u4e0e\u6587\u672c\u8bed\u4e49\u7ed3\u5408\uff0c\u4f7f\u7528\u57fa\u4e8e\u80fd\u91cf\u7684\u7b56\u7565\u9009\u62e9\u548c\u98ce\u683c\u878d\u5408\u5b9e\u73b0\u5171\u60c5\u8bed\u97f3\u5408\u6210\u3002", "result": "ES4R\u5728\u81ea\u52a8\u8bc4\u4f30\u548c\u4eba\u5de5\u8bc4\u4f30\u4e2d\u5747\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u6a21\u578b\uff0c\u4e14\u5728\u4e0d\u540cLLM\u9aa8\u5e72\u7f51\u7edc\u4e0a\u4fdd\u6301\u9c81\u68d2\u6027\u3002", "conclusion": "\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u7ed3\u6784\u5316\u60c5\u611f\u4e0a\u4e0b\u6587\uff0cES4R\u80fd\u591f\u66f4\u597d\u5730\u7406\u89e3\u548c\u751f\u6210\u5171\u60c5\u8bed\u97f3\u54cd\u5e94\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u8bed\u97f3\u5bf9\u8bdd\u6a21\u578b\u5728\u60c5\u611f\u4fe1\u606f\u4fdd\u7559\u548c\u4e0a\u4e0b\u6587\u8fde\u8d2f\u6027\u65b9\u9762\u7684\u4e0d\u8db3\u3002"}}
{"id": "2601.16230", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2601.16230", "abs": "https://arxiv.org/abs/2601.16230", "authors": ["Aditya Kamlesh Parikh", "Cristian Tejedor-Garcia", "Catia Cucchiarini", "Helmer Strik"], "title": "Zero-Shot Speech LLMs for Multi-Aspect Evaluation of L2 Speech: Challenges and Opportunities", "comment": "This publication is part of the project Responsible AI for Voice Diagnostics (RAIVD) with file number NGF.1607.22.013 of the research programme NGF AiNed Fellowship Grants which is financed by the Dutch Research Council (NWO)", "summary": "An accurate assessment of L2 English pronunciation is crucial for language learning, as it provides personalized feedback and ensures a fair evaluation of individual progress. However, automated scoring remains challenging due to the complexity of sentence-level fluency, prosody, and completeness. This paper evaluates the zero-shot performance of Qwen2-Audio-7B-Instruct, an instruction-tuned speech-LLM, on 5,000 Speechocean762 utterances. The model generates rubric-aligned scores for accuracy, fluency, prosody, and completeness, showing strong agreement with human ratings within +-2 tolerance, especially for high-quality speech. However, it tends to overpredict low-quality speech scores and lacks precision in error detection. These findings demonstrate the strong potential of speech LLMs in scalable pronunciation assessment and suggest future improvements through enhanced prompting, calibration, and phonetic integration to advance Computer-Assisted Pronunciation Training.", "AI": {"tldr": "\u8bc4\u4f30Qwen2-Audio-7B-Instruct\u5728L2\u82f1\u8bed\u53d1\u97f3\u81ea\u52a8\u8bc4\u5206\u4e2d\u7684\u96f6\u6837\u672c\u8868\u73b0\uff0c\u53d1\u73b0\u8be5\u8bed\u97f3\u5927\u6a21\u578b\u5728\u9ad8\u8d28\u91cf\u8bed\u97f3\u8bc4\u5206\u4e0a\u4e0e\u4eba\u5de5\u8bc4\u5206\u9ad8\u5ea6\u4e00\u81f4\uff0c\u4f46\u5728\u4f4e\u8d28\u91cf\u8bed\u97f3\u8bc4\u5206\u4e0a\u5b58\u5728\u9ad8\u4f30\u503e\u5411\uff0c\u4e14\u9519\u8bef\u68c0\u6d4b\u7cbe\u5ea6\u4e0d\u8db3\u3002", "motivation": "L2\u82f1\u8bed\u53d1\u97f3\u7684\u51c6\u786e\u8bc4\u4f30\u5bf9\u8bed\u8a00\u5b66\u4e60\u81f3\u5173\u91cd\u8981\uff0c\u80fd\u63d0\u4f9b\u4e2a\u6027\u5316\u53cd\u9988\u5e76\u786e\u4fdd\u516c\u5e73\u8bc4\u4ef7\u3002\u7136\u800c\uff0c\u7531\u4e8e\u53e5\u5b50\u5c42\u9762\u7684\u6d41\u5229\u5ea6\u3001\u97f5\u5f8b\u548c\u5b8c\u6574\u6027\u7b49\u590d\u6742\u6027\uff0c\u81ea\u52a8\u8bc4\u5206\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u4f7f\u7528\u6307\u4ee4\u8c03\u4f18\u7684\u8bed\u97f3\u5927\u6a21\u578bQwen2-Audio-7B-Instruct\uff0c\u57285,000\u4e2aSpeechocean762\u8bed\u97f3\u6837\u672c\u4e0a\u8fdb\u884c\u96f6\u6837\u672c\u8bc4\u4f30\u3002\u6a21\u578b\u751f\u6210\u4e0e\u8bc4\u5206\u6807\u51c6\u5bf9\u9f50\u7684\u51c6\u786e\u6027\u3001\u6d41\u5229\u5ea6\u3001\u97f5\u5f8b\u548c\u5b8c\u6574\u6027\u5206\u6570\u3002", "result": "\u6a21\u578b\u8bc4\u5206\u4e0e\u4eba\u5de5\u8bc4\u5206\u5728\u00b12\u5bb9\u5dee\u8303\u56f4\u5185\u8868\u73b0\u51fa\u5f3a\u4e00\u81f4\u6027\uff0c\u7279\u522b\u662f\u5728\u9ad8\u8d28\u91cf\u8bed\u97f3\u4e0a\u3002\u4f46\u6a21\u578b\u503e\u5411\u4e8e\u9ad8\u4f30\u4f4e\u8d28\u91cf\u8bed\u97f3\u7684\u5206\u6570\uff0c\u5e76\u4e14\u5728\u9519\u8bef\u68c0\u6d4b\u65b9\u9762\u7f3a\u4e4f\u7cbe\u5ea6\u3002", "conclusion": "\u8bed\u97f3\u5927\u6a21\u578b\u5728\u53ef\u6269\u5c55\u7684\u53d1\u97f3\u8bc4\u4f30\u4e2d\u663e\u793a\u51fa\u5f3a\u5927\u6f5c\u529b\uff0c\u672a\u6765\u53ef\u901a\u8fc7\u6539\u8fdb\u63d0\u793a\u5de5\u7a0b\u3001\u6821\u51c6\u6280\u672f\u548c\u97f3\u7d20\u96c6\u6210\u6765\u63a8\u8fdb\u8ba1\u7b97\u673a\u8f85\u52a9\u53d1\u97f3\u8bad\u7ec3\u7684\u53d1\u5c55\u3002"}}
{"id": "2601.16240", "categories": ["eess.AS", "cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2601.16240", "abs": "https://arxiv.org/abs/2601.16240", "authors": ["Jiaheng Dong", "Hong Jia", "Ting Dang"], "title": "Test-Time Adaptation for Speech Emotion Recognition", "comment": "Accepted by 2026 IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP 2026)", "summary": "The practical utility of Speech Emotion Recognition (SER) systems is undermined by their fragility to domain shifts, such as speaker variability, the distinction between acted and naturalistic emotions, and cross-corpus variations. While domain adaptation and fine-tuning are widely studied, they require either source data or labelled target data, which are often unavailable or raise privacy concerns in SER. Test-time adaptation (TTA) bridges this gap by adapting models at inference using only unlabeled target data. Yet, having been predominantly designed for image classification and speech recognition, the efficacy of TTA for mitigating the unique domain shifts in SER has not been investigated. In this paper, we present the first systematic evaluation and comparison covering 11 TTA methods across three representative SER tasks. The results indicate that backpropagation-free TTA methods are the most promising. Conversely, entropy minimization and pseudo-labeling generally fail, as their core assumption of a single, confident ground-truth label is incompatible with the inherent ambiguity of emotional expression. Further, no single method universally excels, and its effectiveness is highly dependent on the distributional shifts and tasks.", "AI": {"tldr": "\u9996\u6b21\u7cfb\u7edf\u8bc4\u4f3011\u79cd\u6d4b\u8bd5\u65f6\u9002\u5e94\u65b9\u6cd5\u5728\u8bed\u97f3\u60c5\u611f\u8bc6\u522b\u4e2d\u7684\u6548\u679c\uff0c\u53d1\u73b0\u65e0\u53cd\u5411\u4f20\u64ad\u65b9\u6cd5\u6700\u6709\u524d\u666f\uff0c\u800c\u57fa\u4e8e\u71b5\u6700\u5c0f\u5316\u548c\u4f2a\u6807\u7b7e\u7684\u65b9\u6cd5\u56e0\u60c5\u611f\u8868\u8fbe\u56fa\u6709\u6a21\u7cca\u6027\u800c\u5931\u6548\u3002", "motivation": "\u8bed\u97f3\u60c5\u611f\u8bc6\u522b\u7cfb\u7edf\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u9762\u4e34\u9886\u57df\u504f\u79fb\u95ee\u9898\uff08\u5982\u8bf4\u8bdd\u4eba\u5dee\u5f02\u3001\u8868\u6f14\u4e0e\u81ea\u7136\u60c5\u611f\u5dee\u5f02\u3001\u8de8\u8bed\u6599\u5e93\u53d8\u5316\uff09\uff0c\u800c\u4f20\u7edf\u7684\u9886\u57df\u9002\u5e94\u548c\u5fae\u8c03\u65b9\u6cd5\u9700\u8981\u6e90\u6570\u636e\u6216\u6807\u6ce8\u76ee\u6807\u6570\u636e\uff0c\u8fd9\u5728SER\u4e2d\u5f80\u5f80\u4e0d\u53ef\u5f97\u6216\u5b58\u5728\u9690\u79c1\u95ee\u9898\u3002\u6d4b\u8bd5\u65f6\u9002\u5e94\u4ec5\u9700\u672a\u6807\u6ce8\u76ee\u6807\u6570\u636e\uff0c\u4f46\u5176\u5728SER\u72ec\u7279\u9886\u57df\u504f\u79fb\u4e2d\u7684\u6548\u679c\u5c1a\u672a\u88ab\u7814\u7a76\u3002", "method": "\u5bf911\u79cd\u6d4b\u8bd5\u65f6\u9002\u5e94\u65b9\u6cd5\u5728\u4e09\u4e2a\u4ee3\u8868\u6027SER\u4efb\u52a1\u4e0a\u8fdb\u884c\u9996\u6b21\u7cfb\u7edf\u8bc4\u4f30\u548c\u6bd4\u8f83\uff0c\u91cd\u70b9\u5173\u6ce8\u4e0d\u540c\u65b9\u6cd5\u5904\u7406SER\u9886\u57df\u504f\u79fb\u7684\u80fd\u529b\u3002", "result": "\u65e0\u53cd\u5411\u4f20\u64ad\u7684TTA\u65b9\u6cd5\u6700\u6709\u524d\u666f\uff1b\u71b5\u6700\u5c0f\u5316\u548c\u4f2a\u6807\u7b7e\u65b9\u6cd5\u666e\u904d\u5931\u8d25\uff0c\u56e0\u4e3a\u8fd9\u4e9b\u65b9\u6cd5\u5047\u8bbe\u5355\u4e00\u3001\u786e\u5b9a\u7684\u771f\u5b9e\u6807\u7b7e\uff0c\u4e0e\u60c5\u611f\u8868\u8fbe\u7684\u56fa\u6709\u6a21\u7cca\u6027\u4e0d\u517c\u5bb9\uff1b\u6ca1\u6709\u5355\u4e00\u65b9\u6cd5\u5728\u6240\u6709\u60c5\u51b5\u4e0b\u90fd\u8868\u73b0\u4f18\u5f02\uff0c\u5176\u6548\u679c\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u5206\u5e03\u504f\u79fb\u548c\u5177\u4f53\u4efb\u52a1\u3002", "conclusion": "\u6d4b\u8bd5\u65f6\u9002\u5e94\u5728SER\u9886\u57df\u504f\u79fb\u7f13\u89e3\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u9700\u8981\u5f00\u53d1\u4e13\u95e8\u8003\u8651\u60c5\u611f\u8868\u8fbe\u6a21\u7cca\u6027\u7684\u65b9\u6cd5\uff0c\u672a\u6765\u7814\u7a76\u5e94\u9488\u5bf9SER\u7684\u72ec\u7279\u6311\u6218\u8bbe\u8ba1\u66f4\u6709\u6548\u7684TTA\u7b56\u7565\u3002"}}
{"id": "2601.16316", "categories": ["eess.AS", "cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2601.16316", "abs": "https://arxiv.org/abs/2601.16316", "authors": ["Oguzhan Buyuksolak", "Alican Gok", "Osman Erman Okman"], "title": "EdgeSpot: Efficient and High-Performance Few-Shot Model for Keyword Spotting", "comment": "Accepted to be presented in IEEE ICASSP 2026", "summary": "We introduce an efficient few-shot keyword spotting model for edge devices, EdgeSpot, that pairs an optimized version of a BC-ResNet-based acoustic backbone with a trainable Per-Channel Energy Normalization frontend and lightweight temporal self-attention. Knowledge distillation is utilized during training by employing a self-supervised teacher model, optimized with Sub-center ArcFace loss. This study demonstrates that the EdgeSpot model consistently provides better accuracy at a fixed false-alarm rate (FAR) than strong BC-ResNet baselines. The largest variant, EdgeSpot-4, improves the 10-shot accuracy at 1% FAR from 73.7% to 82.0%, which requires only 29.4M MACs with 128k parameters.", "AI": {"tldr": "EdgeSpot\uff1a\u7528\u4e8e\u8fb9\u7f18\u8bbe\u5907\u7684\u9ad8\u6548\u5c11\u6837\u672c\u5173\u952e\u8bcd\u68c0\u6d4b\u6a21\u578b\uff0c\u901a\u8fc7\u4f18\u5316BC-ResNet\u58f0\u5b66\u9aa8\u5e72\u3001\u53ef\u8bad\u7ec3PCEN\u524d\u7aef\u548c\u8f7b\u91cf\u7ea7\u65f6\u5e8f\u81ea\u6ce8\u610f\u529b\uff0c\u7ed3\u5408\u77e5\u8bc6\u84b8\u998f\u548cSub-center ArcFace\u635f\u5931\uff0c\u5728\u56fa\u5b9a\u8bef\u62a5\u7387\u4e0b\u5b9e\u73b0\u66f4\u9ad8\u51c6\u786e\u7387\u3002", "motivation": "\u5f00\u53d1\u9002\u7528\u4e8e\u8fb9\u7f18\u8bbe\u5907\u7684\u9ad8\u6548\u5c11\u6837\u672c\u5173\u952e\u8bcd\u68c0\u6d4b\u6a21\u578b\uff0c\u5728\u6709\u9650\u8ba1\u7b97\u8d44\u6e90\u548c\u53c2\u6570\u7ea6\u675f\u4e0b\u63d0\u5347\u68c0\u6d4b\u51c6\u786e\u7387\uff0c\u89e3\u51b3\u4f20\u7edf\u6a21\u578b\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u6027\u80fd\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "1. \u4f18\u5316BC-ResNet\u58f0\u5b66\u9aa8\u5e72\u7f51\u7edc\uff1b2. \u53ef\u8bad\u7ec3\u7684\u6bcf\u901a\u9053\u80fd\u91cf\u5f52\u4e00\u5316\uff08PCEN\uff09\u524d\u7aef\uff1b3. \u8f7b\u91cf\u7ea7\u65f6\u5e8f\u81ea\u6ce8\u610f\u529b\u673a\u5236\uff1b4. \u4f7f\u7528\u81ea\u76d1\u7763\u6559\u5e08\u6a21\u578b\u8fdb\u884c\u77e5\u8bc6\u84b8\u998f\uff1b5. \u91c7\u7528Sub-center ArcFace\u635f\u5931\u4f18\u5316\u8bad\u7ec3\u3002", "result": "EdgeSpot\u5728\u56fa\u5b9a\u8bef\u62a5\u7387\u4e0b\u6301\u7eed\u4f18\u4e8e\u5f3aBC-ResNet\u57fa\u7ebf\u3002\u6700\u5927\u53d8\u4f53EdgeSpot-4\u57281%\u8bef\u62a5\u7387\u4e0b\u5c0610-shot\u51c6\u786e\u7387\u4ece73.7%\u63d0\u5347\u81f382.0%\uff0c\u4ec5\u970029.4M MACs\u548c128k\u53c2\u6570\u3002", "conclusion": "EdgeSpot\u6a21\u578b\u4e3a\u8fb9\u7f18\u8bbe\u5907\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u51c6\u786e\u7684\u5c11\u6837\u672c\u5173\u952e\u8bcd\u68c0\u6d4b\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u6709\u9650\u8ba1\u7b97\u8d44\u6e90\u4e0b\u663e\u8457\u63d0\u5347\u4e86\u68c0\u6d4b\u6027\u80fd\uff0c\u5177\u6709\u5b9e\u9645\u90e8\u7f72\u4ef7\u503c\u3002"}}
{"id": "2601.16231", "categories": ["cs.SD", "cs.AI", "cs.CL", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.16231", "abs": "https://arxiv.org/abs/2601.16231", "authors": ["Aafiya Hussain", "Gaurav Srivastava", "Alvi Ishmam", "Zaber Hakim", "Chris Thomas"], "title": "SoundBreak: A Systematic Study of Audio-Only Adversarial Attacks on Trimodal Models", "comment": null, "summary": "Multimodal foundation models that integrate audio, vision, and language achieve strong performance on reasoning and generation tasks, yet their robustness to adversarial manipulation remains poorly understood. We study a realistic and underexplored threat model: untargeted, audio-only adversarial attacks on trimodal audio-video-language models. We analyze six complementary attack objectives that target different stages of multimodal processing, including audio encoder representations, cross-modal attention, hidden states, and output likelihoods. Across three state-of-the-art models and multiple benchmarks, we show that audio-only perturbations can induce severe multimodal failures, achieving up to 96% attack success rate. We further show that attacks can be successful at low perceptual distortions (LPIPS <= 0.08, SI-SNR >= 0) and benefit more from extended optimization than increased data scale. Transferability across models and encoders remains limited, while speech recognition systems such as Whisper primarily respond to perturbation magnitude, achieving >97% attack success under severe distortion. These results expose a previously overlooked single-modality attack surface in multimodal systems and motivate defenses that enforce cross-modal consistency.", "AI": {"tldr": "\u97f3\u9891\u5bf9\u6297\u653b\u51fb\u53ef\u5728\u4f4e\u611f\u77e5\u5931\u771f\u4e0b\u4e25\u91cd\u7834\u574f\u97f3\u89c6\u9891\u8bed\u8a00\u591a\u6a21\u6001\u6a21\u578b\u7684\u6027\u80fd\uff0c\u653b\u51fb\u6210\u529f\u7387\u9ad8\u8fbe96%\uff0c\u66b4\u9732\u4e86\u591a\u6a21\u6001\u7cfb\u7edf\u4e2d\u88ab\u5ffd\u89c6\u7684\u5355\u6a21\u6001\u653b\u51fb\u9762\u3002", "motivation": "\u867d\u7136\u97f3\u89c6\u9891\u8bed\u8a00\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u5728\u63a8\u7406\u548c\u751f\u6210\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5176\u5bf9\u6297\u653b\u51fb\u9c81\u68d2\u6027\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22\u4e00\u4e2a\u73b0\u5b9e\u4e14\u672a\u88ab\u5145\u5206\u7814\u7a76\u7684\u5a01\u80c1\u6a21\u578b\uff1a\u9488\u5bf9\u4e09\u6a21\u6001\u97f3\u9891-\u89c6\u9891-\u8bed\u8a00\u6a21\u578b\u7684\u975e\u5b9a\u5411\u3001\u7eaf\u97f3\u9891\u5bf9\u6297\u653b\u51fb\u3002", "method": "\u5206\u6790\u4e86\u516d\u79cd\u4e92\u8865\u7684\u653b\u51fb\u76ee\u6807\uff0c\u9488\u5bf9\u591a\u6a21\u6001\u5904\u7406\u7684\u4e0d\u540c\u9636\u6bb5\uff1a\u97f3\u9891\u7f16\u7801\u5668\u8868\u793a\u3001\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u3001\u9690\u85cf\u72b6\u6001\u548c\u8f93\u51fa\u4f3c\u7136\u3002\u5728\u4e09\u4e2a\u6700\u5148\u8fdb\u7684\u6a21\u578b\u548c\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u8bc4\u4f30\u97f3\u9891\u6270\u52a8\u6548\u679c\uff0c\u7814\u7a76\u611f\u77e5\u5931\u771f\u3001\u4f18\u5316\u65f6\u957f\u548c\u6570\u636e\u89c4\u6a21\u7684\u5f71\u54cd\uff0c\u5e76\u6d4b\u8bd5\u653b\u51fb\u5728\u4e0d\u540c\u6a21\u578b\u548c\u7f16\u7801\u5668\u95f4\u7684\u53ef\u8fc1\u79fb\u6027\u3002", "result": "\u7eaf\u97f3\u9891\u6270\u52a8\u53ef\u5bfc\u81f4\u4e25\u91cd\u7684\u591a\u6a21\u6001\u6545\u969c\uff0c\u653b\u51fb\u6210\u529f\u7387\u9ad8\u8fbe96%\u3002\u653b\u51fb\u5728\u4f4e\u611f\u77e5\u5931\u771f\u4e0b\u4ecd\u80fd\u6210\u529f\uff08LPIPS \u2264 0.08\uff0cSI-SNR \u2265 0\uff09\u3002\u5ef6\u957f\u4f18\u5316\u65f6\u95f4\u6bd4\u589e\u52a0\u6570\u636e\u89c4\u6a21\u66f4\u6709\u6548\u3002\u653b\u51fb\u5728\u4e0d\u540c\u6a21\u578b\u548c\u7f16\u7801\u5668\u95f4\u7684\u53ef\u8fc1\u79fb\u6027\u6709\u9650\uff0c\u800cWhisper\u7b49\u8bed\u97f3\u8bc6\u522b\u7cfb\u7edf\u4e3b\u8981\u53d7\u6270\u52a8\u5e45\u5ea6\u5f71\u54cd\uff0c\u5728\u4e25\u91cd\u5931\u771f\u4e0b\u653b\u51fb\u6210\u529f\u7387>97%\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u591a\u6a21\u6001\u7cfb\u7edf\u4e2d\u4e00\u4e2a\u5148\u524d\u88ab\u5ffd\u89c6\u7684\u5355\u6a21\u6001\u653b\u51fb\u9762\uff0c\u8868\u660e\u7eaf\u97f3\u9891\u5bf9\u6297\u653b\u51fb\u53ef\u6709\u6548\u7834\u574f\u591a\u6a21\u6001\u7cfb\u7edf\u7684\u6027\u80fd\u3002\u8fd9\u4fc3\u4f7f\u9700\u8981\u5f00\u53d1\u5f3a\u5236\u8de8\u6a21\u6001\u4e00\u81f4\u6027\u7684\u9632\u5fa1\u673a\u5236\u6765\u589e\u5f3a\u591a\u6a21\u6001\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u3002"}}
{"id": "2601.16301", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.16301", "abs": "https://arxiv.org/abs/2601.16301", "authors": ["Sahar Golipoor", "Richard T. Brophy", "Ying Liu", "Reza Ghazalian", "Stephan Sigg"], "title": "Gesture Recognition from body-Worn RFID under Missing Data", "comment": null, "summary": "We explore hand-gesture recognition through the use of passive body-worn reflective tags. A data processing pipeline is proposed to address the issue of missing data. Specifically, missing information is recovered through linear and exponential interpolation and extrapolation. Furthermore, imputation and proximity-based inference are employed. We represent tags as nodes in a temporal graph, with edges formed based on correlations between received signal strength (RSS) and phase values across successive timestamps, and we train a graph-based convolutional neural network that exploits graph-based self-attention. The system outperforms state-of-the-art methods with an accuracy of 98.13% for the recognition of 21 gestures. We achieve 89.28% accuracy under leave-one-person-out cross-validation. We further investigate the contribution of various body locations on the recognition accuracy. Removing tags from the arms reduces accuracy by more than 10%, while removing the wrist tag only reduces accuracy by around 2%. Therefore, tag placements on the arms are more expressive for gesture recognition than on the wrist.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u88ab\u52a8\u53cd\u5c04\u6807\u7b7e\u7684\u624b\u52bf\u8bc6\u522b\u7cfb\u7edf\uff0c\u901a\u8fc7\u6570\u636e\u63d2\u8865\u548c\u56fe\u5377\u79ef\u7f51\u7edc\u5904\u7406\u7f3a\u5931\u6570\u636e\uff0c\u572821\u79cd\u624b\u52bf\u8bc6\u522b\u4e0a\u8fbe\u523098.13%\u51c6\u786e\u7387", "motivation": "\u63a2\u7d22\u4f7f\u7528\u88ab\u52a8\u8eab\u4f53\u4f69\u6234\u53cd\u5c04\u6807\u7b7e\u8fdb\u884c\u624b\u52bf\u8bc6\u522b\uff0c\u89e3\u51b3\u5b9e\u9645\u5e94\u7528\u4e2d\u5e38\u89c1\u7684\u7f3a\u5931\u6570\u636e\u95ee\u9898", "method": "\u63d0\u51fa\u6570\u636e\u5904\u7406\u6d41\u6c34\u7ebf\uff1a\u4f7f\u7528\u7ebf\u6027\u548c\u6307\u6570\u63d2\u503c/\u5916\u63a8\u6062\u590d\u7f3a\u5931\u6570\u636e\uff1b\u5c06\u6807\u7b7e\u8868\u793a\u4e3a\u65f6\u5e8f\u56fe\u8282\u70b9\uff0c\u57fa\u4e8eRSS\u548c\u76f8\u4f4d\u76f8\u5173\u6027\u6784\u5efa\u8fb9\uff1b\u8bad\u7ec3\u57fa\u4e8e\u56fe\u81ea\u6ce8\u610f\u529b\u7684\u56fe\u5377\u79ef\u795e\u7ecf\u7f51\u7edc", "result": "\u7cfb\u7edf\u572821\u79cd\u624b\u52bf\u8bc6\u522b\u4e0a\u8fbe\u523098.13%\u51c6\u786e\u7387\uff0c\u7559\u4e00\u4eba\u51fa\u4ea4\u53c9\u9a8c\u8bc1\u51c6\u786e\u738789.28%\uff1b\u624b\u81c2\u6807\u7b7e\u6bd4\u624b\u8155\u6807\u7b7e\u5bf9\u8bc6\u522b\u8d21\u732e\u66f4\u5927", "conclusion": "\u88ab\u52a8\u53cd\u5c04\u6807\u7b7e\u7ed3\u5408\u56fe\u795e\u7ecf\u7f51\u7edc\u80fd\u6709\u6548\u8fdb\u884c\u624b\u52bf\u8bc6\u522b\uff0c\u624b\u81c2\u4f4d\u7f6e\u6807\u7b7e\u6bd4\u624b\u8155\u6807\u7b7e\u66f4\u5177\u8868\u8fbe\u529b\uff0c\u7cfb\u7edf\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5"}}
{"id": "2601.16358", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2601.16358", "abs": "https://arxiv.org/abs/2601.16358", "authors": ["Aref Farhadipour", "Jan Marquenie", "Srikanth Madikeri", "Eleanor Chodroff"], "title": "TidyVoice: A Curated Multilingual Dataset for Speaker Verification Derived from Common Voice", "comment": "Accepted at ICASSP 2026", "summary": "The development of robust, multilingual speaker recognition systems is hindered by a lack of large-scale, publicly available and multilingual datasets, particularly for the read-speech style crucial for applications like anti-spoofing. To address this gap, we introduce the TidyVoice dataset derived from the Mozilla Common Voice corpus after mitigating its inherent speaker heterogeneity within the provided client IDs. TidyVoice currently contains training and test data from over 212,000 monolingual speakers (Tidy-M) and around 4,500 multilingual speakers (Tidy-X) from which we derive two distinct conditions. The Tidy-M condition contains target and non-target trials from monolingual speakers across 81 languages. The Tidy-X condition contains target and non-target trials from multilingual speakers in both same- and cross-language trials. We employ two architectures of ResNet models, achieving a 0.35% EER by fine-tuning on our comprehensive Tidy-M partition. Moreover, we show that this fine-tuning enhances the model's generalization, improving performance on unseen conversational interview data from the CANDOR corpus. The complete dataset, evaluation trials, and our models are publicly released to provide a new resource for the community.", "AI": {"tldr": "\u7814\u7a76\u8005\u521b\u5efa\u4e86TidyVoice\u591a\u8bed\u8a00\u8bf4\u8bdd\u4eba\u8bc6\u522b\u6570\u636e\u96c6\uff0c\u57fa\u4e8eMozilla Common Voice\u8bed\u6599\u5e93\uff0c\u5305\u542b\u8d85\u8fc721.2\u4e07\u5355\u8bed\u8bf4\u8bdd\u4eba\u548c\u7ea64500\u540d\u591a\u8bed\u8bf4\u8bdd\u4eba\uff0c\u7528\u4e8e\u63d0\u5347\u8bf4\u8bdd\u4eba\u8bc6\u522b\u7cfb\u7edf\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u5927\u89c4\u6a21\u3001\u516c\u5f00\u53ef\u7528\u7684\u591a\u8bed\u8a00\u8bf4\u8bdd\u4eba\u8bc6\u522b\u6570\u636e\u96c6\uff0c\u7279\u522b\u662f\u9488\u5bf9\u6717\u8bfb\u8bed\u97f3\u98ce\u683c\u7684\u6570\u636e\uff0c\u8fd9\u9650\u5236\u4e86\u9c81\u68d2\u7684\u591a\u8bed\u8a00\u8bf4\u8bdd\u4eba\u8bc6\u522b\u7cfb\u7edf\u7684\u53d1\u5c55\u3002", "method": "\u4eceMozilla Common Voice\u8bed\u6599\u5e93\u4e2d\u63d0\u53d6\u6570\u636e\uff0c\u901a\u8fc7\u7f13\u89e3\u5176\u56fa\u6709\u7684\u8bf4\u8bdd\u4eba\u5f02\u8d28\u6027\uff0c\u6784\u5efaTidyVoice\u6570\u636e\u96c6\u3002\u4f7f\u7528\u4e24\u79cdResNet\u67b6\u6784\u6a21\u578b\uff0c\u5728Tidy-M\u5206\u533a\u4e0a\u8fdb\u884c\u5fae\u8c03\u3002", "result": "\u5728Tidy-M\u5206\u533a\u4e0a\u5fae\u8c03\u540e\uff0c\u6a21\u578b\u8fbe\u52300.35%\u7684\u7b49\u9519\u8bef\u7387\uff08EER\uff09\u3002\u5fae\u8c03\u8fd8\u589e\u5f3a\u4e86\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5728\u672a\u89c1\u8fc7\u7684CANDOR\u8bed\u6599\u5e93\u7684\u5bf9\u8bdd\u8bbf\u8c08\u6570\u636e\u4e0a\u8868\u73b0\u66f4\u597d\u3002", "conclusion": "TidyVoice\u6570\u636e\u96c6\u4e3a\u8bf4\u8bdd\u4eba\u8bc6\u522b\u793e\u533a\u63d0\u4f9b\u4e86\u65b0\u7684\u8d44\u6e90\uff0c\u6709\u52a9\u4e8e\u5f00\u53d1\u66f4\u9c81\u68d2\u7684\u591a\u8bed\u8a00\u8bf4\u8bdd\u4eba\u8bc6\u522b\u7cfb\u7edf\uff0c\u7279\u522b\u662f\u5728\u6717\u8bfb\u8bed\u97f3\u98ce\u683c\u7684\u5e94\u7528\u4e2d\u3002"}}
{"id": "2601.16235", "categories": ["cs.SD", "eess.AS", "eess.SP"], "pdf": "https://arxiv.org/pdf/2601.16235", "abs": "https://arxiv.org/abs/2601.16235", "authors": ["Thomas Serre", "Mathieu Fontaine", "\u00c9ric Benhaim", "Slim Essid"], "title": "Contrastive Knowledge Distillation for Embedding Refinement in Personalized Speech Enhancement", "comment": null, "summary": "Personalized speech enhancement (PSE) has shown convincing results when it comes to extracting a known target voice among interfering ones. The corresponding systems usually incorporate a representation of the target voice within the enhancement system, which is extracted from an enrollment clip of the target voice with upstream models. Those models are generally heavy as the speaker embedding's quality directly affects PSE performances. Yet, embeddings generated beforehand cannot account for the variations of the target voice during inference time. In this paper, we propose to perform on-thefly refinement of the speaker embedding using a tiny speaker encoder. We first introduce a novel contrastive knowledge distillation methodology in order to train a 150k-parameter encoder from complex embeddings. We then use this encoder within the enhancement system during inference and show that the proposed method greatly improves PSE performances while maintaining a low computational load.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5728\u63a8\u7406\u65f6\u52a8\u6001\u4f18\u5316\u8bf4\u8bdd\u4eba\u5d4c\u5165\u7684\u65b9\u6cd5\uff0c\u4f7f\u7528\u8f7b\u91cf\u7ea7\u8bf4\u8bdd\u4eba\u7f16\u7801\u5668\u63d0\u5347\u4e2a\u6027\u5316\u8bed\u97f3\u589e\u5f3a\u6027\u80fd", "motivation": "\u73b0\u6709\u4e2a\u6027\u5316\u8bed\u97f3\u589e\u5f3a\u7cfb\u7edf\u4f7f\u7528\u9884\u5148\u63d0\u53d6\u7684\u8bf4\u8bdd\u4eba\u5d4c\u5165\uff0c\u8fd9\u4e9b\u5d4c\u5165\u65e0\u6cd5\u9002\u5e94\u63a8\u7406\u65f6\u76ee\u6807\u58f0\u97f3\u7684\u53d8\u5316\uff0c\u4e14\u4e0a\u6e38\u6a21\u578b\u901a\u5e38\u5f88\u91cd", "method": "\u63d0\u51fa\u5728\u7ebf\u4f18\u5316\u8bf4\u8bdd\u4eba\u5d4c\u5165\u7684\u65b9\u6cd5\uff0c\u4f7f\u7528\u5bf9\u6bd4\u77e5\u8bc6\u84b8\u998f\u8bad\u7ec3150k\u53c2\u6570\u7684\u8f7b\u91cf\u7f16\u7801\u5668\uff0c\u5728\u63a8\u7406\u65f6\u52a8\u6001\u66f4\u65b0\u5d4c\u5165", "result": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e2a\u6027\u5316\u8bed\u97f3\u589e\u5f3a\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u4f4e\u8ba1\u7b97\u8d1f\u8f7d", "conclusion": "\u901a\u8fc7\u8f7b\u91cf\u7ea7\u7f16\u7801\u5668\u5728\u7ebf\u4f18\u5316\u8bf4\u8bdd\u4eba\u5d4c\u5165\u662f\u6709\u6548\u7684\uff0c\u80fd\u9002\u5e94\u76ee\u6807\u58f0\u97f3\u53d8\u5316\u5e76\u63d0\u5347\u589e\u5f3a\u6027\u80fd"}}
{"id": "2601.16303", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.16303", "abs": "https://arxiv.org/abs/2601.16303", "authors": ["Sahar Golipoor", "Reza Ghazalian", "Ines Lobato Mesquita", "Stephan Sigg"], "title": "Angle of Arrival Estimation for Gesture Recognition from reflective body-worn tags", "comment": null, "summary": "We investigate hand gesture recognition by leveraging passive reflective tags worn on the body. Considering a large set of gestures, distinct patterns are difficult to be captured by learning algorithms using backscattered received signal strength (RSS) and phase signals. This is because these features often exhibit similarities across signals from different gestures. To address this limitation, we explore the estimation of Angle of Arrival (AoA) as a distinguishing feature, since AoA characteristically varies during body motion. To ensure reliable estimation in our system, which employs Smart Antenna Switching (SAS), we first validate AoA estimation using the Multiple SIgnal Classification (MUSIC) algorithm while the tags are fixed at specific angles. Building on this, we propose an AoA tracking method based on Kalman smoothing. Our analysis demonstrates that, while RSS and phase alone are insufficient for distinguishing certain gesture data, AoA tracking can effectively differentiate them. To evaluate the effectiveness of AoA tracking, we implement gesture recognition system benchmarks and show that incorporating AoA features significantly boosts their performance. Improvements of up to 15% confirm the value of AoA-based enhancement.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u5229\u7528\u88ab\u52a8\u53cd\u5c04\u6807\u7b7e\u8fdb\u884c\u624b\u52bf\u8bc6\u522b\uff0c\u63d0\u51fa\u4f7f\u7528\u5230\u8fbe\u89d2(AoA)\u8ddf\u8e2a\u4f5c\u4e3a\u533a\u5206\u7279\u5f81\uff0c\u76f8\u6bd4\u4f20\u7edfRSS\u548c\u76f8\u4f4d\u4fe1\u53f7\u80fd\u663e\u8457\u63d0\u5347\u8bc6\u522b\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u63a5\u6536\u4fe1\u53f7\u5f3a\u5ea6(RSS)\u548c\u76f8\u4f4d\u4fe1\u53f7\u7684\u624b\u52bf\u8bc6\u522b\u65b9\u6cd5\u5728\u533a\u5206\u5927\u91cf\u624b\u52bf\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u56e0\u4e3a\u8fd9\u4e9b\u7279\u5f81\u5728\u4e0d\u540c\u624b\u52bf\u95f4\u5f80\u5f80\u76f8\u4f3c\uff0c\u96be\u4ee5\u88ab\u5b66\u4e60\u7b97\u6cd5\u6709\u6548\u6355\u6349\u3002", "method": "1. \u4f7f\u7528MUSIC\u7b97\u6cd5\u9a8c\u8bc1AoA\u4f30\u8ba1\u7684\u53ef\u9760\u6027\uff1b2. \u63d0\u51fa\u57fa\u4e8e\u5361\u5c14\u66fc\u5e73\u6ed1\u7684AoA\u8ddf\u8e2a\u65b9\u6cd5\uff1b3. \u5c06AoA\u7279\u5f81\u96c6\u6210\u5230\u624b\u52bf\u8bc6\u522b\u7cfb\u7edf\u4e2d\u8fdb\u884c\u6027\u80fd\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cAoA\u8ddf\u8e2a\u80fd\u6709\u6548\u533a\u5206RSS\u548c\u76f8\u4f4d\u65e0\u6cd5\u533a\u5206\u7684\u624b\u52bf\u6570\u636e\uff0c\u5c06AoA\u7279\u5f81\u96c6\u6210\u5230\u624b\u52bf\u8bc6\u522b\u7cfb\u7edf\u4e2d\u53ef\u5e26\u6765\u9ad8\u8fbe15%\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "AoA\u4f5c\u4e3a\u533a\u5206\u7279\u5f81\u5728\u624b\u52bf\u8bc6\u522b\u4e2d\u5177\u6709\u91cd\u8981\u4ef7\u503c\uff0c\u57fa\u4e8eAoA\u8ddf\u8e2a\u7684\u65b9\u6cd5\u80fd\u663e\u8457\u63d0\u5347\u624b\u52bf\u8bc6\u522b\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u4e3a\u89e3\u51b3\u4f20\u7edf\u4fe1\u53f7\u7279\u5f81\u76f8\u4f3c\u6027\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u3002"}}
{"id": "2601.16483", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2601.16483", "abs": "https://arxiv.org/abs/2601.16483", "authors": ["Haoxu Wang", "Biao Tian", "Yiheng Jiang", "Zexu Pan", "Shengkui Zhao", "Bin Ma", "Daren Chen", "Xiangang Li"], "title": "FlowSE-GRPO: Training Flow Matching Speech Enhancement via Online Reinforcement Learning", "comment": "Accepted by ICASSP 2026", "summary": "Generative speech enhancement offers a promising alternative to traditional discriminative methods by modeling the distribution of clean speech conditioned on noisy inputs. Post-training alignment via reinforcement learning (RL) effectively aligns generative models with human preferences and downstream metrics in domains such as natural language processing, but its use in speech enhancement remains limited, especially for online RL. Prior work explores offline methods like Direct Preference Optimization (DPO); online methods such as Group Relative Policy Optimization (GRPO) remain largely uninvestigated. In this paper, we present the first successful integration of online GRPO into a flow-matching speech enhancement framework, enabling efficient post-training alignment to perceptual and task-oriented metrics with few update steps. Unlike prior GRPO work on Large Language Models, we adapt the algorithm to the continuous, time-series nature of speech and to the dynamics of flow-matching generative models. We show that optimizing a single reward yields rapid metric gains but often induces reward hacking that degrades audio fidelity despite higher scores. To mitigate this, we propose a multi-metric reward optimization strategy that balances competing objectives, substantially reducing overfitting and improving overall performance. Our experiments validate online GRPO for speech enhancement and provide practical guidance for RL-based post-training of generative audio models.", "AI": {"tldr": "\u9996\u6b21\u6210\u529f\u5c06\u5728\u7ebfGRPO\u96c6\u6210\u5230\u6d41\u5339\u914d\u8bed\u97f3\u589e\u5f3a\u6846\u67b6\u4e2d\uff0c\u901a\u8fc7\u591a\u6307\u6807\u5956\u52b1\u4f18\u5316\u7b56\u7565\u5e73\u8861\u7ade\u4e89\u76ee\u6807\uff0c\u51cf\u5c11\u8fc7\u62df\u5408\u5e76\u63d0\u5347\u6574\u4f53\u6027\u80fd", "motivation": "\u751f\u6210\u5f0f\u8bed\u97f3\u589e\u5f3a\u76f8\u6bd4\u4f20\u7edf\u5224\u522b\u5f0f\u65b9\u6cd5\u5177\u6709\u4f18\u52bf\uff0c\u4f46\u540e\u8bad\u7ec3\u5bf9\u9f50\u5728\u8bed\u97f3\u589e\u5f3a\u9886\u57df\u5e94\u7528\u6709\u9650\uff0c\u7279\u522b\u662f\u5728\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u65b9\u9762\u3002\u5148\u524d\u5de5\u4f5c\u4e3b\u8981\u63a2\u7d22\u79bb\u7ebf\u65b9\u6cd5\u5982DPO\uff0c\u800c\u5728\u7ebf\u65b9\u6cd5\u5982GRPO\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76", "method": "\u5c06\u5728\u7ebfGRPO\u96c6\u6210\u5230\u6d41\u5339\u914d\u8bed\u97f3\u589e\u5f3a\u6846\u67b6\u4e2d\uff0c\u9002\u5e94\u8bed\u97f3\u7684\u8fde\u7eed\u65f6\u95f4\u5e8f\u5217\u7279\u6027\u548c\u6d41\u5339\u914d\u751f\u6210\u6a21\u578b\u7684\u52a8\u6001\u7279\u6027\u3002\u63d0\u51fa\u591a\u6307\u6807\u5956\u52b1\u4f18\u5316\u7b56\u7565\u6765\u5e73\u8861\u7ade\u4e89\u76ee\u6807\uff0c\u9632\u6b62\u5956\u52b1\u9ed1\u5ba2\u653b\u51fb", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5728\u7ebfGRPO\u5728\u8bed\u97f3\u589e\u5f3a\u4e2d\u7684\u6709\u6548\u6027\uff0c\u4f18\u5316\u5355\u4e00\u5956\u52b1\u80fd\u5feb\u901f\u63d0\u5347\u6307\u6807\u4f46\u4f1a\u5bfc\u81f4\u97f3\u9891\u4fdd\u771f\u5ea6\u4e0b\u964d\uff0c\u591a\u6307\u6807\u5956\u52b1\u4f18\u5316\u7b56\u7565\u80fd\u663e\u8457\u51cf\u5c11\u8fc7\u62df\u5408\u5e76\u6539\u5584\u6574\u4f53\u6027\u80fd", "conclusion": "\u6210\u529f\u5c55\u793a\u4e86\u5728\u7ebfGRPO\u5728\u8bed\u97f3\u589e\u5f3a\u4e2d\u7684\u5e94\u7528\uff0c\u4e3a\u751f\u6210\u5f0f\u97f3\u9891\u6a21\u578b\u7684\u57fa\u4e8eRL\u7684\u540e\u8bad\u7ec3\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\uff0c\u591a\u6307\u6807\u4f18\u5316\u7b56\u7565\u80fd\u6709\u6548\u5e73\u8861\u4e0d\u540c\u76ee\u6807"}}
{"id": "2601.16273", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.16273", "abs": "https://arxiv.org/abs/2601.16273", "authors": ["Shikhar Bharadwaj", "Samuele Cornell", "Kwanghee Choi", "Hye-jin Shim", "Soham Deshmukh", "Satoru Fukayama", "Shinji Watanabe"], "title": "The CMU-AIST submission for the ICME 2025 Audio Encoder Challenge", "comment": null, "summary": "This technical report describes our submission to the ICME 2025 audio encoder challenge. Our submitted system is built on BEATs, a masked speech token prediction based audio encoder. We extend the BEATs model using 74,000 hours of data derived from various speech, music, and sound corpora and scale its architecture upto 300 million parameters. We experiment with speech-heavy and balanced pre-training mixtures to study the impact of different domains on final performance. Our submitted system consists of an ensemble of the Dasheng 1.2 billion model with two custom scaled-up BEATs models trained on the aforementioned pre-training data mixtures. We also propose a simple ensembling technique that retains the best capabilities of constituent models and surpasses both the baseline and Dasheng 1.2B. For open science, we publicly release our trained checkpoints via huggingface at https://huggingface.co/shikhar7ssu/OpenBEATs-ICME-SOUND and https://huggingface.co/shikhar7ssu/OpenBEATs-ICME.", "AI": {"tldr": "\u57fa\u4e8eBEATs\u97f3\u9891\u7f16\u7801\u5668\u6784\u5efa\u7684ICME 2025\u97f3\u9891\u7f16\u7801\u6311\u6218\u8d5b\u63d0\u4ea4\u7cfb\u7edf\uff0c\u901a\u8fc774,000\u5c0f\u65f6\u591a\u9886\u57df\u6570\u636e\u9884\u8bad\u7ec3\u548c\u67b6\u6784\u6269\u5c55\u81f33\u4ebf\u53c2\u6570\uff0c\u91c7\u7528\u96c6\u6210\u65b9\u6cd5\u8d85\u8d8a\u57fa\u7ebf\u6a21\u578b", "motivation": "\u53c2\u52a0ICME 2025\u97f3\u9891\u7f16\u7801\u6311\u6218\u8d5b\uff0c\u63a2\u7d22\u4e0d\u540c\u9886\u57df\u6570\u636e\uff08\u8bed\u97f3\u3001\u97f3\u4e50\u3001\u58f0\u97f3\uff09\u5bf9\u97f3\u9891\u7f16\u7801\u5668\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u5e76\u5f00\u53d1\u8d85\u8d8a\u73b0\u6709\u57fa\u7ebf\u6a21\u578b\u7684\u96c6\u6210\u7cfb\u7edf", "method": "\u6269\u5c55BEATs\u6a21\u578b\u67b6\u6784\u81f33\u4ebf\u53c2\u6570\uff0c\u4f7f\u752874,000\u5c0f\u65f6\u591a\u9886\u57df\u6570\u636e\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u5b9e\u9a8c\u4e0d\u540c\u6570\u636e\u6df7\u5408\u6bd4\u4f8b\uff0c\u91c7\u7528Dasheng 12\u4ebf\u6a21\u578b\u4e0e\u4e24\u4e2a\u5b9a\u5236BEATs\u6a21\u578b\u7684\u96c6\u6210\u7b56\u7565", "result": "\u63d0\u51fa\u7684\u96c6\u6210\u7cfb\u7edf\u8d85\u8d8a\u4e86\u57fa\u7ebf\u6a21\u578b\u548cDasheng 12\u4ebf\u6a21\u578b\uff0c\u8bad\u7ec3\u597d\u7684\u68c0\u67e5\u70b9\u5df2\u901a\u8fc7HuggingFace\u516c\u5f00\u53d1\u5e03", "conclusion": "\u901a\u8fc7\u5927\u89c4\u6a21\u591a\u9886\u57df\u6570\u636e\u9884\u8bad\u7ec3\u548c\u667a\u80fd\u96c6\u6210\u7b56\u7565\uff0c\u6210\u529f\u5f00\u53d1\u4e86\u6027\u80fd\u4f18\u8d8a\u7684\u97f3\u9891\u7f16\u7801\u7cfb\u7edf\uff0c\u4e3a\u97f3\u9891\u7f16\u7801\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u5f00\u6e90\u8d44\u6e90"}}
{"id": "2601.16421", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.16421", "abs": "https://arxiv.org/abs/2601.16421", "authors": ["Gautham Reddy", "Ismail Guvenc", "Mihail L. Sichitiu", "Arupjyoti Bhuyan", "Bryton Petersen", "Jason Abrahamson"], "title": "TransfoREM: Transformer aided 3D Radio Environment Mapping", "comment": "This paper has been submitted to IEEE ICC 2026", "summary": "Providing reliable cellular connectivity to Unmanned Aerial Vehicles (UAV) is a key challenge, as existing terrestrial networks are deployed mainly for ground-level coverage. The cellular network coverage may be available for a limited range from the antenna side lobes, with poor connectivity further exacerbated by UAV flight dynamics. In this work, we propose TransfoREM, a 3D Radio Environment Map (REM) generation method that combines deterministic channel models and real-world data to map terrestrial network coverage at higher altitudes. At the core of our solution is a transformer model that translates radio propagation mapping into a sequence prediction task to construct REMs. Our results demonstrate that TransfoREM offers improved interpolation capability on real-world data compared against conventional Kriging and other machine learning (ML) techniques. Furthermore, TransfoREM is designed for holistic integration into cellular networks at the base station (BS) level, where it can build REMs, which can then be leveraged for enhanced resource allocation, interference management, and spatial spectrum utilization.", "AI": {"tldr": "TransfoREM\uff1a\u4e00\u79cd\u57fa\u4e8eTransformer\u76843D\u65e0\u7ebf\u7535\u73af\u5883\u5730\u56fe\u751f\u6210\u65b9\u6cd5\uff0c\u7528\u4e8e\u9884\u6d4b\u65e0\u4eba\u673a\u5728\u66f4\u9ad8\u6d77\u62d4\u7684\u8702\u7a9d\u7f51\u7edc\u8986\u76d6\u60c5\u51b5\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u5177\u6709\u66f4\u597d\u7684\u63d2\u503c\u80fd\u529b\u3002", "motivation": "\u4e3a\u65e0\u4eba\u673a\u63d0\u4f9b\u53ef\u9760\u7684\u8702\u7a9d\u7f51\u7edc\u8fde\u63a5\u662f\u4e00\u4e2a\u5173\u952e\u6311\u6218\uff0c\u56e0\u4e3a\u73b0\u6709\u7684\u5730\u9762\u7f51\u7edc\u4e3b\u8981\u90e8\u7f72\u5728\u5730\u9762\u8986\u76d6\uff0c\u65e0\u4eba\u673a\u53ea\u80fd\u4ece\u5929\u7ebf\u65c1\u74e3\u83b7\u5f97\u6709\u9650\u8986\u76d6\uff0c\u4e14\u98de\u884c\u52a8\u6001\u4f1a\u8fdb\u4e00\u6b65\u6076\u5316\u8fde\u63a5\u8d28\u91cf\u3002", "method": "\u63d0\u51faTransfoREM\u65b9\u6cd5\uff0c\u7ed3\u5408\u786e\u5b9a\u6027\u4fe1\u9053\u6a21\u578b\u548c\u771f\u5b9e\u4e16\u754c\u6570\u636e\u6765\u751f\u62103D\u65e0\u7ebf\u7535\u73af\u5883\u5730\u56fe\u3002\u6838\u5fc3\u662f\u4f7f\u7528Transformer\u6a21\u578b\uff0c\u5c06\u65e0\u7ebf\u7535\u4f20\u64ad\u6620\u5c04\u8f6c\u5316\u4e3a\u5e8f\u5217\u9884\u6d4b\u4efb\u52a1\u6765\u6784\u5efaREM\u3002", "result": "TransfoREM\u5728\u771f\u5b9e\u4e16\u754c\u6570\u636e\u4e0a\u76f8\u6bd4\u4f20\u7edf\u7684Kriging\u548c\u5176\u4ed6\u673a\u5668\u5b66\u4e60\u6280\u672f\u5177\u6709\u66f4\u597d\u7684\u63d2\u503c\u80fd\u529b\u3002\u8be5\u65b9\u6cd5\u8bbe\u8ba1\u7528\u4e8e\u5728\u57fa\u7ad9\u7ea7\u522b\u96c6\u6210\u5230\u8702\u7a9d\u7f51\u7edc\u4e2d\u3002", "conclusion": "TransfoREM\u80fd\u591f\u6784\u5efa\u53ef\u7528\u4e8e\u589e\u5f3a\u8d44\u6e90\u5206\u914d\u3001\u5e72\u6270\u7ba1\u7406\u548c\u7a7a\u95f4\u9891\u8c31\u5229\u7528\u7684\u65e0\u7ebf\u7535\u73af\u5883\u5730\u56fe\uff0c\u4e3a\u89e3\u51b3\u65e0\u4eba\u673a\u8702\u7a9d\u7f51\u7edc\u8986\u76d6\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.16540", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.16540", "abs": "https://arxiv.org/abs/2601.16540", "authors": ["Haoyun Yang", "Xin Xiao", "Jiang Zhong", "Yu Tian", "Dong Xiaohua", "Yu Mao", "Hao Wu", "Kaiwen Wei"], "title": "Do Models Hear Like Us? Probing the Representational Alignment of Audio LLMs and Naturalistic EEG", "comment": null, "summary": "Audio Large Language Models (Audio LLMs) have demonstrated strong capabilities in integrating speech perception with language understanding. However, whether their internal representations align with human neural dynamics during naturalistic listening remains largely unexplored. In this work, we systematically examine layer-wise representational alignment between 12 open-source Audio LLMs and Electroencephalogram (EEG) signals across 2 datasets. Specifically, we employ 8 similarity metrics, such as Spearman-based Representational Similarity Analysis (RSA), to characterize within-sentence representational geometry. Our analysis reveals 3 key findings: (1) we observe a rank-dependence split, in which model rankings vary substantially across different similarity metrics; (2) we identify spatio-temporal alignment patterns characterized by depth-dependent alignment peaks and a pronounced increase in RSA within the 250-500 ms time window, consistent with N400-related neural dynamics; (3) we find an affective dissociation whereby negative prosody, identified using a proposed Tri-modal Neighborhood Consistency (TNC) criterion, reduces geometric similarity while enhancing covariance-based dependence. These findings provide new neurobiological insights into the representational mechanisms of Audio LLMs.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u7814\u7a76\u4e8612\u4e2a\u5f00\u6e90\u97f3\u9891\u5927\u8bed\u8a00\u6a21\u578b\u4e0e\u4eba\u7c7b\u8111\u7535\u56fe\u4fe1\u53f7\u5728\u81ea\u7136\u542c\u89c9\u8fc7\u7a0b\u4e2d\u7684\u8868\u5f81\u5bf9\u9f50\uff0c\u53d1\u73b0\u6a21\u578b\u6392\u540d\u968f\u76f8\u4f3c\u5ea6\u5ea6\u91cf\u53d8\u5316\u3001\u5b58\u5728\u65f6\u7a7a\u5bf9\u9f50\u6a21\u5f0f\uff08250-500ms\u7a97\u53e3\u4e0eN400\u795e\u7ecf\u52a8\u6001\u4e00\u81f4\uff09\u3001\u4ee5\u53ca\u60c5\u611f\u89e3\u79bb\u73b0\u8c61\uff08\u8d1f\u6027\u97f5\u5f8b\u964d\u4f4e\u51e0\u4f55\u76f8\u4f3c\u5ea6\u4f46\u589e\u5f3a\u534f\u65b9\u5dee\u4f9d\u8d56\uff09\u3002", "motivation": "\u97f3\u9891\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8bed\u97f3\u611f\u77e5\u548c\u8bed\u8a00\u7406\u89e3\u65b9\u9762\u8868\u73b0\u51fa\u5f3a\u5927\u80fd\u529b\uff0c\u4f46\u5176\u5185\u90e8\u8868\u5f81\u662f\u5426\u4e0e\u4eba\u7c7b\u5728\u81ea\u7136\u542c\u89c9\u8fc7\u7a0b\u4e2d\u7684\u795e\u7ecf\u52a8\u6001\u5bf9\u9f50\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u901a\u8fc7\u7cfb\u7edf\u5206\u6790\u97f3\u9891LLMs\u4e0e\u8111\u7535\u56fe\u4fe1\u53f7\u7684\u8868\u5f81\u5bf9\u9f50\uff0c\u4e3a\u7406\u89e3\u8fd9\u4e9b\u6a21\u578b\u7684\u795e\u7ecf\u751f\u7269\u5b66\u57fa\u7840\u63d0\u4f9b\u65b0\u89c1\u89e3\u3002", "method": "\u4f7f\u752812\u4e2a\u5f00\u6e90\u97f3\u9891\u5927\u8bed\u8a00\u6a21\u578b\u548c2\u4e2a\u8111\u7535\u56fe\u6570\u636e\u96c6\uff0c\u91c7\u75288\u79cd\u76f8\u4f3c\u5ea6\u5ea6\u91cf\u65b9\u6cd5\uff08\u5305\u62ec\u57fa\u4e8eSpearman\u7684\u8868\u5f81\u76f8\u4f3c\u6027\u5206\u6790\uff09\u6765\u8868\u5f81\u53e5\u5b50\u5185\u7684\u8868\u5f81\u51e0\u4f55\u7ed3\u6784\u3002\u63d0\u51fa\u4e86\u4e09\u6a21\u6001\u90bb\u57df\u4e00\u81f4\u6027\u51c6\u5219\u6765\u8bc6\u522b\u8d1f\u6027\u97f5\u5f8b\u3002", "result": "\u53d1\u73b0\u4e09\u4e2a\u5173\u952e\u7ed3\u679c\uff1a1\uff09\u6a21\u578b\u6392\u540d\u968f\u76f8\u4f3c\u5ea6\u5ea6\u91cf\u65b9\u6cd5\u53d8\u5316\u663e\u8457\uff1b2\uff09\u5b58\u5728\u6df1\u5ea6\u4f9d\u8d56\u7684\u5bf9\u9f50\u5cf0\u503c\uff0c\u5728250-500ms\u65f6\u95f4\u7a97\u53e3\u5185RSA\u663e\u8457\u589e\u52a0\uff0c\u4e0eN400\u795e\u7ecf\u52a8\u6001\u4e00\u81f4\uff1b3\uff09\u8d1f\u6027\u97f5\u5f8b\uff08\u901a\u8fc7TNC\u51c6\u5219\u8bc6\u522b\uff09\u964d\u4f4e\u51e0\u4f55\u76f8\u4f3c\u5ea6\u4f46\u589e\u5f3a\u534f\u65b9\u5dee\u4f9d\u8d56\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u4e3a\u97f3\u9891\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8868\u5f81\u673a\u5236\u63d0\u4f9b\u4e86\u65b0\u7684\u795e\u7ecf\u751f\u7269\u5b66\u89c1\u89e3\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u5185\u90e8\u8868\u5f81\u4e0e\u4eba\u7c7b\u795e\u7ecf\u52a8\u6001\u4e4b\u95f4\u7684\u590d\u6742\u5bf9\u9f50\u6a21\u5f0f\uff0c\u7279\u522b\u662f\u5728\u65f6\u95f4\u7a97\u53e3\u548c\u60c5\u611f\u5904\u7406\u65b9\u9762\u7684\u7279\u5f02\u6027\u3002"}}
{"id": "2601.16442", "categories": ["eess.SP", "cs.HC", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.16442", "abs": "https://arxiv.org/abs/2601.16442", "authors": ["Masahiro Yoshino", "Haruki Yokota", "Junya Hara", "Yuichi Tanaka", "Hiroshi Higashi"], "title": "Auditory Attention Decoding without Spatial Information: A Diotic EEG Study", "comment": null, "summary": "Auditory attention decoding (AAD) identifies the attended speech stream in multi-speaker environments by decoding brain signals such as electroencephalography (EEG). This technology is essential for realizing smart hearing aids that address the cocktail party problem and for facilitating objective audiometry systems. Existing AAD research mainly utilizes dichotic environments where different speech signals are presented to the left and right ears, enabling models to classify directional attention rather than speech content. However, this spatial reliance limits applicability to real-world scenarios, such as the \"cocktail party\" situation, where speakers overlap or move dynamically. To address this challenge, we propose an AAD framework for diotic environments where identical speech mixtures are presented to both ears, eliminating spatial cues. Our approach maps EEG and speech signals into a shared latent space using independent encoders. We extract speech features using wav2vec 2.0 and encode them with a 2-layer 1D convolutional neural network (CNN), while employing the BrainNetwork architecture for EEG encoding. The model identifies the attended speech by calculating the cosine similarity between EEG and speech representations. We evaluate our method on a diotic EEG dataset and achieve 72.70% accuracy, which is 22.58% higher than the state-of-the-art direction-based AAD method.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7528\u4e8e\u53cc\u8033\u540c\u58f0\u73af\u5883\u7684\u542c\u89c9\u6ce8\u610f\u89e3\u7801\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u8111\u7535\u548c\u8bed\u97f3\u4fe1\u53f7\u6620\u5c04\u5230\u5171\u4eab\u6f5c\u5728\u7a7a\u95f4\uff0c\u6d88\u9664\u7a7a\u95f4\u7ebf\u7d22\u4f9d\u8d56\uff0c\u5728\u9e21\u5c3e\u9152\u4f1a\u7b49\u771f\u5b9e\u573a\u666f\u4e2d\u5b9e\u73b0\u66f4\u51c6\u786e\u7684\u6ce8\u610f\u529b\u89e3\u7801\u3002", "motivation": "\u73b0\u6709\u542c\u89c9\u6ce8\u610f\u89e3\u7801\u7814\u7a76\u4e3b\u8981\u4f9d\u8d56\u53cc\u8033\u5206\u542c\u73af\u5883\uff0c\u5229\u7528\u7a7a\u95f4\u65b9\u5411\u7ebf\u7d22\u8fdb\u884c\u5206\u7c7b\uff0c\u8fd9\u9650\u5236\u4e86\u5728\u771f\u5b9e\u4e16\u754c\u573a\u666f\uff08\u5982\u9e21\u5c3e\u9152\u4f1a\uff09\u4e2d\u7684\u5e94\u7528\uff0c\u56e0\u4e3a\u771f\u5b9e\u573a\u666f\u4e2d\u8bf4\u8bdd\u8005\u53ef\u80fd\u91cd\u53e0\u6216\u52a8\u6001\u79fb\u52a8\u3002", "method": "\u63d0\u51fa\u53cc\u8033\u540c\u58f0\u73af\u5883\u7684AAD\u6846\u67b6\uff0c\u4f7f\u7528\u72ec\u7acb\u7f16\u7801\u5668\u5c06\u8111\u7535\u548c\u8bed\u97f3\u4fe1\u53f7\u6620\u5c04\u5230\u5171\u4eab\u6f5c\u5728\u7a7a\u95f4\uff1a\u7528wav2vec 2.0\u63d0\u53d6\u8bed\u97f3\u7279\u5f81\uff0c2\u5c421D CNN\u7f16\u7801\uff1b\u7528BrainNetwork\u67b6\u6784\u7f16\u7801\u8111\u7535\u4fe1\u53f7\uff1b\u901a\u8fc7\u8ba1\u7b97\u8111\u7535\u548c\u8bed\u97f3\u8868\u793a\u4e4b\u95f4\u7684\u4f59\u5f26\u76f8\u4f3c\u5ea6\u6765\u8bc6\u522b\u6ce8\u610f\u8bed\u97f3\u3002", "result": "\u5728\u53cc\u8033\u540c\u58f0\u8111\u7535\u6570\u636e\u96c6\u4e0a\u8fbe\u523072.70%\u7684\u51c6\u786e\u7387\uff0c\u6bd4\u6700\u5148\u8fdb\u7684\u65b9\u5411\u57faAAD\u65b9\u6cd5\u63d0\u9ad8\u4e8622.58%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6d88\u9664\u4e86\u5bf9\u7a7a\u95f4\u7ebf\u7d22\u7684\u4f9d\u8d56\uff0c\u4e3a\u771f\u5b9e\u4e16\u754c\u542c\u89c9\u573a\u666f\uff08\u5982\u9e21\u5c3e\u9152\u4f1a\uff09\u4e2d\u7684\u542c\u89c9\u6ce8\u610f\u89e3\u7801\u63d0\u4f9b\u4e86\u66f4\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5bf9\u667a\u80fd\u52a9\u542c\u5668\u548c\u5ba2\u89c2\u542c\u529b\u6d4b\u8bd5\u7cfb\u7edf\u7684\u53d1\u5c55\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2601.16547", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.16547", "abs": "https://arxiv.org/abs/2601.16547", "authors": ["Jing Hu", "Danxiang Zhu", "Xianlong Luo", "Dan Zhang", "Shuwei He", "Yishu Lei", "Haitao Zheng", "Shikun Feng", "Jingzhou He", "Yu Sun", "Hua Wu", "Haifeng Wang"], "title": "CORD: Bridging the Audio-Text Reasoning Gap via Weighted On-policy Cross-modal Distillation", "comment": "13 pages, 4 figures", "summary": "Large Audio Language Models (LALMs) have garnered significant research interest. Despite being built upon text-based large language models (LLMs), LALMs frequently exhibit a degradation in knowledge and reasoning capabilities. We hypothesize that this limitation stems from the failure of current training paradigms to effectively bridge the acoustic-semantic gap within the feature representation space. To address this challenge, we propose CORD, a unified alignment framework that performs online cross-modal self-distillation. Specifically, it aligns audio-conditioned reasoning with its text-conditioned counterpart within a unified model. Leveraging the text modality as an internal teacher, CORD performs multi-granularity alignment throughout the audio rollout process. At the token level, it employs on-policy reverse KL divergence with importance-aware weighting to prioritize early and semantically critical tokens. At the sequence level, CORD introduces a judge-based global reward to optimize complete reasoning trajectories via Group Relative Policy Optimization (GRPO). Empirical results across multiple benchmarks demonstrate that CORD consistently enhances audio-conditioned reasoning and substantially bridges the audio-text performance gap with only 80k synthetic training samples, validating the efficacy and data efficiency of our on-policy, multi-level cross-modal alignment approach.", "AI": {"tldr": "CORD\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u8de8\u6a21\u6001\u5bf9\u9f50\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u7ebf\u81ea\u84b8\u998f\u65b9\u6cd5\uff0c\u5229\u7528\u6587\u672c\u6a21\u6001\u4f5c\u4e3a\u5185\u90e8\u6559\u5e08\uff0c\u5728\u97f3\u9891\u63a8\u7406\u8fc7\u7a0b\u4e2d\u8fdb\u884c\u591a\u7c92\u5ea6\u5bf9\u9f50\uff0c\u663e\u8457\u7f29\u5c0f\u97f3\u9891\u4e0e\u6587\u672c\u6027\u80fd\u5dee\u8ddd\u3002", "motivation": "\u5927\u578b\u97f3\u9891\u8bed\u8a00\u6a21\u578b\uff08LALMs\uff09\u867d\u7136\u57fa\u4e8e\u6587\u672c\u5927\u8bed\u8a00\u6a21\u578b\u6784\u5efa\uff0c\u4f46\u5728\u77e5\u8bc6\u548c\u63a8\u7406\u80fd\u529b\u4e0a\u7ecf\u5e38\u51fa\u73b0\u9000\u5316\u3002\u4f5c\u8005\u8ba4\u4e3a\u8fd9\u662f\u56e0\u4e3a\u5f53\u524d\u8bad\u7ec3\u8303\u5f0f\u672a\u80fd\u6709\u6548\u5f25\u5408\u7279\u5f81\u8868\u793a\u7a7a\u95f4\u4e2d\u7684\u58f0\u5b66-\u8bed\u4e49\u9e3f\u6c9f\u3002", "method": "\u63d0\u51faCORD\u6846\u67b6\uff0c\u6267\u884c\u5728\u7ebf\u8de8\u6a21\u6001\u81ea\u84b8\u998f\uff1a1\uff09\u5728token\u7ea7\u522b\u4f7f\u7528\u5e26\u91cd\u8981\u6027\u52a0\u6743\u7684\u7b56\u7565\u53cd\u5411KL\u6563\u5ea6\uff0c\u4f18\u5148\u5904\u7406\u65e9\u671f\u548c\u8bed\u4e49\u5173\u952etoken\uff1b2\uff09\u5728\u5e8f\u5217\u7ea7\u522b\u5f15\u5165\u57fa\u4e8e\u8bc4\u5224\u7684\u5168\u5c40\u5956\u52b1\uff0c\u901a\u8fc7\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\uff08GRPO\uff09\u4f18\u5316\u5b8c\u6574\u63a8\u7406\u8f68\u8ff9\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCORD\u6301\u7eed\u589e\u5f3a\u4e86\u97f3\u9891\u6761\u4ef6\u63a8\u7406\u80fd\u529b\uff0c\u4ec5\u75288\u4e07\u4e2a\u5408\u6210\u8bad\u7ec3\u6837\u672c\u5c31\u663e\u8457\u7f29\u5c0f\u4e86\u97f3\u9891\u4e0e\u6587\u672c\u6027\u80fd\u5dee\u8ddd\uff0c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u6570\u636e\u6548\u7387\u3002", "conclusion": "CORD\u901a\u8fc7\u7b56\u7565\u6027\u3001\u591a\u5c42\u6b21\u7684\u8de8\u6a21\u6001\u5bf9\u9f50\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86LALMs\u4e2d\u7684\u77e5\u8bc6\u9000\u5316\u95ee\u9898\uff0c\u4e3a\u97f3\u9891\u8bed\u8a00\u6a21\u578b\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u5bf9\u9f50\u6846\u67b6\u3002"}}
{"id": "2601.16543", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.16543", "abs": "https://arxiv.org/abs/2601.16543", "authors": ["Xingxiang Peng", "Qingqing Wu", "Ziyuan Zheng", "Yanze Zhu", "Wen Chen", "Penghui Huang", "Ying Gao", "Honghao Wang"], "title": "Cell-Free MIMO with Rotatable Antennas: When Macro-Diversity Meets Antenna Directivity", "comment": "12 pages, 7 figures. Submitted to an IEEE journal for possible publication", "summary": "Cell-free networks leverage distributed access points (APs) to achieve macro-diversity, yet their performance is often constrained by large disparities in channel quality arising from user geometry and blockages. To address this, rotatable antennas (RAs) add a lightweight hardware degree of freedom by steering the antenna boresight toward dominant propagation directions to strengthen unfavorable links, thereby enabling the network to better exploit macro-diversity for higher and more uniform performance. This paper investigates an RA-enabled cell-free downlink network and formulates a max-min rate problem that jointly optimizes transmit beamforming and antenna orientations. To tackle this challenging problem, we develop an alternating-optimization-based algorithm that iteratively updates the beamformers via a second-order cone program (SOCP) and optimizes the antenna orientations using successive convex approximation. To reduce complexity, we further propose an efficient two-stage scheme that first designs orientations by maximizing a proportional-fair log-utility using manifold-aware Frank-Wolfe updates, and then computes the beamformers using an SOCP-based design. Simulation results demonstrate that the proposed orientation-aware designs achieve a substantially higher worst-user rate than conventional beamforming-only benchmarks. Furthermore, larger antenna directivity enhances fairness with proper orientation but can degrade the worst-user performance otherwise.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u5728\u65e0\u8702\u7a9d\u7f51\u7edc\u4e2d\u4f7f\u7528\u53ef\u65cb\u8f6c\u5929\u7ebf\u589e\u5f3a\u6700\u5dee\u7528\u6237\u901f\u7387\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u6ce2\u675f\u6210\u5f62\u548c\u5929\u7ebf\u65b9\u5411\uff0c\u5f00\u53d1\u4e86\u4ea4\u66ff\u4f18\u5316\u7b97\u6cd5\u548c\u9ad8\u6548\u4e24\u9636\u6bb5\u65b9\u6848\u3002", "motivation": "\u65e0\u8702\u7a9d\u7f51\u7edc\u867d\u7136\u5229\u7528\u5206\u5e03\u5f0f\u63a5\u5165\u70b9\u5b9e\u73b0\u5b8f\u5206\u96c6\uff0c\u4f46\u7528\u6237\u51e0\u4f55\u4f4d\u7f6e\u548c\u969c\u788d\u7269\u5bfc\u81f4\u7684\u4fe1\u9053\u8d28\u91cf\u5dee\u5f02\u9650\u5236\u4e86\u6027\u80fd\u3002\u53ef\u65cb\u8f6c\u5929\u7ebf\u901a\u8fc7\u8c03\u6574\u5929\u7ebf\u4e3b\u74e3\u65b9\u5411\u6765\u589e\u5f3a\u4e0d\u5229\u94fe\u8def\uff0c\u4ece\u800c\u66f4\u597d\u5730\u5229\u7528\u5b8f\u5206\u96c6\uff0c\u63d0\u9ad8\u7f51\u7edc\u6027\u80fd\u548c\u516c\u5e73\u6027\u3002", "method": "1) \u63d0\u51fa\u8054\u5408\u4f18\u5316\u6ce2\u675f\u6210\u5f62\u548c\u5929\u7ebf\u65b9\u5411\u7684\u6700\u5927\u6700\u5c0f\u901f\u7387\u95ee\u9898\uff1b2) \u5f00\u53d1\u57fa\u4e8e\u4ea4\u66ff\u4f18\u5316\u7684\u7b97\u6cd5\uff1a\u4f7f\u7528\u4e8c\u9636\u9525\u89c4\u5212\u66f4\u65b0\u6ce2\u675f\u6210\u5f62\u5668\uff0c\u4f7f\u7528\u9010\u6b21\u51f8\u903c\u8fd1\u4f18\u5316\u5929\u7ebf\u65b9\u5411\uff1b3) \u63d0\u51fa\u9ad8\u6548\u4e24\u9636\u6bb5\u65b9\u6848\uff1a\u9996\u5148\u901a\u8fc7\u6d41\u5f62\u611f\u77e5Frank-Wolfe\u66f4\u65b0\u6700\u5927\u5316\u6bd4\u4f8b\u516c\u5e73\u5bf9\u6570\u6548\u7528\u8bbe\u8ba1\u65b9\u5411\uff0c\u7136\u540e\u4f7f\u7528\u57fa\u4e8eSOCP\u7684\u8bbe\u8ba1\u8ba1\u7b97\u6ce2\u675f\u6210\u5f62\u5668\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u5411\u611f\u77e5\u8bbe\u8ba1\u76f8\u6bd4\u4f20\u7edf\u4ec5\u6ce2\u675f\u6210\u5f62\u7684\u57fa\u51c6\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u663e\u8457\u66f4\u9ad8\u7684\u6700\u5dee\u7528\u6237\u901f\u7387\u3002\u6b64\u5916\uff0c\u66f4\u5927\u7684\u5929\u7ebf\u65b9\u5411\u6027\u5728\u9002\u5f53\u5b9a\u5411\u65f6\u80fd\u589e\u5f3a\u516c\u5e73\u6027\uff0c\u4f46\u65b9\u5411\u4e0d\u5f53\u53cd\u800c\u4f1a\u964d\u4f4e\u6700\u5dee\u7528\u6237\u6027\u80fd\u3002", "conclusion": "\u53ef\u65cb\u8f6c\u5929\u7ebf\u662f\u65e0\u8702\u7a9d\u7f51\u7edc\u4e2d\u589e\u5f3a\u6700\u5dee\u7528\u6237\u6027\u80fd\u548c\u516c\u5e73\u6027\u7684\u6709\u6548\u786c\u4ef6\u81ea\u7531\u5ea6\u3002\u8054\u5408\u4f18\u5316\u6ce2\u675f\u6210\u5f62\u548c\u5929\u7ebf\u65b9\u5411\u7684\u8bbe\u8ba1\u80fd\u591f\u663e\u8457\u63d0\u5347\u7f51\u7edc\u6027\u80fd\uff0c\u800c\u63d0\u51fa\u7684\u9ad8\u6548\u7b97\u6cd5\u4e3a\u5b9e\u73b0\u8fd9\u4e00\u76ee\u6807\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.16603", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.16603", "abs": "https://arxiv.org/abs/2601.16603", "authors": ["Ke Xue", "Chang Sun", "Rongfei Fan", "Jing Wang", "Han Hu"], "title": "Omni-directional attention mechanism based on Mamba for speech separation", "comment": null, "summary": "Mamba, a selective state-space model (SSM), has emerged as an efficient alternative to Transformers for speech modeling, enabling long-sequence processing with linear complexity. While effective in speech separation, existing approaches, whether in the time or time-frequency domain, typically decompose the input along a single dimension into short one-dimensional sequences before processing them with Mamba, which restricts it to local 1D modeling and limits its ability to capture global dependencies across the 2D spectrogram. In this work, we propose an efficient omni-directional attention (OA) mechanism built upon unidirectional Mamba, which models global dependencies from ten different directions on the spectrogram. We expand the proposed mechanism into two baseline separation models and evaluate on three public datasets. Experimental results show that our approach consistently achieves significant performance gains over the baselines while preserving linear complexity, outperforming existing state-of-the-art (SOTA) systems.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eMamba\u7684\u9ad8\u6548\u5168\u5411\u6ce8\u610f\u529b\u673a\u5236\uff0c\u7528\u4e8e\u8bed\u97f3\u5206\u79bb\uff0c\u5728\u4fdd\u6301\u7ebf\u6027\u590d\u6742\u5ea6\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u6027\u80fd", "motivation": "\u73b0\u6709\u57fa\u4e8eMamba\u7684\u8bed\u97f3\u5206\u79bb\u65b9\u6cd5\u901a\u5e38\u5c06\u8f93\u5165\u5206\u89e3\u4e3a\u77ed\u4e00\u7ef4\u5e8f\u5217\u5904\u7406\uff0c\u9650\u5236\u4e86\u5176\u5728\u4e8c\u7ef4\u9891\u8c31\u56fe\u4e0a\u6355\u6349\u5168\u5c40\u4f9d\u8d56\u5173\u7cfb\u7684\u80fd\u529b", "method": "\u57fa\u4e8e\u5355\u5411Mamba\u6784\u5efa\u9ad8\u6548\u5168\u5411\u6ce8\u610f\u529b\u673a\u5236\uff0c\u4ece\u9891\u8c31\u56fe\u7684\u5341\u4e2a\u4e0d\u540c\u65b9\u5411\u5efa\u6a21\u5168\u5c40\u4f9d\u8d56\u5173\u7cfb\uff0c\u5e76\u5c06\u8be5\u673a\u5236\u96c6\u6210\u5230\u4e24\u4e2a\u57fa\u7ebf\u5206\u79bb\u6a21\u578b\u4e2d", "result": "\u5728\u4e09\u4e2a\u516c\u5171\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u7ebf\u6027\u590d\u6742\u5ea6\u7684\u540c\u65f6\uff0c\u76f8\u6bd4\u57fa\u7ebf\u6a21\u578b\u83b7\u5f97\u663e\u8457\u6027\u80fd\u63d0\u5347\uff0c\u5e76\u8d85\u8d8a\u4e86\u73b0\u6709\u6700\u5148\u8fdb\u7cfb\u7edf", "conclusion": "\u63d0\u51fa\u7684\u5168\u5411\u6ce8\u610f\u529b\u673a\u5236\u6709\u6548\u6269\u5c55\u4e86Mamba\u5728\u8bed\u97f3\u5206\u79bb\u4e2d\u7684\u5e94\u7528\uff0c\u5b9e\u73b0\u4e86\u5bf9\u4e8c\u7ef4\u9891\u8c31\u56fe\u7684\u5168\u5c40\u5efa\u6a21\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8ba1\u7b97\u6548\u7387"}}
{"id": "2601.16550", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.16550", "abs": "https://arxiv.org/abs/2601.16550", "authors": ["Eike-Manuel Edelmann"], "title": "Spiking Neural Networks for Communication Systems: Encoding Schemes, Learning Algorithms, and Equalization~Techniques", "comment": "PhD thesis (accepted, Karlsruhe Institute of Technology)", "summary": "Machine learning with artificial neural networks (ANNs), provides solutions for the growing complexity of modern communication systems. This complexity, however, increases power consumption, making the systems energy-intensive. Spiking neural networks (SNNs) represent a novel generation of neural networks inspired by the highly efficient human brain. By emulating its event-driven and energy-efficient mechanisms, SNNs enable low-power, real-time signal processing. They differ from ANNs in two key ways: they exhibit inherent temporal dynamics and process and transmit information as short binary signals called spikes. Despite their promise, major challenges remain, e.g., identifying optimal learning rules and effective neural encoding. This thesis investigates the design of SNN-based receivers for nonlinear time-invariant frequency-selective channels. Backpropagation through time with surrogate gradients is identified as a promising update rule and the novel quantization encoding (QE) as promising neural encoding. Given the model of the intensity modulation with direct detection link, we compare two different receiver architectures based on equalization performance and spike count. Using decision feedback and QE achieves both strong performance and low spike counts. Notably, SNN-based receivers significantly outperform ANN-based counterparts. We furthermore introduce policy gradient-based update (PGU), an reinforcement learning-based update algorithm that requires no backpropagation. Using PGU, encoding parameters are optimized, drastically reducing runtime, complexity, and spikes per inference while maintaining performance. This thesis contributes a successful design and optimization framework for SNN-based receivers. By addressing key challenges in SNN optimization, it facilitates future advances in the design and deployment of energy-efficient SNN receivers.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u57fa\u4e8e\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\uff08SNN\uff09\u7684\u63a5\u6536\u673a\u8bbe\u8ba1\uff0c\u7528\u4e8e\u975e\u7ebf\u6027\u65f6\u4e0d\u53d8\u9891\u7387\u9009\u62e9\u6027\u4fe1\u9053\uff0c\u63d0\u51fa\u4e86\u91cf\u5316\u7f16\u7801\u548c\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u66f4\u65b0\u7b97\u6cd5\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u80fd\u8017\u548c\u590d\u6742\u5ea6\uff0c\u6027\u80fd\u4f18\u4e8e\u4f20\u7edf\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\u63a5\u6536\u673a\u3002", "motivation": "\u73b0\u4ee3\u901a\u4fe1\u7cfb\u7edf\u590d\u6742\u5ea6\u4e0d\u65ad\u589e\u52a0\u5bfc\u81f4\u529f\u8017\u4e0a\u5347\uff0c\u800c\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\uff08SNN\uff09\u53d7\u9ad8\u6548\u4eba\u8111\u542f\u53d1\uff0c\u5177\u6709\u4e8b\u4ef6\u9a71\u52a8\u548c\u4f4e\u529f\u8017\u7279\u6027\uff0c\u6709\u671b\u89e3\u51b3\u901a\u4fe1\u7cfb\u7edf\u7684\u80fd\u8017\u95ee\u9898\u3002\u7136\u800cSNN\u9762\u4e34\u5b66\u4e60\u89c4\u5219\u548c\u795e\u7ecf\u7f16\u7801\u7b49\u5173\u952e\u6311\u6218\u3002", "method": "1. \u4f7f\u7528\u5e26\u4ee3\u7406\u68af\u5ea6\u7684\u901a\u8fc7\u65f6\u95f4\u53cd\u5411\u4f20\u64ad\u4f5c\u4e3a\u66f4\u65b0\u89c4\u5219\uff1b2. \u63d0\u51fa\u65b0\u9896\u7684\u91cf\u5316\u7f16\u7801\uff08QE\uff09\u4f5c\u4e3a\u795e\u7ecf\u7f16\u7801\u65b9\u6cd5\uff1b3. \u6bd4\u8f83\u4e24\u79cd\u57fa\u4e8e\u5f3a\u5ea6\u8c03\u5236\u76f4\u63a5\u68c0\u6d4b\u94fe\u8def\u7684\u63a5\u6536\u673a\u67b6\u6784\uff1b4. \u5f15\u5165\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u7b56\u7565\u68af\u5ea6\u66f4\u65b0\u7b97\u6cd5\uff08PGU\uff09\u4f18\u5316\u7f16\u7801\u53c2\u6570\u3002", "result": "1. \u4f7f\u7528\u51b3\u7b56\u53cd\u9988\u548c\u91cf\u5316\u7f16\u7801\u7684SNN\u63a5\u6536\u673a\u5728\u5747\u8861\u6027\u80fd\u548c\u8109\u51b2\u8ba1\u6570\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff1b2. SNN\u63a5\u6536\u673a\u663e\u8457\u4f18\u4e8eANN\u63a5\u6536\u673a\uff1b3. PGU\u7b97\u6cd5\u5927\u5e45\u51cf\u5c11\u8fd0\u884c\u65f6\u95f4\u3001\u590d\u6742\u5ea6\u548c\u6bcf\u6b21\u63a8\u7406\u7684\u8109\u51b2\u6570\uff0c\u540c\u65f6\u4fdd\u6301\u6027\u80fd\u3002", "conclusion": "\u8be5\u8bba\u6587\u6210\u529f\u5f00\u53d1\u4e86SNN\u63a5\u6536\u673a\u7684\u8bbe\u8ba1\u548c\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u51b3SNN\u4f18\u5316\u7684\u5173\u952e\u6311\u6218\uff0c\u4e3a\u672a\u6765\u8bbe\u8ba1\u548c\u90e8\u7f72\u9ad8\u80fd\u6548SNN\u63a5\u6536\u673a\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u63a8\u52a8\u4e86\u4f4e\u529f\u8017\u5b9e\u65f6\u4fe1\u53f7\u5904\u7406\u7684\u53d1\u5c55\u3002"}}
{"id": "2601.16675", "categories": ["cs.SD", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.16675", "abs": "https://arxiv.org/abs/2601.16675", "authors": ["David A. Kelly", "Hana Chockler"], "title": "I Guess That's Why They Call it the Blues: Causal Analysis for Audio Classifiers", "comment": null, "summary": "It is well-known that audio classifiers often rely on non-musically relevant features and spurious correlations to classify audio. Hence audio classifiers are easy to manipulate or confuse, resulting in wrong classifications. While inducing a misclassification is not hard, until now the set of features that the classifiers rely on was not well understood.\n  In this paper we introduce a new method that uses causal reasoning to discover features of the frequency space that are sufficient and necessary for a given classification. We describe an implementation of this algorithm in the tool FreqReX and provide experimental results on a number of standard benchmark datasets. Our experiments show that causally sufficient and necessary subsets allow us to manipulate the outputs of the models in a variety of ways by changing the input very slightly. Namely, a change to one out of 240,000 frequencies results in a change in classification 58% of the time, and the change can be so small that it is practically inaudible. These results show that causal analysis is useful for understanding the reasoning process of audio classifiers and can be used to successfully manipulate their outputs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faFreqReX\u5de5\u5177\uff0c\u4f7f\u7528\u56e0\u679c\u63a8\u7406\u53d1\u73b0\u97f3\u9891\u5206\u7c7b\u5668\u4f9d\u8d56\u7684\u9891\u7387\u7279\u5f81\uff0c\u901a\u8fc7\u5fae\u5c0f\u4fee\u6539\uff081/240000\u9891\u7387\uff09\u5373\u53ef\u6539\u53d8\u5206\u7c7b\u7ed3\u679c\uff0858%\u6210\u529f\u7387\uff09\uff0c\u63ed\u793a\u97f3\u9891\u5206\u7c7b\u5668\u6613\u53d7\u5fae\u5c0f\u9891\u7387\u6270\u52a8\u5f71\u54cd\u3002", "motivation": "\u97f3\u9891\u5206\u7c7b\u5668\u901a\u5e38\u4f9d\u8d56\u975e\u97f3\u4e50\u76f8\u5173\u7279\u5f81\u548c\u865a\u5047\u76f8\u5173\u6027\u8fdb\u884c\u5206\u7c7b\uff0c\u5bb9\u6613\u88ab\u64cd\u7eb5\u6216\u6df7\u6dc6\u5bfc\u81f4\u9519\u8bef\u5206\u7c7b\u3002\u867d\u7136\u8bf1\u5bfc\u8bef\u5206\u7c7b\u4e0d\u96be\uff0c\u4f46\u5206\u7c7b\u5668\u4f9d\u8d56\u7684\u7279\u5f81\u96c6\u4e00\u76f4\u672a\u88ab\u5145\u5206\u7406\u89e3\u3002", "method": "\u63d0\u51fa\u65b0\u65b9\u6cd5\u4f7f\u7528\u56e0\u679c\u63a8\u7406\u53d1\u73b0\u9891\u7387\u7a7a\u95f4\u4e2d\u5bf9\u4e8e\u7ed9\u5b9a\u5206\u7c7b\u65e2\u5145\u5206\u53c8\u5fc5\u8981\u7684\u7279\u5f81\u5b50\u96c6\uff0c\u5728FreqReX\u5de5\u5177\u4e2d\u5b9e\u73b0\u8be5\u7b97\u6cd5\uff0c\u5e76\u5728\u591a\u4e2a\u6807\u51c6\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u56e0\u679c\u5145\u5206\u5fc5\u8981\u5b50\u96c6\u5141\u8bb8\u901a\u8fc7\u5fae\u5c0f\u6539\u53d8\u8f93\u5165\u6765\u64cd\u7eb5\u6a21\u578b\u8f93\u51fa\uff1a\u6539\u53d8240,000\u4e2a\u9891\u7387\u4e2d\u76841\u4e2a\u5373\u53ef\u572858%\u60c5\u51b5\u4e0b\u6539\u53d8\u5206\u7c7b\uff0c\u4e14\u6539\u53d8\u5c0f\u5230\u51e0\u4e4e\u542c\u4e0d\u89c1\u3002\u8bc1\u660e\u56e0\u679c\u5206\u6790\u6709\u52a9\u4e8e\u7406\u89e3\u97f3\u9891\u5206\u7c7b\u5668\u7684\u63a8\u7406\u8fc7\u7a0b\u3002", "conclusion": "\u56e0\u679c\u5206\u6790\u5bf9\u4e8e\u7406\u89e3\u97f3\u9891\u5206\u7c7b\u5668\u7684\u63a8\u7406\u8fc7\u7a0b\u5f88\u6709\u7528\uff0c\u5e76\u80fd\u6210\u529f\u64cd\u7eb5\u5176\u8f93\u51fa\uff0c\u63ed\u793a\u4e86\u97f3\u9891\u5206\u7c7b\u5668\u5bf9\u5fae\u5c0f\u9891\u7387\u53d8\u5316\u7684\u8106\u5f31\u6027\u3002"}}
{"id": "2601.16577", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.16577", "abs": "https://arxiv.org/abs/2601.16577", "authors": ["Ga\u00ebl Pages", "Priot Beno\u00eet", "Guillaume Beaugendre"], "title": "Real-Time Evaluation of an Ultra-Tight GNSS/INS Integration Based on Adaptive PLL Bandwidth", "comment": null, "summary": "In this contribution, we propose a GNSS/INS ultra-tight coupling in which the GNSS receiver architecture is based on a vector tracking loop type architecture. In the proposed approach, the phase lock loop bandwidth is adapted according to the inertial navigation system information. The latter has the advantage to be easily implementable on a System-on-Chip component such as an FPGA (Field-Programmable Gate Arrays), and can be implemented with minor modifications on an existing GNSS receiver platform. Moreover, compared to classical vector-based solutions, the proposed architecture decodes the navigation message in the loop, without the need to run scalar loops in parallel or having to store pre-downloaded ephemeris data. This architecture therefore does not increase the area occupied on the FPGA and does not use additional resources for storage. The proposed GNSS receiver architecture uses GPS L1/C and Galileo E1 signals and is composed of one acquisition module and 16 tracking channels (8 GPS and 8 Galileo) which are implemented within a FPGA (Zynq-Ultrascale).", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5411\u91cf\u8ddf\u8e2a\u73af\u67b6\u6784\u7684GNSS/INS\u8d85\u7d27\u8026\u5408\u7cfb\u7edf\uff0c\u901a\u8fc7INS\u4fe1\u606f\u81ea\u9002\u5e94\u8c03\u6574PLL\u5e26\u5bbd\uff0c\u53ef\u5728FPGA\u4e0a\u5b9e\u73b0\u4e14\u5360\u7528\u8d44\u6e90\u5c11", "motivation": "\u4f20\u7edf\u5411\u91cf\u8ddf\u8e2a\u89e3\u51b3\u65b9\u6848\u9700\u8981\u5e76\u884c\u8fd0\u884c\u6807\u91cf\u73af\u6216\u5b58\u50a8\u9884\u4e0b\u8f7d\u661f\u5386\u6570\u636e\uff0c\u589e\u52a0\u4e86FPGA\u9762\u79ef\u548c\u5b58\u50a8\u8d44\u6e90\u6d88\u8017\u3002\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u3001\u8d44\u6e90\u5360\u7528\u66f4\u5c11\u7684GNSS/INS\u8d85\u7d27\u8026\u5408\u67b6\u6784", "method": "\u91c7\u7528\u5411\u91cf\u8ddf\u8e2a\u73af\u67b6\u6784\u7684GNSS\u63a5\u6536\u673a\uff0c\u6839\u636e\u60ef\u6027\u5bfc\u822a\u7cfb\u7edf\u4fe1\u606f\u81ea\u9002\u5e94\u8c03\u6574\u76f8\u4f4d\u9501\u5b9a\u73af\u5e26\u5bbd\u3002\u5728\u73af\u5185\u89e3\u7801\u5bfc\u822a\u6d88\u606f\uff0c\u65e0\u9700\u5e76\u884c\u6807\u91cf\u73af\u6216\u9884\u5b58\u661f\u5386\u6570\u636e\u3002\u4f7f\u7528GPS L1/C\u548cGalileo E1\u4fe1\u53f7\uff0c\u5305\u542b1\u4e2a\u6355\u83b7\u6a21\u5757\u548c16\u4e2a\u8ddf\u8e2a\u901a\u9053\uff088\u4e2aGPS\u548c8\u4e2aGalileo\uff09\uff0c\u5728Zynq-Ultrascale FPGA\u4e0a\u5b9e\u73b0", "result": "\u63d0\u51fa\u7684\u67b6\u6784\u6613\u4e8e\u5728FPGA\u7b49\u7247\u4e0a\u7cfb\u7edf\u7ec4\u4ef6\u4e0a\u5b9e\u73b0\uff0c\u5bf9\u73b0\u6709GNSS\u63a5\u6536\u673a\u5e73\u53f0\u53ea\u9700\u5c11\u91cf\u4fee\u6539\u3002\u76f8\u6bd4\u4f20\u7edf\u5411\u91cf\u89e3\u51b3\u65b9\u6848\uff0c\u4e0d\u589e\u52a0FPGA\u5360\u7528\u9762\u79ef\uff0c\u4e0d\u4f7f\u7528\u989d\u5916\u5b58\u50a8\u8d44\u6e90", "conclusion": "\u8be5GNSS/INS\u8d85\u7d27\u8026\u5408\u67b6\u6784\u63d0\u4f9b\u4e86\u4e00\u79cd\u8d44\u6e90\u9ad8\u6548\u3001\u6613\u4e8e\u5b9e\u73b0\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u9002\u5408\u5728FPGA\u5e73\u53f0\u4e0a\u90e8\u7f72\uff0c\u5177\u6709\u826f\u597d\u7684\u5b9e\u7528\u6027\u548c\u53ef\u6269\u5c55\u6027"}}
{"id": "2601.16774", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.16774", "abs": "https://arxiv.org/abs/2601.16774", "authors": ["Yiheng Jiang", "Biao Tian", "Haoxu Wang", "Shengkui Zhao", "Bin Ma", "Daren Chen", "Xiangang Li"], "title": "E2E-AEC: Implementing an end-to-end neural network learning approach for acoustic echo cancellation", "comment": "This paper has been accepted by ICASSP2026", "summary": "We propose a novel neural network-based end-to-end acoustic echo cancellation (E2E-AEC) method capable of streaming inference, which operates effectively without reliance on traditional linear AEC (LAEC) techniques and time delay estimation. Our approach includes several key strategies: First, we introduce and refine progressive learning to gradually enhance echo suppression. Second, our model employs knowledge transfer by initializing with a pre-trained LAECbased model, harnessing the insights gained from LAEC training. Third, we optimize the attention mechanism with a loss function applied on attention weights to achieve precise time alignment between the reference and microphone signals. Lastly, we incorporate voice activity detection to enhance speech quality and improve echo removal by masking the network output when near-end speech is absent. The effectiveness of our approach is validated through experiments conducted on public datasets.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684\u7aef\u5230\u7aef\u58f0\u5b66\u56de\u58f0\u6d88\u9664\u65b9\u6cd5\uff0c\u652f\u6301\u6d41\u5f0f\u63a8\u7406\uff0c\u65e0\u9700\u4f9d\u8d56\u4f20\u7edf\u7ebf\u6027AEC\u6280\u672f\u548c\u65f6\u5ef6\u4f30\u8ba1", "motivation": "\u4f20\u7edf\u7ebf\u6027AEC\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u65f6\u5ef6\u4f30\u8ba1\u4e14\u6027\u80fd\u6709\u9650\u3002\u672c\u6587\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u7aef\u5230\u7aef\u7684\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\uff0c\u80fd\u591f\u5b9e\u73b0\u6d41\u5f0f\u63a8\u7406\uff0c\u540c\u65f6\u6446\u8131\u5bf9\u4f20\u7edf\u7ebf\u6027AEC\u6280\u672f\u7684\u4f9d\u8d56", "method": "\u91c7\u7528\u56db\u79cd\u5173\u952e\u6280\u672f\uff1a1\uff09\u6e10\u8fdb\u5f0f\u5b66\u4e60\u9010\u6b65\u589e\u5f3a\u56de\u58f0\u6291\u5236\uff1b2\uff09\u901a\u8fc7\u9884\u8bad\u7ec3\u7684\u7ebf\u6027AEC\u6a21\u578b\u8fdb\u884c\u77e5\u8bc6\u8fc1\u79fb\uff1b3\uff09\u4f18\u5316\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5728\u6ce8\u610f\u529b\u6743\u91cd\u4e0a\u5e94\u7528\u635f\u5931\u51fd\u6570\u4ee5\u5b9e\u73b0\u7cbe\u786e\u65f6\u95f4\u5bf9\u9f50\uff1b4\uff09\u7ed3\u5408\u8bed\u97f3\u6d3b\u52a8\u68c0\u6d4b\uff0c\u5728\u8fd1\u7aef\u8bed\u97f3\u7f3a\u5931\u65f6\u63a9\u7801\u7f51\u7edc\u8f93\u51fa", "result": "\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u80fd\u591f\u6709\u6548\u6d88\u9664\u56de\u58f0\u5e76\u63d0\u5347\u8bed\u97f3\u8d28\u91cf", "conclusion": "\u63d0\u51fa\u7684\u7aef\u5230\u7aef\u58f0\u5b66\u56de\u58f0\u6d88\u9664\u65b9\u6cd5\u6210\u529f\u5b9e\u73b0\u4e86\u6d41\u5f0f\u63a8\u7406\uff0c\u65e0\u9700\u4f20\u7edf\u7ebf\u6027AEC\u6280\u672f\u548c\u65f6\u5ef6\u4f30\u8ba1\uff0c\u901a\u8fc7\u6e10\u8fdb\u5b66\u4e60\u3001\u77e5\u8bc6\u8fc1\u79fb\u3001\u6ce8\u610f\u529b\u4f18\u5316\u548c\u8bed\u97f3\u6d3b\u52a8\u68c0\u6d4b\u7b49\u7b56\u7565\u53d6\u5f97\u4e86\u826f\u597d\u6548\u679c"}}
{"id": "2601.16586", "categories": ["eess.SP", "cs.IT", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.16586", "abs": "https://arxiv.org/abs/2601.16586", "authors": ["Benedikt Fesl", "Fatih Capar"], "title": "Learning Successive Interference Cancellation for Low-Complexity Soft-Output MIMO Detection", "comment": null, "summary": "Low-complexity multiple-input multiple-output (MIMO) detection remains a key challenge in modern wireless systems, particularly for 5G reduced capability (RedCap) and internet-of-things (IoT) devices. In this context, the growing interest in deploying machine learning on edge devices must be balanced against stringent constraints on computational complexity and memory while supporting high-order modulation. Beyond accurate hard detection, reliable soft information is equally critical, as modern receivers rely on soft-input channel decoding, imposing additional requirements on the detector design. In this work, we propose recurSIC, a lightweight learning-based MIMO detection framework that is structurally inspired by successive interference cancellation (SIC) and incorporates learned processing stages. It generates reliable soft information via multi-path hypothesis tracking with a tunable complexity parameter while requiring only a single forward pass and a minimal parameter count. Numerical results in realistic wireless scenarios show that recurSIC achieves strong hard- and soft-detection performance at very low complexity, making it well suited for edge-constrained MIMO receivers.", "AI": {"tldr": "\u63d0\u51farecurSIC\u2014\u2014\u4e00\u79cd\u8f7b\u91cf\u7ea7\u5b66\u4e60\u578bMIMO\u68c0\u6d4b\u6846\u67b6\uff0c\u7ed3\u5408SIC\u7ed3\u6784\u548c\u5b66\u4e60\u5904\u7406\uff0c\u5728\u4f4e\u590d\u6742\u5ea6\u4e0b\u5b9e\u73b0\u53ef\u9760\u7684\u786c\u68c0\u6d4b\u548c\u8f6f\u4fe1\u606f\u751f\u6210\uff0c\u9002\u7528\u4e8e\u8fb9\u7f18\u8bbe\u5907\u3002", "motivation": "5G RedCap\u548cIoT\u8bbe\u5907\u9700\u8981\u4f4e\u590d\u6742\u5ea6MIMO\u68c0\u6d4b\uff0c\u540c\u65f6\u8981\u652f\u6301\u9ad8\u9636\u8c03\u5236\u5e76\u751f\u6210\u53ef\u9760\u7684\u8f6f\u4fe1\u606f\u7528\u4e8e\u4fe1\u9053\u89e3\u7801\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5e73\u8861\u8fd9\u4e9b\u9700\u6c42\u3002", "method": "\u57fa\u4e8eSIC\u7ed3\u6784\u8bbe\u8ba1\u8f7b\u91cf\u7ea7\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u8def\u5f84\u5047\u8bbe\u8ddf\u8e2a\u751f\u6210\u8f6f\u4fe1\u606f\uff0c\u4ec5\u9700\u5355\u6b21\u524d\u5411\u4f20\u64ad\u548c\u6781\u5c11\u53c2\u6570\uff0c\u590d\u6742\u5ea6\u53ef\u8c03\u3002", "result": "\u5728\u771f\u5b9e\u65e0\u7ebf\u573a\u666f\u4e2d\uff0crecurSIC\u5728\u6781\u4f4e\u590d\u6742\u5ea6\u4e0b\u5b9e\u73b0\u4e86\u4f18\u5f02\u7684\u786c\u68c0\u6d4b\u548c\u8f6f\u68c0\u6d4b\u6027\u80fd\u3002", "conclusion": "recurSIC\u662f\u9002\u5408\u8fb9\u7f18\u53d7\u9650MIMO\u63a5\u6536\u5668\u7684\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5e73\u8861\u4e86\u6027\u80fd\u3001\u590d\u6742\u5ea6\u548c\u5185\u5b58\u9700\u6c42\u3002"}}
{"id": "2601.16793", "categories": ["cs.SD", "cs.NE", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.16793", "abs": "https://arxiv.org/abs/2601.16793", "authors": ["Rafiul Islam", "Md. Taimur Ahad"], "title": "A Novel Transfer Learning Approach for Mental Stability Classification from Voice Signal", "comment": null, "summary": "This study presents a novel transfer learning approach and data augmentation technique for mental stability classification using human voice signals and addresses the challenges associated with limited data availability. Convolutional neural networks (CNNs) have been employed to analyse spectrogram images generated from voice recordings. Three CNN architectures, VGG16, InceptionV3, and DenseNet121, were evaluated across three experimental phases: training on non-augmented data, augmented data, and transfer learning. This proposed transfer learning approach involves pre-training models on the augmented dataset and fine-tuning them on the non-augmented dataset while ensuring strict data separation to prevent data leakage. The results demonstrate significant improvements in classification performance compared to the baseline approach. Among three CNN architectures, DenseNet121 achieved the highest accuracy of 94% and an AUC score of 99% using the proposed transfer learning approach. This finding highlights the effectiveness of combining data augmentation and transfer learning to enhance CNN-based classification of mental stability using voice spectrograms, offering a promising non-invasive tool for mental health diagnostics.", "AI": {"tldr": "\u63d0\u51fa\u7ed3\u5408\u6570\u636e\u589e\u5f3a\u548c\u8fc1\u79fb\u5b66\u4e60\u7684\u65b0\u65b9\u6cd5\uff0c\u4f7f\u7528\u8bed\u97f3\u4fe1\u53f7\u548cCNN\u8fdb\u884c\u5fc3\u7406\u7a33\u5b9a\u6027\u5206\u7c7b\uff0cDenseNet121\u8fbe\u523094%\u51c6\u786e\u7387\u548c99%AUC", "motivation": "\u89e3\u51b3\u5fc3\u7406\u7a33\u5b9a\u6027\u5206\u7c7b\u4e2d\u8bed\u97f3\u4fe1\u53f7\u6570\u636e\u6709\u9650\u7684\u95ee\u9898\uff0c\u5f00\u53d1\u975e\u4fb5\u5165\u6027\u7684\u5fc3\u7406\u5065\u5eb7\u8bca\u65ad\u5de5\u5177", "method": "\u4f7f\u7528VGG16\u3001InceptionV3\u548cDenseNet121\u4e09\u79cdCNN\u67b6\u6784\u5206\u6790\u8bed\u97f3\u9891\u8c31\u56fe\uff0c\u91c7\u7528\u6570\u636e\u589e\u5f3a\u548c\u8fc1\u79fb\u5b66\u4e60\u7b56\u7565\uff1a\u5148\u5728\u589e\u5f3a\u6570\u636e\u4e0a\u9884\u8bad\u7ec3\uff0c\u518d\u5728\u539f\u59cb\u6570\u636e\u4e0a\u5fae\u8c03", "result": "DenseNet121\u5728\u8fc1\u79fb\u5b66\u4e60\u65b9\u6cd5\u4e0b\u83b7\u5f97\u6700\u9ad8\u6027\u80fd\uff1a94%\u51c6\u786e\u7387\u548c99%AUC\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5", "conclusion": "\u6570\u636e\u589e\u5f3a\u4e0e\u8fc1\u79fb\u5b66\u4e60\u7ed3\u5408\u80fd\u6709\u6548\u63d0\u5347\u57fa\u4e8eCNN\u7684\u5fc3\u7406\u7a33\u5b9a\u6027\u5206\u7c7b\u6027\u80fd\uff0c\u4e3a\u5fc3\u7406\u5065\u5eb7\u8bca\u65ad\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u975e\u4fb5\u5165\u6027\u5de5\u5177"}}
{"id": "2601.16606", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.16606", "abs": "https://arxiv.org/abs/2601.16606", "authors": ["Antonio Bracale", "Pasquale De Falco", "Piotr Kuwa\u0142ek", "Grzegorz Wiczy\u0144ski"], "title": "Assessment of Errors of Fundamental Frequency Estimation Methods in the Presence of Voltage Fluctuations and Distortions", "comment": "5 pages, 7 figures, submitted to IEEE conferences", "summary": "The fundamental frequency is one of the parameters that define power quality. Correctly determining this parameter under the conditions that prevail in modern power grids is crucial. Diagnostic purposes often require an efficient estimation of this parameter within short time windows. Therefore, this article presents the results of numerical simulation studies that allow the assessment of errors in various fundamental frequency estimation methods, including the standard IEC 61000-4-30 method, when the analyzed signal has a form similar to that found in modern power grids. For the purposes of this study, a test signal was adopted recreating the states of the power grid, including the simultaneous occurrence of voltage fluctuations and distortions. Conclusions are presented based on conducted research.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u6570\u503c\u6a21\u62df\u7814\u7a76\u8bc4\u4f30\u4e86\u73b0\u4ee3\u7535\u7f51\u6761\u4ef6\u4e0b\u5404\u79cd\u57fa\u9891\u4f30\u8ba1\u65b9\u6cd5\u7684\u8bef\u5dee\uff0c\u5305\u62ecIEC 61000-4-30\u6807\u51c6\u65b9\u6cd5\uff0c\u7279\u522b\u5173\u6ce8\u7535\u538b\u6ce2\u52a8\u548c\u7578\u53d8\u540c\u65f6\u5b58\u5728\u7684\u60c5\u51b5\u3002", "motivation": "\u57fa\u9891\u662f\u5b9a\u4e49\u7535\u80fd\u8d28\u91cf\u7684\u5173\u952e\u53c2\u6570\u4e4b\u4e00\u3002\u5728\u73b0\u4ee3\u7535\u7f51\u6761\u4ef6\u4e0b\u6b63\u786e\u786e\u5b9a\u8be5\u53c2\u6570\u81f3\u5173\u91cd\u8981\uff0c\u7279\u522b\u662f\u8bca\u65ad\u76ee\u7684\u9700\u8981\u5728\u77ed\u65f6\u95f4\u5185\u7a97\u53e3\u5185\u9ad8\u6548\u4f30\u8ba1\u8be5\u53c2\u6570\u3002", "method": "\u91c7\u7528\u6570\u503c\u6a21\u62df\u7814\u7a76\u65b9\u6cd5\uff0c\u4f7f\u7528\u6a21\u62df\u73b0\u4ee3\u7535\u7f51\u72b6\u6001\u7684\u6d4b\u8bd5\u4fe1\u53f7\uff08\u5305\u62ec\u7535\u538b\u6ce2\u52a8\u548c\u7578\u53d8\u540c\u65f6\u53d1\u751f\u7684\u60c5\u51b5\uff09\uff0c\u8bc4\u4f30\u5404\u79cd\u57fa\u9891\u4f30\u8ba1\u65b9\u6cd5\u7684\u8bef\u5dee\uff0c\u5305\u62ec\u6807\u51c6IEC 61000-4-30\u65b9\u6cd5\u3002", "result": "\u7814\u7a76\u63d0\u4f9b\u4e86\u5404\u79cd\u57fa\u9891\u4f30\u8ba1\u65b9\u6cd5\u5728\u6a21\u62df\u73b0\u4ee3\u7535\u7f51\u6761\u4ef6\u4e0b\u7684\u8bef\u5dee\u8bc4\u4f30\u7ed3\u679c\uff0c\u5305\u62ecIEC 61000-4-30\u6807\u51c6\u65b9\u6cd5\u7684\u8868\u73b0\u3002", "conclusion": "\u57fa\u4e8e\u7814\u7a76\u7ed3\u679c\u5f97\u51fa\u7ed3\u8bba\uff0c\u4e3a\u73b0\u4ee3\u7535\u7f51\u6761\u4ef6\u4e0b\u57fa\u9891\u4f30\u8ba1\u65b9\u6cd5\u7684\u9009\u62e9\u548c\u5e94\u7528\u63d0\u4f9b\u6307\u5bfc\u3002"}}
{"id": "2601.16662", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.16662", "abs": "https://arxiv.org/abs/2601.16662", "authors": ["Sahar Golipoor", "Lingyun Yao", "Martin Andraud", "Stephan Sigg"], "title": "Low-Power On-Device Gesture Recognition with Einsum Networks", "comment": null, "summary": "We design a gesture-recognition pipeline for networks of distributed, resource constrained devices utilising Einsum Networks. Einsum Networks are probabilistic circuits that feature a tractable inference, explainability, and energy efficiency. The system is validated in a scenario of low-power, body-worn, passive Radio Frequency Identification-based gesture recognition. Each constrained device includes task-specific processing units responsible for Received Signal Strength (RSS) and phase processing or Angle of Arrival (AoA) estimation, along with feature extraction, as well as dedicated Einsum hardware that processes the extracted features. The output of all constrained devices is then fused in a decision aggregation module to predict gestures. Experimental results demonstrate that the method outperforms the benchmark models.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eEinsum\u7f51\u7edc\u7684\u5206\u5e03\u5f0f\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u624b\u52bf\u8bc6\u522b\u7cfb\u7edf\uff0c\u5728\u4f4e\u529f\u8017RFID\u624b\u52bf\u8bc6\u522b\u573a\u666f\u4e2d\u4f18\u4e8e\u57fa\u51c6\u6a21\u578b", "motivation": "\u4e3a\u5206\u5e03\u5f0f\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u7f51\u7edc\u8bbe\u8ba1\u9ad8\u6548\u7684\u624b\u52bf\u8bc6\u522b\u7cfb\u7edf\uff0c\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u5728\u8ba1\u7b97\u8d44\u6e90\u3001\u80fd\u8017\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u7684\u9650\u5236", "method": "\u91c7\u7528Einsum\u7f51\u7edc\u4f5c\u4e3a\u6982\u7387\u7535\u8def\uff0c\u7ed3\u5408RSS/\u76f8\u4f4d\u5904\u7406\u548cAoA\u4f30\u8ba1\u7684\u7279\u5f81\u63d0\u53d6\uff0c\u901a\u8fc7\u4e13\u7528\u786c\u4ef6\u5904\u7406\u7279\u5f81\uff0c\u6700\u540e\u5728\u51b3\u7b56\u805a\u5408\u6a21\u5757\u878d\u5408\u6240\u6709\u8bbe\u5907\u8f93\u51fa\u8fdb\u884c\u624b\u52bf\u9884\u6d4b", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u4f4e\u529f\u8017\u3001\u88ab\u52a8RFID\u624b\u52bf\u8bc6\u522b\u573a\u666f\u4e2d\u4f18\u4e8e\u57fa\u51c6\u6a21\u578b", "conclusion": "\u57fa\u4e8eEinsum\u7f51\u7edc\u7684\u5206\u5e03\u5f0f\u624b\u52bf\u8bc6\u522b\u7cfb\u7edf\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u53ef\u89e3\u91ca\u7684\u624b\u52bf\u8bc6\u522b\uff0c\u4e3a\u8fb9\u7f18\u8ba1\u7b97\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848"}}
{"id": "2601.16664", "categories": ["eess.SP", "eess.IV"], "pdf": "https://arxiv.org/pdf/2601.16664", "abs": "https://arxiv.org/abs/2601.16664", "authors": ["Michael Negosanti", "Lorenzo Pucci", "Andrea Giorgetti"], "title": "OFDM-Based ISAC Imaging of Extended Targets via Inverse Virtual Aperture Processing", "comment": "6 pages; This paper was presented at the IEEE JC&S Symposium 2026", "summary": "This work investigates the performance of an integrated sensing and communication (ISAC) system exploiting inverse virtual aperture (IVA) for imaging moving extended targets in vehicular scenarios. A base station (BS) operates as a monostatic sensor using MIMO-OFDM waveforms. Echoes reflected by the target are processed through motion-compensation techniques to form an IVA range-Doppler (cross-range) image. A case study considers a 5G NR waveform in the upper mid-band, with the target model defined in 3GPP Release 19, representing a vehicle as a set of spatially distributed scatterers. Performance is evaluated in terms of image contrast (IC) and the root mean squared error (RMSE) of the estimated target-centroid range. Finally, the trade-off between sensing accuracy and communication efficiency is examined by varying the subcarrier allocation for IVA imaging. The results provide insights for designing effective sensing strategies in next-generation radio networks.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u5229\u7528\u9006\u865a\u62df\u5b54\u5f84(IVA)\u7684\u96c6\u6210\u611f\u77e5\u4e0e\u901a\u4fe1\u7cfb\u7edf\u5728\u8f66\u8f7d\u573a\u666f\u4e2d\u5bf9\u79fb\u52a8\u6269\u5c55\u76ee\u6807\u8fdb\u884c\u6210\u50cf\u7684\u6027\u80fd\uff0c\u5206\u6790\u4e86\u611f\u77e5\u7cbe\u5ea6\u4e0e\u901a\u4fe1\u6548\u7387\u4e4b\u95f4\u7684\u6743\u8861\u3002", "motivation": "\u968f\u7740\u4e0b\u4e00\u4ee3\u65e0\u7ebf\u7f51\u7edc\u7684\u53d1\u5c55\uff0c\u96c6\u6210\u611f\u77e5\u4e0e\u901a\u4fe1(ISAC)\u7cfb\u7edf\u5728\u8f66\u8f7d\u573a\u666f\u4e2d\u53d8\u5f97\u8d8a\u6765\u8d8a\u91cd\u8981\u3002\u9700\u8981\u7814\u7a76\u5982\u4f55\u6709\u6548\u5229\u7528\u901a\u4fe1\u6ce2\u5f62\u540c\u65f6\u5b9e\u73b0\u611f\u77e5\u529f\u80fd\uff0c\u7279\u522b\u662f\u5bf9\u79fb\u52a8\u6269\u5c55\u76ee\u6807\uff08\u5982\u8f66\u8f86\uff09\u7684\u6210\u50cf\u80fd\u529b\u3002", "method": "\u91c7\u7528MIMO-OFDM\u6ce2\u5f62\uff0c\u57fa\u7ad9\u4f5c\u4e3a\u5355\u57fa\u5730\u4f20\u611f\u5668\u3002\u901a\u8fc7\u8fd0\u52a8\u8865\u507f\u6280\u672f\u5904\u7406\u76ee\u6807\u53cd\u5c04\u7684\u56de\u6ce2\uff0c\u5f62\u6210IVA\u8ddd\u79bb-\u591a\u666e\u52d2\uff08\u8de8\u8ddd\u79bb\uff09\u56fe\u50cf\u3002\u4f7f\u75285G NR\u6ce2\u5f62\uff08\u4e2d\u9ad8\u9891\u6bb5\uff09\uff0c\u76ee\u6807\u6a21\u578b\u57fa\u4e8e3GPP Release 19\u5b9a\u4e49\uff0c\u5c06\u8f66\u8f86\u5efa\u6a21\u4e3a\u4e00\u7ec4\u7a7a\u95f4\u5206\u5e03\u7684\u6563\u5c04\u4f53\u3002", "result": "\u901a\u8fc7\u56fe\u50cf\u5bf9\u6bd4\u5ea6(IC)\u548c\u76ee\u6807\u8d28\u5fc3\u8ddd\u79bb\u4f30\u8ba1\u7684\u5747\u65b9\u6839\u8bef\u5dee(RMSE)\u8bc4\u4f30\u6027\u80fd\u3002\u901a\u8fc7\u6539\u53d8\u7528\u4e8eIVA\u6210\u50cf\u7684\u5b50\u8f7d\u6ce2\u5206\u914d\uff0c\u7814\u7a76\u4e86\u611f\u77e5\u7cbe\u5ea6\u4e0e\u901a\u4fe1\u6548\u7387\u4e4b\u95f4\u7684\u6743\u8861\u5173\u7cfb\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u4e0b\u4e00\u4ee3\u65e0\u7ebf\u7f51\u7edc\u4e2d\u8bbe\u8ba1\u6709\u6548\u7684\u611f\u77e5\u7b56\u7565\u63d0\u4f9b\u4e86\u89c1\u89e3\uff0c\u5c55\u793a\u4e86\u5229\u7528\u901a\u4fe1\u6ce2\u5f62\u5b9e\u73b0\u9ad8\u8d28\u91cf\u76ee\u6807\u6210\u50cf\u7684\u53ef\u884c\u6027\uff0c\u5e76\u63ed\u793a\u4e86\u611f\u77e5\u7cbe\u5ea6\u4e0e\u901a\u4fe1\u8d44\u6e90\u5206\u914d\u4e4b\u95f4\u7684\u91cd\u8981\u6743\u8861\u3002"}}
{"id": "2601.16727", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.16727", "abs": "https://arxiv.org/abs/2601.16727", "authors": ["Julian Block", "Andreas K\u00f6nsgen", "Jens Dede", "Anna F\u00f6rster"], "title": "Precise Low-Current Measurement Techniques for IoT Devices: A Case Study on MoleNet", "comment": null, "summary": "Power consumption is a crucial aspect of IoT devices which often have to run on a battery for an extended period of time. Therefore, supply current measurements are crucial before deploying a device in the field. Multimeters and oscilloscopes are not well suited when it comes to measuring very small currents which occur e.g. when an IoT device is in sleep mode. In this report, we compare dedicated source measurement units (SMUs) which allow to measure very small currents with high precision. As an application example, we demonstrate current measurements on our MoleNet IoT sensor board.", "AI": {"tldr": "\u6bd4\u8f83\u4e13\u7528\u6e90\u6d4b\u91cf\u5355\u5143(SMU)\u5728\u7269\u8054\u7f51\u8bbe\u5907\u5fae\u5c0f\u7535\u6d41\u6d4b\u91cf\u4e2d\u7684\u6027\u80fd\uff0c\u5e76\u4ee5MoleNet\u4f20\u611f\u5668\u677f\u4e3a\u4f8b\u8fdb\u884c\u5e94\u7528\u6f14\u793a", "motivation": "\u7269\u8054\u7f51\u8bbe\u5907\u901a\u5e38\u9700\u8981\u957f\u65f6\u95f4\u7535\u6c60\u4f9b\u7535\uff0c\u529f\u8017\u81f3\u5173\u91cd\u8981\u3002\u4f20\u7edf\u4e07\u7528\u8868\u548c\u793a\u6ce2\u5668\u96be\u4ee5\u7cbe\u786e\u6d4b\u91cf\u8bbe\u5907\u4f11\u7720\u65f6\u7684\u5fae\u5c0f\u7535\u6d41\uff0c\u9700\u8981\u4e13\u7528\u6d4b\u91cf\u5de5\u5177", "method": "\u6bd4\u8f83\u591a\u79cd\u4e13\u7528\u6e90\u6d4b\u91cf\u5355\u5143(SMU)\uff0c\u8fd9\u4e9b\u8bbe\u5907\u80fd\u591f\u4ee5\u9ad8\u7cbe\u5ea6\u6d4b\u91cf\u5fae\u5c0f\u7535\u6d41\uff0c\u5e76\u4ee5MoleNet\u7269\u8054\u7f51\u4f20\u611f\u5668\u677f\u4f5c\u4e3a\u5e94\u7528\u5b9e\u4f8b\u8fdb\u884c\u7535\u6d41\u6d4b\u91cf\u6f14\u793a", "result": "\u4e13\u7528SMU\u76f8\u6bd4\u4f20\u7edf\u6d4b\u91cf\u5de5\u5177\u66f4\u9002\u5408\u7269\u8054\u7f51\u8bbe\u5907\u7684\u5fae\u5c0f\u7535\u6d41\u6d4b\u91cf\uff0c\u80fd\u591f\u63d0\u4f9b\u66f4\u9ad8\u7cbe\u5ea6\u7684\u529f\u8017\u6570\u636e", "conclusion": "\u5728\u7269\u8054\u7f51\u8bbe\u5907\u90e8\u7f72\u524d\uff0c\u4f7f\u7528\u4e13\u7528\u6e90\u6d4b\u91cf\u5355\u5143\u8fdb\u884c\u7cbe\u786e\u7535\u6d41\u6d4b\u91cf\u5bf9\u4e8e\u8bc4\u4f30\u529f\u8017\u548c\u5ef6\u957f\u7535\u6c60\u5bff\u547d\u81f3\u5173\u91cd\u8981"}}
{"id": "2601.16792", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.16792", "abs": "https://arxiv.org/abs/2601.16792", "authors": ["Yingtong Zhou", "Yiang Zhou", "Zhengxian Qu", "Kang Liu", "Ting Tan"], "title": "A Dynamic Parametric Simulator for Fetal Heart Sounds", "comment": null, "summary": "Research on fetal phonocardiogram (fPCG) is challenged by the limited number of abdominal recordings, substantial maternal interference, and marked transmissioninduced signal attenuation that complicate reproducible benchmarking. We present a reproducible dynamic parametric simulator that generates long abdominal fPCG sequences by combining cycle-level fetal S1/S2 event synthesis with a convolutional transmission module and configurable interference and background noise. Model parameters are calibrated cyclewise from real abdominal recordings to capture beat-to-beat variability and to define data-driven admissible ranges for controllable synthesis. The generated signals are validated against real recordings in terms of envelope-based temporal structure and frequency-domain characteristics. The simulator is released as open software to support rapid, reproducible evaluation of fPCG processing methods under controlled acquisition conditions.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u53ef\u91cd\u590d\u7684\u80ce\u513f\u5fc3\u97f3\u56fe\u52a8\u6001\u53c2\u6570\u6a21\u62df\u5668\uff0c\u901a\u8fc7\u5408\u6210\u80ce\u513fS1/S2\u4e8b\u4ef6\u3001\u5377\u79ef\u4f20\u8f93\u6a21\u5757\u548c\u53ef\u914d\u7f6e\u5e72\u6270\u6765\u751f\u6210\u8179\u90e8fPCG\u4fe1\u53f7\uff0c\u652f\u6301fPCG\u5904\u7406\u65b9\u6cd5\u8bc4\u4f30\u3002", "motivation": "\u80ce\u513f\u5fc3\u97f3\u56fe\u7814\u7a76\u9762\u4e34\u8179\u90e8\u8bb0\u5f55\u6570\u636e\u6709\u9650\u3001\u6bcd\u4f53\u5e72\u6270\u4e25\u91cd\u3001\u4fe1\u53f7\u8870\u51cf\u660e\u663e\u7b49\u95ee\u9898\uff0c\u5bfc\u81f4\u96be\u4ee5\u8fdb\u884c\u53ef\u91cd\u590d\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002", "method": "\u7ed3\u5408\u5468\u671f\u7ea7\u80ce\u513fS1/S2\u4e8b\u4ef6\u5408\u6210\u3001\u5377\u79ef\u4f20\u8f93\u6a21\u5757\u3001\u53ef\u914d\u7f6e\u5e72\u6270\u548c\u80cc\u666f\u566a\u58f0\uff0c\u4ece\u771f\u5b9e\u8179\u90e8\u8bb0\u5f55\u4e2d\u9010\u5468\u671f\u6821\u51c6\u6a21\u578b\u53c2\u6570\uff0c\u6355\u6349\u5fc3\u8df3\u95f4\u53d8\u5f02\u6027\u3002", "result": "\u751f\u6210\u7684\u4fe1\u53f7\u5728\u5305\u7edc\u65f6\u95f4\u7ed3\u6784\u548c\u9891\u57df\u7279\u6027\u65b9\u9762\u4e0e\u771f\u5b9e\u8bb0\u5f55\u9a8c\u8bc1\u4e00\u81f4\uff0c\u6a21\u62df\u5668\u4f5c\u4e3a\u5f00\u6e90\u8f6f\u4ef6\u53d1\u5e03\u3002", "conclusion": "\u8be5\u6a21\u62df\u5668\u652f\u6301\u5728\u53d7\u63a7\u91c7\u96c6\u6761\u4ef6\u4e0b\u5feb\u901f\u3001\u53ef\u91cd\u590d\u5730\u8bc4\u4f30fPCG\u5904\u7406\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u80ce\u513f\u5fc3\u97f3\u56fe\u7814\u7a76\u4e2d\u7684\u57fa\u51c6\u6d4b\u8bd5\u6311\u6218\u3002"}}
{"id": "2601.16847", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.16847", "abs": "https://arxiv.org/abs/2601.16847", "authors": ["Pantea Nadimi Goki", "Luca Pot\u00ec"], "title": "Hierarchical Distribution Matcher Design for Probabilistic Constellation Shaping Based on a Novel Semi-Analytical Optimization Approach", "comment": "The manuscript has been submitted for publication in the Journal of Lightwave Technologies (JLT)", "summary": "A novel design procedure for practical hierarchical distribution matchers (HiDMs) in probabilistically shaped constellation systems is presented. The proposed approach enables the determination of optimal parameters for any target distribution matcher rate. Specifically, lower bounds on energy loss, rate loss, and memory requirements are analytically estimated for HiDM architectures approximating the Maxwell Boltzmann (MB) distribution. A semi analytical optimization framework is employed to jointly optimize rate and energy loss, allowing the selection of the number of hierarchical layers, memory size, and block length required to optimize channel capacity. The accuracy of the proposed model is validated through probabilistic amplitude shaping of 16QAM (PAS 16QAM), showing good agreement between analytical predictions and simulated results. The proposed analytical tool facilitates the design of HiDM structures that are compatible with practical hardware and implementation constraints, such as those imposed by state-of-the-art application-specific integrated circuits (ASICs) and field-programmable gate arrays (FPGAs). Furthermore, the performance of the optimized HiDM structure, incorporating layer selection based on lower-bound energy loss, is evaluated over the AWGN channel in terms of normalized generalized mutual information (NGMI) as a function of the optical signal-to-noise ratio (OSNR). At a net data rate of 200 Gbps with 25% forward error correction (FEC) overhead, the proposed scheme achieves a shaping gain improvement of 2.8% compared to previously reported solutions.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5b9e\u7528\u7684\u5206\u5c42\u5206\u5e03\u5339\u914d\u5668\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u7528\u4e8e\u6982\u7387\u6574\u5f62\u661f\u5ea7\u7cfb\u7edf\uff0c\u901a\u8fc7\u5206\u6790\u4f18\u5316\u5b9e\u73b0\u80fd\u91cf\u635f\u5931\u3001\u901f\u7387\u635f\u5931\u548c\u5185\u5b58\u9700\u6c42\u7684\u5e73\u8861\uff0c\u572816QAM\u7cfb\u7edf\u4e2d\u9a8c\u8bc1\u4e862.8%\u7684\u6574\u5f62\u589e\u76ca\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u7684\u5206\u5c42\u5206\u5e03\u5339\u914d\u5668\u8bbe\u8ba1\u7f3a\u4e4f\u7cfb\u7edf\u5316\u7684\u4f18\u5316\u65b9\u6cd5\uff0c\u96be\u4ee5\u5728\u5b9e\u9645\u786c\u4ef6\u7ea6\u675f\u4e0b\u5b9e\u73b0\u6700\u4f73\u6027\u80fd\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5e73\u8861\u80fd\u91cf\u635f\u5931\u3001\u901f\u7387\u635f\u5931\u548c\u5185\u5b58\u9700\u6c42\u7684\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u4ee5\u9002\u5e94ASIC\u548cFPGA\u7b49\u5b9e\u9645\u786c\u4ef6\u5e73\u53f0\u7684\u9650\u5236\u3002", "method": "\u63d0\u51fa\u534a\u89e3\u6790\u4f18\u5316\u6846\u67b6\uff0c\u8054\u5408\u4f18\u5316\u901f\u7387\u548c\u80fd\u91cf\u635f\u5931\uff0c\u786e\u5b9a\u5206\u5c42\u5c42\u6570\u3001\u5185\u5b58\u5927\u5c0f\u548c\u5757\u957f\u5ea6\u3002\u901a\u8fc7\u5206\u6790\u4f30\u8ba1\u80fd\u91cf\u635f\u5931\u3001\u901f\u7387\u635f\u5931\u548c\u5185\u5b58\u9700\u6c42\u7684\u4e0b\u754c\uff0c\u7279\u522b\u9488\u5bf9\u8fd1\u4f3cMaxwell Boltzmann\u5206\u5e03\u7684\u5206\u5c42\u67b6\u6784\u3002\u91c7\u7528\u6982\u7387\u5e45\u5ea6\u6574\u5f6216QAM\u7cfb\u7edf\u9a8c\u8bc1\u65b9\u6cd5\u51c6\u786e\u6027\u3002", "result": "\u5728AWGN\u4fe1\u9053\u4e2d\u8bc4\u4f30\u4f18\u5316\u540e\u7684HiDM\u7ed3\u6784\uff0c\u4f7f\u7528NGMI\u4f5c\u4e3aOSNR\u7684\u51fd\u6570\u3002\u5728200Gbps\u51c0\u6570\u636e\u901f\u7387\u548c25%\u524d\u5411\u7ea0\u9519\u5f00\u9500\u4e0b\uff0c\u76f8\u6bd4\u5148\u524d\u65b9\u6848\u5b9e\u73b0\u4e862.8%\u7684\u6574\u5f62\u589e\u76ca\u63d0\u5347\u3002\u5206\u6790\u9884\u6d4b\u4e0e\u4eff\u771f\u7ed3\u679c\u543b\u5408\u826f\u597d\u3002", "conclusion": "\u8be5\u8bbe\u8ba1\u65b9\u6cd5\u4e3a\u5206\u5c42\u5206\u5e03\u5339\u914d\u5668\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u5206\u6790\u5de5\u5177\uff0c\u80fd\u591f\u5728\u786c\u4ef6\u7ea6\u675f\u4e0b\u4f18\u5316\u4fe1\u9053\u5bb9\u91cf\uff0c\u5728\u6982\u7387\u6574\u5f62\u7cfb\u7edf\u4e2d\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2601.16915", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.16915", "abs": "https://arxiv.org/abs/2601.16915", "authors": ["Aleksey S. Gvozdarev"], "title": "IRS Compensation of Hyper-Rayleigh Fading: How Many Elements Are Needed?", "comment": null, "summary": "The letter introduces and studies the problem of defining the minimum number of Intelligent Reflecting Surface (IRS) elements needed to compensate for heavy fading conditions in multipath fading channels. The fading severity is quantified in terms of Hyper-Rayleigh Regimes (HRRs) (i.e., full-HRR (worst-case conditions), strong-, weak-, and no-HRR), and the channel model used (Inverse Power Lomax (IPL)) was chosen since it can account for all HRRs. The research presents the derived closed-form channel coefficient envelope statistics for the single IRS-element channel with IPL statistics in both subchannels and total IRS-assisted channel, as well as tight approximations for the channel coefficient and instantaneous signal-to-noise ratio (SNR) statistics for the latter. The derived expressions helped estimate channel parameters corresponding to the specific HRRs of the total channel and demonstrate that while both single links (i.e., ''source-IRS'' and ''IRS-destination'') are in full-HRR, the minimum number of IRS elements needed to bring the total IRS-assisted link (''source-IRS-destination'') out of full-HRR is no less than $6$ (for the whole range on the IPL scale parameter corresponding full-HRR). Furthermore, the minimum number of IRS elements required to bring the total IRS-assisted link into no-HRR is $14$ (under the same conditions).", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5728\u8d85\u745e\u5229\u8870\u843d\u6761\u4ef6\u4e0b\uff0c\u667a\u80fd\u53cd\u5c04\u8868\u9762(IRS)\u6240\u9700\u7684\u6700\u5c0f\u53cd\u5c04\u5355\u5143\u6570\u91cf\u95ee\u9898\uff0c\u786e\u5b9a\u4e86\u5728\u4e0d\u540c\u8870\u843d\u4e25\u91cd\u7a0b\u5ea6\u4e0b\u8865\u507f\u4fe1\u9053\u8870\u843d\u6240\u9700\u7684\u6700\u5c0fIRS\u5355\u5143\u6570\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u786e\u5b9a\u5728\u4e25\u91cd\u8870\u843d\u7684\u591a\u5f84\u4fe1\u9053\u4e2d\uff0c\u9700\u8981\u591a\u5c11IRS\u5355\u5143\u624d\u80fd\u8865\u507f\u4fe1\u9053\u8870\u843d\u3002\u8870\u843d\u4e25\u91cd\u7a0b\u5ea6\u7528\u91cf\u5316\u7684\u8d85\u745e\u5229\u673a\u5236(HRRs)\u6765\u8861\u91cf\uff0c\u5305\u62ec\u5b8c\u5168HRR\uff08\u6700\u574f\u60c5\u51b5\uff09\u3001\u5f3aHRR\u3001\u5f31HRR\u548c\u65e0HRR\u3002", "method": "\u4f7f\u7528\u9006\u5e42Lomax(IPL)\u5206\u5e03\u4f5c\u4e3a\u4fe1\u9053\u6a21\u578b\uff0c\u56e0\u4e3a\u5b83\u80fd\u6db5\u76d6\u6240\u6709HRR\u60c5\u51b5\u3002\u63a8\u5bfc\u4e86\u5355IRS\u5355\u5143\u4fe1\u9053\u7684\u95ed\u5f0f\u4fe1\u9053\u7cfb\u6570\u5305\u7edc\u7edf\u8ba1\u91cf\uff0c\u5305\u62ec\u5b50\u4fe1\u9053\u548c\u603bIRS\u8f85\u52a9\u4fe1\u9053\u7684\u7edf\u8ba1\u91cf\uff0c\u5e76\u4e3a\u603b\u4fe1\u9053\u7684\u4fe1\u9053\u7cfb\u6570\u548c\u77ac\u65f6\u4fe1\u566a\u6bd4(SNR)\u63d0\u4f9b\u4e86\u7d27\u5bc6\u8fd1\u4f3c\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff1a\u5f53\u6e90-IRS\u548cIRS-\u76ee\u7684\u5730\u4e24\u4e2a\u5355\u94fe\u8def\u90fd\u5904\u4e8e\u5b8c\u5168HRR\u65f6\uff0c\u5c06\u603bIRS\u8f85\u52a9\u94fe\u8def\u5e26\u51fa\u5b8c\u5168HRR\u6240\u9700\u7684\u6700\u5c0fIRS\u5355\u5143\u6570\u4e0d\u5c11\u4e8e6\u4e2a\uff1b\u5c06\u603b\u94fe\u8def\u5e26\u5165\u65e0HRR\u6240\u9700\u7684\u6700\u5c0fIRS\u5355\u5143\u6570\u4e3a14\u4e2a\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3aIRS\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u91cd\u8981\u6307\u5bfc\uff0c\u786e\u5b9a\u4e86\u5728\u4e0d\u540c\u8870\u843d\u4e25\u91cd\u7a0b\u5ea6\u4e0b\u8865\u507f\u4fe1\u9053\u8870\u843d\u6240\u9700\u7684\u6700\u5c0fIRS\u5355\u5143\u6570\u91cf\uff0c\u8fd9\u5bf9\u4e8e\u5b9e\u9645IRS\u90e8\u7f72\u5177\u6709\u91cd\u8981\u53c2\u8003\u4ef7\u503c\u3002"}}

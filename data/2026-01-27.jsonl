{"id": "2601.17129", "categories": ["eess.SP", "physics.app-ph"], "pdf": "https://arxiv.org/pdf/2601.17129", "abs": "https://arxiv.org/abs/2601.17129", "authors": ["Eric Danson", "Jeffrey S. Walling"], "title": "Inverter-Based Differential Amplifiers With Back-Gate Feedback Linearization", "comment": "4 pages, 8 figures, 1 table", "summary": "Feeding the common-source amplifier output to the back-gate terminal in fully depleted silicon on insulator (FD-SOI) technology exploits the linearizing effect of negative feedback. Analysis and simulation results in 22 nm FD-SOI show that back-gate feedback sets the overall gain approximately independent of the load, contributes no additional noise, and improves linearity by the back-gate voltage gain. Third-order intercept point (IP3) enhancement is at least $60\\times$ compared to without feedback in inverter-based, or complementary common-source, differential amplifiers."}
{"id": "2601.17154", "categories": ["eess.SP", "eess.SY"], "pdf": "https://arxiv.org/pdf/2601.17154", "abs": "https://arxiv.org/abs/2601.17154", "authors": ["Shivanshu Tripathi", "Hossein Mohsenzadeh Yazdi", "Maziar Raissi", "Hamed Mohsenian-Rad"], "title": "Data-Efficient Physics-Informed Learning to Model Synchro-Waveform Dynamics of Grid-Integrated Inverter-Based Resources", "comment": null, "summary": "Inverter-based resources (IBRs) exhibit fast transient dynamics during network disturbances, which often cannot be properly captured by phasor and SCADA measurements. This shortcoming has recently been addressed with the advent of waveform measurement units (WMUs), which provide high-resolution, time-synchronized raw voltage and current waveform samples from multiple locations in the power system. However, transient model learning based on synchro-waveform measurements remains constrained by the scarcity of network disturbances and the complexity of the underlying nonlinear dynamics of IBRs. We propose to address these problems by developing a data-efficient physics-informed machine learning (PIML) framework for synchro-waveform analytics that estimates the IBR terminal current response from only a few network disturbance signatures. Here, the physics of the electrical circuits are used to compensate for limited data availability by constraining the learning process through known circuit relationships. Two cases are considered, with known and unknown circuit parameters. In the latter case, the framework jointly learns the transient dynamics of the IBRs and the parameters of the electrical circuit. Case studies using WMU disturbance data across multiple sampling rates shows consistently lower current estimation error with substantially fewer training events than a purely data-driven baseline."}
{"id": "2601.17320", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.17320", "abs": "https://arxiv.org/abs/2601.17320", "authors": ["Ioannis Gavras", "Giuseppe Thadeu Freitas de Abreu", "George C. Alexandropoulos"], "title": "RIS-Enabled Spoofing Against Adversary Sensing: CRB-Maximizing Design and Decoying Analysis", "comment": "6 pages, 4 figures", "summary": "This paper studies the capability of a Reconfigurable Intelligent Surface (RIS), when transparently covering a User Equipment (UE), to deceive an adversary monostatic radar system. A compact RIS kernel model that explicitly links the radar's angular response to the RIS phase profile is introduced, enabling an analytical investigation of the Angle of Arrival (AoA) estimation accuracy with respect to the kernel's power. This model is also leveraged to formulate an RIS-based spoofing design with the dual objective to enforce strict nulls around the UE's true reflection AoA and maximize the channel gain towards a decoy direction. The RIS's deception capability is quantified using point-wise and angle-range robust criteria, and a configuration-independent placement score guiding decoy selection is proposed. Selected numerical results confirm deep nulls at the true reflection AoA together with a pronounced decoy peak, rendering maximum-likelihood sensing at the adversary radar unreliable."}
{"id": "2601.17419", "categories": ["eess.SP", "cs.IT", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.17419", "abs": "https://arxiv.org/abs/2601.17419", "authors": ["Ahmad Halimi Razlighi", "Pallavi Dhingra", "Edgar Beck", "Bho Matthiesen", "Armin Dekorsy"], "title": "Semantic-Aware Task Clustering for Federated Cooperative Multi-Task Semantic Communication", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Task-oriented semantic communication (SemCom) prioritizes task execution over accurate symbol reconstruction and is well-suited to emerging intelligent applications. Cooperative multi-task SemCom (CMT-SemCom) further improves task execution performance. However, [1] demonstrates that cooperative multi-tasking can be either constructive or destructive. Moreover, the existing CMT-SemCom framework is not directly applicable to distributed multi-user scenarios, such as non-terrestrial satellite networks, where each satellite employs an individual semantic encoder. In this paper, we extend our earlier CMT-SemCom framework to distributed settings by proposing a federated learning (FL) based CMT-SemCom that enables cooperative multi-tasking across distributed users. Moreover, to address performance degradation caused by negative information transfer among heterogeneous tasks, we propose a semantic-aware task clustering method integrated in the FL process to ensure constructive cooperation based on an information-theoretic approach. Unlike common clustering methods that rely on high-dimensional data or feature space similarity, our proposed approach operates in the low-dimensional semantic domain to identify meaningful task relationships. Simulation results based on a LEO satellite network setup demonstrate the effectiveness of our approach and performance gain over unclustered FL and individual single-task SemCom."}
{"id": "2601.17086", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.17086", "abs": "https://arxiv.org/abs/2601.17086", "authors": ["Ayush Pratap Singh", "Harshit Singh", "Nityanand Mathur", "Akshat Mandloi", "Sudarshan Kamath"], "title": "SonoEdit: Null-Space Constrained Knowledge Editing for Pronunciation Correction in LLM-Based TTS", "comment": null, "summary": "Neural text-to-speech (TTS) systems systematically mispronounce low-resource proper nouns, particularly non-English names, brands, and geographic locations, due to their underrepresentation in predominantly English training corpora. Existing solutions typically rely on expensive multilingual data collection, supervised finetuning, or manual phonetic annotation, which limits the deployment of TTS systems in linguistically diverse settings. We introduce SonoEdit, a model editing technique that surgically corrects pronunciation errors in pre-trained TTS models without retraining. Instead of costly finetuning or explicit phoneme injection, we propose a parsimonious alternative based on Null-Space Pronunciation Editing, which performs a single-shot parameter update to modify the pronunciation of specific words while provably preserving all other model behavior. We first adapt Acoustic Causal Tracing to identify the Transformer layers responsible for text-to-pronunciation mapping. We then apply Null-Space Constrained Editing to compute a closed-form weight update that corrects the target pronunciation while remaining mathematically orthogonal to the subspace governing general speech generation. This constrained update steers the model's acoustic output toward a desired pronunciation exemplar while guaranteeing zero first-order change on a preserved speech corpus."}
{"id": "2601.16989", "categories": ["eess.AS", "cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2601.16989", "abs": "https://arxiv.org/abs/2601.16989", "authors": ["Yasaman Haghbin", "Sina Rashidi", "Ali Zolnour", "Maryam Zolnoori"], "title": "The Voice of Equity: A Systematic Evaluation of Bias Mitigation Techniques for Speech-Based Cognitive Impairment Detection Across Architectures and Demographics", "comment": null, "summary": "Speech-based detection of cognitive impairment offers a scalable, non-invasive screening, yet algorithmic bias across demographic and linguistic subgroups remains critically underexplored. We present the first comprehensive fairness analysis framework for speech-based multi-class cognitive impairment detection, systematically evaluating bias mitigation across architectures, and demographic subgroups. We developed two transformer-based architectures, SpeechCARE-AGF and Whisper-LWF-LoRA, on the multilingual NIA PREPARE Challenge dataset. Unlike prior work that typically examines single mitigation techniques, we compared pre-processing, in-processing, and post-processing approaches, assessing fairness via Equality of Opportunity and Equalized Odds across gender, age, education, and language. Both models achieved strong performance (F1: SpeechCARE-AGF 70.87, Whisper-LWF-LoRA 71.46) but exhibited substantial fairness disparities. Adults >=80 showed lower sensitivity versus younger groups; Spanish speakers demonstrated reduced TPR versus English speakers. Mitigation effectiveness varied by architecture: oversampling improved SpeechCARE-AGF for older adults (80+ TPR: 46.19%=>49.97%) but minimally affected Whisper-LWF-LoRA. This study addresses a critical healthcare AI gap by demonstrating that architectural design fundamentally shapes bias patterns and mitigation effectiveness. Adaptive fusion mechanisms enable flexible responses to data interventions, while frequency reweighting offers robust improvements across architectures. Our findings establish that fairness interventions must be tailored to both model architecture and demographic characteristics, providing a systematic framework for developing equitable speech-based screening tools essential for reducing diagnostic disparities in cognitive healthcare."}
{"id": "2601.17571", "categories": ["eess.SP", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17571", "abs": "https://arxiv.org/abs/2601.17571", "authors": ["Javier González-Alonso", "Paula Martín-Tapia", "David González-Ortega", "Míriam Antón-Rodríguez", "Francisco Javier Díaz-Pernas", "Mario Martínez-Zarzuela"], "title": "ME-WARD: A multimodal ergonomic analysis tool for musculoskeletal risk assessment from inertial and video data in working plac", "comment": "19 pages", "summary": "This study presents ME-WARD (Multimodal Ergonomic Workplace Assessment and Risk from Data), a novel system for ergonomic assessment and musculoskeletal risk evaluation that implements the Rapid Upper Limb Assessment (RULA) method. ME-WARD is designed to process joint angle data from motion capture systems, including inertial measurement unit (IMU)-based setups, and deep learning human body pose tracking models. The tool's flexibility enables ergonomic risk assessment using any system capable of reliably measuring joint angles, extending the applicability of RULA beyond proprietary setups. To validate its performance, the tool was tested in an industrial setting during the assembly of conveyor belts, which involved high-risk tasks such as inserting rods and pushing conveyor belt components. The experiments leveraged gold standard IMU systems alongside a state-of-the-art monocular 3D pose estimation system. The results confirmed that ME-WARD produces reliable RULA scores that closely align with IMU-derived metrics for flexion-dominated movements and comparable performance with the monocular system, despite limitations in tracking lateral and rotational motions. This work highlights the potential of integrating multiple motion capture technologies into a unified and accessible ergonomic assessment pipeline. By supporting diverse input sources, including low-cost video-based systems, the proposed multimodal approach offers a scalable, cost-effective solution for ergonomic assessments, paving the way for broader adoption in resource-constrained industrial environments."}
{"id": "2601.17097", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.17097", "abs": "https://arxiv.org/abs/2601.17097", "authors": ["Federico Bruzzone", "Walter Cazzola", "Matteo Brancaleoni", "Dario Pellegrino"], "title": "Sink or SWIM: Tackling Real-Time ASR at Scale", "comment": "14 pages, 7 figures", "summary": "Real-time automatic speech recognition systems are increasingly integrated into interactive applications, from voice assistants to live transcription services. However, scaling these systems to support multiple concurrent clients while maintaining low latency and high accuracy remains a major challenge. In this work, we present SWIM, a novel real-time ASR system built on top of OpenAI's Whisper model that enables true model-level parallelization for scalable, multilingual transcription. SWIM supports multiple concurrent audio streams without modifying the underlying model. It introduces a buffer merging strategy that maintains transcription fidelity while ensuring efficient resource usage. We evaluate SWIM in multi-client settings -- scaling up to 20 concurrent users -- and show that it delivers accurate real-time transcriptions in English, Italian, and Spanish, while maintaining low latency and high throughput. While Whisper-Streaming achieves a word error rate of approximately 8.2% with an average delay of approximately 3.4 s in a single-client, English-only setting, SWIM extends this capability to multilingual, multi-client environments. It maintains comparable accuracy with significantly lower delay -- around 2.4 s with 5 clients -- and continues to scale effectively up to 20 concurrent clients without degrading transcription quality and increasing overall throughput. Our approach advances scalable ASR by improving robustness and efficiency in dynamic, multi-user environments."}
{"id": "2601.17014", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2601.17014", "abs": "https://arxiv.org/abs/2601.17014", "authors": ["Kayley Seow", "Alexander Arovas", "Grace Steinmetz", "Emily Bick"], "title": "BickGraphing: Web-Based Application for Visual Inspection of Audio Recordings", "comment": "11 pages, 4 figures for submission in Journal of Open Research Software", "summary": "BickGraphing is a browser based research tool that enables visual inspection of acoustic recordings. The tool was built in support of visualizing crop feeding pest sounds in support of the Insect Eavesdropper project; however, it is widely applicable to all audiovisualizations in research. It allows multiple uploads of large .wav files, computes waveforms and spectrograms locally, and supports interactive exploration of audio events in time and frequency. The application is implemented as a SvelteKit and TypeScript web app with a client side signal processing pipeline using WebAssembly compiled FFmpeg and custom FFT utilities. The software is released on an open Git repository (https://github.com/bicklabuw/BickGraphing) and archived under a standard MIT license and can be reused for rapid visual quality checks of .wav recordings in insect bioacoustics and related fields. BickGraphing has the potential to be a local, easy to use coding free visualization platform for audio data in research."}
{"id": "2601.17574", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.17574", "abs": "https://arxiv.org/abs/2601.17574", "authors": ["Javier González-Alonso", "Cristina Simón-Martínez", "Míriam Antón-Rodríguez", "David González-Ortega", "Francisco Javier Díaz-Pernas", "Mario Martínez-Zarzuela"], "title": "Development of an end-to-end hardware and software pipeline for affordable and feasible ergonomics assessment in the automotive industry", "comment": "13 pages", "summary": "An end-to-end hardware-software pipeline is introduced to automatize ergonomics assessment in industrial workplaces. The proposed modular solution can interoperate with commercial systems throughout the ergonomics assessment phases involved in the process. The pipeline includes custom-designed Inertial Measurement Unit (IMU) sensors, two real-time worker movement acquisition tools, inverse kinematics processing and Rapid Upper Limb Assessment (RULA) report generation. It is based on free tools such as Unity3D and OpenSim to avoid the problems derived from using proprietary technologies, such as security decisions being made under \"black box\" conditions. Experiments were conducted in an automotive factory in a workplace with WMSDs risk among workers. The proposed solution obtained comparable results to a gold standard solution, reaching measured joint angles a 0.95 cross-correlation and a Root Mean Square Error (RMSE) lower than 10 for elbows and 12 for shoulders between both systems. In addition, the global RULA score difference is lower than 5% between both systems. This work provides a low-cost solution for WMSDs risk assessment in the workplace to reduce musculoskeletal disorders and associated sick leave in industry, impacting the health of workers in the long term. Our study can ease further research and popularize the use of wearable systems for ergonomics analysis allowing these workplace prevention systems to reach different industrial environments."}
{"id": "2601.17270", "categories": ["cs.SD", "cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.17270", "abs": "https://arxiv.org/abs/2601.17270", "authors": ["Max McKinnon", "Samir Khaki", "Chandan KA Reddy", "William Huang"], "title": "Window Size Versus Accuracy Experiments in Voice Activity Detectors", "comment": null, "summary": "Voice activity detection (VAD) plays a vital role in enabling applications such as speech recognition. We analyze the impact of window size on the accuracy of three VAD algorithms: Silero, WebRTC, and Root Mean Square (RMS) across a set of diverse real-world digital audio streams. We additionally explore the use of hysteresis on top of each VAD output. Our results offer practical references for optimizing VAD systems. Silero significantly outperforms WebRTC and RMS, and hysteresis provides a benefit for WebRTC."}
{"id": "2601.17080", "categories": ["eess.AS", "cs.AI", "cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2601.17080", "abs": "https://arxiv.org/abs/2601.17080", "authors": ["Seung Gyu Jeong", "Seong-Eun Kim"], "title": "PC-MCL: Patient-Consistent Multi-Cycle Learning with multi-label bias correction for respiratory sound classification", "comment": null, "summary": "Automated respiratory sound classification supports the diagnosis of pulmonary diseases. However, many deep models still rely on cycle-level analysis and suffer from patient-specific overfitting. We propose PC-MCL (Patient-Consistent Multi-Cycle Learning) to address these limitations by utilizing three key components: multi-cycle concatenation, a 3-label formulation, and a patient-matching auxiliary task. Our work resolves a multi-label distributional bias in respiratory sound classification, a critical issue inherent to applying multi-cycle concatenation with the conventional 2-label formulation (crackle, wheeze). This bias manifests as a systematic loss of normal signal information when normal and abnormal cycles are combined. Our proposed 3-label formulation (normal, crackle, wheeze) corrects this by preserving information from all constituent cycles in mixed samples. Furthermore, the patient-matching auxiliary task acts as a multi-task regularizer, encouraging the model to learn more robust features and improving generalization. On the ICBHI 2017 benchmark, PC-MCL achieves an ICBHI Score of 65.37%, outperforming existing baselines. Ablation studies confirm that all three components are essential, working synergistically to improve the detection of abnormal respiratory events."}
{"id": "2601.17710", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.17710", "abs": "https://arxiv.org/abs/2601.17710", "authors": ["Huy Trinh", "Phuong Thai", "Elliot Creager", "George Shaker"], "title": "Reliable Quasi-Static Post-Fall Floor-Occupancy Detection Using Low-Cost Millimetre-Wave Radar", "comment": null, "summary": "As the population ages rapidly, long-term care (LTC) facilities across North America face growing pressure to monitor residents safely while keeping staff workload manageable. Falls are among the most critical events to monitor due to their timely response requirement, yet frequent false alarms or uncertain detections can overwhelm caregivers and contribute to alarm fatigue. This motivates the design of reliable, whole end-to-end ambient monitoring systems from occupancy and activity awareness to fall and post-fall detection. In this paper, we focus on robust post-fall floor-occupancy detection using an off-the-shelf 60 GHz FMCW radar and evaluate its deployment in a realistic, furnished indoor environment representative of LTC facilities. Post-fall detection is challenging since motion is minimal, and reflections from the floor and surrounding objects can dominate the radar signal return. We compare a vendor-provided digital beamforming (DBF) pipeline against a proposed preprocessing approach based on Capon or minimum variance distortionless response (MVDR) beamforming. A cell-averaging constant false alarm rate (CA-CFAR) detector is applied and evaluated on the resulting range-azimuth maps across 7 participants. The proposed method improves the mean frame-positive rate from 0.823 (DBF) to 0.916 (Proposed)."}
{"id": "2601.17517", "categories": ["cs.SD", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.17517", "abs": "https://arxiv.org/abs/2601.17517", "authors": ["Luca Cerovaz", "Michele Mancusi", "Emanuele Rodolà"], "title": "EuleroDec: A Complex-Valued RVQ-VAE for Efficient and Robust Audio Coding", "comment": "Accepted at ICASSP 2026", "summary": "Audio codecs power discrete music generative modelling, music streaming, and immersive media by shrinking PCM audio to bandwidth-friendly bitrates. Recent works have gravitated towards processing in the spectral domain; however, spectrogram domains typically struggle with phase modeling, which is naturally complex-valued. Most frequency-domain neural codecs either disregard phase information or encode it as two separate real-valued channels, limiting spatial fidelity. This entails the need to introduce adversarial discriminators at the expense of convergence speed and training stability to compensate for the inadequate representation power of the audio signal. In this work we introduce an end-to-end complex-valued RVQ-VAE audio codec that preserves magnitude-phase coupling across the entire analysis-quantization-synthesis pipeline and removes adversarial discriminators and diffusion post-filters. Without GANs or diffusion, we match or surpass much longer-trained baselines in-domain and reach SOTA out-of-domain performance on phase coherence and waveform fidelity. Compared to standard baselines that train for hundreds of thousands of steps, our model, which reduces the training budget by an order of magnitude, is markedly more compute-efficient while preserving high perceptual quality."}
{"id": "2601.17085", "categories": ["eess.AS", "cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2601.17085", "abs": "https://arxiv.org/abs/2601.17085", "authors": ["Esther Sun", "Abinay Reddy Naini", "Carlos Busso"], "title": "Recovering Performance in Speech Emotion Recognition from Discrete Tokens via Multi-Layer Fusion and Paralinguistic Feature Integration", "comment": "Accepted to ICASSP 2026", "summary": "Discrete speech tokens offer significant advantages for storage and language model integration, but their application in speech emotion recognition (SER) is limited by paralinguistic information loss during quantization. This paper presents a comprehensive investigation of discrete tokens for SER. Using a fine-tuned WavLM-Large model, we systematically quantify performance degradation across different layer configurations and k-means quantization granularities. To recover the information loss, we propose two key strategies: (1) attention-based multi-layer fusion to recapture complementary information from different layers, and (2) integration of openSMILE features to explicitly reintroduce paralinguistic cues. We also compare mainstream neural codec tokenizers (SpeechTokenizer, DAC, EnCodec) and analyze their behaviors when fused with acoustic features. Our findings demonstrate that through multi-layer fusion and acoustic feature integration, discrete tokens can close the performance gap with continuous representations in SER tasks."}
{"id": "2601.17721", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.17721", "abs": "https://arxiv.org/abs/2601.17721", "authors": ["Huy Trinh", "Elliot Creager", "George Shaker"], "title": "Doppler-Domain Respiratory Amplification for Semi-Static Human Occupancy Detection Using Low-Resolution SIMO FMCW Radar", "comment": null, "summary": "Radar-based sensing is a promising privacy-preserving alternative to cameras and wearables in settings such as long-term care. Yet detecting quasi-static presence (lying, sitting, or standing with only subtle micro-motions) is difficult for low-resolution SIMO FMCW radar because near-zero Doppler energy is often buried under static clutter. We present Respiratory-Amplification Semi-Static Occupancy (RASSO), an invertible Doppler-domain non-linear remapping that densifies the slow-time FFT (Doppler) grid around 0 m/s before adaptive Capon beamforming. The resulting range-azimuth (RA) maps exhibit higher effective SNR, sharper target peaks, and lower background variance, making thresholding and learning more reliable. On a real nursing-home dataset collected with a short-range 1Tx-3Rx radar, RASSO-RA improves classical detection performance, achieving AUC = 0.981 and recall = 0.920/0.947 at FAR = 1%/5%, outperforming conventional Capon processing and a recent baseline. RASSO-RA also benefits data-driven models: a frame-based CNN reaches 95-99% accuracy and a sequence-based CNN-LSTM reaches 99.4-99.6% accuracy across subjects. A paired session-level bootstrap test confirms statistically significant macro-F1 gains of 2.6-3.6 points (95% confidence intervals above zero) over the non-warped pipeline. These results show that simple Doppler-domain warping before spatial processing can materially improve semi-static occupancy detection with low-resolution radar in real clinical environments."}
{"id": "2601.17645", "categories": ["cs.SD", "cs.CL", "cs.CV", "cs.MM", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.17645", "abs": "https://arxiv.org/abs/2601.17645", "authors": ["Xilin Jiang", "Qiaolin Wang", "Junkai Wu", "Xiaomin He", "Zhongweiyang Xu", "Yinghao Ma", "Minshuo Piao", "Kaiyi Yang", "Xiuwen Zheng", "Riki Shimizu", "Yicong Chen", "Arsalan Firoozi", "Gavin Mischler", "Sukru Samet Dindar", "Richard Antonello", "Linyang He", "Tsun-An Hsieh", "Xulin Fan", "Yulun Wu", "Yuesheng Ma", "Chaitanya Amballa", "Weixiong Chen", "Jiarui Hai", "Ruisi Li", "Vishal Choudhari", "Cong Han", "Yinghao Aaron Li", "Adeen Flinker", "Mounya Elhilali", "Emmanouil Benetos", "Mark Hasegawa-Johnson", "Romit Roy Choudhury", "Nima Mesgarani"], "title": "AVMeme Exam: A Multimodal Multilingual Multicultural Benchmark for LLMs' Contextual and Cultural Knowledge and Thinking", "comment": "avmemeexam.github.io/public", "summary": "Internet audio-visual clips convey meaning through time-varying sound and motion, which extend beyond what text alone can represent. To examine whether AI models can understand such signals in human cultural contexts, we introduce AVMeme Exam, a human-curated benchmark of over one thousand iconic Internet sounds and videos spanning speech, songs, music, and sound effects. Each meme is paired with a unique Q&A assessing levels of understanding from surface content to context and emotion to usage and world knowledge, along with metadata such as original year, transcript, summary, and sensitivity. We systematically evaluate state-of-the-art multimodal large language models (MLLMs) alongside human participants using this benchmark. Our results reveal a consistent limitation: current models perform poorly on textless music and sound effects, and struggle to think in context and in culture compared to surface content. These findings highlight a key gap in human-aligned multimodal intelligence and call for models that can perceive contextually and culturally beyond the surface of what they hear and see. Project page: avmemeexam.github.io/public"}
{"id": "2601.17557", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2601.17557", "abs": "https://arxiv.org/abs/2601.17557", "authors": ["Aref Farhadipour", "Ming Jin", "Valeriia Vyshnevetska", "Xiyang Li", "Elisa Pellegrino", "Srikanth Madikeri"], "title": "Spoofing-Aware Speaker Verification via Wavelet Prompt Tuning and Multi-Model Ensembles", "comment": "System description of the T03 team in the WildSpoof Challenge at ICASSP 2026", "summary": "This paper describes the UZH-CL system submitted to the SASV section of the WildSpoof 2026 challenge. The challenge focuses on the integrated defense against generative spoofing attacks by requiring the simultaneous verification of speaker identity and audio authenticity. We proposed a cascaded Spoofing-Aware Speaker Verification framework that integrates a Wavelet Prompt-Tuned XLSR-AASIST countermeasure with a multi-model ensemble. The ASV component utilizes the ResNet34, ResNet293, and WavLM-ECAPA-TDNN architectures, with Z-score normalization followed by score averaging. Trained on VoxCeleb2 and SpoofCeleb, the system obtained a Macro a-DCF of 0.2017 and a SASV EER of 2.08%. While the system achieved a 0.16% EER in spoof detection on the in-domain data, results on unseen datasets, such as the ASVspoof5, highlight the critical challenge of cross-domain generalization."}
{"id": "2601.17731", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.17731", "abs": "https://arxiv.org/abs/2601.17731", "authors": ["Hui Cao", "Rui Meng", "Shujun Han", "Song Gao", "Xiaodong Xu", "Ping Zhang"], "title": "S-MDMA: Sensitivity-Aware Model Division Multiple Access for Satellite-Ground Semantic Communication", "comment": null, "summary": "Satellite-ground semantic communication (SemCom) is expected to play a pivotal role in convergence of communication and AI (ComAI), particularly in enabling intelligent and efficient multi-user data transmission. However, the inherent bandwidth constraints and user interference in satellite-ground systems pose significant challenges to semantic fidelity and transmission robustness. To address these issues, we propose a sensitivity-aware model division multiple access (S-MDMA) framework tailored for bandwidth-limited multi-user scenarios. The proposed framework first performs semantic extraction and merging based on the MDMA architecture to consolidate redundant information. To further improve transmission efficiency, a semantic sensitivity sorting algorithm is presented, which can selectively retain key semantic features. In addition, to mitigate inter-user interference, the framework incorporates orthogonal embedding of semantic features and introduces a multi-user reconstruction loss function to guide joint optimization. Experimental results on open-source datasets demonstrate that S-MDMA consistently outperforms existing methods, achieving robust and high-fidelity reconstruction across diverse signal-to-noise ratio (SNR) conditions and user configurations."}
{"id": "2601.17679", "categories": ["cs.SD", "cs.CL", "cs.CV", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.17679", "abs": "https://arxiv.org/abs/2601.17679", "authors": ["Md Sazzadul Islam Ridoy", "Mubaswira Ibnat Zidney", "Sumi Akter", "Md. Aminur Rahman"], "title": "BanglaRobustNet: A Hybrid Denoising-Attention Architecture for Robust Bangla Speech Recognition", "comment": null, "summary": "Bangla, one of the most widely spoken languages, remains underrepresented in state-of-the-art automatic speech recognition (ASR) research, particularly under noisy and speaker-diverse conditions. This paper presents BanglaRobustNet, a hybrid denoising-attention framework built on Wav2Vec-BERT, designed to address these challenges. The architecture integrates a diffusion-based denoising module to suppress environmental noise while preserving Bangla-specific phonetic cues, and a contextual cross-attention module that conditions recognition on speaker embeddings for robustness across gender, age, and dialects. Trained end-to-end with a composite objective combining CTC loss, phonetic consistency, and speaker alignment, BanglaRobustNet achieves substantial reductions in word error rate (WER) and character error rate (CER) compared to Wav2Vec-BERT and Whisper baselines. Evaluations on Mozilla Common Voice Bangla and augmented noisy speech confirm the effectiveness of our approach, establishing BanglaRobustNet as a robust ASR system tailored to low-resource, noise-prone linguistic settings."}
{"id": "2601.17611", "categories": ["eess.AS", "cs.LG", "cs.SD", "eess.IV", "eess.SP"], "pdf": "https://arxiv.org/pdf/2601.17611", "abs": "https://arxiv.org/abs/2601.17611", "authors": ["Davide Berghi", "Philip J. B. Jackson"], "title": "ToS: A Team of Specialists ensemble framework for Stereo Sound Event Localization and Detection with distance estimation in Video", "comment": null, "summary": "Sound event localization and detection with distance estimation (3D SELD) in video involves identifying active sound events at each time frame while estimating their spatial coordinates. This multimodal task requires joint reasoning across semantic, spatial, and temporal dimensions, a challenge that single models often struggle to address effectively. To tackle this, we introduce the Team of Specialists (ToS) ensemble framework, which integrates three complementary sub-networks: a spatio-linguistic model, a spatio-temporal model, and a tempo-linguistic model. Each sub-network specializes in a unique pair of dimensions, contributing distinct insights to the final prediction, akin to a collaborative team with diverse expertise. ToS has been benchmarked against state-of-the-art audio-visual models for 3D SELD on the DCASE2025 Task 3 Stereo SELD development set, consistently outperforming existing methods across key metrics. Future work will extend this proof of concept by strengthening the specialists with appropriate tasks, training, and pre-training curricula."}
{"id": "2601.17749", "categories": ["eess.SP", "cs.ET", "cs.LG", "cs.NE"], "pdf": "https://arxiv.org/pdf/2601.17749", "abs": "https://arxiv.org/abs/2601.17749", "authors": ["Kyriakos Stylianopoulos", "Mattia Fabiani", "Giulia Torcolacci", "Davide Dardari", "George C. Alexandropoulos"], "title": "Over-The-Air Extreme Learning Machines with XL Reception via Nonlinear Cascaded Metasurfaces", "comment": "6 pages, 5 figures, to be presented at a conference", "summary": "The recently envisioned goal-oriented communications paradigm calls for the application of inference on wirelessly transferred data via Machine Learning (ML) tools. An emerging research direction deals with the realization of inference ML models directly in the physical layer of Multiple-Input Multiple-Output (MIMO) systems, which, however, entails certain significant challenges. In this paper, leveraging the technology of programmable MetaSurfaces (MSs), we present an eXtremely Large (XL) MIMO system that acts as an Extreme Learning Machine (ELM) performing binary classification tasks completely Over-The-Air (OTA), which can be trained in closed form. The proposed system comprises a receiver architecture consisting of densely parallel placed diffractive layers of XL MSs followed by a single reception radio-frequency chain. The front layer facing the MIMO channel consists of identical unit cells of a fixed NonLinear (NL) response, while the remaining layers of elements of tunable linear responses are utilized to approximate OTA the trained ELM weights. Our numerical investigations showcase that, in the XL regime of MS elements, the proposed XL-MIMO-ELM system achieves performance comparable to that of digital and idealized ML models across diverse datasets and wireless scenarios, thereby demonstrating the feasibility of embedding OTA learning capabilities into future communication systems."}
{"id": "2601.17690", "categories": ["cs.SD", "cs.AI", "cs.IR", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.17690", "abs": "https://arxiv.org/abs/2601.17690", "authors": ["Ziling Gong", "Yunyan Ouyang", "Iram Kamdar", "Melody Ma", "Hongjie Chen", "Franck Dernoncourt", "Ryan A. Rossi", "Nesreen K. Ahmed"], "title": "Segment Length Matters: A Study of Segment Lengths on Audio Fingerprinting Performance", "comment": null, "summary": "Audio fingerprinting provides an identifiable representation of acoustic signals, which can be later used for identification and retrieval systems. To obtain a discriminative representation, the input audio is usually segmented into shorter time intervals, allowing local acoustic features to be extracted and analyzed. Modern neural approaches typically operate on short, fixed-duration audio segments, yet the choice of segment duration is often made heuristically and rarely examined in depth. In this paper, we study how segment length affects audio fingerprinting performance. We extend an existing neural fingerprinting architecture to adopt various segment lengths and evaluate retrieval accuracy across different segment lengths and query durations. Our results show that short segment lengths (0.5-second) generally achieve better performance. Moreover, we evaluate LLM capacity in recommending the best segment length, which shows that GPT-5-mini consistently gives the best suggestions across five considerations among three studied LLMs. Our findings provide practical guidance for selecting segment duration in large-scale neural audio retrieval systems."}
{"id": "2601.17640", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2601.17640", "abs": "https://arxiv.org/abs/2601.17640", "authors": ["Anfeng Xu", "Tiantian Feng", "Somer Bishop", "Catherine Lord", "Shrikanth Narayanan"], "title": "End-to-End Joint ASR and Speaker Role Diarization with Child-Adult Interactions", "comment": "Under review for IEEE", "summary": "Accurate transcription and speaker diarization of child-adult spoken interactions are crucial for developmental and clinical research. However, manual annotation is time-consuming and challenging to scale. Existing automated systems typically rely on cascaded speaker diarization and speech recognition pipelines, which can lead to error propagation. This paper presents a unified end-to-end framework that extends the Whisper encoder-decoder architecture to jointly model ASR and child-adult speaker role diarization. The proposed approach integrates: (i) a serialized output training scheme that emits speaker tags and start/end timestamps, (ii) a lightweight frame-level diarization head that enhances speaker-discriminative encoder representations, (iii) diarization-guided silence suppression for improved temporal precision, and (iv) a state-machine-based forced decoding procedure that guarantees structurally valid outputs. Comprehensive evaluations on two datasets demonstrate consistent and substantial improvements over two cascaded baselines, achieving lower multi-talker word error rates and demonstrating competitive diarization accuracy across both Whisper-small and Whisper-large models. These findings highlight the effectiveness and practical utility of the proposed joint modeling framework for generating reliable, speaker-attributed transcripts of child-adult interactions at scale. The code and model weights are publicly available"}
{"id": "2601.17751", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.17751", "abs": "https://arxiv.org/abs/2601.17751", "authors": ["Hong-Bae Jeon", "Chan-Byoung Chae"], "title": "Ampli-Flection for 6G: Active-RIS-Aided Aerial Backhaul with Full 3D Coverage", "comment": "15 pages, 9 figures", "summary": "In this paper, we propose a novel aerial backhaul architecture that employs an aerial active reconfigurable intelligent surface (RIS) to achieve energy-efficient, {full 3D coverage including UAV-BSs and ground users in 6G wireless networks}. Unlike prior aerial-RIS approaches limited to {2D coverage with only servicing ground users} or passive operation, the proposed design integrates an active-RIS onto a high-altitude aerial platform, enabling reliable line-of-sight links and overcoming multiplicative fading through amplification. In a scenario with UAV-BSs deployed to handle sudden traffic surges in urban areas, the aerial-active-RIS both reflects and amplifies backhaul signals to overcome blockage. We jointly optimize the aerial platform placement, array partitioning, and RIS phase configuration to maximize UAV-BS energy-efficiency. Simulation results confirm that the proposed method significantly outperforms benchmarks, demonstrating its strong potential to deliver resilient backhaul connectivity with comprehensive 3D coverage in 6G networks."}
{"id": "2601.17711", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.17711", "abs": "https://arxiv.org/abs/2601.17711", "authors": ["Chengqian Jiang", "Jie Zhang", "Haoyin Yan"], "title": "CaSNet: Compress-and-Send Network Based Multi-Device Speech Enhancement Model for Distributed Microphone Arrays", "comment": "this paper has been accept by ICASSP2026", "summary": "Distributed microphone array (DMA) is a promising next-generation platform for speech interaction, where speech enhancement (SE) is still required to improve the speech quality in noisy cases. Existing SE methods usually first gather raw waveforms at a fusion center (FC) from all devices and then design a multi-microphone model, causing high bandwidth and energy costs. In this work, we propose a \\emph{Compress-and-Send Network (CaSNet)} for resource-constrained DMAs, where one microphone serves as the FC and reference. Each of other devices encodes the measured raw data into a feature matrix, which is then compressed by singular value decomposition (SVD) to produce a more compact representation. The received features at the FC are aligned via cross window query with respect to the reference, followed by neural decoding to yield spatially coherent enhanced speech. Experiments on multiple datasets show that the proposed CaSNet can save the data amount with a negligible impact on the performance compared to the uncompressed case. The reproducible code is available at https://github.com/Jokejiangv/CaSNet."}
{"id": "2601.17901", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2601.17901", "abs": "https://arxiv.org/abs/2601.17901", "authors": ["Yuanchao Li"], "title": "Speech Emotion Recognition with ASR Integration", "comment": "PhD Thesis", "summary": "Speech Emotion Recognition (SER) plays a pivotal role in understanding human communication, enabling emotionally intelligent systems, and serving as a fundamental component in the development of Artificial General Intelligence (AGI). However, deploying SER in real-world, spontaneous, and low-resource scenarios remains a significant challenge due to the complexity of emotional expression and the limitations of current speech and language technologies. This thesis investigates the integration of Automatic Speech Recognition (ASR) into SER, with the goal of enhancing the robustness, scalability, and practical applicability of emotion recognition from spoken language."}
{"id": "2601.17770", "categories": ["eess.SP", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17770", "abs": "https://arxiv.org/abs/2601.17770", "authors": ["Junyong Shin", "Joohyuk Park", "Jihong Park", "Jinho Choi", "Yo-Seb Jeon"], "title": "Context-Aware Iterative Token Detection and Masked Transmission for Wireless Token Communication", "comment": null, "summary": "The success of large-scale language models has established tokens as compact and meaningful units for natural-language representation, which motivates token communication over wireless channels, where tokens are considered fundamental units for wireless transmission. We propose a context-aware token communication framework that uses a pretrained masked language model (MLM) as a shared contextual probability model between the transmitter (Tx) and receiver (Rx). At Rx, we develop an iterative token detection method that jointly exploits MLM-guided contextual priors and channel observations based on a Bayesian perspective. At Tx, we additionally introduce a context-aware masking strategy which skips highly predictable token transmission to reduce transmission rate. Simulation results demonstrate that the proposed framework substantially improves reconstructed sentence quality and supports effective rate adaptation under various channel conditions."}
{"id": "2601.17902", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.17902", "abs": "https://arxiv.org/abs/2601.17902", "authors": ["Wenjie Tian", "Bingshen Mu", "Guobin Ma", "Xuelong Geng", "Zhixian Zhao", "Lei Xie"], "title": "dLLM-ASR: A Faster Diffusion LLM-based Framework for Speech Recognition", "comment": null, "summary": "Automatic speech recognition (ASR) systems based on large language models (LLMs) achieve superior performance by leveraging pretrained LLMs as decoders, but their token-by-token generation mechanism leads to inference latency that grows linearly with sequence length. Meanwhile, discrete diffusion large language models (dLLMs) offer a promising alternative, enabling high-quality parallel sequence generation with pretrained decoders. However, directly applying native text-oriented dLLMs to ASR leads to a fundamental mismatch between open-ended text generation and the acoustically conditioned transcription paradigm required by ASR. As a result, it introduces unnecessary difficulty and computational redundancy, such as denoising from pure noise, inflexible generation lengths, and fixed denoising steps. We propose dLLM-ASR, an efficient dLLM-based ASR framework that formulates dLLM's decoding as a prior-guided and adaptive denoising process. It leverages an ASR prior to initialize the denoising process and provide an anchor for sequence length. Building upon this prior, length-adaptive pruning dynamically removes redundant tokens, while confidence-based denoising allows converged tokens to exit the denoising loop early, enabling token-level adaptive computation. Experiments demonstrate that dLLM-ASR achieves recognition accuracy comparable to autoregressive LLM-based ASR systems and delivers a 4.44$\\times$ inference speedup, establishing a practical and efficient paradigm for ASR."}
{"id": "2601.18010", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2601.18010", "abs": "https://arxiv.org/abs/2601.18010", "authors": ["Jingyao Wu", "Grace Lin", "Yinuo Song", "Rosalind Picard"], "title": "AmbER$^2$: Dual Ambiguity-Aware Emotion Recognition Applied to Speech and Text", "comment": "Accepted in ICASSP 2026", "summary": "Emotion recognition is inherently ambiguous, with uncertainty arising both from rater disagreement and from discrepancies across modalities such as speech and text. There is growing interest in modeling rater ambiguity using label distributions. However, modality ambiguity remains underexplored, and multimodal approaches often rely on simple feature fusion without explicitly addressing conflicts between modalities. In this work, we propose AmbER$^2$, a dual ambiguity-aware framework that simultaneously models rater-level and modality-level ambiguity through a teacher-student architecture with a distribution-wise training objective. Evaluations on IEMOCAP and MSP-Podcast show that AmbER$^2$ consistently improves distributional fidelity over conventional cross-entropy baselines and achieves performance competitive with, or superior to, recent state-of-the-art systems. For example, on IEMOCAP, AmbER$^2$ achieves relative improvements of 20.3% on Bhattacharyya coefficient (0.83 vs. 0.69), 13.6% on R$^2$ (0.67 vs. 0.59), 3.8% on accuracy (0.683 vs. 0.658), and 4.5% on F1 (0.675 vs. 0.646). Further analysis across ambiguity levels shows that explicitly modeling ambiguity is particularly beneficial for highly uncertain samples. These findings highlight the importance of jointly addressing rater and modality ambiguity when building robust emotion recognition systems."}
{"id": "2601.17803", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.17803", "abs": "https://arxiv.org/abs/2601.17803", "authors": ["Dongdong Zou", "Fan Li", "Wei Wang", "Zhongxing Tian", "Yuheng Liu", "Gangxiang Shen", "Yi Cai"], "title": "Comparison of Single Carrier FTN-QAM and PCS-QAM for Amplifier-less Coherent Communication Systems", "comment": null, "summary": "A performance comparison of FTN-QAM and PCS-QAM for amplifier-less short-reach coherent communication systems is provided. With the applications of phase tracking partial response DFE and turbo equalization strategy, FTN-16QAM exhibits about 0.9dB power margin advantage over PCS-64QAM."}
{"id": "2601.18086", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.18086", "abs": "https://arxiv.org/abs/2601.18086", "authors": ["Mengcheng Huang", "Xue Zhou", "Chen Xu", "Dapeng Man"], "title": "From Human Speech to Ocean Signals: Transferring Speech Large Models for Underwater Acoustic Target Recognition", "comment": null, "summary": "Underwater acoustic target recognition (UATR) plays a vital role in marine applications but remains challenging due to limited labeled data and the complexity of ocean environments. This paper explores a central question: can speech large models (SLMs), trained on massive human speech corpora, be effectively transferred to underwater acoustics? To investigate this, we propose UATR-SLM, a simple framework that reuses the speech feature pipeline, adapts the SLM as an acoustic encoder, and adds a lightweight classifier.Experiments on the DeepShip and ShipsEar benchmarks show that UATR-SLM achieves over 99% in-domain accuracy, maintains strong robustness across variable signal lengths, and reaches up to 96.67% accuracy in cross-domain evaluation. These results highlight the strong transferability of SLMs to UATR, establishing a promising paradigm for leveraging speech foundation models in underwater acoustics."}
{"id": "2601.18037", "categories": ["eess.AS", "cs.AI", "cs.SD"], "pdf": "https://arxiv.org/pdf/2601.18037", "abs": "https://arxiv.org/abs/2601.18037", "authors": ["Yiwen Shao", "Yong Xu", "Sanjeev Khudanpur", "Dong Yu"], "title": "SpatialEmb: Extract and Encode Spatial Information for 1-Stage Multi-channel Multi-speaker ASR on Arbitrary Microphone Arrays", "comment": "SLT 2024", "summary": "Spatial information is a critical clue for multi-channel multi-speaker target speech recognition. Most state-of-the-art multi-channel Automatic Speech Recognition (ASR) systems extract spatial features only during the speech separation stage, followed by standard single-channel ASR on the separated speech. This approach results in an inefficient, lengthy pipeline and sub-optimal ASR performance due to the accumulated errors from preprocessing modules. Furthermore, most spatial feature extraction methods depend on the knowledge of speaker positions and microphone topology, making the systems reliant on specific settings and challenging to adapt to new equipment. In this work, we propose a solution to these issues with a lightweight embedding module named SpatialEmb, which extracts and encodes spatial information directly for the ASR model, supporting both fixed and arbitrary microphone topology. We conduct comprehensive experiments on AliMeeting, a real meeting corpus, to determine the optimal model design for SpatialEmb in terms of both performance and efficiency. Our best model trained with 105 hours Train-Ali-far achieves 17.04% and 20.32% character error rates (CER) on the Eval and Test sets, establishing a new state-of-the-art result with the same training data."}
{"id": "2601.17825", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.17825", "abs": "https://arxiv.org/abs/2601.17825", "authors": ["Shun Yang", "Xin Wei", "Nianbing Su", "Weidong Mei", "Zhi Chen", "Boyu Ning"], "title": "Movable Antenna-Enhanced Near-Field Flexible Beamforming: Performance Analysis and Optimization", "comment": null, "summary": "As an emerging wireless communication technology, movable antennas (MAs) offer the ability to adjust the spatial correlation of steering vectors, enabling more flexible beamforming compared to fixed-position antennas (FPAs). In this paper, we investigate the use of MAs for two typical near-field beamforming scenarios: beam nulling and multi-beam forming. In the first scenario, we aim to jointly optimize the positions of multiple MAs and the beamforming vector to maximize the beam gain toward a desired direction while nulling interference toward multiple undesired directions. In the second scenario, the objective is to maximize the minimum beam gain among all the above directions. However, both problems are non-convex and challenging to solve optimally. To gain insights, we first analyze several special cases and show that, with proper positioning of the MAs, directing the beam toward a specific direction can lead to nulls or full gains in other directions in the two scenarios, respectively. For the general cases, we propose a discrete sampling method and an alternating optimization algorithm to obtain high-quality suboptimal solutions to the two formulated problems. Furthermore, considering the practical limitations in antenna positioning accuracy, we analyze the impact of position errors on the performance of the optimized beamforming and MA positions, by introducing a Taylor series approximation for the near-field beam gain at each target. Numerical results validate our theoretical findings and demonstrate the effectiveness of our proposed algorithms."}
{"id": "2601.18184", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.18184", "abs": "https://arxiv.org/abs/2601.18184", "authors": ["Zhiliang Peng", "Jianwei Yu", "Yaoyao Chang", "Zilong Wang", "Li Dong", "Yingbo Hao", "Yujie Tu", "Chenyu Yang", "Wenhui Wang", "Songchen Xu", "Yutao Sun", "Hangbo Bao", "Weijiang Xu", "Yi Zhu", "Zehua Wang", "Ting Song", "Yan Xia", "Zewen Chi", "Shaohan Huang", "Liang Wang", "Chuang Ding", "Shuai Wang", "Xie Chen", "Furu Wei"], "title": "VIBEVOICE-ASR Technical Report", "comment": null, "summary": "This report presents VibeVoice-ASR, a general-purpose speech understanding framework built upon VibeVoice, designed to address the persistent challenges of context fragmentation and multi-speaker complexity in long-form audio (e.g., meetings, podcasts) that remain despite recent advancements in short-form speech recognition. Unlike traditional pipelined approaches that rely on audio chunking, VibeVoice-ASRsupports single-pass processing for up to 60 minutes of audio. It unifies Automatic Speech Recognition, Speaker Diarization, and Timestamping into a single end-to-end generation task. In addition, VibeVoice-ASR supports over 50 languages, requires no explicit language setting, and natively handles code-switching within and across utterances. Furthermore, we introduce a prompt-based context injection mechanism that allows users to supply customized conetxt, significantly improving accuracy on domain-specific terminology and polyphonic character disambiguation."}
{"id": "2601.18094", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2601.18094", "abs": "https://arxiv.org/abs/2601.18094", "authors": ["Zhichao Wang", "Tao Li", "Wenshuo Ge", "Zihao Cui", "Shilei Zhang", "Junlan Feng"], "title": "OneVoice: One Model, Triple Scenarios-Towards Unified Zero-shot Voice Conversion", "comment": "Work in progress", "summary": "Recent progress of voice conversion~(VC) has achieved a new milestone in speaker cloning and linguistic preservation. But the field remains fragmented, relying on specialized models for linguistic-preserving, expressive, and singing scenarios. We propose OneVoice, a unified zero-shot framework capable of handling all three scenarios within a single model. OneVoice is built upon a continuous language model trained with VAE-free next-patch diffusion, ensuring high fidelity and efficient sequence modeling. Its core design for unification lies in a Mixture-of-Experts (MoE) designed to explicitly model shared conversion knowledge and scenario-specific expressivity. Expert selection is coordinated by a dual-path routing mechanism, including shared expert isolation and scenario-aware domain expert assignment with global-local cues. For precise conditioning, scenario-specific prosodic features are fused into each layer via a gated mechanism, allowing adaptive usage of prosody information. Furthermore, to enable the core idea and alleviate the imbalanced issue (abundant speech vs. scarce singing), we adopt a two-stage progressive training that includes foundational pre-training and scenario enhancement with LoRA-based domain experts. Experiments show that OneVoice matches or surpasses specialized models across all three scenarios, while verifying flexible control over scenarios and offering a fast decoding version as few as 2 steps. Code and model will be released soon."}
{"id": "2601.17871", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.17871", "abs": "https://arxiv.org/abs/2601.17871", "authors": ["Huy Trinh", "Sebastian Ratto", "Elliot Creager", "George Shaker"], "title": "A Physics-Informed Digital Twin Framework for Calibrated Sim-to-Real FMCW Radar Occupancy Estimation", "comment": null, "summary": "Learning robust radar perception models directly from real measurements is costly due to the need for controlled experiments, repeated calibration, and extensive annotation. This paper proposes a lightweight simulation-to-real (sim2real) framework that enables reliable Frequency Modulated Continuous Wave (FMCW) radar occupancy detection and people counting using only a physics-informed geometric simulator and a small unlabeled real calibration set. We introduce calibrated domain randomization (CDR) to align the global noise-floor statistics of simulated range-Doppler (RD) maps with those observed in real environments while preserving discriminative micro-Doppler structure. Across real-world evaluations, ResNet18 models trained purely on CDR-adjusted simulation achieve 97 percent accuracy for occupancy detection and 72 percent accuracy for people counting, outperforming ray-tracing baseline simulation and conventional random domain randomization baselines."}
{"id": "2601.18220", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.18220", "abs": "https://arxiv.org/abs/2601.18220", "authors": ["Bingshen Mu", "Xian Shi", "Xiong Wang", "Hexin Liu", "Jin Xu", "Lei Xie"], "title": "LLM-ForcedAligner: A Non-Autoregressive and Accurate LLM-Based Forced Aligner for Multilingual and Long-Form Speech", "comment": null, "summary": "Forced alignment (FA) predicts start and end timestamps for words or characters in speech, but existing methods are language-specific and prone to cumulative temporal shifts. The multilingual speech understanding and long-sequence processing abilities of speech large language models (SLLMs) make them promising for FA in multilingual, crosslingual, and long-form speech settings. However, directly applying the next-token prediction paradigm of SLLMs to FA results in hallucinations and slow inference. To bridge the gap, we propose LLM-ForcedAligner, reformulating FA as a slot-filling paradigm: timestamps are treated as discrete indices, and special timestamp tokens are inserted as slots into the transcript. Conditioned on the speech embeddings and the transcript with slots, the SLLM directly predicts the time indices at slots. During training, causal attention masking with non-shifted input and label sequences allows each slot to predict its own timestamp index based on itself and preceding context, with loss computed only at slot positions. Dynamic slot insertion enables FA at arbitrary positions. Moreover, non-autoregressive inference is supported, avoiding hallucinations and improving speed. Experiments across multilingual, crosslingual, and long-form speech scenarios show that LLM-ForcedAligner achieves a 69%~78% relative reduction in accumulated averaging shift compared with prior methods. The checkpoint and inference code will be released later."}
{"id": "2601.18266", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2601.18266", "abs": "https://arxiv.org/abs/2601.18266", "authors": ["Steven Vander Eeckt", "Hugo Van hamme"], "title": "Efficient Rehearsal for Continual Learning in ASR via Singular Value Tuning", "comment": "Accepted for publication in IEEE Transactions on Audio, Speech, and Language Processing", "summary": "Continual Learning (CL) in Automatic Speech Recognition (ASR) suffers from catastrophic forgetting when adapting to new tasks, domains, or speakers. A common strategy to mitigate this is to store a subset of past data in memory for rehearsal. However, rehearsal-based methods face key limitations: storing data is often costly, infeasible with pre-trained models, or restricted by privacy regulations. Running existing rehearsal-based methods with smaller memory sizes to alleviate these issues usually leads to degraded performance.\n  We propose a rehearsal-based CL method that remains effective even with minimal memory. It operates in two stages: first, fine-tuning on the new task; second, applying Singular Value Decomposition (SVD) to the changes in linear layers and, in a parameter-efficient manner, retraining only gating vectors on the singular values, which control to extent to which updates from the first stage are accepted, using rehearsal. We extensively test and analyze our method on two monolingual and two multilingual benchmarks. Our method reduces forgetting and outperforms state-of-the-art CL approaches for ASR, even when limited to a single utterance per previous task."}
{"id": "2601.18333", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.18333", "abs": "https://arxiv.org/abs/2601.18333", "authors": ["Jionghui Wang", "Jun Fang", "Hongbin Li", "Boyu Ning"], "title": "Integrated Channel Estimation and Sensing for Near-Field ELAA Systems", "comment": null, "summary": "In this paper, we study the problem of uplink channel estimation for near-filed orthogonal frequency division multiplexing (OFDM) systems, where a base station (BS), equipped with an extremely large-scale antenna array (ELAA), serves multiple users over the same time-frequency resource block. A non-orthogonal pilot transmission scheme is considered to accommodate a larger number of users that can be supported by ELAA systems without incurring an excessive amount of training overhead. To facilitate efficient multi-user channel estimation, we express the received signal as a third-order low-rank tensor, which admits a canonical polyadic decomposition (CPD) model for line-of-sight (LoS) scenarios and a block term decomposition (BTD) model for non-line-of-sight (NLoS) scenarios. An alternating least squares (ALS) algorithm and a non-linear least squares (NLS) algorithm are employed to perform CPD and BTD, respectively. Channel parameters are then efficiently extracted from the recovered factor matrices. By exploiting the geometry of the propagation paths in the estimated channel, users' positions can be precisely determined in LoS scenarios. Moreover, our uniqueness analysis shows that the proposed tensor-based joint multi-user channel estimation framework is effective even when the number of pilot symbols is much smaller than the number of users, revealing its potential in training overhead reduction. Simulation results demonstrate that the proposed method achieves markedly higher channel estimation accuracy than compressed sensing (CS)-based approaches."}
{"id": "2601.18335", "categories": ["cs.SD", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18335", "abs": "https://arxiv.org/abs/2601.18335", "authors": ["Zexia Fan", "Yu Chen", "Qiquan Zhang", "Kainan Chen", "Xinyuan Qian"], "title": "Analytic Incremental Learning For Sound Source Localization With Imbalance Rectification", "comment": "Accepted by ICASSP26", "summary": "Sound source localization (SSL) demonstrates remarkable results in controlled settings but struggles in real-world deployment due to dual imbalance challenges: intra-task imbalance arising from long-tailed direction-of-arrival (DoA) distributions, and inter-task imbalance induced by cross-task skews and overlaps. These often lead to catastrophic forgetting, significantly degrading the localization accuracy. To mitigate these issues, we propose a unified framework with two key innovations. Specifically, we design a GCC-PHAT-based data augmentation (GDA) method that leverages peak characteristics to alleviate intra-task distribution skews. We also propose an Analytic dynamic imbalance rectifier (ADIR) with task-adaption regularization, which enables analytic updates that adapt to inter-task dynamics. On the SSLR benchmark, our proposal achieves state-of-the-art (SoTA) results of 89.0% accuracy, 5.3° mean absolute error, and 1.6 backward transfer, demonstrating robustness to evolving imbalances without exemplar storage."}
{"id": "2601.18295", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2601.18295", "abs": "https://arxiv.org/abs/2601.18295", "authors": ["Milan Marocchi", "Matthew Fynn", "Yue Rong"], "title": "Noise-Robust Contrastive Learning with an MFCC-Conformer For Coronary Artery Disease Detection", "comment": "This paper has been accepted for presentation at ICASSP 2026. \\c{opyright} 2026 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses. 5 pages, 1 figure", "summary": "Cardiovascular diseases (CVD) are the leading cause of death worldwide, with coronary artery disease (CAD) comprising the largest subcategory of CVDs. Recently, there has been increased focus on detecting CAD using phonocardiogram (PCG) signals, with high success in clinical environments with low noise and optimal sensor placement. Multichannel techniques have been found to be more robust to noise; however, achieving robust performance on real-world data remains a challenge. This work utilises a novel multichannel energy-based noisy-segment rejection algorithm, using heart and noise-reference microphones, to discard audio segments with large amounts of nonstationary noise before training a deep learning classifier. This conformer-based classifier takes mel-frequency cepstral coefficients (MFCCs) from multiple channels, further helping improve the model's noise robustness. The proposed method achieved 78.4% accuracy and 78.2% balanced accuracy on 297 subjects, representing improvements of 4.1% and 4.3%, respectively, compared to training without noisy-segment rejection."}
{"id": "2601.18453", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.18453", "abs": "https://arxiv.org/abs/2601.18453", "authors": ["Phuong Nam Tran", "Nhan Thanh Nguyen", "Markku Juntti"], "title": "Deep Reinforcement Learning for Hybrid RIS Assisted MIMO Communications", "comment": "This version corresponds to the paper accepted for presentation at the 2025 Asilomar Conference on Signals, Systems, and Computers", "summary": "Hybrid reconfigurable intelligent surfaces (HRIS) enhance wireless systems by combining passive reflection with active signal amplification. However, jointly optimizing the transmit beamforming with the HRIS reflection and amplification coefficients to maximize spectral efficiency (SE) is a non-convex problem, and conventional iterative solutions are computationally intensive. To address this, we propose a deep reinforcement learning (DRL) framework that learns a direct mapping from channel state information to the near-optimal transmit beamforming and HRIS configurations. The DRL model is trained offline, after which it can compute the beamforming and HRIS configurations with low complexity and latency. Simulation results demonstrate that our DRL-based method achieves 95% of the SE obtained by the alternating optimization benchmark, while significantly lowering the computational complexity."}
{"id": "2601.18339", "categories": ["cs.SD", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18339", "abs": "https://arxiv.org/abs/2601.18339", "authors": ["Reemt Hinrichs", "Sonja Stephan", "Alexander Lange", "Jörn Ostermann"], "title": "A Dataset for Automatic Vocal Mode Classification", "comment": "Part of the proceedings of the EvoMUSART 2026: 15th International Conference on Artificial Intelligence in Music, Sound, Art and Design", "summary": "The Complete Vocal Technique (CVT) is a school of singing developed in the past decades by Cathrin Sadolin et al.. CVT groups the use of the voice into so called vocal modes, namely Neutral, Curbing, Overdrive and Edge. Knowledge of the desired vocal mode can be helpful for singing students. Automatic classification of vocal modes can thus be important for technology-assisted singing teaching. Previously, automatic classification of vocal modes has been attempted without major success, potentially due to a lack of data. Therefore, we recorded a novel vocal mode dataset consisting of sustained vowels recorded from four singers, three of which professional singers with more than five years of CVT-experience. The dataset covers the entire vocal range of the subjects, totaling 3,752 unique samples. By using four microphones, thereby offering a natural data augmentation, the dataset consists of more than 13,000 samples combined. An annotation was created using three CVT-experienced annotators, each providing an individual annotation. The merged annotation as well as the three individual annotations come with the published dataset. Additionally, we provide some baseline classification results. The best balanced accuracy across a 5-fold cross validation of 81.3\\,\\% was achieved with a ResNet18. The dataset can be downloaded under https://zenodo.org/records/14276415."}
{"id": "2601.18322", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2601.18322", "abs": "https://arxiv.org/abs/2601.18322", "authors": ["Thomas Deppisch", "Yang Gao", "Manan Mittal", "Benjamin Stahl", "Christoph Hold", "David Alon", "Zamir Ben-Hur"], "title": "Residual Learning for Neural Ambisonics Encoders", "comment": null, "summary": "Emerging wearable devices such as smartglasses and extended reality headsets demand high-quality spatial audio capture from compact, head-worn microphone arrays. Ambisonics provides a device-agnostic spatial audio representation by mapping array signals to spherical harmonic (SH) coefficients. In practice, however, accurate encoding remains challenging. While traditional linear encoders are signal-independent and robust, they amplify low-frequency noise and suffer from high-frequency spatial aliasing. On the other hand, neural network approaches can outperform linear encoders but they often assume idealized microphones and may perform inconsistently in real-world scenarios. To leverage their complementary strengths, we introduce a residual-learning framework that refines a linear encoder with corrections from a neural network. Using measured array transfer functions from smartglasses, we compare a UNet-based encoder from the literature with a new recurrent attention model. Our analysis reveals that both neural encoders only consistently outperform the linear baseline when integrated within the residual learning framework. In the residual configuration, both neural models achieve consistent and significant improvements across all tested metrics for in-domain data and moderate gains for out-of-domain data. Yet, coherence analysis indicates that all neural encoder configurations continue to struggle with directionally accurate high-frequency encoding."}
{"id": "2601.18473", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.18473", "abs": "https://arxiv.org/abs/2601.18473", "authors": ["Yuan Gao", "Xinyu Guo", "Wenjing Xie", "Zifan Wang", "Hongwen Yu", "Gongyang Li", "Shugong Xu"], "title": "Dynamic Channel Charting: An LSTM-AE-based Approach", "comment": "in Chinese language", "summary": "With the development of the sixth-generation (6G) communication system, Channel State Information (CSI) plays a crucial role in improving network performance. Traditional Channel Charting (CC) methods map high-dimensional CSI data to low-dimensional spaces to help reveal the geometric structure of wireless channels. However, most existing CC methods focus on learning static geometric structures and ignore the dynamic nature of the channel over time, leading to instability and poor topological consistency of the channel charting in complex environments. To address this issue, this paper proposes a novel time-series channel charting approach based on the integration of Long Short-Term Memory (LSTM) networks and Auto encoders (AE) (LSTM-AE-CC). This method incorporates a temporal modeling mechanism into the traditional CC framework, capturing temporal dependencies in CSI using LSTM and learning continuous latent representations with AE. The proposed method ensures both geometric consistency of the channel and explicit modeling of the time-varying properties. Experimental results demonstrate that the proposed method outperforms traditional CC methods in various real-world communication scenarios, particularly in terms of channel charting stability, trajectory continuity, and long-term predictability."}
{"id": "2601.18393", "categories": ["cs.SD", "cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.18393", "abs": "https://arxiv.org/abs/2601.18393", "authors": ["Junli Chen", "Changli Tang", "Yixuan Li", "Guangzhi Sun", "Chao Zhang"], "title": "OCR-Enhanced Multimodal ASR Can Read While Listening", "comment": "4 pages, 2 figures. Submitted to ICASSP 2026", "summary": "Visual information, such as subtitles in a movie, often helps automatic speech recognition. In this paper, we propose Donut-Whisper, an audio-visual ASR model with dual encoder to leverage visual information to improve speech recognition performance in both English and Chinese. Donut-Whisper combines the advantage of the linear and the Q-Former-based modality alignment structures via a cross-attention module, generating more powerful audio-visual features. Meanwhile, we propose a lightweight knowledge distillation scheme showcasing the potential of using audio-visual models to teach audio-only models to achieve better performance. Moreover, we propose a new multilingual audio-visual speech recognition dataset based on movie clips containing both Chinese and English partitions. As a result, Donut-Whisper achieved significantly better performance on both English and Chinese partition of the dataset compared to both Donut and Whisper large V3 baselines. In particular, an absolute 5.75% WER reduction and a 16.5% absolute CER reduction were achieved on the English and Chinese sets respectively compared to the Whisper ASR baseline."}
{"id": "2601.18396", "categories": ["eess.AS", "cs.CL", "cs.CV", "cs.SD"], "pdf": "https://arxiv.org/pdf/2601.18396", "abs": "https://arxiv.org/abs/2601.18396", "authors": ["Zhengyang Li", "Thomas Graave", "Björn Möller", "Zehang Wu", "Matthias Franz", "Tim Fingscheidt"], "title": "Noise-Robust AV-ASR Using Visual Features Both in the Whisper Encoder and Decoder", "comment": "accepted at ICASSP2026", "summary": "In audiovisual automatic speech recognition (AV-ASR) systems, information fusion of visual features in a pre-trained ASR has been proven as a promising method to improve noise robustness. In this work, based on the prominent Whisper ASR, first, we propose a simple and effective visual fusion method -- use of visual features both in encoder and decoder (dual-use) -- to learn the audiovisual interactions in the encoder and to weigh modalities in the decoder. Second, we compare visual fusion methods in Whisper models of various sizes. Our proposed dual-use method shows consistent noise robustness improvement, e.g., a 35% relative improvement (WER: 4.41% vs. 6.83%) based on Whisper small, and a 57% relative improvement (WER: 4.07% vs. 9.53%) based on Whisper medium, compared to typical reference middle fusion in babble noise with a signal-to-noise ratio (SNR) of 0dB. Third, we conduct ablation studies examining the impact of various module designs and fusion options. Fine-tuned on 1929 hours of audiovisual data, our dual-use method using Whisper medium achieves 4.08% (MUSAN babble noise) and 4.43% (NoiseX babble noise) average WER across various SNRs, thereby establishing a new state-of-the-art in noisy conditions on the LRS3 AV-ASR benchmark. Our code is at https://github.com/ifnspaml/Dual-Use-AVASR"}
{"id": "2601.18478", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.18478", "abs": "https://arxiv.org/abs/2601.18478", "authors": ["Jialun Kou", "Achiel Colpaert", "Zhuangzhuang Cui", "Sofie Pollin"], "title": "Dualband OFDM Delay Estimation for Multi-Target Localization", "comment": null, "summary": "Integrated localization and communication systems aim to reuse communication waveforms for simultaneous data transmission and localization, but delay resolution is fundamentally limited by the available bandwidth. In practice, large contiguous bandwidths are difficult to obtain due to hardware constraints and spectrum fragmentation. Aggregating non-contiguous narrow bands can increase the effective frequency span, but a non-contiguous frequency layout introduces challenges such as elevated sidelobes and ambiguity in delay estimation. This paper introduces a point-spread-function (PSF)-centric framework for dual-band OFDM delay estimation. We model the observed delay profile as the convolution of the true target response with a PSF determined by the dual-band subcarrier selection pattern, explicitly linking band configuration to resolution and ambiguity. To suppress PSF-induced artifacts, we adapt the RELAX algorithm for dual-band multi-target delay estimation. Simulations demonstrate improved robustness and accuracy in dual-band scenarios, supporting ILC under fragmented spectrum."}
{"id": "2601.18438", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2601.18438", "abs": "https://arxiv.org/abs/2601.18438", "authors": ["Wei Wang", "Wangyou Zhang", "Chenda Li", "Jiahe Wang", "Samuele Cornell", "Marvin Sach", "Kohei Saijo", "Yihui Fu", "Zhaoheng Ni", "Bing Han", "Xun Gong", "Mengxiao Bi", "Tim Fingscheidt", "Shinji Watanabe", "Yanmin Qian"], "title": "UrgentMOS: Unified Multi-Metric and Preference Learning for Robust Speech Quality Assessment", "comment": null, "summary": "Automatic speech quality assessment has become increasingly important as modern speech generation systems continue to advance, while human listening tests remain costly, time-consuming, and difficult to scale. Most existing learning-based assessment models rely primarily on scarce human-annotated mean opinion score (MOS) data, which limits robustness and generalization, especially when training across heterogeneous datasets. In this work, we propose UrgentMOS, a unified speech quality assessment framework that jointly learns from diverse objective and perceptual quality metrics, while explicitly tolerating the absence of arbitrary subsets of metrics during training. By leveraging complementary quality facets under heterogeneous supervision, UrgentMOS enables effective utilization of partially annotated data and improves robustness when trained on large-scale, multi-source datasets. Beyond absolute score prediction, UrgentMOS explicitly models pairwise quality preferences by directly predicting comparative MOS (CMOS), making it well suited for preference-based evaluation scenarios commonly adopted in system benchmarking. Extensive experiments across a wide range of speech quality datasets, including simulated distortions, speech enhancement, and speech synthesis, demonstrate that UrgentMOS consistently achieves state-of-the-art performance in both absolute and comparative evaluation settings."}
{"id": "2601.18535", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2601.18535", "abs": "https://arxiv.org/abs/2601.18535", "authors": ["Peter Balušík", "Pavel Rajmic"], "title": "Audio Inpainting in Time-Frequency Domain with Phase-Aware Prior", "comment": "submitted to IEEE Transactions on Audio, Speech and Language Processing", "summary": "The so-called audio inpainting problem in the time domain refers to estimating missing segments of samples within a signal. Over the years, several methods have been developed for such type of audio inpainting. In contrast to this case, a time-frequency variant of inpainting appeared in the literature, where the challenge is to reconstruct missing spectrogram columns with reliable information. We propose a method to address this time-frequency audio inpainting problem. Our approach is based on the recently introduced phase-aware signal prior that exploits an estimate of the instantaneous frequency. An optimization problem is formulated and solved using the generalized Chambolle-Pock algorithm. The proposed method is evaluated both objectively and subjectively against other time-frequency inpainting methods, specifically a deep-prior neural network and the autoregression-based approach known as Janssen-TF. Our proposed approach surpassed these methods in the objective evaluation as well as in the conducted listening test. Moreover, this outcome is achieved with a substantially reduced computational requirement compared to alternative methods."}
{"id": "2601.18539", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.18539", "abs": "https://arxiv.org/abs/2601.18539", "authors": ["Akhileswar Chowdary", "Ahmad Bazzi", "Marwa Chafii"], "title": "Hybrid Radar Fusion with Quantization: CRB-Rate Trade-offs and ADC Dynamic Range", "comment": null, "summary": "Recent advancements have underscored the relevance of low-resolution analog-to-digital converters (ADCs) in integrated sensing and communication (ISAC) systems. Nevertheless, their specific impact on hybrid radar fusion (HRF) remains largely unexplored. In HRF systems, where uplink (UL) paths carry direct and reflected signals in the same frequency band, the reflected signal is often significantly weaker, making HRF performance particularly sensitive to ADC resolution. To study this effect, we use the quantized Cramér-Rao bound (CRB) to measure sensing accuracy. This work derives an upper bound on the quantized CRB for angle of arrival (AoA) estimation and explores CRB-rate trade-offs through two formulated optimization problems. Simulation results indicate that HRF becomes infeasible when the dynamic range of the received signal exceeds the dynamic range supported by the ADC, which is inherently limited by its resolution. Furthermore, the UL communication rate does not increase significantly when the ADC resolution is raised beyond a certain threshold. These observations highlight a fundamental trade-off between sensing and communication performance: while HRF performance benefits from higher ADC resolutions, the corresponding gains in communication rate plateau. This trade-off is effectively characterized using CRB-rate boundaries derived through simulation."}
{"id": "2601.18456", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.18456", "abs": "https://arxiv.org/abs/2601.18456", "authors": ["Kohei Asai", "Wataru Nakata", "Yuki Saito", "Hiroshi Saruwatari"], "title": "Geneses: Unified Generative Speech Enhancement and Separation", "comment": "Accepted to ICASSP 2025 workshop", "summary": "Real-world audio recordings often contain multiple speakers and various degradations, which limit both the quantity and quality of speech data available for building state-of-the-art speech processing models. Although end-to-end approaches that concatenate speech enhancement (SE) and speech separation (SS) to obtain a clean speech signal for each speaker are promising, conventional SE-SS methods suffer from complex degradations beyond additive noise. To this end, we propose \\textbf{Geneses}, a generative framework to achieve unified, high-quality SE--SS. Our Geneses leverages latent flow matching to estimate each speaker's clean speech features using multi-modal diffusion Transformer conditioned on self-supervised learning representation from noisy mixture. We conduct experimental evaluation using two-speaker mixtures from LibriTTS-R under two conditions: additive-noise-only and complex degradations. The results demonstrate that Geneses significantly outperforms a conventional mask-based SE--SS method across various objective metrics with high robustness against complex degradations. Audio samples are available in our demo page."}
{"id": "2601.18766", "categories": ["eess.AS", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18766", "abs": "https://arxiv.org/abs/2601.18766", "authors": ["Parampreet Singh", "Somya Kumar", "Chaitanya Shailendra Nitawe", "Vipul Arora"], "title": "Learning to Discover: A Generalized Framework for Raga Identification without Forgetting", "comment": "Accepted at NCC 2026 conference", "summary": "Raga identification in Indian Art Music (IAM) remains challenging due to the presence of numerous rarely performed Ragas that are not represented in available training datasets. Traditional classification models struggle in this setting, as they assume a closed set of known categories and therefore fail to recognise or meaningfully group previously unseen Ragas. Recent works have tried categorizing unseen Ragas, but they run into a problem of catastrophic forgetting, where the knowledge of previously seen Ragas is diminished. To address this problem, we adopt a unified learning framework that leverages both labeled and unlabeled audio, enabling the model to discover coherent categories corresponding to the unseen Ragas, while retaining the knowledge of previously known ones. We test our model on benchmark Raga Identification datasets and demonstrate its performance in categorizing previously seen, unseen, and all Raga classes. The proposed approach surpasses the previous NCD-based pipeline even in discovering the unseen Raga categories, offering new insights into representation learning for IAM tasks."}
{"id": "2601.18643", "categories": ["eess.SP", "eess.SY"], "pdf": "https://arxiv.org/pdf/2601.18643", "abs": "https://arxiv.org/abs/2601.18643", "authors": ["Dominik Neudert-Schulz", "Thomas Dallmann"], "title": "Synchronization and Localization in Ad-Hoc ICAS Networks Using a Two-Stage Kuramoto Method", "comment": "6 pages, conference", "summary": "To enable Integrated Communications and Sensing (ICAS) in a peer-to-peer vehicular network, precise synchronization in frequency and phase among the communicating entities is required. In addition, self-driving cars need accurate position estimates of the surrounding vehicles. In this work, we propose a joint, distributed synchronization and localization scheme for a network of communicating entities. Our proposed scheme is mostly signal-agnostic and therefore can be applied to a wide range of possible ICAS signals. We also mitigate the effect of finite sampling frequencies, which otherwise would degrade the synchronization and localization performance severely."}
{"id": "2601.18694", "categories": ["cs.SD", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18694", "abs": "https://arxiv.org/abs/2601.18694", "authors": ["Aayush M. Shrestha", "Aditya Bajracharya", "Projan Shakya", "Dinesh B. Kshatri"], "title": "Neural Multi-Speaker Voice Cloning for Nepali in Low-Resource Settings", "comment": "16 pages with appendix included", "summary": "This research presents a few-shot voice cloning system for Nepali speakers, designed to synthesize speech in a specific speaker's voice from Devanagari text using minimal data. Voice cloning in Nepali remains largely unexplored due to its low-resource nature. To address this, we constructed separate datasets: untranscribed audio for training a speaker encoder and paired text-audio data for training a Tacotron2-based synthesizer. The speaker encoder, optimized with Generative End2End loss, generates embeddings that capture the speaker's vocal identity, validated through Uniform Manifold Approximation and Projection (UMAP) for dimension reduction visualizations. These embeddings are fused with Tacotron2's text embeddings to produce mel-spectrograms, which are then converted into audio using a WaveRNN vocoder. Audio data were collected from various sources, including self-recordings, and underwent thorough preprocessing for quality and alignment. Training was performed using mel and gate loss functions under multiple hyperparameter settings. The system effectively clones speaker characteristics even for unseen voices, demonstrating the feasibility of few-shot voice cloning for the Nepali language and establishing a foundation for personalized speech synthesis in low-resource scenarios."}
{"id": "2601.17086", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.17086", "abs": "https://arxiv.org/abs/2601.17086", "authors": ["Ayush Pratap Singh", "Harshit Singh", "Nityanand Mathur", "Akshat Mandloi", "Sudarshan Kamath"], "title": "SonoEdit: Null-Space Constrained Knowledge Editing for Pronunciation Correction in LLM-Based TTS", "comment": null, "summary": "Neural text-to-speech (TTS) systems systematically mispronounce low-resource proper nouns, particularly non-English names, brands, and geographic locations, due to their underrepresentation in predominantly English training corpora. Existing solutions typically rely on expensive multilingual data collection, supervised finetuning, or manual phonetic annotation, which limits the deployment of TTS systems in linguistically diverse settings. We introduce SonoEdit, a model editing technique that surgically corrects pronunciation errors in pre-trained TTS models without retraining. Instead of costly finetuning or explicit phoneme injection, we propose a parsimonious alternative based on Null-Space Pronunciation Editing, which performs a single-shot parameter update to modify the pronunciation of specific words while provably preserving all other model behavior. We first adapt Acoustic Causal Tracing to identify the Transformer layers responsible for text-to-pronunciation mapping. We then apply Null-Space Constrained Editing to compute a closed-form weight update that corrects the target pronunciation while remaining mathematically orthogonal to the subspace governing general speech generation. This constrained update steers the model's acoustic output toward a desired pronunciation exemplar while guaranteeing zero first-order change on a preserved speech corpus."}
{"id": "2601.18690", "categories": ["eess.SP", "cs.NI", "eess.SY"], "pdf": "https://arxiv.org/pdf/2601.18690", "abs": "https://arxiv.org/abs/2601.18690", "authors": ["Seyed Bagher Hashemi Natanzi", "Hossein Mohammadi", "Bo Tang", "Vuk Marojevic"], "title": "AI-Driven Fuzzing for Vulnerability Assessment of 5G Traffic Steering Algorithms", "comment": null, "summary": "Traffic Steering (TS) dynamically allocates user traffic across cells to enhance Quality of Experience (QoE), load balance, and spectrum efficiency in 5G networks. However, TS algorithms remain vulnerable to adversarial conditions such as interference spikes, handover storms, and localized outages. To address this, an AI-driven fuzz testing framework based on the Non-Dominated Sorting Genetic Algorithm II (NSGA-II) is proposed to systematically expose hidden vulnerabilities. Using NVIDIA Sionna, five TS algorithms are evaluated across six scenarios. Results show that AI-driven fuzzing detects 34.3% more total vulnerabilities and 5.8% more critical failures than traditional testing, achieving superior diversity and edge-case discovery. The observed variance in critical failure detection underscores the stochastic nature of rare vulnerabilities. These findings demonstrate that AI-driven fuzzing offers an effective and scalable validation approach for improving TS algorithm robustness and ensuring resilient 6G-ready networks."}
{"id": "2601.16989", "categories": ["eess.AS", "cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2601.16989", "abs": "https://arxiv.org/abs/2601.16989", "authors": ["Yasaman Haghbin", "Sina Rashidi", "Ali Zolnour", "Maryam Zolnoori"], "title": "The Voice of Equity: A Systematic Evaluation of Bias Mitigation Techniques for Speech-Based Cognitive Impairment Detection Across Architectures and Demographics", "comment": null, "summary": "Speech-based detection of cognitive impairment offers a scalable, non-invasive screening, yet algorithmic bias across demographic and linguistic subgroups remains critically underexplored. We present the first comprehensive fairness analysis framework for speech-based multi-class cognitive impairment detection, systematically evaluating bias mitigation across architectures, and demographic subgroups. We developed two transformer-based architectures, SpeechCARE-AGF and Whisper-LWF-LoRA, on the multilingual NIA PREPARE Challenge dataset. Unlike prior work that typically examines single mitigation techniques, we compared pre-processing, in-processing, and post-processing approaches, assessing fairness via Equality of Opportunity and Equalized Odds across gender, age, education, and language. Both models achieved strong performance (F1: SpeechCARE-AGF 70.87, Whisper-LWF-LoRA 71.46) but exhibited substantial fairness disparities. Adults >=80 showed lower sensitivity versus younger groups; Spanish speakers demonstrated reduced TPR versus English speakers. Mitigation effectiveness varied by architecture: oversampling improved SpeechCARE-AGF for older adults (80+ TPR: 46.19%=>49.97%) but minimally affected Whisper-LWF-LoRA. This study addresses a critical healthcare AI gap by demonstrating that architectural design fundamentally shapes bias patterns and mitigation effectiveness. Adaptive fusion mechanisms enable flexible responses to data interventions, while frequency reweighting offers robust improvements across architectures. Our findings establish that fairness interventions must be tailored to both model architecture and demographic characteristics, providing a systematic framework for developing equitable speech-based screening tools essential for reducing diagnostic disparities in cognitive healthcare."}
{"id": "2601.17097", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.17097", "abs": "https://arxiv.org/abs/2601.17097", "authors": ["Federico Bruzzone", "Walter Cazzola", "Matteo Brancaleoni", "Dario Pellegrino"], "title": "Sink or SWIM: Tackling Real-Time ASR at Scale", "comment": "14 pages, 7 figures", "summary": "Real-time automatic speech recognition systems are increasingly integrated into interactive applications, from voice assistants to live transcription services. However, scaling these systems to support multiple concurrent clients while maintaining low latency and high accuracy remains a major challenge. In this work, we present SWIM, a novel real-time ASR system built on top of OpenAI's Whisper model that enables true model-level parallelization for scalable, multilingual transcription. SWIM supports multiple concurrent audio streams without modifying the underlying model. It introduces a buffer merging strategy that maintains transcription fidelity while ensuring efficient resource usage. We evaluate SWIM in multi-client settings -- scaling up to 20 concurrent users -- and show that it delivers accurate real-time transcriptions in English, Italian, and Spanish, while maintaining low latency and high throughput. While Whisper-Streaming achieves a word error rate of approximately 8.2% with an average delay of approximately 3.4 s in a single-client, English-only setting, SWIM extends this capability to multilingual, multi-client environments. It maintains comparable accuracy with significantly lower delay -- around 2.4 s with 5 clients -- and continues to scale effectively up to 20 concurrent clients without degrading transcription quality and increasing overall throughput. Our approach advances scalable ASR by improving robustness and efficiency in dynamic, multi-user environments."}
{"id": "2601.18782", "categories": ["eess.SP", "eess.IV", "math.GR", "math.NA", "math.OC"], "pdf": "https://arxiv.org/pdf/2601.18782", "abs": "https://arxiv.org/abs/2601.18782", "authors": ["Felix Krahmer", "He Lyu", "Rayan Saab", "Jinna Qian", "Anna Veselovska", "Rongrong Wang"], "title": "Low-Bit Quantization of Bandlimited Graph Signals via Iterative Methods", "comment": "17 pages, 5 figures", "summary": "We study the quantization of real-valued bandlimited signals on graphs, focusing on low-bit representations. We propose iterative noise-shaping algorithms for quantization, including sampling approaches with and without vertex replacement. The methods leverage the spectral properties of the graph Laplacian and exploit graph incoherence to achieve high-fidelity approximations. Theoretical guarantees are provided for the random sampling method, and extensive numerical experiments on synthetic and real-world graphs illustrate the efficiency and robustness of the proposed schemes."}
{"id": "2601.17014", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2601.17014", "abs": "https://arxiv.org/abs/2601.17014", "authors": ["Kayley Seow", "Alexander Arovas", "Grace Steinmetz", "Emily Bick"], "title": "BickGraphing: Web-Based Application for Visual Inspection of Audio Recordings", "comment": "11 pages, 4 figures for submission in Journal of Open Research Software", "summary": "BickGraphing is a browser based research tool that enables visual inspection of acoustic recordings. The tool was built in support of visualizing crop feeding pest sounds in support of the Insect Eavesdropper project; however, it is widely applicable to all audiovisualizations in research. It allows multiple uploads of large .wav files, computes waveforms and spectrograms locally, and supports interactive exploration of audio events in time and frequency. The application is implemented as a SvelteKit and TypeScript web app with a client side signal processing pipeline using WebAssembly compiled FFmpeg and custom FFT utilities. The software is released on an open Git repository (https://github.com/bicklabuw/BickGraphing) and archived under a standard MIT license and can be reused for rapid visual quality checks of .wav recordings in insect bioacoustics and related fields. BickGraphing has the potential to be a local, easy to use coding free visualization platform for audio data in research."}
{"id": "2601.17270", "categories": ["cs.SD", "cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.17270", "abs": "https://arxiv.org/abs/2601.17270", "authors": ["Max McKinnon", "Samir Khaki", "Chandan KA Reddy", "William Huang"], "title": "Window Size Versus Accuracy Experiments in Voice Activity Detectors", "comment": null, "summary": "Voice activity detection (VAD) plays a vital role in enabling applications such as speech recognition. We analyze the impact of window size on the accuracy of three VAD algorithms: Silero, WebRTC, and Root Mean Square (RMS) across a set of diverse real-world digital audio streams. We additionally explore the use of hysteresis on top of each VAD output. Our results offer practical references for optimizing VAD systems. Silero significantly outperforms WebRTC and RMS, and hysteresis provides a benefit for WebRTC."}
{"id": "2601.17611", "categories": ["eess.AS", "cs.LG", "cs.SD", "eess.IV", "eess.SP"], "pdf": "https://arxiv.org/pdf/2601.17611", "abs": "https://arxiv.org/abs/2601.17611", "authors": ["Davide Berghi", "Philip J. B. Jackson"], "title": "ToS: A Team of Specialists ensemble framework for Stereo Sound Event Localization and Detection with distance estimation in Video", "comment": null, "summary": "Sound event localization and detection with distance estimation (3D SELD) in video involves identifying active sound events at each time frame while estimating their spatial coordinates. This multimodal task requires joint reasoning across semantic, spatial, and temporal dimensions, a challenge that single models often struggle to address effectively. To tackle this, we introduce the Team of Specialists (ToS) ensemble framework, which integrates three complementary sub-networks: a spatio-linguistic model, a spatio-temporal model, and a tempo-linguistic model. Each sub-network specializes in a unique pair of dimensions, contributing distinct insights to the final prediction, akin to a collaborative team with diverse expertise. ToS has been benchmarked against state-of-the-art audio-visual models for 3D SELD on the DCASE2025 Task 3 Stereo SELD development set, consistently outperforming existing methods across key metrics. Future work will extend this proof of concept by strengthening the specialists with appropriate tasks, training, and pre-training curricula."}
{"id": "2601.17080", "categories": ["eess.AS", "cs.AI", "cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2601.17080", "abs": "https://arxiv.org/abs/2601.17080", "authors": ["Seung Gyu Jeong", "Seong-Eun Kim"], "title": "PC-MCL: Patient-Consistent Multi-Cycle Learning with multi-label bias correction for respiratory sound classification", "comment": null, "summary": "Automated respiratory sound classification supports the diagnosis of pulmonary diseases. However, many deep models still rely on cycle-level analysis and suffer from patient-specific overfitting. We propose PC-MCL (Patient-Consistent Multi-Cycle Learning) to address these limitations by utilizing three key components: multi-cycle concatenation, a 3-label formulation, and a patient-matching auxiliary task. Our work resolves a multi-label distributional bias in respiratory sound classification, a critical issue inherent to applying multi-cycle concatenation with the conventional 2-label formulation (crackle, wheeze). This bias manifests as a systematic loss of normal signal information when normal and abnormal cycles are combined. Our proposed 3-label formulation (normal, crackle, wheeze) corrects this by preserving information from all constituent cycles in mixed samples. Furthermore, the patient-matching auxiliary task acts as a multi-task regularizer, encouraging the model to learn more robust features and improving generalization. On the ICBHI 2017 benchmark, PC-MCL achieves an ICBHI Score of 65.37%, outperforming existing baselines. Ablation studies confirm that all three components are essential, working synergistically to improve the detection of abnormal respiratory events."}
{"id": "2601.17517", "categories": ["cs.SD", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.17517", "abs": "https://arxiv.org/abs/2601.17517", "authors": ["Luca Cerovaz", "Michele Mancusi", "Emanuele Rodolà"], "title": "EuleroDec: A Complex-Valued RVQ-VAE for Efficient and Robust Audio Coding", "comment": "Accepted at ICASSP 2026", "summary": "Audio codecs power discrete music generative modelling, music streaming, and immersive media by shrinking PCM audio to bandwidth-friendly bitrates. Recent works have gravitated towards processing in the spectral domain; however, spectrogram domains typically struggle with phase modeling, which is naturally complex-valued. Most frequency-domain neural codecs either disregard phase information or encode it as two separate real-valued channels, limiting spatial fidelity. This entails the need to introduce adversarial discriminators at the expense of convergence speed and training stability to compensate for the inadequate representation power of the audio signal. In this work we introduce an end-to-end complex-valued RVQ-VAE audio codec that preserves magnitude-phase coupling across the entire analysis-quantization-synthesis pipeline and removes adversarial discriminators and diffusion post-filters. Without GANs or diffusion, we match or surpass much longer-trained baselines in-domain and reach SOTA out-of-domain performance on phase coherence and waveform fidelity. Compared to standard baselines that train for hundreds of thousands of steps, our model, which reduces the training budget by an order of magnitude, is markedly more compute-efficient while preserving high perceptual quality."}
{"id": "2601.17085", "categories": ["eess.AS", "cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2601.17085", "abs": "https://arxiv.org/abs/2601.17085", "authors": ["Esther Sun", "Abinay Reddy Naini", "Carlos Busso"], "title": "Recovering Performance in Speech Emotion Recognition from Discrete Tokens via Multi-Layer Fusion and Paralinguistic Feature Integration", "comment": "Accepted to ICASSP 2026", "summary": "Discrete speech tokens offer significant advantages for storage and language model integration, but their application in speech emotion recognition (SER) is limited by paralinguistic information loss during quantization. This paper presents a comprehensive investigation of discrete tokens for SER. Using a fine-tuned WavLM-Large model, we systematically quantify performance degradation across different layer configurations and k-means quantization granularities. To recover the information loss, we propose two key strategies: (1) attention-based multi-layer fusion to recapture complementary information from different layers, and (2) integration of openSMILE features to explicitly reintroduce paralinguistic cues. We also compare mainstream neural codec tokenizers (SpeechTokenizer, DAC, EnCodec) and analyze their behaviors when fused with acoustic features. Our findings demonstrate that through multi-layer fusion and acoustic feature integration, discrete tokens can close the performance gap with continuous representations in SER tasks."}
{"id": "2601.17645", "categories": ["cs.SD", "cs.CL", "cs.CV", "cs.MM", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.17645", "abs": "https://arxiv.org/abs/2601.17645", "authors": ["Xilin Jiang", "Qiaolin Wang", "Junkai Wu", "Xiaomin He", "Zhongweiyang Xu", "Yinghao Ma", "Minshuo Piao", "Kaiyi Yang", "Xiuwen Zheng", "Riki Shimizu", "Yicong Chen", "Arsalan Firoozi", "Gavin Mischler", "Sukru Samet Dindar", "Richard Antonello", "Linyang He", "Tsun-An Hsieh", "Xulin Fan", "Yulun Wu", "Yuesheng Ma", "Chaitanya Amballa", "Weixiong Chen", "Jiarui Hai", "Ruisi Li", "Vishal Choudhari", "Cong Han", "Yinghao Aaron Li", "Adeen Flinker", "Mounya Elhilali", "Emmanouil Benetos", "Mark Hasegawa-Johnson", "Romit Roy Choudhury", "Nima Mesgarani"], "title": "AVMeme Exam: A Multimodal Multilingual Multicultural Benchmark for LLMs' Contextual and Cultural Knowledge and Thinking", "comment": "avmemeexam.github.io/public", "summary": "Internet audio-visual clips convey meaning through time-varying sound and motion, which extend beyond what text alone can represent. To examine whether AI models can understand such signals in human cultural contexts, we introduce AVMeme Exam, a human-curated benchmark of over one thousand iconic Internet sounds and videos spanning speech, songs, music, and sound effects. Each meme is paired with a unique Q&A assessing levels of understanding from surface content to context and emotion to usage and world knowledge, along with metadata such as original year, transcript, summary, and sensitivity. We systematically evaluate state-of-the-art multimodal large language models (MLLMs) alongside human participants using this benchmark. Our results reveal a consistent limitation: current models perform poorly on textless music and sound effects, and struggle to think in context and in culture compared to surface content. These findings highlight a key gap in human-aligned multimodal intelligence and call for models that can perceive contextually and culturally beyond the surface of what they hear and see. Project page: avmemeexam.github.io/public"}
{"id": "2601.17557", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2601.17557", "abs": "https://arxiv.org/abs/2601.17557", "authors": ["Aref Farhadipour", "Ming Jin", "Valeriia Vyshnevetska", "Xiyang Li", "Elisa Pellegrino", "Srikanth Madikeri"], "title": "Spoofing-Aware Speaker Verification via Wavelet Prompt Tuning and Multi-Model Ensembles", "comment": "System description of the T03 team in the WildSpoof Challenge at ICASSP 2026", "summary": "This paper describes the UZH-CL system submitted to the SASV section of the WildSpoof 2026 challenge. The challenge focuses on the integrated defense against generative spoofing attacks by requiring the simultaneous verification of speaker identity and audio authenticity. We proposed a cascaded Spoofing-Aware Speaker Verification framework that integrates a Wavelet Prompt-Tuned XLSR-AASIST countermeasure with a multi-model ensemble. The ASV component utilizes the ResNet34, ResNet293, and WavLM-ECAPA-TDNN architectures, with Z-score normalization followed by score averaging. Trained on VoxCeleb2 and SpoofCeleb, the system obtained a Macro a-DCF of 0.2017 and a SASV EER of 2.08%. While the system achieved a 0.16% EER in spoof detection on the in-domain data, results on unseen datasets, such as the ASVspoof5, highlight the critical challenge of cross-domain generalization."}
{"id": "2601.17679", "categories": ["cs.SD", "cs.CL", "cs.CV", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.17679", "abs": "https://arxiv.org/abs/2601.17679", "authors": ["Md Sazzadul Islam Ridoy", "Mubaswira Ibnat Zidney", "Sumi Akter", "Md. Aminur Rahman"], "title": "BanglaRobustNet: A Hybrid Denoising-Attention Architecture for Robust Bangla Speech Recognition", "comment": null, "summary": "Bangla, one of the most widely spoken languages, remains underrepresented in state-of-the-art automatic speech recognition (ASR) research, particularly under noisy and speaker-diverse conditions. This paper presents BanglaRobustNet, a hybrid denoising-attention framework built on Wav2Vec-BERT, designed to address these challenges. The architecture integrates a diffusion-based denoising module to suppress environmental noise while preserving Bangla-specific phonetic cues, and a contextual cross-attention module that conditions recognition on speaker embeddings for robustness across gender, age, and dialects. Trained end-to-end with a composite objective combining CTC loss, phonetic consistency, and speaker alignment, BanglaRobustNet achieves substantial reductions in word error rate (WER) and character error rate (CER) compared to Wav2Vec-BERT and Whisper baselines. Evaluations on Mozilla Common Voice Bangla and augmented noisy speech confirm the effectiveness of our approach, establishing BanglaRobustNet as a robust ASR system tailored to low-resource, noise-prone linguistic settings."}
{"id": "2601.17611", "categories": ["eess.AS", "cs.LG", "cs.SD", "eess.IV", "eess.SP"], "pdf": "https://arxiv.org/pdf/2601.17611", "abs": "https://arxiv.org/abs/2601.17611", "authors": ["Davide Berghi", "Philip J. B. Jackson"], "title": "ToS: A Team of Specialists ensemble framework for Stereo Sound Event Localization and Detection with distance estimation in Video", "comment": null, "summary": "Sound event localization and detection with distance estimation (3D SELD) in video involves identifying active sound events at each time frame while estimating their spatial coordinates. This multimodal task requires joint reasoning across semantic, spatial, and temporal dimensions, a challenge that single models often struggle to address effectively. To tackle this, we introduce the Team of Specialists (ToS) ensemble framework, which integrates three complementary sub-networks: a spatio-linguistic model, a spatio-temporal model, and a tempo-linguistic model. Each sub-network specializes in a unique pair of dimensions, contributing distinct insights to the final prediction, akin to a collaborative team with diverse expertise. ToS has been benchmarked against state-of-the-art audio-visual models for 3D SELD on the DCASE2025 Task 3 Stereo SELD development set, consistently outperforming existing methods across key metrics. Future work will extend this proof of concept by strengthening the specialists with appropriate tasks, training, and pre-training curricula."}
{"id": "2601.17690", "categories": ["cs.SD", "cs.AI", "cs.IR", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.17690", "abs": "https://arxiv.org/abs/2601.17690", "authors": ["Ziling Gong", "Yunyan Ouyang", "Iram Kamdar", "Melody Ma", "Hongjie Chen", "Franck Dernoncourt", "Ryan A. Rossi", "Nesreen K. Ahmed"], "title": "Segment Length Matters: A Study of Segment Lengths on Audio Fingerprinting Performance", "comment": null, "summary": "Audio fingerprinting provides an identifiable representation of acoustic signals, which can be later used for identification and retrieval systems. To obtain a discriminative representation, the input audio is usually segmented into shorter time intervals, allowing local acoustic features to be extracted and analyzed. Modern neural approaches typically operate on short, fixed-duration audio segments, yet the choice of segment duration is often made heuristically and rarely examined in depth. In this paper, we study how segment length affects audio fingerprinting performance. We extend an existing neural fingerprinting architecture to adopt various segment lengths and evaluate retrieval accuracy across different segment lengths and query durations. Our results show that short segment lengths (0.5-second) generally achieve better performance. Moreover, we evaluate LLM capacity in recommending the best segment length, which shows that GPT-5-mini consistently gives the best suggestions across five considerations among three studied LLMs. Our findings provide practical guidance for selecting segment duration in large-scale neural audio retrieval systems."}
{"id": "2601.17640", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2601.17640", "abs": "https://arxiv.org/abs/2601.17640", "authors": ["Anfeng Xu", "Tiantian Feng", "Somer Bishop", "Catherine Lord", "Shrikanth Narayanan"], "title": "End-to-End Joint ASR and Speaker Role Diarization with Child-Adult Interactions", "comment": "Under review for IEEE", "summary": "Accurate transcription and speaker diarization of child-adult spoken interactions are crucial for developmental and clinical research. However, manual annotation is time-consuming and challenging to scale. Existing automated systems typically rely on cascaded speaker diarization and speech recognition pipelines, which can lead to error propagation. This paper presents a unified end-to-end framework that extends the Whisper encoder-decoder architecture to jointly model ASR and child-adult speaker role diarization. The proposed approach integrates: (i) a serialized output training scheme that emits speaker tags and start/end timestamps, (ii) a lightweight frame-level diarization head that enhances speaker-discriminative encoder representations, (iii) diarization-guided silence suppression for improved temporal precision, and (iv) a state-machine-based forced decoding procedure that guarantees structurally valid outputs. Comprehensive evaluations on two datasets demonstrate consistent and substantial improvements over two cascaded baselines, achieving lower multi-talker word error rates and demonstrating competitive diarization accuracy across both Whisper-small and Whisper-large models. These findings highlight the effectiveness and practical utility of the proposed joint modeling framework for generating reliable, speaker-attributed transcripts of child-adult interactions at scale. The code and model weights are publicly available"}
{"id": "2601.17711", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.17711", "abs": "https://arxiv.org/abs/2601.17711", "authors": ["Chengqian Jiang", "Jie Zhang", "Haoyin Yan"], "title": "CaSNet: Compress-and-Send Network Based Multi-Device Speech Enhancement Model for Distributed Microphone Arrays", "comment": "this paper has been accept by ICASSP2026", "summary": "Distributed microphone array (DMA) is a promising next-generation platform for speech interaction, where speech enhancement (SE) is still required to improve the speech quality in noisy cases. Existing SE methods usually first gather raw waveforms at a fusion center (FC) from all devices and then design a multi-microphone model, causing high bandwidth and energy costs. In this work, we propose a \\emph{Compress-and-Send Network (CaSNet)} for resource-constrained DMAs, where one microphone serves as the FC and reference. Each of other devices encodes the measured raw data into a feature matrix, which is then compressed by singular value decomposition (SVD) to produce a more compact representation. The received features at the FC are aligned via cross window query with respect to the reference, followed by neural decoding to yield spatially coherent enhanced speech. Experiments on multiple datasets show that the proposed CaSNet can save the data amount with a negligible impact on the performance compared to the uncompressed case. The reproducible code is available at https://github.com/Jokejiangv/CaSNet."}
{"id": "2601.17901", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2601.17901", "abs": "https://arxiv.org/abs/2601.17901", "authors": ["Yuanchao Li"], "title": "Speech Emotion Recognition with ASR Integration", "comment": "PhD Thesis", "summary": "Speech Emotion Recognition (SER) plays a pivotal role in understanding human communication, enabling emotionally intelligent systems, and serving as a fundamental component in the development of Artificial General Intelligence (AGI). However, deploying SER in real-world, spontaneous, and low-resource scenarios remains a significant challenge due to the complexity of emotional expression and the limitations of current speech and language technologies. This thesis investigates the integration of Automatic Speech Recognition (ASR) into SER, with the goal of enhancing the robustness, scalability, and practical applicability of emotion recognition from spoken language."}
{"id": "2601.17902", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.17902", "abs": "https://arxiv.org/abs/2601.17902", "authors": ["Wenjie Tian", "Bingshen Mu", "Guobin Ma", "Xuelong Geng", "Zhixian Zhao", "Lei Xie"], "title": "dLLM-ASR: A Faster Diffusion LLM-based Framework for Speech Recognition", "comment": null, "summary": "Automatic speech recognition (ASR) systems based on large language models (LLMs) achieve superior performance by leveraging pretrained LLMs as decoders, but their token-by-token generation mechanism leads to inference latency that grows linearly with sequence length. Meanwhile, discrete diffusion large language models (dLLMs) offer a promising alternative, enabling high-quality parallel sequence generation with pretrained decoders. However, directly applying native text-oriented dLLMs to ASR leads to a fundamental mismatch between open-ended text generation and the acoustically conditioned transcription paradigm required by ASR. As a result, it introduces unnecessary difficulty and computational redundancy, such as denoising from pure noise, inflexible generation lengths, and fixed denoising steps. We propose dLLM-ASR, an efficient dLLM-based ASR framework that formulates dLLM's decoding as a prior-guided and adaptive denoising process. It leverages an ASR prior to initialize the denoising process and provide an anchor for sequence length. Building upon this prior, length-adaptive pruning dynamically removes redundant tokens, while confidence-based denoising allows converged tokens to exit the denoising loop early, enabling token-level adaptive computation. Experiments demonstrate that dLLM-ASR achieves recognition accuracy comparable to autoregressive LLM-based ASR systems and delivers a 4.44$\\times$ inference speedup, establishing a practical and efficient paradigm for ASR."}
{"id": "2601.18010", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2601.18010", "abs": "https://arxiv.org/abs/2601.18010", "authors": ["Jingyao Wu", "Grace Lin", "Yinuo Song", "Rosalind Picard"], "title": "AmbER$^2$: Dual Ambiguity-Aware Emotion Recognition Applied to Speech and Text", "comment": "Accepted in ICASSP 2026", "summary": "Emotion recognition is inherently ambiguous, with uncertainty arising both from rater disagreement and from discrepancies across modalities such as speech and text. There is growing interest in modeling rater ambiguity using label distributions. However, modality ambiguity remains underexplored, and multimodal approaches often rely on simple feature fusion without explicitly addressing conflicts between modalities. In this work, we propose AmbER$^2$, a dual ambiguity-aware framework that simultaneously models rater-level and modality-level ambiguity through a teacher-student architecture with a distribution-wise training objective. Evaluations on IEMOCAP and MSP-Podcast show that AmbER$^2$ consistently improves distributional fidelity over conventional cross-entropy baselines and achieves performance competitive with, or superior to, recent state-of-the-art systems. For example, on IEMOCAP, AmbER$^2$ achieves relative improvements of 20.3% on Bhattacharyya coefficient (0.83 vs. 0.69), 13.6% on R$^2$ (0.67 vs. 0.59), 3.8% on accuracy (0.683 vs. 0.658), and 4.5% on F1 (0.675 vs. 0.646). Further analysis across ambiguity levels shows that explicitly modeling ambiguity is particularly beneficial for highly uncertain samples. These findings highlight the importance of jointly addressing rater and modality ambiguity when building robust emotion recognition systems."}
{"id": "2601.18086", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.18086", "abs": "https://arxiv.org/abs/2601.18086", "authors": ["Mengcheng Huang", "Xue Zhou", "Chen Xu", "Dapeng Man"], "title": "From Human Speech to Ocean Signals: Transferring Speech Large Models for Underwater Acoustic Target Recognition", "comment": null, "summary": "Underwater acoustic target recognition (UATR) plays a vital role in marine applications but remains challenging due to limited labeled data and the complexity of ocean environments. This paper explores a central question: can speech large models (SLMs), trained on massive human speech corpora, be effectively transferred to underwater acoustics? To investigate this, we propose UATR-SLM, a simple framework that reuses the speech feature pipeline, adapts the SLM as an acoustic encoder, and adds a lightweight classifier.Experiments on the DeepShip and ShipsEar benchmarks show that UATR-SLM achieves over 99% in-domain accuracy, maintains strong robustness across variable signal lengths, and reaches up to 96.67% accuracy in cross-domain evaluation. These results highlight the strong transferability of SLMs to UATR, establishing a promising paradigm for leveraging speech foundation models in underwater acoustics."}
{"id": "2601.18037", "categories": ["eess.AS", "cs.AI", "cs.SD"], "pdf": "https://arxiv.org/pdf/2601.18037", "abs": "https://arxiv.org/abs/2601.18037", "authors": ["Yiwen Shao", "Yong Xu", "Sanjeev Khudanpur", "Dong Yu"], "title": "SpatialEmb: Extract and Encode Spatial Information for 1-Stage Multi-channel Multi-speaker ASR on Arbitrary Microphone Arrays", "comment": "SLT 2024", "summary": "Spatial information is a critical clue for multi-channel multi-speaker target speech recognition. Most state-of-the-art multi-channel Automatic Speech Recognition (ASR) systems extract spatial features only during the speech separation stage, followed by standard single-channel ASR on the separated speech. This approach results in an inefficient, lengthy pipeline and sub-optimal ASR performance due to the accumulated errors from preprocessing modules. Furthermore, most spatial feature extraction methods depend on the knowledge of speaker positions and microphone topology, making the systems reliant on specific settings and challenging to adapt to new equipment. In this work, we propose a solution to these issues with a lightweight embedding module named SpatialEmb, which extracts and encodes spatial information directly for the ASR model, supporting both fixed and arbitrary microphone topology. We conduct comprehensive experiments on AliMeeting, a real meeting corpus, to determine the optimal model design for SpatialEmb in terms of both performance and efficiency. Our best model trained with 105 hours Train-Ali-far achieves 17.04% and 20.32% character error rates (CER) on the Eval and Test sets, establishing a new state-of-the-art result with the same training data."}
{"id": "2601.18184", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.18184", "abs": "https://arxiv.org/abs/2601.18184", "authors": ["Zhiliang Peng", "Jianwei Yu", "Yaoyao Chang", "Zilong Wang", "Li Dong", "Yingbo Hao", "Yujie Tu", "Chenyu Yang", "Wenhui Wang", "Songchen Xu", "Yutao Sun", "Hangbo Bao", "Weijiang Xu", "Yi Zhu", "Zehua Wang", "Ting Song", "Yan Xia", "Zewen Chi", "Shaohan Huang", "Liang Wang", "Chuang Ding", "Shuai Wang", "Xie Chen", "Furu Wei"], "title": "VIBEVOICE-ASR Technical Report", "comment": null, "summary": "This report presents VibeVoice-ASR, a general-purpose speech understanding framework built upon VibeVoice, designed to address the persistent challenges of context fragmentation and multi-speaker complexity in long-form audio (e.g., meetings, podcasts) that remain despite recent advancements in short-form speech recognition. Unlike traditional pipelined approaches that rely on audio chunking, VibeVoice-ASRsupports single-pass processing for up to 60 minutes of audio. It unifies Automatic Speech Recognition, Speaker Diarization, and Timestamping into a single end-to-end generation task. In addition, VibeVoice-ASR supports over 50 languages, requires no explicit language setting, and natively handles code-switching within and across utterances. Furthermore, we introduce a prompt-based context injection mechanism that allows users to supply customized conetxt, significantly improving accuracy on domain-specific terminology and polyphonic character disambiguation."}
{"id": "2601.18094", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2601.18094", "abs": "https://arxiv.org/abs/2601.18094", "authors": ["Zhichao Wang", "Tao Li", "Wenshuo Ge", "Zihao Cui", "Shilei Zhang", "Junlan Feng"], "title": "OneVoice: One Model, Triple Scenarios-Towards Unified Zero-shot Voice Conversion", "comment": "Work in progress", "summary": "Recent progress of voice conversion~(VC) has achieved a new milestone in speaker cloning and linguistic preservation. But the field remains fragmented, relying on specialized models for linguistic-preserving, expressive, and singing scenarios. We propose OneVoice, a unified zero-shot framework capable of handling all three scenarios within a single model. OneVoice is built upon a continuous language model trained with VAE-free next-patch diffusion, ensuring high fidelity and efficient sequence modeling. Its core design for unification lies in a Mixture-of-Experts (MoE) designed to explicitly model shared conversion knowledge and scenario-specific expressivity. Expert selection is coordinated by a dual-path routing mechanism, including shared expert isolation and scenario-aware domain expert assignment with global-local cues. For precise conditioning, scenario-specific prosodic features are fused into each layer via a gated mechanism, allowing adaptive usage of prosody information. Furthermore, to enable the core idea and alleviate the imbalanced issue (abundant speech vs. scarce singing), we adopt a two-stage progressive training that includes foundational pre-training and scenario enhancement with LoRA-based domain experts. Experiments show that OneVoice matches or surpasses specialized models across all three scenarios, while verifying flexible control over scenarios and offering a fast decoding version as few as 2 steps. Code and model will be released soon."}
{"id": "2601.18220", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.18220", "abs": "https://arxiv.org/abs/2601.18220", "authors": ["Bingshen Mu", "Xian Shi", "Xiong Wang", "Hexin Liu", "Jin Xu", "Lei Xie"], "title": "LLM-ForcedAligner: A Non-Autoregressive and Accurate LLM-Based Forced Aligner for Multilingual and Long-Form Speech", "comment": null, "summary": "Forced alignment (FA) predicts start and end timestamps for words or characters in speech, but existing methods are language-specific and prone to cumulative temporal shifts. The multilingual speech understanding and long-sequence processing abilities of speech large language models (SLLMs) make them promising for FA in multilingual, crosslingual, and long-form speech settings. However, directly applying the next-token prediction paradigm of SLLMs to FA results in hallucinations and slow inference. To bridge the gap, we propose LLM-ForcedAligner, reformulating FA as a slot-filling paradigm: timestamps are treated as discrete indices, and special timestamp tokens are inserted as slots into the transcript. Conditioned on the speech embeddings and the transcript with slots, the SLLM directly predicts the time indices at slots. During training, causal attention masking with non-shifted input and label sequences allows each slot to predict its own timestamp index based on itself and preceding context, with loss computed only at slot positions. Dynamic slot insertion enables FA at arbitrary positions. Moreover, non-autoregressive inference is supported, avoiding hallucinations and improving speed. Experiments across multilingual, crosslingual, and long-form speech scenarios show that LLM-ForcedAligner achieves a 69%~78% relative reduction in accumulated averaging shift compared with prior methods. The checkpoint and inference code will be released later."}
{"id": "2601.18266", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2601.18266", "abs": "https://arxiv.org/abs/2601.18266", "authors": ["Steven Vander Eeckt", "Hugo Van hamme"], "title": "Efficient Rehearsal for Continual Learning in ASR via Singular Value Tuning", "comment": "Accepted for publication in IEEE Transactions on Audio, Speech, and Language Processing", "summary": "Continual Learning (CL) in Automatic Speech Recognition (ASR) suffers from catastrophic forgetting when adapting to new tasks, domains, or speakers. A common strategy to mitigate this is to store a subset of past data in memory for rehearsal. However, rehearsal-based methods face key limitations: storing data is often costly, infeasible with pre-trained models, or restricted by privacy regulations. Running existing rehearsal-based methods with smaller memory sizes to alleviate these issues usually leads to degraded performance.\n  We propose a rehearsal-based CL method that remains effective even with minimal memory. It operates in two stages: first, fine-tuning on the new task; second, applying Singular Value Decomposition (SVD) to the changes in linear layers and, in a parameter-efficient manner, retraining only gating vectors on the singular values, which control to extent to which updates from the first stage are accepted, using rehearsal. We extensively test and analyze our method on two monolingual and two multilingual benchmarks. Our method reduces forgetting and outperforms state-of-the-art CL approaches for ASR, even when limited to a single utterance per previous task."}
{"id": "2601.18393", "categories": ["cs.SD", "cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.18393", "abs": "https://arxiv.org/abs/2601.18393", "authors": ["Junli Chen", "Changli Tang", "Yixuan Li", "Guangzhi Sun", "Chao Zhang"], "title": "OCR-Enhanced Multimodal ASR Can Read While Listening", "comment": "4 pages, 2 figures. Submitted to ICASSP 2026", "summary": "Visual information, such as subtitles in a movie, often helps automatic speech recognition. In this paper, we propose Donut-Whisper, an audio-visual ASR model with dual encoder to leverage visual information to improve speech recognition performance in both English and Chinese. Donut-Whisper combines the advantage of the linear and the Q-Former-based modality alignment structures via a cross-attention module, generating more powerful audio-visual features. Meanwhile, we propose a lightweight knowledge distillation scheme showcasing the potential of using audio-visual models to teach audio-only models to achieve better performance. Moreover, we propose a new multilingual audio-visual speech recognition dataset based on movie clips containing both Chinese and English partitions. As a result, Donut-Whisper achieved significantly better performance on both English and Chinese partition of the dataset compared to both Donut and Whisper large V3 baselines. In particular, an absolute 5.75% WER reduction and a 16.5% absolute CER reduction were achieved on the English and Chinese sets respectively compared to the Whisper ASR baseline."}
{"id": "2601.18295", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2601.18295", "abs": "https://arxiv.org/abs/2601.18295", "authors": ["Milan Marocchi", "Matthew Fynn", "Yue Rong"], "title": "Noise-Robust Contrastive Learning with an MFCC-Conformer For Coronary Artery Disease Detection", "comment": "This paper has been accepted for presentation at ICASSP 2026. \\c{opyright} 2026 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses. 5 pages, 1 figure", "summary": "Cardiovascular diseases (CVD) are the leading cause of death worldwide, with coronary artery disease (CAD) comprising the largest subcategory of CVDs. Recently, there has been increased focus on detecting CAD using phonocardiogram (PCG) signals, with high success in clinical environments with low noise and optimal sensor placement. Multichannel techniques have been found to be more robust to noise; however, achieving robust performance on real-world data remains a challenge. This work utilises a novel multichannel energy-based noisy-segment rejection algorithm, using heart and noise-reference microphones, to discard audio segments with large amounts of nonstationary noise before training a deep learning classifier. This conformer-based classifier takes mel-frequency cepstral coefficients (MFCCs) from multiple channels, further helping improve the model's noise robustness. The proposed method achieved 78.4% accuracy and 78.2% balanced accuracy on 297 subjects, representing improvements of 4.1% and 4.3%, respectively, compared to training without noisy-segment rejection."}
{"id": "2601.18456", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.18456", "abs": "https://arxiv.org/abs/2601.18456", "authors": ["Kohei Asai", "Wataru Nakata", "Yuki Saito", "Hiroshi Saruwatari"], "title": "Geneses: Unified Generative Speech Enhancement and Separation", "comment": "Accepted to ICASSP 2025 workshop", "summary": "Real-world audio recordings often contain multiple speakers and various degradations, which limit both the quantity and quality of speech data available for building state-of-the-art speech processing models. Although end-to-end approaches that concatenate speech enhancement (SE) and speech separation (SS) to obtain a clean speech signal for each speaker are promising, conventional SE-SS methods suffer from complex degradations beyond additive noise. To this end, we propose \\textbf{Geneses}, a generative framework to achieve unified, high-quality SE--SS. Our Geneses leverages latent flow matching to estimate each speaker's clean speech features using multi-modal diffusion Transformer conditioned on self-supervised learning representation from noisy mixture. We conduct experimental evaluation using two-speaker mixtures from LibriTTS-R under two conditions: additive-noise-only and complex degradations. The results demonstrate that Geneses significantly outperforms a conventional mask-based SE--SS method across various objective metrics with high robustness against complex degradations. Audio samples are available in our demo page."}
{"id": "2601.18322", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2601.18322", "abs": "https://arxiv.org/abs/2601.18322", "authors": ["Thomas Deppisch", "Yang Gao", "Manan Mittal", "Benjamin Stahl", "Christoph Hold", "David Alon", "Zamir Ben-Hur"], "title": "Residual Learning for Neural Ambisonics Encoders", "comment": null, "summary": "Emerging wearable devices such as smartglasses and extended reality headsets demand high-quality spatial audio capture from compact, head-worn microphone arrays. Ambisonics provides a device-agnostic spatial audio representation by mapping array signals to spherical harmonic (SH) coefficients. In practice, however, accurate encoding remains challenging. While traditional linear encoders are signal-independent and robust, they amplify low-frequency noise and suffer from high-frequency spatial aliasing. On the other hand, neural network approaches can outperform linear encoders but they often assume idealized microphones and may perform inconsistently in real-world scenarios. To leverage their complementary strengths, we introduce a residual-learning framework that refines a linear encoder with corrections from a neural network. Using measured array transfer functions from smartglasses, we compare a UNet-based encoder from the literature with a new recurrent attention model. Our analysis reveals that both neural encoders only consistently outperform the linear baseline when integrated within the residual learning framework. In the residual configuration, both neural models achieve consistent and significant improvements across all tested metrics for in-domain data and moderate gains for out-of-domain data. Yet, coherence analysis indicates that all neural encoder configurations continue to struggle with directionally accurate high-frequency encoding."}
{"id": "2601.18396", "categories": ["eess.AS", "cs.CL", "cs.CV", "cs.SD"], "pdf": "https://arxiv.org/pdf/2601.18396", "abs": "https://arxiv.org/abs/2601.18396", "authors": ["Zhengyang Li", "Thomas Graave", "Björn Möller", "Zehang Wu", "Matthias Franz", "Tim Fingscheidt"], "title": "Noise-Robust AV-ASR Using Visual Features Both in the Whisper Encoder and Decoder", "comment": "accepted at ICASSP2026", "summary": "In audiovisual automatic speech recognition (AV-ASR) systems, information fusion of visual features in a pre-trained ASR has been proven as a promising method to improve noise robustness. In this work, based on the prominent Whisper ASR, first, we propose a simple and effective visual fusion method -- use of visual features both in encoder and decoder (dual-use) -- to learn the audiovisual interactions in the encoder and to weigh modalities in the decoder. Second, we compare visual fusion methods in Whisper models of various sizes. Our proposed dual-use method shows consistent noise robustness improvement, e.g., a 35% relative improvement (WER: 4.41% vs. 6.83%) based on Whisper small, and a 57% relative improvement (WER: 4.07% vs. 9.53%) based on Whisper medium, compared to typical reference middle fusion in babble noise with a signal-to-noise ratio (SNR) of 0dB. Third, we conduct ablation studies examining the impact of various module designs and fusion options. Fine-tuned on 1929 hours of audiovisual data, our dual-use method using Whisper medium achieves 4.08% (MUSAN babble noise) and 4.43% (NoiseX babble noise) average WER across various SNRs, thereby establishing a new state-of-the-art in noisy conditions on the LRS3 AV-ASR benchmark. Our code is at https://github.com/ifnspaml/Dual-Use-AVASR"}
{"id": "2601.18535", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2601.18535", "abs": "https://arxiv.org/abs/2601.18535", "authors": ["Peter Balušík", "Pavel Rajmic"], "title": "Audio Inpainting in Time-Frequency Domain with Phase-Aware Prior", "comment": "submitted to IEEE Transactions on Audio, Speech and Language Processing", "summary": "The so-called audio inpainting problem in the time domain refers to estimating missing segments of samples within a signal. Over the years, several methods have been developed for such type of audio inpainting. In contrast to this case, a time-frequency variant of inpainting appeared in the literature, where the challenge is to reconstruct missing spectrogram columns with reliable information. We propose a method to address this time-frequency audio inpainting problem. Our approach is based on the recently introduced phase-aware signal prior that exploits an estimate of the instantaneous frequency. An optimization problem is formulated and solved using the generalized Chambolle-Pock algorithm. The proposed method is evaluated both objectively and subjectively against other time-frequency inpainting methods, specifically a deep-prior neural network and the autoregression-based approach known as Janssen-TF. Our proposed approach surpassed these methods in the objective evaluation as well as in the conducted listening test. Moreover, this outcome is achieved with a substantially reduced computational requirement compared to alternative methods."}

<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 23]
- [eess.AS](#eess.AS) [Total: 17]
- [cs.SD](#cs.SD) [Total: 18]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [Inverter-Based Differential Amplifiers With Back-Gate Feedback Linearization](https://arxiv.org/abs/2601.17129)
*Eric Danson,Jeffrey S. Walling*

Main category: eess.SP

TL;DR: 在22nm FD-SOI技术中，将共源放大器输出反馈到背栅端，利用负反馈的线性化效应，实现增益与负载无关、无额外噪声、线性度显著提升


<details>
  <summary>Details</summary>
Motivation: 利用FD-SOI技术中背栅控制的特性，通过负反馈改善放大器的线性度，解决传统放大器线性度不足的问题

Method: 将共源放大器的输出信号反馈到背栅端，形成负反馈结构，在22nm FD-SOI工艺中进行分析和仿真验证

Result: 背栅反馈使整体增益基本与负载无关，不引入额外噪声，线性度通过背栅电压增益显著改善，三阶截点(IP3)提升至少60倍

Conclusion: 背栅反馈是FD-SOI技术中改善放大器线性度的有效方法，特别适用于反相器基或互补共源差分放大器设计

Abstract: Feeding the common-source amplifier output to the back-gate terminal in fully depleted silicon on insulator (FD-SOI) technology exploits the linearizing effect of negative feedback. Analysis and simulation results in 22 nm FD-SOI show that back-gate feedback sets the overall gain approximately independent of the load, contributes no additional noise, and improves linearity by the back-gate voltage gain. Third-order intercept point (IP3) enhancement is at least $60\times$ compared to without feedback in inverter-based, or complementary common-source, differential amplifiers.

</details>


### [2] [Data-Efficient Physics-Informed Learning to Model Synchro-Waveform Dynamics of Grid-Integrated Inverter-Based Resources](https://arxiv.org/abs/2601.17154)
*Shivanshu Tripathi,Hossein Mohsenzadeh Yazdi,Maziar Raissi,Hamed Mohsenian-Rad*

Main category: eess.SP

TL;DR: 提出基于物理信息机器学习(PIML)的同步波形分析框架，利用电路物理知识补偿数据稀缺性，从少量网络扰动中估计逆变器终端电流响应


<details>
  <summary>Details</summary>
Motivation: 传统相量和SCADA测量无法准确捕捉逆变器快速暂态动态，同步波形测量单元(WMUs)虽提供高分辨率数据，但暂态模型学习仍受限于网络扰动稀缺性和逆变器非线性动态复杂性

Method: 开发数据高效的物理信息机器学习框架，利用已知电路关系约束学习过程，考虑电路参数已知和未知两种情况，后者联合学习逆变器暂态动态和电路参数

Result: 使用WMU扰动数据进行案例研究，相比纯数据驱动基线方法，在多种采样率下均获得更低的电流估计误差，且所需训练事件显著减少

Conclusion: 物理信息机器学习框架能有效利用电路物理知识补偿数据稀缺性，提高逆变器暂态动态估计的准确性和数据效率

Abstract: Inverter-based resources (IBRs) exhibit fast transient dynamics during network disturbances, which often cannot be properly captured by phasor and SCADA measurements. This shortcoming has recently been addressed with the advent of waveform measurement units (WMUs), which provide high-resolution, time-synchronized raw voltage and current waveform samples from multiple locations in the power system. However, transient model learning based on synchro-waveform measurements remains constrained by the scarcity of network disturbances and the complexity of the underlying nonlinear dynamics of IBRs. We propose to address these problems by developing a data-efficient physics-informed machine learning (PIML) framework for synchro-waveform analytics that estimates the IBR terminal current response from only a few network disturbance signatures. Here, the physics of the electrical circuits are used to compensate for limited data availability by constraining the learning process through known circuit relationships. Two cases are considered, with known and unknown circuit parameters. In the latter case, the framework jointly learns the transient dynamics of the IBRs and the parameters of the electrical circuit. Case studies using WMU disturbance data across multiple sampling rates shows consistently lower current estimation error with substantially fewer training events than a purely data-driven baseline.

</details>


### [3] [RIS-Enabled Spoofing Against Adversary Sensing: CRB-Maximizing Design and Decoying Analysis](https://arxiv.org/abs/2601.17320)
*Ioannis Gavras,Giuseppe Thadeu Freitas de Abreu,George C. Alexandropoulos*

Main category: eess.SP

TL;DR: RIS透明覆盖用户设备时能欺骗单站雷达，通过相位调控在真实反射方向形成深零陷，在诱饵方向产生强峰值，使雷达无法可靠感知目标。


<details>
  <summary>Details</summary>
Motivation: 研究可重构智能表面(RIS)在透明覆盖用户设备时欺骗敌方单站雷达系统的能力，解决雷达对目标的可靠探测问题。

Method: 提出紧凑RIS核模型，将雷达角度响应与RIS相位剖面显式关联；基于该模型设计RIS欺骗方案，在真实反射方向强制严格零陷，在诱饵方向最大化信道增益；使用点式和角度范围鲁棒准则量化欺骗能力，提出配置无关的放置评分指导诱饵选择。

Result: 数值结果显示在真实反射到达角形成深零陷，同时在诱饵方向产生显著峰值，使敌方雷达的最大似然感知变得不可靠。

Conclusion: RIS能够有效欺骗单站雷达系统，通过相位调控在真实目标方向形成零陷，在诱饵方向增强信号，破坏雷达的可靠感知能力。

Abstract: This paper studies the capability of a Reconfigurable Intelligent Surface (RIS), when transparently covering a User Equipment (UE), to deceive an adversary monostatic radar system. A compact RIS kernel model that explicitly links the radar's angular response to the RIS phase profile is introduced, enabling an analytical investigation of the Angle of Arrival (AoA) estimation accuracy with respect to the kernel's power. This model is also leveraged to formulate an RIS-based spoofing design with the dual objective to enforce strict nulls around the UE's true reflection AoA and maximize the channel gain towards a decoy direction. The RIS's deception capability is quantified using point-wise and angle-range robust criteria, and a configuration-independent placement score guiding decoy selection is proposed. Selected numerical results confirm deep nulls at the true reflection AoA together with a pronounced decoy peak, rendering maximum-likelihood sensing at the adversary radar unreliable.

</details>


### [4] [Semantic-Aware Task Clustering for Federated Cooperative Multi-Task Semantic Communication](https://arxiv.org/abs/2601.17419)
*Ahmad Halimi Razlighi,Pallavi Dhingra,Edgar Beck,Bho Matthiesen,Armin Dekorsy*

Main category: eess.SP

TL;DR: 提出基于联邦学习的分布式协同多任务语义通信框架，通过语义感知任务聚类解决异构任务间的负向信息传递问题，在低轨卫星网络中验证了性能优势。


<details>
  <summary>Details</summary>
Motivation: 现有协同多任务语义通信框架不适用于分布式多用户场景（如卫星网络），且协同多任务可能产生破坏性影响，需要解决异构任务间的负向信息传递问题。

Method: 1) 将协同多任务语义通信扩展到分布式设置，提出基于联邦学习的框架；2) 在联邦学习过程中集成语义感知任务聚类方法，基于信息论方法确保建设性合作；3) 在低维语义域而非高维数据/特征空间进行任务关系识别。

Result: 在低轨卫星网络设置下的仿真结果表明，该方法相比未聚类的联邦学习和单任务语义通信具有更好的性能表现。

Conclusion: 提出的基于联邦学习的分布式协同多任务语义通信框架有效解决了分布式场景下的应用问题，通过语义感知任务聚类确保了异构任务间的建设性合作，在卫星网络等场景中具有实用价值。

Abstract: Task-oriented semantic communication (SemCom) prioritizes task execution over accurate symbol reconstruction and is well-suited to emerging intelligent applications. Cooperative multi-task SemCom (CMT-SemCom) further improves task execution performance. However, [1] demonstrates that cooperative multi-tasking can be either constructive or destructive. Moreover, the existing CMT-SemCom framework is not directly applicable to distributed multi-user scenarios, such as non-terrestrial satellite networks, where each satellite employs an individual semantic encoder. In this paper, we extend our earlier CMT-SemCom framework to distributed settings by proposing a federated learning (FL) based CMT-SemCom that enables cooperative multi-tasking across distributed users. Moreover, to address performance degradation caused by negative information transfer among heterogeneous tasks, we propose a semantic-aware task clustering method integrated in the FL process to ensure constructive cooperation based on an information-theoretic approach. Unlike common clustering methods that rely on high-dimensional data or feature space similarity, our proposed approach operates in the low-dimensional semantic domain to identify meaningful task relationships. Simulation results based on a LEO satellite network setup demonstrate the effectiveness of our approach and performance gain over unclustered FL and individual single-task SemCom.

</details>


### [5] [ME-WARD: A multimodal ergonomic analysis tool for musculoskeletal risk assessment from inertial and video data in working plac](https://arxiv.org/abs/2601.17571)
*Javier González-Alonso,Paula Martín-Tapia,David González-Ortega,Míriam Antón-Rodríguez,Francisco Javier Díaz-Pernas,Mario Martínez-Zarzuela*

Main category: eess.SP

TL;DR: ME-WARD是一个多模态工效学评估系统，实现了RULA方法，能够处理来自不同运动捕捉系统（IMU和深度学习姿态跟踪）的关节角度数据，在工业环境中验证了其可靠性。


<details>
  <summary>Details</summary>
Motivation: 传统工效学评估通常依赖专有系统，限制了RULA方法的广泛应用。需要开发一个灵活的系统，能够整合多种运动捕捉技术，为资源受限的工业环境提供可扩展、经济高效的解决方案。

Method: 开发了ME-WARD系统，实现RULA方法，能够处理来自IMU系统和深度学习单目3D姿态估计系统的关节角度数据。系统在传送带组装任务中进行验证，比较了IMU金标准和单目系统的性能。

Result: ME-WARD能够产生可靠的RULA评分，对于屈曲主导的运动与IMU系统结果高度一致，与单目系统性能相当。尽管在跟踪侧向和旋转运动方面存在限制，但整体表现良好。

Conclusion: ME-WARD展示了将多种运动捕捉技术整合到统一工效学评估流程中的潜力，支持包括低成本视频系统在内的多样化输入源，为资源受限的工业环境提供了可扩展的解决方案。

Abstract: This study presents ME-WARD (Multimodal Ergonomic Workplace Assessment and Risk from Data), a novel system for ergonomic assessment and musculoskeletal risk evaluation that implements the Rapid Upper Limb Assessment (RULA) method. ME-WARD is designed to process joint angle data from motion capture systems, including inertial measurement unit (IMU)-based setups, and deep learning human body pose tracking models. The tool's flexibility enables ergonomic risk assessment using any system capable of reliably measuring joint angles, extending the applicability of RULA beyond proprietary setups. To validate its performance, the tool was tested in an industrial setting during the assembly of conveyor belts, which involved high-risk tasks such as inserting rods and pushing conveyor belt components. The experiments leveraged gold standard IMU systems alongside a state-of-the-art monocular 3D pose estimation system. The results confirmed that ME-WARD produces reliable RULA scores that closely align with IMU-derived metrics for flexion-dominated movements and comparable performance with the monocular system, despite limitations in tracking lateral and rotational motions. This work highlights the potential of integrating multiple motion capture technologies into a unified and accessible ergonomic assessment pipeline. By supporting diverse input sources, including low-cost video-based systems, the proposed multimodal approach offers a scalable, cost-effective solution for ergonomic assessments, paving the way for broader adoption in resource-constrained industrial environments.

</details>


### [6] [Development of an end-to-end hardware and software pipeline for affordable and feasible ergonomics assessment in the automotive industry](https://arxiv.org/abs/2601.17574)
*Javier González-Alonso,Cristina Simón-Martínez,Míriam Antón-Rodríguez,David González-Ortega,Francisco Javier Díaz-Pernas,Mario Martínez-Zarzuela*

Main category: eess.SP

TL;DR: 提出端到端硬件软件管道，自动化工业工作场所人体工学评估，使用低成本IMU传感器和开源工具，在汽车工厂验证效果，与黄金标准系统结果相当。


<details>
  <summary>Details</summary>
Motivation: 解决工业工作场所中工作相关肌肉骨骼疾病（WMSDs）风险评估问题，避免使用专有技术的"黑箱"安全决策问题，提供低成本解决方案来减少肌肉骨骼疾病和相关病假。

Method: 开发模块化端到端管道，包括定制IMU传感器、实时工人运动采集工具、逆运动学处理和RULA报告生成，基于Unity3D和OpenSim等免费工具，避免专有技术问题。

Result: 在汽车工厂高风险WMSDs工作场所测试，与黄金标准系统相比：关节角度交叉相关0.95，肘部RMSE<10，肩部RMSE<12，全局RULA评分差异<5%，结果相当。

Conclusion: 提供低成本WMSDs风险评估解决方案，可减少肌肉骨骼疾病和病假，促进可穿戴系统在人体工学分析中的研究和普及，使工作场所预防系统能应用于不同工业环境。

Abstract: An end-to-end hardware-software pipeline is introduced to automatize ergonomics assessment in industrial workplaces. The proposed modular solution can interoperate with commercial systems throughout the ergonomics assessment phases involved in the process. The pipeline includes custom-designed Inertial Measurement Unit (IMU) sensors, two real-time worker movement acquisition tools, inverse kinematics processing and Rapid Upper Limb Assessment (RULA) report generation. It is based on free tools such as Unity3D and OpenSim to avoid the problems derived from using proprietary technologies, such as security decisions being made under "black box" conditions. Experiments were conducted in an automotive factory in a workplace with WMSDs risk among workers. The proposed solution obtained comparable results to a gold standard solution, reaching measured joint angles a 0.95 cross-correlation and a Root Mean Square Error (RMSE) lower than 10 for elbows and 12 for shoulders between both systems. In addition, the global RULA score difference is lower than 5% between both systems. This work provides a low-cost solution for WMSDs risk assessment in the workplace to reduce musculoskeletal disorders and associated sick leave in industry, impacting the health of workers in the long term. Our study can ease further research and popularize the use of wearable systems for ergonomics analysis allowing these workplace prevention systems to reach different industrial environments.

</details>


### [7] [Reliable Quasi-Static Post-Fall Floor-Occupancy Detection Using Low-Cost Millimetre-Wave Radar](https://arxiv.org/abs/2601.17710)
*Huy Trinh,Phuong Thai,Elliot Creager,George Shaker*

Main category: eess.SP

TL;DR: 使用60GHz FMCW雷达在长期护理设施中进行稳健的跌倒后地板占用检测，通过Capon/MVDR波束成形改进检测性能


<details>
  <summary>Details</summary>
Motivation: 随着人口老龄化，长期护理设施需要可靠的环境监测系统来检测跌倒等关键事件，但现有系统存在误报率高、检测不确定等问题，导致护理人员警报疲劳

Method: 使用商用60GHz FMCW雷达，在模拟长期护理设施的室内环境中部署。比较供应商提供的数字波束成形(DBF)管道与提出的基于Capon/MVDR波束成形的预处理方法，并应用单元平均恒虚警率(CA-CFAR)检测器

Result: 提出的方法将平均帧正率从0.823(DBF)提升到0.916，在7名参与者上验证了改进的检测性能

Conclusion: 基于Capon/MVDR波束成形的预处理方法能有效改善跌倒后地板占用检测，为长期护理设施提供更可靠的监测解决方案

Abstract: As the population ages rapidly, long-term care (LTC) facilities across North America face growing pressure to monitor residents safely while keeping staff workload manageable. Falls are among the most critical events to monitor due to their timely response requirement, yet frequent false alarms or uncertain detections can overwhelm caregivers and contribute to alarm fatigue. This motivates the design of reliable, whole end-to-end ambient monitoring systems from occupancy and activity awareness to fall and post-fall detection. In this paper, we focus on robust post-fall floor-occupancy detection using an off-the-shelf 60 GHz FMCW radar and evaluate its deployment in a realistic, furnished indoor environment representative of LTC facilities. Post-fall detection is challenging since motion is minimal, and reflections from the floor and surrounding objects can dominate the radar signal return. We compare a vendor-provided digital beamforming (DBF) pipeline against a proposed preprocessing approach based on Capon or minimum variance distortionless response (MVDR) beamforming. A cell-averaging constant false alarm rate (CA-CFAR) detector is applied and evaluated on the resulting range-azimuth maps across 7 participants. The proposed method improves the mean frame-positive rate from 0.823 (DBF) to 0.916 (Proposed).

</details>


### [8] [Doppler-Domain Respiratory Amplification for Semi-Static Human Occupancy Detection Using Low-Resolution SIMO FMCW Radar](https://arxiv.org/abs/2601.17721)
*Huy Trinh,Elliot Creager,George Shaker*

Main category: eess.SP

TL;DR: RASSO是一种可逆多普勒域非线性重映射方法，通过在自适应Capon波束成形前对慢时FFT进行0m/s附近的密度化处理，提高低分辨率雷达对半静态人体存在的检测性能。


<details>
  <summary>Details</summary>
Motivation: 雷达传感作为隐私保护的摄像头和可穿戴设备替代方案，在长期护理等场景中具有前景。但低分辨率SIMO FMCW雷达检测半静态存在（躺、坐、站等仅有微动）很困难，因为接近零多普勒的能量常被静态杂波淹没。

Method: 提出RASSO（呼吸放大半静态占用）方法，这是一种可逆的多普勒域非线性重映射技术，在自适应Capon波束成形前对慢时FFT（多普勒）网格在0 m/s附近进行密度化处理。

Result: 在真实养老院数据集上，RASSO-RA显著改善检测性能：AUC=0.981，在FAR=1%/5%时召回率=0.920/0.947。数据驱动模型也受益：帧级CNN达到95-99%准确率，序列级CNN-LSTM达到99.4-99.6%准确率。配对会话级bootstrap测试显示宏F1显著提升2.6-3.6点。

Conclusion: 简单的多普勒域扭曲处理在空间处理前能够实质性改善低分辨率雷达在真实临床环境中的半静态占用检测性能，为隐私保护的长期监测提供了有效解决方案。

Abstract: Radar-based sensing is a promising privacy-preserving alternative to cameras and wearables in settings such as long-term care. Yet detecting quasi-static presence (lying, sitting, or standing with only subtle micro-motions) is difficult for low-resolution SIMO FMCW radar because near-zero Doppler energy is often buried under static clutter. We present Respiratory-Amplification Semi-Static Occupancy (RASSO), an invertible Doppler-domain non-linear remapping that densifies the slow-time FFT (Doppler) grid around 0 m/s before adaptive Capon beamforming. The resulting range-azimuth (RA) maps exhibit higher effective SNR, sharper target peaks, and lower background variance, making thresholding and learning more reliable. On a real nursing-home dataset collected with a short-range 1Tx-3Rx radar, RASSO-RA improves classical detection performance, achieving AUC = 0.981 and recall = 0.920/0.947 at FAR = 1%/5%, outperforming conventional Capon processing and a recent baseline. RASSO-RA also benefits data-driven models: a frame-based CNN reaches 95-99% accuracy and a sequence-based CNN-LSTM reaches 99.4-99.6% accuracy across subjects. A paired session-level bootstrap test confirms statistically significant macro-F1 gains of 2.6-3.6 points (95% confidence intervals above zero) over the non-warped pipeline. These results show that simple Doppler-domain warping before spatial processing can materially improve semi-static occupancy detection with low-resolution radar in real clinical environments.

</details>


### [9] [S-MDMA: Sensitivity-Aware Model Division Multiple Access for Satellite-Ground Semantic Communication](https://arxiv.org/abs/2601.17731)
*Hui Cao,Rui Meng,Shujun Han,Song Gao,Xiaodong Xu,Ping Zhang*

Main category: eess.SP

TL;DR: 提出S-MDMA框架解决卫星-地面语义通信中的带宽限制和用户干扰问题，通过语义提取合并、敏感度排序和正交嵌入实现高效多用户传输


<details>
  <summary>Details</summary>
Motivation: 卫星-地面语义通信在通信与AI融合中至关重要，但带宽限制和用户干扰对语义保真度和传输鲁棒性构成挑战，需要解决多用户场景下的效率问题

Method: 基于MDMA架构进行语义提取和合并，提出语义敏感度排序算法选择性保留关键特征，采用正交嵌入语义特征和引入多用户重建损失函数进行联合优化

Result: 在开源数据集上的实验表明，S-MDMA在不同信噪比条件和用户配置下均优于现有方法，实现了鲁棒且高保真的重建

Conclusion: S-MDMA框架有效解决了卫星-地面语义通信中的带宽限制和用户干扰问题，为通信与AI融合提供了高效的多用户传输解决方案

Abstract: Satellite-ground semantic communication (SemCom) is expected to play a pivotal role in convergence of communication and AI (ComAI), particularly in enabling intelligent and efficient multi-user data transmission. However, the inherent bandwidth constraints and user interference in satellite-ground systems pose significant challenges to semantic fidelity and transmission robustness. To address these issues, we propose a sensitivity-aware model division multiple access (S-MDMA) framework tailored for bandwidth-limited multi-user scenarios. The proposed framework first performs semantic extraction and merging based on the MDMA architecture to consolidate redundant information. To further improve transmission efficiency, a semantic sensitivity sorting algorithm is presented, which can selectively retain key semantic features. In addition, to mitigate inter-user interference, the framework incorporates orthogonal embedding of semantic features and introduces a multi-user reconstruction loss function to guide joint optimization. Experimental results on open-source datasets demonstrate that S-MDMA consistently outperforms existing methods, achieving robust and high-fidelity reconstruction across diverse signal-to-noise ratio (SNR) conditions and user configurations.

</details>


### [10] [Over-The-Air Extreme Learning Machines with XL Reception via Nonlinear Cascaded Metasurfaces](https://arxiv.org/abs/2601.17749)
*Kyriakos Stylianopoulos,Mattia Fabiani,Giulia Torcolacci,Davide Dardari,George C. Alexandropoulos*

Main category: eess.SP

TL;DR: 本文提出了一种基于可编程超表面的XL-MIMO系统，该系统作为极限学习机，能够完全在无线信道中执行二元分类任务，并可通过闭式解进行训练。


<details>
  <summary>Details</summary>
Motivation: 面向目标通信范式需要在无线传输数据上应用机器学习推理。现有研究尝试在MIMO系统物理层直接实现推理ML模型，但面临显著挑战。本文旨在利用可编程超表面技术，将OTA学习能力嵌入未来通信系统。

Method: 提出XL-MIMO-ELM系统：接收端架构包含密集并行放置的衍射层XL超表面，后接单个射频链。前层面向MIMO信道，由具有固定非线性响应的相同单元组成；其余层由可调线性响应元件构成，用于近似OTA训练ELM权重。

Result: 数值研究表明，在超表面元件的XL体制下，所提XL-MIMO-ELM系统在不同数据集和无线场景中，性能可与数字化和理想化ML模型相媲美。

Conclusion: 该工作证明了将OTA学习能力嵌入未来通信系统的可行性，为在物理层实现机器学习推理提供了新途径。

Abstract: The recently envisioned goal-oriented communications paradigm calls for the application of inference on wirelessly transferred data via Machine Learning (ML) tools. An emerging research direction deals with the realization of inference ML models directly in the physical layer of Multiple-Input Multiple-Output (MIMO) systems, which, however, entails certain significant challenges. In this paper, leveraging the technology of programmable MetaSurfaces (MSs), we present an eXtremely Large (XL) MIMO system that acts as an Extreme Learning Machine (ELM) performing binary classification tasks completely Over-The-Air (OTA), which can be trained in closed form. The proposed system comprises a receiver architecture consisting of densely parallel placed diffractive layers of XL MSs followed by a single reception radio-frequency chain. The front layer facing the MIMO channel consists of identical unit cells of a fixed NonLinear (NL) response, while the remaining layers of elements of tunable linear responses are utilized to approximate OTA the trained ELM weights. Our numerical investigations showcase that, in the XL regime of MS elements, the proposed XL-MIMO-ELM system achieves performance comparable to that of digital and idealized ML models across diverse datasets and wireless scenarios, thereby demonstrating the feasibility of embedding OTA learning capabilities into future communication systems.

</details>


### [11] [Ampli-Flection for 6G: Active-RIS-Aided Aerial Backhaul with Full 3D Coverage](https://arxiv.org/abs/2601.17751)
*Hong-Bae Jeon,Chan-Byoung Chae*

Main category: eess.SP

TL;DR: 提出一种新型空中回程架构，利用空中主动可重构智能表面实现6G网络中包括无人机基站和地面用户的全3D覆盖，相比现有2D覆盖方案具有显著性能优势。


<details>
  <summary>Details</summary>
Motivation: 现有空中RIS方案主要局限于2D覆盖（仅服务地面用户）或被动操作，无法满足6G网络中无人机基站和地面用户的全3D覆盖需求，特别是在城市突发流量场景下需要克服信号阻塞问题。

Method: 将主动RIS集成到高空平台，通过信号反射和放大克服阻塞；联合优化空中平台部署、阵列分区和RIS相位配置，最大化无人机基站能效。

Result: 仿真结果表明，所提方法显著优于基准方案，展示了在6G网络中提供具有全面3D覆盖的弹性回程连接的强大潜力。

Conclusion: 提出的空中主动RIS架构能够实现能量高效、全3D覆盖的回程连接，为6G网络中处理突发流量和克服信号阻塞提供了有效解决方案。

Abstract: In this paper, we propose a novel aerial backhaul architecture that employs an aerial active reconfigurable intelligent surface (RIS) to achieve energy-efficient, {full 3D coverage including UAV-BSs and ground users in 6G wireless networks}. Unlike prior aerial-RIS approaches limited to {2D coverage with only servicing ground users} or passive operation, the proposed design integrates an active-RIS onto a high-altitude aerial platform, enabling reliable line-of-sight links and overcoming multiplicative fading through amplification. In a scenario with UAV-BSs deployed to handle sudden traffic surges in urban areas, the aerial-active-RIS both reflects and amplifies backhaul signals to overcome blockage. We jointly optimize the aerial platform placement, array partitioning, and RIS phase configuration to maximize UAV-BS energy-efficiency. Simulation results confirm that the proposed method significantly outperforms benchmarks, demonstrating its strong potential to deliver resilient backhaul connectivity with comprehensive 3D coverage in 6G networks.

</details>


### [12] [Context-Aware Iterative Token Detection and Masked Transmission for Wireless Token Communication](https://arxiv.org/abs/2601.17770)
*Junyong Shin,Joohyuk Park,Jihong Park,Jinho Choi,Yo-Seb Jeon*

Main category: eess.SP

TL;DR: 提出基于预训练掩码语言模型的上下文感知令牌通信框架，通过联合利用上下文先验和信道观测进行迭代令牌检测，并引入上下文感知掩码策略跳过可预测令牌以降低传输速率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型成功将令牌作为自然语言表示的紧凑且有意义的单元，这启发了无线信道中的令牌通信，将令牌视为无线传输的基本单元。

Method: 1) 使用预训练掩码语言模型作为发射机和接收机共享的上下文概率模型；2) 接收端开发基于贝叶斯视角的迭代令牌检测方法，联合利用MLM引导的上下文先验和信道观测；3) 发射端引入上下文感知掩码策略，跳过高度可预测的令牌传输以降低传输速率。

Result: 仿真结果表明，该框架显著提高了重构句子质量，并在各种信道条件下支持有效的速率自适应。

Conclusion: 提出的上下文感知令牌通信框架通过利用语言模型的上下文知识，实现了更高效的无线通信，在保持通信质量的同时降低了传输需求。

Abstract: The success of large-scale language models has established tokens as compact and meaningful units for natural-language representation, which motivates token communication over wireless channels, where tokens are considered fundamental units for wireless transmission. We propose a context-aware token communication framework that uses a pretrained masked language model (MLM) as a shared contextual probability model between the transmitter (Tx) and receiver (Rx). At Rx, we develop an iterative token detection method that jointly exploits MLM-guided contextual priors and channel observations based on a Bayesian perspective. At Tx, we additionally introduce a context-aware masking strategy which skips highly predictable token transmission to reduce transmission rate. Simulation results demonstrate that the proposed framework substantially improves reconstructed sentence quality and supports effective rate adaptation under various channel conditions.

</details>


### [13] [Comparison of Single Carrier FTN-QAM and PCS-QAM for Amplifier-less Coherent Communication Systems](https://arxiv.org/abs/2601.17803)
*Dongdong Zou,Fan Li,Wei Wang,Zhongxing Tian,Yuheng Liu,Gangxiang Shen,Yi Cai*

Main category: eess.SP

TL;DR: FTN-16QAM在无放大器短距离相干通信系统中比PCS-64QAM具有约0.9dB的功率裕度优势


<details>
  <summary>Details</summary>
Motivation: 比较FTN-QAM和PCS-QAM在无放大器短距离相干通信系统中的性能，为系统设计提供参考

Method: 应用相位跟踪部分响应DFE和turbo均衡策略，对FTN-16QAM和PCS-64QAM进行性能比较

Result: FTN-16QAM比PCS-64QAM具有约0.9dB的功率裕度优势

Conclusion: 在无放大器短距离相干通信系统中，FTN-16QAM相比PCS-64QAM具有显著的性能优势

Abstract: A performance comparison of FTN-QAM and PCS-QAM for amplifier-less short-reach coherent communication systems is provided. With the applications of phase tracking partial response DFE and turbo equalization strategy, FTN-16QAM exhibits about 0.9dB power margin advantage over PCS-64QAM.

</details>


### [14] [Movable Antenna-Enhanced Near-Field Flexible Beamforming: Performance Analysis and Optimization](https://arxiv.org/abs/2601.17825)
*Shun Yang,Xin Wei,Nianbing Su,Weidong Mei,Zhi Chen,Boyu Ning*

Main category: eess.SP

TL;DR: 该论文研究可移动天线在近场波束赋形中的应用，针对波束置零和多波束形成两种场景，提出优化算法并分析位置误差影响。


<details>
  <summary>Details</summary>
Motivation: 可移动天线相比固定位置天线能更灵活地调整波束赋形，但在近场波束赋形中的应用潜力尚未充分探索，特别是在波束置零和多波束形成这两种典型场景中。

Method: 首先分析特殊案例，证明通过合理放置MA可以在特定方向产生零点或全增益；针对一般情况，提出离散采样方法和交替优化算法；考虑实际定位误差，引入泰勒级数近似分析性能影响。

Result: 数值结果验证了理论发现，证明所提算法在波束置零和多波束形成场景中的有效性，并分析了天线位置误差对优化性能的影响。

Conclusion: 可移动天线在近场波束赋形中具有显著优势，所提出的优化算法能有效解决非凸优化问题，为实际应用提供了可行的解决方案。

Abstract: As an emerging wireless communication technology, movable antennas (MAs) offer the ability to adjust the spatial correlation of steering vectors, enabling more flexible beamforming compared to fixed-position antennas (FPAs). In this paper, we investigate the use of MAs for two typical near-field beamforming scenarios: beam nulling and multi-beam forming. In the first scenario, we aim to jointly optimize the positions of multiple MAs and the beamforming vector to maximize the beam gain toward a desired direction while nulling interference toward multiple undesired directions. In the second scenario, the objective is to maximize the minimum beam gain among all the above directions. However, both problems are non-convex and challenging to solve optimally. To gain insights, we first analyze several special cases and show that, with proper positioning of the MAs, directing the beam toward a specific direction can lead to nulls or full gains in other directions in the two scenarios, respectively. For the general cases, we propose a discrete sampling method and an alternating optimization algorithm to obtain high-quality suboptimal solutions to the two formulated problems. Furthermore, considering the practical limitations in antenna positioning accuracy, we analyze the impact of position errors on the performance of the optimized beamforming and MA positions, by introducing a Taylor series approximation for the near-field beam gain at each target. Numerical results validate our theoretical findings and demonstrate the effectiveness of our proposed algorithms.

</details>


### [15] [A Physics-Informed Digital Twin Framework for Calibrated Sim-to-Real FMCW Radar Occupancy Estimation](https://arxiv.org/abs/2601.17871)
*Huy Trinh,Sebastian Ratto,Elliot Creager,George Shaker*

Main category: eess.SP

TL;DR: 提出轻量级仿真到真实框架，仅需物理几何模拟器和少量未标注真实校准数据，即可实现可靠的FMCW雷达占用检测和人数统计


<details>
  <summary>Details</summary>
Motivation: 直接从真实测量中学习鲁棒的雷达感知模型成本高昂，需要受控实验、重复校准和大量标注。需要一种更高效的训练方法。

Method: 提出校准域随机化(CDR)方法，将模拟的距离-多普勒(RD)图的全局噪声统计与真实环境对齐，同时保留有区别性的微多普勒结构。使用ResNet18模型在CDR调整后的模拟数据上训练。

Result: 在真实世界评估中，仅使用CDR调整模拟数据训练的模型在占用检测上达到97%准确率，在人数统计上达到72%准确率，优于光线追踪基线模拟和传统随机域随机化基线。

Conclusion: 提出的轻量级sim2real框架能够有效利用物理几何模拟器和少量未标注真实数据，实现可靠的雷达感知任务，显著降低了训练成本。

Abstract: Learning robust radar perception models directly from real measurements is costly due to the need for controlled experiments, repeated calibration, and extensive annotation. This paper proposes a lightweight simulation-to-real (sim2real) framework that enables reliable Frequency Modulated Continuous Wave (FMCW) radar occupancy detection and people counting using only a physics-informed geometric simulator and a small unlabeled real calibration set. We introduce calibrated domain randomization (CDR) to align the global noise-floor statistics of simulated range-Doppler (RD) maps with those observed in real environments while preserving discriminative micro-Doppler structure. Across real-world evaluations, ResNet18 models trained purely on CDR-adjusted simulation achieve 97 percent accuracy for occupancy detection and 72 percent accuracy for people counting, outperforming ray-tracing baseline simulation and conventional random domain randomization baselines.

</details>


### [16] [Integrated Channel Estimation and Sensing for Near-Field ELAA Systems](https://arxiv.org/abs/2601.18333)
*Jionghui Wang,Jun Fang,Hongbin Li,Boyu Ning*

Main category: eess.SP

TL;DR: 提出基于张量分解的ELAA-OFDM系统上行信道估计方法，利用CPD和BTD模型分别处理LoS和NLoS场景，显著降低训练开销并提高估计精度。


<details>
  <summary>Details</summary>
Motivation: 解决ELAA-OFDM系统中多用户信道估计问题，传统正交导频方案训练开销过大，需要开发能支持更多用户且降低训练开销的非正交导频传输方案。

Method: 将接收信号建模为三阶低秩张量：LoS场景使用CPD分解，NLoS场景使用BTD分解。采用ALS算法进行CPD，NLS算法进行BTD，从恢复的因子矩阵中提取信道参数。

Result: 仿真结果表明，该方法比基于压缩感知的方法获得显著更高的信道估计精度。在LoS场景下还能精确确定用户位置，且导频符号数可远小于用户数。

Conclusion: 提出的张量分解框架能有效解决ELAA-OFDM系统的多用户信道估计问题，显著降低训练开销，为大规模天线系统提供了高效的信道估计方案。

Abstract: In this paper, we study the problem of uplink channel estimation for near-filed orthogonal frequency division multiplexing (OFDM) systems, where a base station (BS), equipped with an extremely large-scale antenna array (ELAA), serves multiple users over the same time-frequency resource block. A non-orthogonal pilot transmission scheme is considered to accommodate a larger number of users that can be supported by ELAA systems without incurring an excessive amount of training overhead. To facilitate efficient multi-user channel estimation, we express the received signal as a third-order low-rank tensor, which admits a canonical polyadic decomposition (CPD) model for line-of-sight (LoS) scenarios and a block term decomposition (BTD) model for non-line-of-sight (NLoS) scenarios. An alternating least squares (ALS) algorithm and a non-linear least squares (NLS) algorithm are employed to perform CPD and BTD, respectively. Channel parameters are then efficiently extracted from the recovered factor matrices. By exploiting the geometry of the propagation paths in the estimated channel, users' positions can be precisely determined in LoS scenarios. Moreover, our uniqueness analysis shows that the proposed tensor-based joint multi-user channel estimation framework is effective even when the number of pilot symbols is much smaller than the number of users, revealing its potential in training overhead reduction. Simulation results demonstrate that the proposed method achieves markedly higher channel estimation accuracy than compressed sensing (CS)-based approaches.

</details>


### [17] [Deep Reinforcement Learning for Hybrid RIS Assisted MIMO Communications](https://arxiv.org/abs/2601.18453)
*Phuong Nam Tran,Nhan Thanh Nguyen,Markku Juntti*

Main category: eess.SP

TL;DR: 提出基于深度强化学习的混合可重构智能表面优化框架，通过离线训练学习信道状态信息到波束成形和HRIS配置的直接映射，实现低复杂度、低延迟的优化


<details>
  <summary>Details</summary>
Motivation: 混合可重构智能表面结合了被动反射和主动信号放大，能增强无线系统性能。但联合优化发射波束成形与HRIS反射放大系数以最大化频谱效率是一个非凸问题，传统迭代方法计算复杂度高

Method: 提出深度强化学习框架，学习从信道状态信息到近最优发射波束成形和HRIS配置的直接映射。模型离线训练后，能以低复杂度和低延迟计算配置

Result: 仿真结果表明，所提DRL方法能达到交替优化基准方法95%的频谱效率，同时显著降低计算复杂度

Conclusion: 深度强化学习为混合可重构智能表面优化提供了一种高效解决方案，在保持性能的同时大幅降低计算复杂度，适合实际部署

Abstract: Hybrid reconfigurable intelligent surfaces (HRIS) enhance wireless systems by combining passive reflection with active signal amplification. However, jointly optimizing the transmit beamforming with the HRIS reflection and amplification coefficients to maximize spectral efficiency (SE) is a non-convex problem, and conventional iterative solutions are computationally intensive. To address this, we propose a deep reinforcement learning (DRL) framework that learns a direct mapping from channel state information to the near-optimal transmit beamforming and HRIS configurations. The DRL model is trained offline, after which it can compute the beamforming and HRIS configurations with low complexity and latency. Simulation results demonstrate that our DRL-based method achieves 95% of the SE obtained by the alternating optimization benchmark, while significantly lowering the computational complexity.

</details>


### [18] [Dynamic Channel Charting: An LSTM-AE-based Approach](https://arxiv.org/abs/2601.18473)
*Yuan Gao,Xinyu Guo,Wenjing Xie,Zifan Wang,Hongwen Yu,Gongyang Li,Shugong Xu*

Main category: eess.SP

TL;DR: 提出基于LSTM-AE的时间序列信道图表方法，解决传统CC方法忽略信道时变特性的问题，在6G通信中提升信道图表稳定性和拓扑一致性。


<details>
  <summary>Details</summary>
Motivation: 传统信道图表方法主要学习静态几何结构，忽略了信道随时间变化的动态特性，导致在复杂环境中信道图表不稳定且拓扑一致性差。随着6G通信系统发展，信道状态信息对提升网络性能至关重要，需要能够捕捉信道时变特性的新方法。

Method: 提出LSTM-AE-CC方法，将长短期记忆网络与自编码器结合。LSTM用于捕捉CSI中的时间依赖关系，AE学习连续潜在表示，将时间建模机制融入传统CC框架，确保信道几何一致性和时变特性的显式建模。

Result: 实验结果表明，该方法在多种实际通信场景中优于传统CC方法，特别是在信道图表稳定性、轨迹连续性和长期可预测性方面表现更佳。

Conclusion: LSTM-AE-CC方法成功解决了传统信道图表方法忽略时变特性的问题，为6G通信系统中的信道状态信息处理提供了更稳定、更准确的时间序列分析方法。

Abstract: With the development of the sixth-generation (6G) communication system, Channel State Information (CSI) plays a crucial role in improving network performance. Traditional Channel Charting (CC) methods map high-dimensional CSI data to low-dimensional spaces to help reveal the geometric structure of wireless channels. However, most existing CC methods focus on learning static geometric structures and ignore the dynamic nature of the channel over time, leading to instability and poor topological consistency of the channel charting in complex environments. To address this issue, this paper proposes a novel time-series channel charting approach based on the integration of Long Short-Term Memory (LSTM) networks and Auto encoders (AE) (LSTM-AE-CC). This method incorporates a temporal modeling mechanism into the traditional CC framework, capturing temporal dependencies in CSI using LSTM and learning continuous latent representations with AE. The proposed method ensures both geometric consistency of the channel and explicit modeling of the time-varying properties. Experimental results demonstrate that the proposed method outperforms traditional CC methods in various real-world communication scenarios, particularly in terms of channel charting stability, trajectory continuity, and long-term predictability.

</details>


### [19] [Dualband OFDM Delay Estimation for Multi-Target Localization](https://arxiv.org/abs/2601.18478)
*Jialun Kou,Achiel Colpaert,Zhuangzhuang Cui,Sofie Pollin*

Main category: eess.SP

TL;DR: 提出PSF中心框架解决双频段OFDM时延估计问题，通过建模时延剖面为真实目标响应与PSF的卷积，并采用RELAX算法抑制伪影，提升碎片化频谱下的定位精度。


<details>
  <summary>Details</summary>
Motivation: 在集成定位与通信系统中，由于硬件限制和频谱碎片化，难以获得大连续带宽。虽然聚合非连续窄带可以增加有效频率跨度，但非连续频率布局会引入高旁瓣和时延估计模糊等问题。

Method: 提出点扩散函数（PSF）中心框架，将观测时延剖面建模为真实目标响应与由双频段子载波选择模式决定的PSF的卷积。采用RELAX算法进行双频段多目标时延估计，抑制PSF引起的伪影。

Result: 仿真结果表明，该方法在双频段场景下提高了鲁棒性和准确性，支持碎片化频谱下的集成定位与通信。

Conclusion: PSF中心框架和RELAX算法能够有效解决双频段OFDM时延估计中的伪影和模糊问题，为碎片化频谱下的集成定位与通信提供了可行的解决方案。

Abstract: Integrated localization and communication systems aim to reuse communication waveforms for simultaneous data transmission and localization, but delay resolution is fundamentally limited by the available bandwidth. In practice, large contiguous bandwidths are difficult to obtain due to hardware constraints and spectrum fragmentation. Aggregating non-contiguous narrow bands can increase the effective frequency span, but a non-contiguous frequency layout introduces challenges such as elevated sidelobes and ambiguity in delay estimation. This paper introduces a point-spread-function (PSF)-centric framework for dual-band OFDM delay estimation. We model the observed delay profile as the convolution of the true target response with a PSF determined by the dual-band subcarrier selection pattern, explicitly linking band configuration to resolution and ambiguity. To suppress PSF-induced artifacts, we adapt the RELAX algorithm for dual-band multi-target delay estimation. Simulations demonstrate improved robustness and accuracy in dual-band scenarios, supporting ILC under fragmented spectrum.

</details>


### [20] [Hybrid Radar Fusion with Quantization: CRB-Rate Trade-offs and ADC Dynamic Range](https://arxiv.org/abs/2601.18539)
*Akhileswar Chowdary,Ahmad Bazzi,Marwa Chafii*

Main category: eess.SP

TL;DR: 该论文研究了低分辨率ADC对混合雷达融合系统的影响，发现当接收信号动态范围超过ADC支持范围时HRF不可行，且通信速率在ADC分辨率达到一定阈值后不再显著提升，揭示了感知与通信性能之间的基本权衡。


<details>
  <summary>Details</summary>
Motivation: 虽然低分辨率ADC在ISAC系统中越来越重要，但其对混合雷达融合系统的具体影响尚未充分研究。HRF系统中上行链路同时传输直接信号和反射信号，反射信号通常弱得多，使得HRF性能对ADC分辨率特别敏感。

Method: 使用量化Cramér-Rao界来衡量感知精度，推导了到达角估计的量化CRB上界，并通过两个优化问题探索CRB与速率的权衡关系。

Result: 仿真结果表明：当接收信号动态范围超过ADC支持范围时HRF不可行；上行通信速率在ADC分辨率超过一定阈值后不再显著增加；感知与通信性能存在基本权衡，HRF性能受益于更高ADC分辨率，但通信速率增益会达到平台期。

Conclusion: 该研究通过CRB-速率边界有效表征了感知与通信性能之间的权衡关系，为低分辨率ADC在HRF系统中的实际应用提供了重要指导。

Abstract: Recent advancements have underscored the relevance of low-resolution analog-to-digital converters (ADCs) in integrated sensing and communication (ISAC) systems. Nevertheless, their specific impact on hybrid radar fusion (HRF) remains largely unexplored. In HRF systems, where uplink (UL) paths carry direct and reflected signals in the same frequency band, the reflected signal is often significantly weaker, making HRF performance particularly sensitive to ADC resolution. To study this effect, we use the quantized Cramér-Rao bound (CRB) to measure sensing accuracy. This work derives an upper bound on the quantized CRB for angle of arrival (AoA) estimation and explores CRB-rate trade-offs through two formulated optimization problems. Simulation results indicate that HRF becomes infeasible when the dynamic range of the received signal exceeds the dynamic range supported by the ADC, which is inherently limited by its resolution. Furthermore, the UL communication rate does not increase significantly when the ADC resolution is raised beyond a certain threshold. These observations highlight a fundamental trade-off between sensing and communication performance: while HRF performance benefits from higher ADC resolutions, the corresponding gains in communication rate plateau. This trade-off is effectively characterized using CRB-rate boundaries derived through simulation.

</details>


### [21] [Synchronization and Localization in Ad-Hoc ICAS Networks Using a Two-Stage Kuramoto Method](https://arxiv.org/abs/2601.18643)
*Dominik Neudert-Schulz,Thomas Dallmann*

Main category: eess.SP

TL;DR: 提出一种用于车对车网络的联合分布式同步与定位方案，支持集成通信与感知，信号无关且能缓解有限采样频率的影响


<details>
  <summary>Details</summary>
Motivation: 在车对车网络中实现集成通信与感知需要精确的频率和相位同步，同时自动驾驶汽车需要准确获取周围车辆的位置信息

Method: 提出联合分布式同步与定位方案，方案具有信号无关性，可应用于多种ICAS信号，并缓解有限采样频率对性能的影响

Result: 方案能够实现精确的同步和定位，适用于广泛的ICAS信号，并有效减轻了有限采样频率带来的性能下降

Conclusion: 提出的联合分布式同步与定位方案为车对车网络中的集成通信与感知提供了有效的解决方案，具有信号无关性和对有限采样频率的鲁棒性

Abstract: To enable Integrated Communications and Sensing (ICAS) in a peer-to-peer vehicular network, precise synchronization in frequency and phase among the communicating entities is required. In addition, self-driving cars need accurate position estimates of the surrounding vehicles. In this work, we propose a joint, distributed synchronization and localization scheme for a network of communicating entities. Our proposed scheme is mostly signal-agnostic and therefore can be applied to a wide range of possible ICAS signals. We also mitigate the effect of finite sampling frequencies, which otherwise would degrade the synchronization and localization performance severely.

</details>


### [22] [AI-Driven Fuzzing for Vulnerability Assessment of 5G Traffic Steering Algorithms](https://arxiv.org/abs/2601.18690)
*Seyed Bagher Hashemi Natanzi,Hossein Mohammadi,Bo Tang,Vuk Marojevic*

Main category: eess.SP

TL;DR: 提出基于NSGA-II的AI驱动模糊测试框架，用于系统性地暴露5G网络流量调度算法的隐藏漏洞，相比传统测试能发现更多漏洞和关键故障。


<details>
  <summary>Details</summary>
Motivation: 5G网络中的流量调度算法容易受到干扰峰值、切换风暴和局部中断等对抗性条件的影响，需要更有效的测试方法来发现隐藏漏洞，确保网络鲁棒性。

Method: 采用基于非支配排序遗传算法II（NSGA-II）的AI驱动模糊测试框架，使用NVIDIA Sionna平台，在六种场景下评估五种流量调度算法。

Result: AI驱动模糊测试比传统测试多检测出34.3%的总漏洞和5.8%的关键故障，在多样性和边缘案例发现方面表现更优，但关键故障检测存在方差，反映了罕见漏洞的随机性。

Conclusion: AI驱动模糊测试为提升流量调度算法鲁棒性提供了有效且可扩展的验证方法，有助于构建面向6G的弹性网络。

Abstract: Traffic Steering (TS) dynamically allocates user traffic across cells to enhance Quality of Experience (QoE), load balance, and spectrum efficiency in 5G networks. However, TS algorithms remain vulnerable to adversarial conditions such as interference spikes, handover storms, and localized outages. To address this, an AI-driven fuzz testing framework based on the Non-Dominated Sorting Genetic Algorithm II (NSGA-II) is proposed to systematically expose hidden vulnerabilities. Using NVIDIA Sionna, five TS algorithms are evaluated across six scenarios. Results show that AI-driven fuzzing detects 34.3% more total vulnerabilities and 5.8% more critical failures than traditional testing, achieving superior diversity and edge-case discovery. The observed variance in critical failure detection underscores the stochastic nature of rare vulnerabilities. These findings demonstrate that AI-driven fuzzing offers an effective and scalable validation approach for improving TS algorithm robustness and ensuring resilient 6G-ready networks.

</details>


### [23] [Low-Bit Quantization of Bandlimited Graph Signals via Iterative Methods](https://arxiv.org/abs/2601.18782)
*Felix Krahmer,He Lyu,Rayan Saab,Jinna Qian,Anna Veselovska,Rongrong Wang*

Main category: eess.SP

TL;DR: 该论文研究了图信号的低比特量化问题，提出了基于噪声整形的迭代算法，利用图拉普拉斯谱特性和图不相干性实现高保真近似。


<details>
  <summary>Details</summary>
Motivation: 研究图信号的低比特量化问题，旨在开发高效的量化方法以压缩图信号表示，同时保持高保真度。

Method: 提出了迭代噪声整形算法，包括有顶点替换和无顶点替换的采样方法，利用图拉普拉斯算子的谱特性和图不相干性。

Result: 为随机采样方法提供了理论保证，在合成和真实图上的大量数值实验证明了所提方案的高效性和鲁棒性。

Conclusion: 提出的噪声整形量化算法能够有效实现图信号的低比特表示，在保持高保真度的同时具有理论保证和实际应用价值。

Abstract: We study the quantization of real-valued bandlimited signals on graphs, focusing on low-bit representations. We propose iterative noise-shaping algorithms for quantization, including sampling approaches with and without vertex replacement. The methods leverage the spectral properties of the graph Laplacian and exploit graph incoherence to achieve high-fidelity approximations. Theoretical guarantees are provided for the random sampling method, and extensive numerical experiments on synthetic and real-world graphs illustrate the efficiency and robustness of the proposed schemes.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [24] [The Voice of Equity: A Systematic Evaluation of Bias Mitigation Techniques for Speech-Based Cognitive Impairment Detection Across Architectures and Demographics](https://arxiv.org/abs/2601.16989)
*Yasaman Haghbin,Sina Rashidi,Ali Zolnour,Maryam Zolnoori*

Main category: eess.AS

TL;DR: 该研究首次提出了语音认知障碍检测的全面公平性分析框架，系统评估了不同架构和人口亚组间的算法偏见缓解效果，发现架构设计对偏见模式和缓解效果有根本性影响。


<details>
  <summary>Details</summary>
Motivation: 语音认知障碍检测虽具可扩展性，但算法在不同人口和语言亚组间的偏见问题研究不足。现有研究通常只考察单一缓解技术，缺乏系统性公平性分析框架。

Method: 开发了两种基于Transformer的架构（SpeechCARE-AGF和Whisper-LWF-LoRA），使用多语言NIA PREPARE数据集。系统比较了预处理、处理中和后处理三种偏见缓解方法，通过机会均等和平等化几率评估性别、年龄、教育和语言维度的公平性。

Result: 两种模型性能良好（F1分数：70.87和71.46），但存在显著公平性差异：80岁以上成人敏感性较低，西班牙语使用者真阳性率低于英语使用者。缓解效果因架构而异：过采样对SpeechCARE-AGF在老年人中有效（80+ TPR从46.19%提升至49.97%），但对Whisper-LWF-LoRA影响有限。

Conclusion: 架构设计从根本上影响偏见模式和缓解效果。公平性干预必须针对模型架构和人口特征进行定制，自适应融合机制和频率重加权提供了稳健改进。该研究为开发公平的语音筛查工具提供了系统性框架，对减少认知医疗诊断差异至关重要。

Abstract: Speech-based detection of cognitive impairment offers a scalable, non-invasive screening, yet algorithmic bias across demographic and linguistic subgroups remains critically underexplored. We present the first comprehensive fairness analysis framework for speech-based multi-class cognitive impairment detection, systematically evaluating bias mitigation across architectures, and demographic subgroups. We developed two transformer-based architectures, SpeechCARE-AGF and Whisper-LWF-LoRA, on the multilingual NIA PREPARE Challenge dataset. Unlike prior work that typically examines single mitigation techniques, we compared pre-processing, in-processing, and post-processing approaches, assessing fairness via Equality of Opportunity and Equalized Odds across gender, age, education, and language. Both models achieved strong performance (F1: SpeechCARE-AGF 70.87, Whisper-LWF-LoRA 71.46) but exhibited substantial fairness disparities. Adults >=80 showed lower sensitivity versus younger groups; Spanish speakers demonstrated reduced TPR versus English speakers. Mitigation effectiveness varied by architecture: oversampling improved SpeechCARE-AGF for older adults (80+ TPR: 46.19%=>49.97%) but minimally affected Whisper-LWF-LoRA. This study addresses a critical healthcare AI gap by demonstrating that architectural design fundamentally shapes bias patterns and mitigation effectiveness. Adaptive fusion mechanisms enable flexible responses to data interventions, while frequency reweighting offers robust improvements across architectures. Our findings establish that fairness interventions must be tailored to both model architecture and demographic characteristics, providing a systematic framework for developing equitable speech-based screening tools essential for reducing diagnostic disparities in cognitive healthcare.

</details>


### [25] [BickGraphing: Web-Based Application for Visual Inspection of Audio Recordings](https://arxiv.org/abs/2601.17014)
*Kayley Seow,Alexander Arovas,Grace Steinmetz,Emily Bick*

Main category: eess.AS

TL;DR: BickGraphing是一个基于浏览器的研究工具，用于可视化声学录音，特别适用于昆虫生物声学领域，支持本地处理大型.wav文件并生成波形图和频谱图。


<details>
  <summary>Details</summary>
Motivation: 开发一个易于使用的本地音频可视化平台，支持昆虫生物声学研究中的作物害虫声音分析，同时适用于广泛的视听化研究需求。

Method: 采用SvelteKit和TypeScript构建的Web应用，使用WebAssembly编译的FFmpeg和自定义FFT工具在客户端进行信号处理，支持多文件上传和交互式音频事件探索。

Result: 成功开发出开源工具，支持本地计算波形和频谱图，无需编码即可进行音频数据的可视化质量检查，已发布在GitHub上并采用MIT许可证。

Conclusion: BickGraphing有潜力成为研究音频数据的本地化、易于使用且无需编码的可视化平台，特别适用于昆虫生物声学及相关领域。

Abstract: BickGraphing is a browser based research tool that enables visual inspection of acoustic recordings. The tool was built in support of visualizing crop feeding pest sounds in support of the Insect Eavesdropper project; however, it is widely applicable to all audiovisualizations in research. It allows multiple uploads of large .wav files, computes waveforms and spectrograms locally, and supports interactive exploration of audio events in time and frequency. The application is implemented as a SvelteKit and TypeScript web app with a client side signal processing pipeline using WebAssembly compiled FFmpeg and custom FFT utilities. The software is released on an open Git repository (https://github.com/bicklabuw/BickGraphing) and archived under a standard MIT license and can be reused for rapid visual quality checks of .wav recordings in insect bioacoustics and related fields. BickGraphing has the potential to be a local, easy to use coding free visualization platform for audio data in research.

</details>


### [26] [PC-MCL: Patient-Consistent Multi-Cycle Learning with multi-label bias correction for respiratory sound classification](https://arxiv.org/abs/2601.17080)
*Seung Gyu Jeong,Seong-Eun Kim*

Main category: eess.AS

TL;DR: PC-MCL提出了一种患者一致的多周期学习方法，通过多周期拼接、三标签公式和患者匹配辅助任务，解决了呼吸音分类中的患者特异性过拟合问题，在ICBHI 2017基准测试中取得了65.37%的分数。


<details>
  <summary>Details</summary>
Motivation: 现有深度模型依赖周期级分析且存在患者特异性过拟合问题，同时多周期拼接与传统的二标签公式（爆裂音、哮鸣音）结合时会产生多标签分布偏差，导致正常信号信息丢失。

Method: 提出PC-MCL框架，包含三个关键组件：1）多周期拼接；2）三标签公式（正常、爆裂音、哮鸣音）以纠正分布偏差；3）患者匹配辅助任务作为多任务正则化器，学习更鲁棒的特征。

Result: 在ICBHI 2017基准测试中，PC-MCL取得了65.37%的ICBHI分数，优于现有基线方法。消融研究证实所有三个组件都是必需的，协同工作改善了异常呼吸事件的检测。

Conclusion: PC-MCL通过解决多标签分布偏差和患者特异性过拟合问题，显著提高了呼吸音分类性能，为肺部疾病的自动诊断提供了更可靠的解决方案。

Abstract: Automated respiratory sound classification supports the diagnosis of pulmonary diseases. However, many deep models still rely on cycle-level analysis and suffer from patient-specific overfitting. We propose PC-MCL (Patient-Consistent Multi-Cycle Learning) to address these limitations by utilizing three key components: multi-cycle concatenation, a 3-label formulation, and a patient-matching auxiliary task. Our work resolves a multi-label distributional bias in respiratory sound classification, a critical issue inherent to applying multi-cycle concatenation with the conventional 2-label formulation (crackle, wheeze). This bias manifests as a systematic loss of normal signal information when normal and abnormal cycles are combined. Our proposed 3-label formulation (normal, crackle, wheeze) corrects this by preserving information from all constituent cycles in mixed samples. Furthermore, the patient-matching auxiliary task acts as a multi-task regularizer, encouraging the model to learn more robust features and improving generalization. On the ICBHI 2017 benchmark, PC-MCL achieves an ICBHI Score of 65.37%, outperforming existing baselines. Ablation studies confirm that all three components are essential, working synergistically to improve the detection of abnormal respiratory events.

</details>


### [27] [Recovering Performance in Speech Emotion Recognition from Discrete Tokens via Multi-Layer Fusion and Paralinguistic Feature Integration](https://arxiv.org/abs/2601.17085)
*Esther Sun,Abinay Reddy Naini,Carlos Busso*

Main category: eess.AS

TL;DR: 该论文研究了离散语音标记在语音情感识别中的应用，通过多层融合和声学特征集成来弥补量化过程中的副语言信息损失，使离散标记性能接近连续表示。


<details>
  <summary>Details</summary>
Motivation: 离散语音标记在存储和语言模型集成方面具有优势，但在语音情感识别中应用受限，主要因为量化过程中副语言信息损失。

Method: 使用微调的WavLM-Large模型，系统量化不同层配置和k-means量化粒度下的性能退化。提出两种关键策略：1）基于注意力的多层融合以从不同层重新捕获互补信息；2）集成openSMILE特征以显式重新引入副语言线索。同时比较主流神经编解码标记器。

Result: 研究发现通过多层融合和声学特征集成，离散标记可以在语音情感识别任务中缩小与连续表示的性能差距。

Conclusion: 通过适当的多层融合策略和声学特征补充，离散语音标记能够有效用于语音情感识别，克服量化过程中的信息损失问题。

Abstract: Discrete speech tokens offer significant advantages for storage and language model integration, but their application in speech emotion recognition (SER) is limited by paralinguistic information loss during quantization. This paper presents a comprehensive investigation of discrete tokens for SER. Using a fine-tuned WavLM-Large model, we systematically quantify performance degradation across different layer configurations and k-means quantization granularities. To recover the information loss, we propose two key strategies: (1) attention-based multi-layer fusion to recapture complementary information from different layers, and (2) integration of openSMILE features to explicitly reintroduce paralinguistic cues. We also compare mainstream neural codec tokenizers (SpeechTokenizer, DAC, EnCodec) and analyze their behaviors when fused with acoustic features. Our findings demonstrate that through multi-layer fusion and acoustic feature integration, discrete tokens can close the performance gap with continuous representations in SER tasks.

</details>


### [28] [Spoofing-Aware Speaker Verification via Wavelet Prompt Tuning and Multi-Model Ensembles](https://arxiv.org/abs/2601.17557)
*Aref Farhadipour,Ming Jin,Valeriia Vyshnevetska,Xiyang Li,Elisa Pellegrino,Srikanth Madikeri*

Main category: eess.AS

TL;DR: UZH-CL系统参加WildSpoof 2026挑战赛SASV任务，提出级联式欺骗感知说话人验证框架，集成小波提示调优XLSR-AASIST反欺骗模块和多模型集成，在挑战中取得2.08% SASV EER。


<details>
  <summary>Details</summary>
Motivation: 应对生成式欺骗攻击的集成防御挑战，需要同时验证说话人身份和音频真实性，解决跨领域泛化问题。

Method: 级联式欺骗感知说话人验证框架，集成小波提示调优XLSR-AASIST反欺骗模块，ASV组件使用ResNet34、ResNet293和WavLM-ECAPA-TDNN架构，采用Z-score归一化和分数平均。

Result: 系统在VoxCeleb2和SpoofCeleb上训练，获得Macro a-DCF 0.2017和SASV EER 2.08%，域内欺骗检测EER为0.16%，但在ASVspoof5等未见数据集上显示跨领域泛化挑战。

Conclusion: 提出的级联框架在集成防御生成式欺骗攻击方面有效，但跨领域泛化仍是关键挑战，需要在未见数据集上提升性能。

Abstract: This paper describes the UZH-CL system submitted to the SASV section of the WildSpoof 2026 challenge. The challenge focuses on the integrated defense against generative spoofing attacks by requiring the simultaneous verification of speaker identity and audio authenticity. We proposed a cascaded Spoofing-Aware Speaker Verification framework that integrates a Wavelet Prompt-Tuned XLSR-AASIST countermeasure with a multi-model ensemble. The ASV component utilizes the ResNet34, ResNet293, and WavLM-ECAPA-TDNN architectures, with Z-score normalization followed by score averaging. Trained on VoxCeleb2 and SpoofCeleb, the system obtained a Macro a-DCF of 0.2017 and a SASV EER of 2.08%. While the system achieved a 0.16% EER in spoof detection on the in-domain data, results on unseen datasets, such as the ASVspoof5, highlight the critical challenge of cross-domain generalization.

</details>


### [29] [ToS: A Team of Specialists ensemble framework for Stereo Sound Event Localization and Detection with distance estimation in Video](https://arxiv.org/abs/2601.17611)
*Davide Berghi,Philip J. B. Jackson*

Main category: eess.AS

TL;DR: 提出Team of Specialists (ToS)集成框架，通过三个专门处理不同维度对的子网络协同解决3D声音事件定位与检测任务，在DCASE2025基准测试中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 3D声音事件定位与检测需要同时处理语义、空间和时间三个维度，单一模型难以有效应对这种多模态任务的复杂性，需要更有效的解决方案。

Method: 提出ToS集成框架，包含三个互补的子网络：1) 空间-语言模型处理空间和语义维度；2) 空间-时间模型处理空间和时间维度；3) 时间-语言模型处理时间和语义维度。每个子网络专注于不同的维度组合，通过协同工作提供综合预测。

Result: 在DCASE2025 Task 3 Stereo SELD开发集上，ToS框架在关键指标上持续优于现有的最先进音频-视觉模型。

Conclusion: ToS框架通过专家团队协作的方式有效解决了3D SELD任务的多维度挑战，未来将通过增强各专家的任务设计、训练策略和预训练课程来进一步扩展这一概念验证。

Abstract: Sound event localization and detection with distance estimation (3D SELD) in video involves identifying active sound events at each time frame while estimating their spatial coordinates. This multimodal task requires joint reasoning across semantic, spatial, and temporal dimensions, a challenge that single models often struggle to address effectively. To tackle this, we introduce the Team of Specialists (ToS) ensemble framework, which integrates three complementary sub-networks: a spatio-linguistic model, a spatio-temporal model, and a tempo-linguistic model. Each sub-network specializes in a unique pair of dimensions, contributing distinct insights to the final prediction, akin to a collaborative team with diverse expertise. ToS has been benchmarked against state-of-the-art audio-visual models for 3D SELD on the DCASE2025 Task 3 Stereo SELD development set, consistently outperforming existing methods across key metrics. Future work will extend this proof of concept by strengthening the specialists with appropriate tasks, training, and pre-training curricula.

</details>


### [30] [End-to-End Joint ASR and Speaker Role Diarization with Child-Adult Interactions](https://arxiv.org/abs/2601.17640)
*Anfeng Xu,Tiantian Feng,Somer Bishop,Catherine Lord,Shrikanth Narayanan*

Main category: eess.AS

TL;DR: 提出统一的端到端框架，扩展Whisper架构，联合建模语音识别和儿童-成人说话人角色分离，在儿童-成人互动场景中优于级联基线方法。


<details>
  <summary>Details</summary>
Motivation: 儿童-成人语音互动的准确转录和说话人分离对发育和临床研究至关重要，但手动标注耗时且难以扩展。现有自动化系统通常采用级联的说话人分离和语音识别流程，容易导致错误传播。

Method: 扩展Whisper编码器-解码器架构，联合建模ASR和说话人角色分离：1）序列化输出训练方案，输出说话人标签和起止时间戳；2）轻量级帧级分离头，增强说话人区分性编码表示；3）分离引导的静音抑制，提高时间精度；4）基于状态机的强制解码过程，保证结构有效输出。

Result: 在两个数据集上的综合评估显示，相比两个级联基线方法，该方法在多说话人词错误率方面取得显著降低，并在Whisper-small和Whisper-large模型上都表现出竞争力的分离准确性。

Conclusion: 提出的联合建模框架能有效生成可靠、说话人归属的儿童-成人互动转录，具有实际应用价值，代码和模型权重已公开。

Abstract: Accurate transcription and speaker diarization of child-adult spoken interactions are crucial for developmental and clinical research. However, manual annotation is time-consuming and challenging to scale. Existing automated systems typically rely on cascaded speaker diarization and speech recognition pipelines, which can lead to error propagation. This paper presents a unified end-to-end framework that extends the Whisper encoder-decoder architecture to jointly model ASR and child-adult speaker role diarization. The proposed approach integrates: (i) a serialized output training scheme that emits speaker tags and start/end timestamps, (ii) a lightweight frame-level diarization head that enhances speaker-discriminative encoder representations, (iii) diarization-guided silence suppression for improved temporal precision, and (iv) a state-machine-based forced decoding procedure that guarantees structurally valid outputs. Comprehensive evaluations on two datasets demonstrate consistent and substantial improvements over two cascaded baselines, achieving lower multi-talker word error rates and demonstrating competitive diarization accuracy across both Whisper-small and Whisper-large models. These findings highlight the effectiveness and practical utility of the proposed joint modeling framework for generating reliable, speaker-attributed transcripts of child-adult interactions at scale. The code and model weights are publicly available

</details>


### [31] [Speech Emotion Recognition with ASR Integration](https://arxiv.org/abs/2601.17901)
*Yuanchao Li*

Main category: eess.AS

TL;DR: 该论文研究将自动语音识别(ASR)集成到语音情感识别(SER)中，以增强情感识别的鲁棒性、可扩展性和实际应用性


<details>
  <summary>Details</summary>
Motivation: 语音情感识别在理解人类交流、开发情感智能系统和AGI中至关重要，但在真实世界、自发和低资源场景中部署SER仍面临重大挑战，主要由于情感表达的复杂性和当前语音语言技术的局限性

Method: 研究将自动语音识别(ASR)集成到语音情感识别(SER)中，通过结合语音识别技术来增强情感识别能力

Result: 未在摘要中明确说明具体结果，但研究目标是提高情感识别的鲁棒性、可扩展性和实际应用性

Conclusion: 集成ASR到SER中是解决真实世界、自发和低资源场景中语音情感识别挑战的重要研究方向，有望推动情感智能系统和AGI的发展

Abstract: Speech Emotion Recognition (SER) plays a pivotal role in understanding human communication, enabling emotionally intelligent systems, and serving as a fundamental component in the development of Artificial General Intelligence (AGI). However, deploying SER in real-world, spontaneous, and low-resource scenarios remains a significant challenge due to the complexity of emotional expression and the limitations of current speech and language technologies. This thesis investigates the integration of Automatic Speech Recognition (ASR) into SER, with the goal of enhancing the robustness, scalability, and practical applicability of emotion recognition from spoken language.

</details>


### [32] [AmbER$^2$: Dual Ambiguity-Aware Emotion Recognition Applied to Speech and Text](https://arxiv.org/abs/2601.18010)
*Jingyao Wu,Grace Lin,Yinuo Song,Rosalind Picard*

Main category: eess.AS

TL;DR: AmbER²是一个双重模糊感知框架，同时建模评分者级别和模态级别的模糊性，通过教师-学生架构和分布训练目标提升情感识别的分布保真度和性能。


<details>
  <summary>Details</summary>
Motivation: 情感识别存在固有的模糊性，包括评分者之间的分歧和不同模态（如语音和文本）之间的差异。现有研究主要关注评分者模糊性，而模态模糊性研究不足，多模态方法通常简单融合特征而不处理模态冲突。

Method: 提出AmbER²框架，采用教师-学生架构，通过分布训练目标同时建模评分者级别和模态级别的模糊性。该框架能够处理多模态情感识别中的双重模糊性问题。

Result: 在IEMOCAP和MSP-Podcast数据集上，AmbER²相比传统交叉熵基线显著提升了分布保真度，性能达到或优于最新SOTA系统。在IEMOCAP上，Bhattacharyya系数提升20.3%，R²提升13.6%，准确率提升3.8%，F1提升4.5%。

Conclusion: 明确建模模糊性对高度不确定样本特别有益，联合处理评分者和模态模糊性对于构建鲁棒的情感识别系统至关重要。

Abstract: Emotion recognition is inherently ambiguous, with uncertainty arising both from rater disagreement and from discrepancies across modalities such as speech and text. There is growing interest in modeling rater ambiguity using label distributions. However, modality ambiguity remains underexplored, and multimodal approaches often rely on simple feature fusion without explicitly addressing conflicts between modalities. In this work, we propose AmbER$^2$, a dual ambiguity-aware framework that simultaneously models rater-level and modality-level ambiguity through a teacher-student architecture with a distribution-wise training objective. Evaluations on IEMOCAP and MSP-Podcast show that AmbER$^2$ consistently improves distributional fidelity over conventional cross-entropy baselines and achieves performance competitive with, or superior to, recent state-of-the-art systems. For example, on IEMOCAP, AmbER$^2$ achieves relative improvements of 20.3% on Bhattacharyya coefficient (0.83 vs. 0.69), 13.6% on R$^2$ (0.67 vs. 0.59), 3.8% on accuracy (0.683 vs. 0.658), and 4.5% on F1 (0.675 vs. 0.646). Further analysis across ambiguity levels shows that explicitly modeling ambiguity is particularly beneficial for highly uncertain samples. These findings highlight the importance of jointly addressing rater and modality ambiguity when building robust emotion recognition systems.

</details>


### [33] [SpatialEmb: Extract and Encode Spatial Information for 1-Stage Multi-channel Multi-speaker ASR on Arbitrary Microphone Arrays](https://arxiv.org/abs/2601.18037)
*Yiwen Shao,Yong Xu,Sanjeev Khudanpur,Dong Yu*

Main category: eess.AS

TL;DR: 提出SpatialEmb轻量级嵌入模块，直接为ASR模型提取编码空间信息，支持固定和任意麦克风拓扑，在AliMeeting数据集上达到SOTA效果


<details>
  <summary>Details</summary>
Motivation: 现有多通道ASR系统仅在语音分离阶段提取空间特征，导致流程冗长、性能次优，且多数方法依赖扬声器位置和麦克风拓扑知识，难以适应新设备

Method: 提出SpatialEmb轻量级嵌入模块，直接为ASR模型提取和编码空间信息，支持固定和任意麦克风拓扑，避免依赖预处理模块

Result: 在AliMeeting真实会议语料上，使用105小时Train-Ali-far训练的最佳模型在Eval和Test集上分别达到17.04%和20.32%的字符错误率，创下相同训练数据下的SOTA结果

Conclusion: SpatialEmb通过直接为ASR模型提取空间信息，解决了现有多通道ASR系统的效率低下和适应性差的问题，在真实会议场景中取得了显著性能提升

Abstract: Spatial information is a critical clue for multi-channel multi-speaker target speech recognition. Most state-of-the-art multi-channel Automatic Speech Recognition (ASR) systems extract spatial features only during the speech separation stage, followed by standard single-channel ASR on the separated speech. This approach results in an inefficient, lengthy pipeline and sub-optimal ASR performance due to the accumulated errors from preprocessing modules. Furthermore, most spatial feature extraction methods depend on the knowledge of speaker positions and microphone topology, making the systems reliant on specific settings and challenging to adapt to new equipment. In this work, we propose a solution to these issues with a lightweight embedding module named SpatialEmb, which extracts and encodes spatial information directly for the ASR model, supporting both fixed and arbitrary microphone topology. We conduct comprehensive experiments on AliMeeting, a real meeting corpus, to determine the optimal model design for SpatialEmb in terms of both performance and efficiency. Our best model trained with 105 hours Train-Ali-far achieves 17.04% and 20.32% character error rates (CER) on the Eval and Test sets, establishing a new state-of-the-art result with the same training data.

</details>


### [34] [OneVoice: One Model, Triple Scenarios-Towards Unified Zero-shot Voice Conversion](https://arxiv.org/abs/2601.18094)
*Zhichao Wang,Tao Li,Wenshuo Ge,Zihao Cui,Shilei Zhang,Junlan Feng*

Main category: eess.AS

TL;DR: OneVoice是一个统一的零样本语音转换框架，能够处理语音克隆、情感表达和歌唱三种场景，使用基于MoE的扩散模型实现高效序列建模和场景适应性。


<details>
  <summary>Details</summary>
Motivation: 当前语音转换领域存在碎片化问题，需要针对不同场景（语音克隆、情感表达、歌唱）使用专门的模型。作者希望开发一个统一的框架来处理所有这些场景，提高效率和灵活性。

Method: 基于连续语言模型和VAE-free的next-patch扩散训练，采用混合专家(MoE)架构，包含共享专家隔离和场景感知的领域专家分配的双路径路由机制。使用门控机制融合场景特定的韵律特征，采用两阶段渐进训练（基础预训练+LoRA领域专家增强）。

Result: 实验表明OneVoice在所有三种场景中都能匹配或超越专门的模型，同时验证了对场景的灵活控制，并提供了仅需2步的快速解码版本。

Conclusion: OneVoice成功实现了语音转换的统一框架，解决了领域碎片化问题，在保持高质量的同时提供了灵活性和效率，为语音转换领域提供了新的解决方案。

Abstract: Recent progress of voice conversion~(VC) has achieved a new milestone in speaker cloning and linguistic preservation. But the field remains fragmented, relying on specialized models for linguistic-preserving, expressive, and singing scenarios. We propose OneVoice, a unified zero-shot framework capable of handling all three scenarios within a single model. OneVoice is built upon a continuous language model trained with VAE-free next-patch diffusion, ensuring high fidelity and efficient sequence modeling. Its core design for unification lies in a Mixture-of-Experts (MoE) designed to explicitly model shared conversion knowledge and scenario-specific expressivity. Expert selection is coordinated by a dual-path routing mechanism, including shared expert isolation and scenario-aware domain expert assignment with global-local cues. For precise conditioning, scenario-specific prosodic features are fused into each layer via a gated mechanism, allowing adaptive usage of prosody information. Furthermore, to enable the core idea and alleviate the imbalanced issue (abundant speech vs. scarce singing), we adopt a two-stage progressive training that includes foundational pre-training and scenario enhancement with LoRA-based domain experts. Experiments show that OneVoice matches or surpasses specialized models across all three scenarios, while verifying flexible control over scenarios and offering a fast decoding version as few as 2 steps. Code and model will be released soon.

</details>


### [35] [Efficient Rehearsal for Continual Learning in ASR via Singular Value Tuning](https://arxiv.org/abs/2601.18266)
*Steven Vander Eeckt,Hugo Van hamme*

Main category: eess.AS

TL;DR: 提出一种基于排练的持续学习方法，通过SVD分解和门控向量训练，在极小内存下有效缓解ASR中的灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: ASR中的持续学习面临灾难性遗忘问题，现有基于排练的方法需要大量存储数据，但实际应用中存储成本高、隐私限制多，小内存下性能下降严重。

Method: 两阶段方法：1) 在新任务上微调；2) 对线性层变化进行SVD分解，仅训练控制更新接受程度的门控向量，使用排练数据进行参数高效训练。

Result: 在单语和多语基准测试中，该方法显著减少遗忘，性能优于最先进的CL方法，即使每个先前任务仅存储单个话语也能有效工作。

Conclusion: 提出的方法通过SVD分解和门控向量训练，实现了在极小内存需求下的高效持续学习，为ASR中的CL问题提供了实用的解决方案。

Abstract: Continual Learning (CL) in Automatic Speech Recognition (ASR) suffers from catastrophic forgetting when adapting to new tasks, domains, or speakers. A common strategy to mitigate this is to store a subset of past data in memory for rehearsal. However, rehearsal-based methods face key limitations: storing data is often costly, infeasible with pre-trained models, or restricted by privacy regulations. Running existing rehearsal-based methods with smaller memory sizes to alleviate these issues usually leads to degraded performance.
  We propose a rehearsal-based CL method that remains effective even with minimal memory. It operates in two stages: first, fine-tuning on the new task; second, applying Singular Value Decomposition (SVD) to the changes in linear layers and, in a parameter-efficient manner, retraining only gating vectors on the singular values, which control to extent to which updates from the first stage are accepted, using rehearsal. We extensively test and analyze our method on two monolingual and two multilingual benchmarks. Our method reduces forgetting and outperforms state-of-the-art CL approaches for ASR, even when limited to a single utterance per previous task.

</details>


### [36] [Noise-Robust Contrastive Learning with an MFCC-Conformer For Coronary Artery Disease Detection](https://arxiv.org/abs/2601.18295)
*Milan Marocchi,Matthew Fynn,Yue Rong*

Main category: eess.AS

TL;DR: 提出一种基于多通道能量噪声段剔除算法的心音图CAD检测方法，通过噪声参考麦克风剔除非平稳噪声段，结合Conformer分类器提升噪声鲁棒性，在297名受试者上取得78.4%准确率。


<details>
  <summary>Details</summary>
Motivation: 心血管疾病是全球主要死因，其中冠心病占比最大。虽然心音图检测CAD在临床低噪声环境中已有较高成功率，但多通道技术对噪声更鲁棒，在真实世界数据中仍面临挑战。需要开发能有效处理非平稳噪声的鲁棒检测方法。

Method: 1) 提出新颖的多通道能量噪声段剔除算法，使用心音麦克风和噪声参考麦克风，在训练前剔除含大量非平稳噪声的音频段；2) 采用基于Conformer的深度学习分类器，处理多通道的MFCC特征，进一步提升模型噪声鲁棒性。

Result: 在297名受试者上达到78.4%准确率和78.2%平衡准确率。相比不使用噪声段剔除的方法，准确率提升4.1%，平衡准确率提升4.3%。

Conclusion: 提出的多通道噪声段剔除算法与Conformer分类器相结合，显著提升了心音图CAD检测在真实噪声环境下的性能，为临床实际应用提供了更鲁棒的解决方案。

Abstract: Cardiovascular diseases (CVD) are the leading cause of death worldwide, with coronary artery disease (CAD) comprising the largest subcategory of CVDs. Recently, there has been increased focus on detecting CAD using phonocardiogram (PCG) signals, with high success in clinical environments with low noise and optimal sensor placement. Multichannel techniques have been found to be more robust to noise; however, achieving robust performance on real-world data remains a challenge. This work utilises a novel multichannel energy-based noisy-segment rejection algorithm, using heart and noise-reference microphones, to discard audio segments with large amounts of nonstationary noise before training a deep learning classifier. This conformer-based classifier takes mel-frequency cepstral coefficients (MFCCs) from multiple channels, further helping improve the model's noise robustness. The proposed method achieved 78.4% accuracy and 78.2% balanced accuracy on 297 subjects, representing improvements of 4.1% and 4.3%, respectively, compared to training without noisy-segment rejection.

</details>


### [37] [Residual Learning for Neural Ambisonics Encoders](https://arxiv.org/abs/2601.18322)
*Thomas Deppisch,Yang Gao,Manan Mittal,Benjamin Stahl,Christoph Hold,David Alon,Zamir Ben-Hur*

Main category: eess.AS

TL;DR: 提出残差学习框架，结合线性编码器和神经网络的优势，改进智能眼镜等可穿戴设备的空间音频编码性能


<details>
  <summary>Details</summary>
Motivation: 可穿戴设备需要高质量的空间音频采集，但现有方法各有局限：线性编码器稳健但存在低频噪声放大和高频空间混叠问题，神经网络方法性能更好但假设理想麦克风且实际场景表现不稳定

Method: 提出残差学习框架，用神经网络修正线性编码器的输出。使用智能眼镜实测阵列传递函数，比较UNet编码器和新的循环注意力模型，分析它们在残差配置下的性能

Result: 在残差配置下，两种神经模型在所有测试指标上均取得一致且显著的改进（域内数据），域外数据也有适度提升。但相干性分析显示所有神经编码器配置在高频方向准确编码方面仍有困难

Conclusion: 残差学习框架能有效结合线性编码器和神经网络的互补优势，实现更稳健的空间音频编码，但高频方向编码精度仍需进一步改进

Abstract: Emerging wearable devices such as smartglasses and extended reality headsets demand high-quality spatial audio capture from compact, head-worn microphone arrays. Ambisonics provides a device-agnostic spatial audio representation by mapping array signals to spherical harmonic (SH) coefficients. In practice, however, accurate encoding remains challenging. While traditional linear encoders are signal-independent and robust, they amplify low-frequency noise and suffer from high-frequency spatial aliasing. On the other hand, neural network approaches can outperform linear encoders but they often assume idealized microphones and may perform inconsistently in real-world scenarios. To leverage their complementary strengths, we introduce a residual-learning framework that refines a linear encoder with corrections from a neural network. Using measured array transfer functions from smartglasses, we compare a UNet-based encoder from the literature with a new recurrent attention model. Our analysis reveals that both neural encoders only consistently outperform the linear baseline when integrated within the residual learning framework. In the residual configuration, both neural models achieve consistent and significant improvements across all tested metrics for in-domain data and moderate gains for out-of-domain data. Yet, coherence analysis indicates that all neural encoder configurations continue to struggle with directionally accurate high-frequency encoding.

</details>


### [38] [Noise-Robust AV-ASR Using Visual Features Both in the Whisper Encoder and Decoder](https://arxiv.org/abs/2601.18396)
*Zhengyang Li,Thomas Graave,Björn Möller,Zehang Wu,Matthias Franz,Tim Fingscheidt*

Main category: eess.AS

TL;DR: 基于Whisper ASR，提出了一种简单有效的视听融合方法——在编码器和解码器中同时使用视觉特征（双重使用），显著提升了噪声鲁棒性，在LRS3 AV-ASR基准上达到了新的SOTA。


<details>
  <summary>Details</summary>
Motivation: 在视听自动语音识别系统中，将视觉特征融合到预训练的ASR中已被证明是提高噪声鲁棒性的有效方法。本研究基于先进的Whisper ASR，探索更有效的视觉特征融合方法。

Method: 提出双重使用视觉特征的方法：1）在编码器中学习视听交互；2）在解码器中权衡不同模态的重要性。基于不同规模的Whisper模型比较视觉融合方法，并进行消融研究分析模块设计和融合选项的影响。

Result: 双重使用方法在噪声鲁棒性方面表现出显著改进：基于Whisper small相对改进35%（WER：4.41% vs. 6.83%），基于Whisper medium相对改进57%（WER：4.07% vs. 9.53%）。在LRS3 AV-ASR基准上，使用Whisper medium的双重使用方法在MUSAN和NoiseX噪声条件下分别达到4.08%和4.43%的平均WER，创造了新的SOTA。

Conclusion: 提出的双重使用视觉特征方法是一种简单有效的视听融合策略，能够显著提升ASR系统在噪声环境下的性能，为视听语音识别提供了新的技术方向。

Abstract: In audiovisual automatic speech recognition (AV-ASR) systems, information fusion of visual features in a pre-trained ASR has been proven as a promising method to improve noise robustness. In this work, based on the prominent Whisper ASR, first, we propose a simple and effective visual fusion method -- use of visual features both in encoder and decoder (dual-use) -- to learn the audiovisual interactions in the encoder and to weigh modalities in the decoder. Second, we compare visual fusion methods in Whisper models of various sizes. Our proposed dual-use method shows consistent noise robustness improvement, e.g., a 35% relative improvement (WER: 4.41% vs. 6.83%) based on Whisper small, and a 57% relative improvement (WER: 4.07% vs. 9.53%) based on Whisper medium, compared to typical reference middle fusion in babble noise with a signal-to-noise ratio (SNR) of 0dB. Third, we conduct ablation studies examining the impact of various module designs and fusion options. Fine-tuned on 1929 hours of audiovisual data, our dual-use method using Whisper medium achieves 4.08% (MUSAN babble noise) and 4.43% (NoiseX babble noise) average WER across various SNRs, thereby establishing a new state-of-the-art in noisy conditions on the LRS3 AV-ASR benchmark. Our code is at https://github.com/ifnspaml/Dual-Use-AVASR

</details>


### [39] [Audio Inpainting in Time-Frequency Domain with Phase-Aware Prior](https://arxiv.org/abs/2601.18535)
*Peter Balušík,Pavel Rajmic*

Main category: eess.AS

TL;DR: 本文提出了一种基于相位感知信号先验的时间-频率音频修复方法，在客观和主观评估中均优于现有方法，且计算需求更低。


<details>
  <summary>Details</summary>
Motivation: 传统的音频修复主要针对时域信号，而时间-频率域的修复（重建缺失的频谱图列）是一个相对较新的挑战。现有方法如深度先验神经网络和Janssen-TF自回归方法存在改进空间，需要更高效准确的解决方案。

Method: 提出基于相位感知信号先验的方法，利用瞬时频率估计，通过广义Chambolle-Pock算法求解优化问题。

Result: 在客观评估和主观听力测试中均优于深度先验神经网络和Janssen-TF方法，同时计算需求显著降低。

Conclusion: 提出的相位感知时间-频率音频修复方法在性能和效率方面均优于现有方法，为时间-频率域音频修复提供了有效的解决方案。

Abstract: The so-called audio inpainting problem in the time domain refers to estimating missing segments of samples within a signal. Over the years, several methods have been developed for such type of audio inpainting. In contrast to this case, a time-frequency variant of inpainting appeared in the literature, where the challenge is to reconstruct missing spectrogram columns with reliable information. We propose a method to address this time-frequency audio inpainting problem. Our approach is based on the recently introduced phase-aware signal prior that exploits an estimate of the instantaneous frequency. An optimization problem is formulated and solved using the generalized Chambolle-Pock algorithm. The proposed method is evaluated both objectively and subjectively against other time-frequency inpainting methods, specifically a deep-prior neural network and the autoregression-based approach known as Janssen-TF. Our proposed approach surpassed these methods in the objective evaluation as well as in the conducted listening test. Moreover, this outcome is achieved with a substantially reduced computational requirement compared to alternative methods.

</details>


### [40] [Learning to Discover: A Generalized Framework for Raga Identification without Forgetting](https://arxiv.org/abs/2601.18766)
*Parampreet Singh,Somya Kumar,Chaitanya Shailendra Nitawe,Vipul Arora*

Main category: eess.AS

TL;DR: 提出统一学习框架解决印度艺术音乐中罕见拉格识别问题，利用有标签和无标签音频同时学习已知和未知拉格类别，避免灾难性遗忘


<details>
  <summary>Details</summary>
Motivation: 印度艺术音乐中许多罕见拉格在训练数据中缺失，传统分类模型假设封闭类别集，无法识别或分组未见拉格，现有方法存在灾难性遗忘问题

Method: 采用统一学习框架，利用有标签和无标签音频，使模型能够发现对应未见拉格的连贯类别，同时保留已知拉格知识

Result: 在基准拉格识别数据集上测试，在已知、未知和所有拉格类别分类中表现优异，超越先前基于NCD的流程，在发现未知拉格类别方面提供新见解

Conclusion: 提出的方法有效解决了罕见拉格识别中的灾难性遗忘问题，为印度艺术音乐任务的表示学习提供了新思路

Abstract: Raga identification in Indian Art Music (IAM) remains challenging due to the presence of numerous rarely performed Ragas that are not represented in available training datasets. Traditional classification models struggle in this setting, as they assume a closed set of known categories and therefore fail to recognise or meaningfully group previously unseen Ragas. Recent works have tried categorizing unseen Ragas, but they run into a problem of catastrophic forgetting, where the knowledge of previously seen Ragas is diminished. To address this problem, we adopt a unified learning framework that leverages both labeled and unlabeled audio, enabling the model to discover coherent categories corresponding to the unseen Ragas, while retaining the knowledge of previously known ones. We test our model on benchmark Raga Identification datasets and demonstrate its performance in categorizing previously seen, unseen, and all Raga classes. The proposed approach surpasses the previous NCD-based pipeline even in discovering the unseen Raga categories, offering new insights into representation learning for IAM tasks.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [41] [SonoEdit: Null-Space Constrained Knowledge Editing for Pronunciation Correction in LLM-Based TTS](https://arxiv.org/abs/2601.17086)
*Ayush Pratap Singh,Harshit Singh,Nityanand Mathur,Akshat Mandloi,Sudarshan Kamath*

Main category: cs.SD

TL;DR: SonoEdit：一种无需重新训练即可修正预训练TTS模型中特定单词发音错误的模型编辑技术，通过单次参数更新实现发音校正，同时保证其他模型行为不变。


<details>
  <summary>Details</summary>
Motivation: 神经TTS系统在处理低资源专有名词（特别是非英语名称、品牌和地名）时存在系统性发音错误，因为这些词汇在主要基于英语的训练语料中代表性不足。现有解决方案依赖昂贵的多语言数据收集、监督微调或手动音标标注，限制了TTS系统在语言多样化环境中的部署。

Method: 提出SonoEdit模型编辑技术，基于零空间发音编辑方法。首先使用声学因果追踪识别负责文本到发音映射的Transformer层，然后应用零空间约束编辑计算闭式权重更新，该更新修正目标发音的同时在数学上与控制一般语音生成的子空间正交，确保对保留语音语料库的零一阶变化。

Result: 该方法能够通过单次参数更新手术式修正预训练TTS模型中的发音错误，无需重新训练、微调或显式音素注入，在修正特定单词发音的同时可证明地保留所有其他模型行为。

Conclusion: SonoEdit提供了一种高效、精确的TTS发音校正方法，解决了低资源专有名词发音问题，为在语言多样化环境中部署TTS系统提供了实用解决方案，避免了传统方法的高成本和复杂性。

Abstract: Neural text-to-speech (TTS) systems systematically mispronounce low-resource proper nouns, particularly non-English names, brands, and geographic locations, due to their underrepresentation in predominantly English training corpora. Existing solutions typically rely on expensive multilingual data collection, supervised finetuning, or manual phonetic annotation, which limits the deployment of TTS systems in linguistically diverse settings. We introduce SonoEdit, a model editing technique that surgically corrects pronunciation errors in pre-trained TTS models without retraining. Instead of costly finetuning or explicit phoneme injection, we propose a parsimonious alternative based on Null-Space Pronunciation Editing, which performs a single-shot parameter update to modify the pronunciation of specific words while provably preserving all other model behavior. We first adapt Acoustic Causal Tracing to identify the Transformer layers responsible for text-to-pronunciation mapping. We then apply Null-Space Constrained Editing to compute a closed-form weight update that corrects the target pronunciation while remaining mathematically orthogonal to the subspace governing general speech generation. This constrained update steers the model's acoustic output toward a desired pronunciation exemplar while guaranteeing zero first-order change on a preserved speech corpus.

</details>


### [42] [Sink or SWIM: Tackling Real-Time ASR at Scale](https://arxiv.org/abs/2601.17097)
*Federico Bruzzone,Walter Cazzola,Matteo Brancaleoni,Dario Pellegrino*

Main category: cs.SD

TL;DR: SWIM是基于Whisper的实时ASR系统，支持多客户端并行处理，在保持准确率的同时显著降低延迟，可扩展到20个并发用户。


<details>
  <summary>Details</summary>
Motivation: 实时语音识别系统需要支持多并发客户端，同时保持低延迟和高准确率，这是当前的主要挑战。现有系统难以在模型层面实现并行化，特别是在多语言多用户环境中。

Method: 基于OpenAI的Whisper模型构建SWIM系统，引入缓冲合并策略，在不修改底层模型的情况下支持多个并发音频流，实现真正的模型级并行化。

Result: 在20个并发用户的多客户端设置中，SWIM在英语、意大利语和西班牙语上都能提供准确的实时转录。相比Whisper-Streaming单客户端8.2%词错误率和3.4秒延迟，SWIM在5个客户端时延迟降至2.4秒，同时保持可比较的准确率，并能有效扩展到20个客户端而不降低转录质量。

Conclusion: SWIM通过改进动态多用户环境中的鲁棒性和效率，推进了可扩展ASR系统的发展，为多语言多客户端实时语音识别提供了有效解决方案。

Abstract: Real-time automatic speech recognition systems are increasingly integrated into interactive applications, from voice assistants to live transcription services. However, scaling these systems to support multiple concurrent clients while maintaining low latency and high accuracy remains a major challenge. In this work, we present SWIM, a novel real-time ASR system built on top of OpenAI's Whisper model that enables true model-level parallelization for scalable, multilingual transcription. SWIM supports multiple concurrent audio streams without modifying the underlying model. It introduces a buffer merging strategy that maintains transcription fidelity while ensuring efficient resource usage. We evaluate SWIM in multi-client settings -- scaling up to 20 concurrent users -- and show that it delivers accurate real-time transcriptions in English, Italian, and Spanish, while maintaining low latency and high throughput. While Whisper-Streaming achieves a word error rate of approximately 8.2% with an average delay of approximately 3.4 s in a single-client, English-only setting, SWIM extends this capability to multilingual, multi-client environments. It maintains comparable accuracy with significantly lower delay -- around 2.4 s with 5 clients -- and continues to scale effectively up to 20 concurrent clients without degrading transcription quality and increasing overall throughput. Our approach advances scalable ASR by improving robustness and efficiency in dynamic, multi-user environments.

</details>


### [43] [Window Size Versus Accuracy Experiments in Voice Activity Detectors](https://arxiv.org/abs/2601.17270)
*Max McKinnon,Samir Khaki,Chandan KA Reddy,William Huang*

Main category: cs.SD

TL;DR: 研究分析窗口大小对三种语音活动检测算法性能的影响，发现Silero显著优于WebRTC和RMS，迟滞对WebRTC有改善作用


<details>
  <summary>Details</summary>
Motivation: 语音活动检测在语音识别等应用中至关重要，需要了解不同算法在不同窗口大小下的性能表现，为优化VAD系统提供实用参考

Method: 分析窗口大小对三种VAD算法（Silero、WebRTC、RMS）准确率的影响，在多样化的真实数字音频流上进行测试，并在每种VAD输出上探索迟滞技术的应用

Result: Silero算法显著优于WebRTC和RMS算法，迟滞技术对WebRTC算法有改善作用，为优化VAD系统提供了实用参考

Conclusion: 窗口大小对VAD算法性能有重要影响，Silero是最优选择，迟滞技术可改善WebRTC性能，研究结果为VAD系统优化提供了实用指导

Abstract: Voice activity detection (VAD) plays a vital role in enabling applications such as speech recognition. We analyze the impact of window size on the accuracy of three VAD algorithms: Silero, WebRTC, and Root Mean Square (RMS) across a set of diverse real-world digital audio streams. We additionally explore the use of hysteresis on top of each VAD output. Our results offer practical references for optimizing VAD systems. Silero significantly outperforms WebRTC and RMS, and hysteresis provides a benefit for WebRTC.

</details>


### [44] [EuleroDec: A Complex-Valued RVQ-VAE for Efficient and Robust Audio Coding](https://arxiv.org/abs/2601.17517)
*Luca Cerovaz,Michele Mancusi,Emanuele Rodolà*

Main category: cs.SD

TL;DR: 提出一种端到端的复数RVQ-VAE音频编解码器，保留幅度-相位耦合，无需对抗判别器或扩散后处理，在相位一致性和波形保真度上达到SOTA性能，训练效率提升一个数量级。


<details>
  <summary>Details</summary>
Motivation: 当前频域神经编解码器要么忽略相位信息，要么将其编码为两个独立的实值通道，限制了空间保真度。为了补偿音频信号表示能力的不足，需要引入对抗判别器，但这会牺牲收敛速度和训练稳定性。

Method: 提出端到端的复数RVQ-VAE音频编解码器，在整个分析-量化-合成流程中保持幅度-相位耦合，移除了对抗判别器和扩散后滤波器。模型采用复数表示，无需GAN或扩散技术。

Result: 在域内匹配或超越训练时间更长的基线模型，在域外达到相位一致性和波形保真度的SOTA性能。相比需要数十万步训练的标准基线，本模型训练预算减少一个数量级，计算效率显著提升同时保持高感知质量。

Conclusion: 复数RVQ-VAE音频编解码器通过保留幅度-相位耦合，无需对抗训练即可实现高质量音频压缩，在性能和训练效率方面都有显著优势，为音频编解码提供了更稳定高效的解决方案。

Abstract: Audio codecs power discrete music generative modelling, music streaming, and immersive media by shrinking PCM audio to bandwidth-friendly bitrates. Recent works have gravitated towards processing in the spectral domain; however, spectrogram domains typically struggle with phase modeling, which is naturally complex-valued. Most frequency-domain neural codecs either disregard phase information or encode it as two separate real-valued channels, limiting spatial fidelity. This entails the need to introduce adversarial discriminators at the expense of convergence speed and training stability to compensate for the inadequate representation power of the audio signal. In this work we introduce an end-to-end complex-valued RVQ-VAE audio codec that preserves magnitude-phase coupling across the entire analysis-quantization-synthesis pipeline and removes adversarial discriminators and diffusion post-filters. Without GANs or diffusion, we match or surpass much longer-trained baselines in-domain and reach SOTA out-of-domain performance on phase coherence and waveform fidelity. Compared to standard baselines that train for hundreds of thousands of steps, our model, which reduces the training budget by an order of magnitude, is markedly more compute-efficient while preserving high perceptual quality.

</details>


### [45] [AVMeme Exam: A Multimodal Multilingual Multicultural Benchmark for LLMs' Contextual and Cultural Knowledge and Thinking](https://arxiv.org/abs/2601.17645)
*Xilin Jiang,Qiaolin Wang,Junkai Wu,Xiaomin He,Zhongweiyang Xu,Yinghao Ma,Minshuo Piao,Kaiyi Yang,Xiuwen Zheng,Riki Shimizu,Yicong Chen,Arsalan Firoozi,Gavin Mischler,Sukru Samet Dindar,Richard Antonello,Linyang He,Tsun-An Hsieh,Xulin Fan,Yulun Wu,Yuesheng Ma,Chaitanya Amballa,Weixiong Chen,Jiarui Hai,Ruisi Li,Vishal Choudhari,Cong Han,Yinghao Aaron Li,Adeen Flinker,Mounya Elhilali,Emmanouil Benetos,Mark Hasegawa-Johnson,Romit Roy Choudhury,Nima Mesgarani*

Main category: cs.SD

TL;DR: AVMeme Exam是一个评估AI模型理解互联网音视频文化含义的基准测试，包含1000多个标志性网络梗，测试结果显示当前多模态大语言模型在无文本音乐、音效以及文化背景理解方面存在明显不足。


<details>
  <summary>Details</summary>
Motivation: 互联网音视频通过随时间变化的声音和动作传达意义，这超出了纯文本所能表达的范围。为了研究AI模型是否能在人类文化背景下理解这些信号，需要建立一个专门的评估基准。

Method: 创建AVMeme Exam基准，包含1000多个标志性互联网声音和视频，涵盖演讲、歌曲、音乐和音效。每个梗都配有独特的问答，评估从表面内容到背景、情感、使用方式和世界知识等不同层次的理解，并包含原始年份、转录、摘要和敏感性等元数据。使用该基准系统评估最先进的多模态大语言模型和人类参与者。

Result: 当前模型在无文本音乐和音效方面表现不佳，在背景和文化思维方面相比表面内容理解存在困难。与人类相比，模型在文化背景理解方面存在明显差距。

Conclusion: 研究揭示了人类对齐多模态智能的关键差距，呼吁开发能够超越表面感知、理解背景和文化的模型。

Abstract: Internet audio-visual clips convey meaning through time-varying sound and motion, which extend beyond what text alone can represent. To examine whether AI models can understand such signals in human cultural contexts, we introduce AVMeme Exam, a human-curated benchmark of over one thousand iconic Internet sounds and videos spanning speech, songs, music, and sound effects. Each meme is paired with a unique Q&A assessing levels of understanding from surface content to context and emotion to usage and world knowledge, along with metadata such as original year, transcript, summary, and sensitivity. We systematically evaluate state-of-the-art multimodal large language models (MLLMs) alongside human participants using this benchmark. Our results reveal a consistent limitation: current models perform poorly on textless music and sound effects, and struggle to think in context and in culture compared to surface content. These findings highlight a key gap in human-aligned multimodal intelligence and call for models that can perceive contextually and culturally beyond the surface of what they hear and see. Project page: avmemeexam.github.io/public

</details>


### [46] [BanglaRobustNet: A Hybrid Denoising-Attention Architecture for Robust Bangla Speech Recognition](https://arxiv.org/abs/2601.17679)
*Md Sazzadul Islam Ridoy,Mubaswira Ibnat Zidney,Sumi Akter,Md. Aminur Rahman*

Main category: cs.SD

TL;DR: BanglaRobustNet：基于Wav2Vec-BERT的混合去噪注意力框架，专门针对嘈杂和说话者多样的孟加拉语ASR场景，通过扩散去噪和上下文交叉注意力模块显著降低错误率。


<details>
  <summary>Details</summary>
Motivation: 孟加拉语作为广泛使用的语言，在自动语音识别研究中代表性不足，特别是在嘈杂环境和说话者多样性条件下。现有ASR系统在处理噪声和方言变化时表现不佳。

Method: 提出BanglaRobustNet混合框架：1）扩散去噪模块抑制环境噪声同时保留孟加拉语特定语音特征；2）上下文交叉注意力模块基于说话者嵌入增强识别鲁棒性；3）端到端训练，结合CTC损失、语音一致性和说话者对齐的复合目标函数。

Result: 在Mozilla Common Voice Bangla和增强噪声语音数据集上评估，相比Wav2Vec-BERT和Whisper基线，在词错误率（WER）和字符错误率（CER）上取得显著降低。

Conclusion: BanglaRobustNet成为针对低资源、易受噪声影响语言环境的鲁棒ASR系统，有效解决了孟加拉语在嘈杂和说话者多样条件下的ASR挑战。

Abstract: Bangla, one of the most widely spoken languages, remains underrepresented in state-of-the-art automatic speech recognition (ASR) research, particularly under noisy and speaker-diverse conditions. This paper presents BanglaRobustNet, a hybrid denoising-attention framework built on Wav2Vec-BERT, designed to address these challenges. The architecture integrates a diffusion-based denoising module to suppress environmental noise while preserving Bangla-specific phonetic cues, and a contextual cross-attention module that conditions recognition on speaker embeddings for robustness across gender, age, and dialects. Trained end-to-end with a composite objective combining CTC loss, phonetic consistency, and speaker alignment, BanglaRobustNet achieves substantial reductions in word error rate (WER) and character error rate (CER) compared to Wav2Vec-BERT and Whisper baselines. Evaluations on Mozilla Common Voice Bangla and augmented noisy speech confirm the effectiveness of our approach, establishing BanglaRobustNet as a robust ASR system tailored to low-resource, noise-prone linguistic settings.

</details>


### [47] [Segment Length Matters: A Study of Segment Lengths on Audio Fingerprinting Performance](https://arxiv.org/abs/2601.17690)
*Ziling Gong,Yunyan Ouyang,Iram Kamdar,Melody Ma,Hongjie Chen,Franck Dernoncourt,Ryan A. Rossi,Nesreen K. Ahmed*

Main category: cs.SD

TL;DR: 研究音频指纹识别中片段长度对性能的影响，发现0.5秒短片段表现最佳，并评估了LLM在推荐最佳片段长度方面的能力。


<details>
  <summary>Details</summary>
Motivation: 当前神经音频指纹识别方法通常使用固定时长的短音频片段，但片段长度的选择往往是启发式的，缺乏深入研究。需要系统研究片段长度如何影响音频指纹识别性能。

Method: 扩展现有神经指纹识别架构以支持不同片段长度，评估不同片段长度和查询时长下的检索准确率，并测试三种LLM（包括GPT-5-mini）在推荐最佳片段长度方面的能力。

Result: 短片段长度（0.5秒）通常获得更好的性能表现。在三种研究的LLM中，GPT-5-mini在五个考量维度上始终给出最佳建议。

Conclusion: 研究结果为大规模神经音频检索系统中片段时长的选择提供了实用指导，短片段（0.5秒）是更优选择，且LLM（特别是GPT-5-mini）能够有效推荐最佳片段长度。

Abstract: Audio fingerprinting provides an identifiable representation of acoustic signals, which can be later used for identification and retrieval systems. To obtain a discriminative representation, the input audio is usually segmented into shorter time intervals, allowing local acoustic features to be extracted and analyzed. Modern neural approaches typically operate on short, fixed-duration audio segments, yet the choice of segment duration is often made heuristically and rarely examined in depth. In this paper, we study how segment length affects audio fingerprinting performance. We extend an existing neural fingerprinting architecture to adopt various segment lengths and evaluate retrieval accuracy across different segment lengths and query durations. Our results show that short segment lengths (0.5-second) generally achieve better performance. Moreover, we evaluate LLM capacity in recommending the best segment length, which shows that GPT-5-mini consistently gives the best suggestions across five considerations among three studied LLMs. Our findings provide practical guidance for selecting segment duration in large-scale neural audio retrieval systems.

</details>


### [48] [CaSNet: Compress-and-Send Network Based Multi-Device Speech Enhancement Model for Distributed Microphone Arrays](https://arxiv.org/abs/2601.17711)
*Chengqian Jiang,Jie Zhang,Haoyin Yan*

Main category: cs.SD

TL;DR: 提出CaSNet用于分布式麦克风阵列的语音增强，通过SVD压缩特征减少带宽需求，在FC端进行特征对齐和解码以生成增强语音


<details>
  <summary>Details</summary>
Motivation: 分布式麦克风阵列需要语音增强来提升嘈杂环境下的语音质量，但现有方法需要将所有原始波形传输到融合中心，导致高带宽和能耗成本

Method: CaSNet采用压缩发送网络：一个麦克风作为FC和参考，其他设备将原始数据编码为特征矩阵，通过SVD压缩，在FC端通过跨窗口查询对齐特征，最后神经解码生成增强语音

Result: 在多个数据集上的实验表明，CaSNet能显著减少数据传输量，同时性能与未压缩情况相比影响可忽略

Conclusion: CaSNet为资源受限的分布式麦克风阵列提供了一种高效的语音增强解决方案，在保持性能的同时大幅降低带宽需求

Abstract: Distributed microphone array (DMA) is a promising next-generation platform for speech interaction, where speech enhancement (SE) is still required to improve the speech quality in noisy cases. Existing SE methods usually first gather raw waveforms at a fusion center (FC) from all devices and then design a multi-microphone model, causing high bandwidth and energy costs. In this work, we propose a \emph{Compress-and-Send Network (CaSNet)} for resource-constrained DMAs, where one microphone serves as the FC and reference. Each of other devices encodes the measured raw data into a feature matrix, which is then compressed by singular value decomposition (SVD) to produce a more compact representation. The received features at the FC are aligned via cross window query with respect to the reference, followed by neural decoding to yield spatially coherent enhanced speech. Experiments on multiple datasets show that the proposed CaSNet can save the data amount with a negligible impact on the performance compared to the uncompressed case. The reproducible code is available at https://github.com/Jokejiangv/CaSNet.

</details>


### [49] [dLLM-ASR: A Faster Diffusion LLM-based Framework for Speech Recognition](https://arxiv.org/abs/2601.17902)
*Wenjie Tian,Bingshen Mu,Guobin Ma,Xuelong Geng,Zhixian Zhao,Lei Xie*

Main category: cs.SD

TL;DR: dLLM-ASR：一种基于离散扩散大语言模型的高效自动语音识别框架，通过先验引导的自适应去噪过程，在保持识别精度的同时实现4.44倍推理加速


<details>
  <summary>Details</summary>
Motivation: 传统基于LLM的ASR系统存在token-by-token生成导致的线性推理延迟问题，而直接应用文本导向的离散扩散LLM到ASR任务存在开放文本生成与声学条件转录之间的不匹配，导致不必要的计算冗余和困难

Method: 提出dLLM-ASR框架，将dLLM解码重新定义为先验引导的自适应去噪过程：1）利用ASR先验初始化去噪过程并提供序列长度锚点；2）长度自适应剪枝动态移除冗余token；3）基于置信度的去噪让收敛token提前退出循环，实现token级自适应计算

Result: 实验表明dLLM-ASR在识别准确率上与自回归LLM-based ASR系统相当，同时实现了4.44倍的推理加速，为ASR建立了实用高效的新范式

Conclusion: dLLM-ASR成功解决了离散扩散LLM在ASR任务中的不匹配问题，通过先验引导的自适应去噪机制，在保持性能的同时显著提升推理效率，为ASR系统提供了更实用的解决方案

Abstract: Automatic speech recognition (ASR) systems based on large language models (LLMs) achieve superior performance by leveraging pretrained LLMs as decoders, but their token-by-token generation mechanism leads to inference latency that grows linearly with sequence length. Meanwhile, discrete diffusion large language models (dLLMs) offer a promising alternative, enabling high-quality parallel sequence generation with pretrained decoders. However, directly applying native text-oriented dLLMs to ASR leads to a fundamental mismatch between open-ended text generation and the acoustically conditioned transcription paradigm required by ASR. As a result, it introduces unnecessary difficulty and computational redundancy, such as denoising from pure noise, inflexible generation lengths, and fixed denoising steps. We propose dLLM-ASR, an efficient dLLM-based ASR framework that formulates dLLM's decoding as a prior-guided and adaptive denoising process. It leverages an ASR prior to initialize the denoising process and provide an anchor for sequence length. Building upon this prior, length-adaptive pruning dynamically removes redundant tokens, while confidence-based denoising allows converged tokens to exit the denoising loop early, enabling token-level adaptive computation. Experiments demonstrate that dLLM-ASR achieves recognition accuracy comparable to autoregressive LLM-based ASR systems and delivers a 4.44$\times$ inference speedup, establishing a practical and efficient paradigm for ASR.

</details>


### [50] [From Human Speech to Ocean Signals: Transferring Speech Large Models for Underwater Acoustic Target Recognition](https://arxiv.org/abs/2601.18086)
*Mengcheng Huang,Xue Zhou,Chen Xu,Dapeng Man*

Main category: cs.SD

TL;DR: 论文提出UATR-SLM框架，利用预训练语音大模型进行水下声学目标识别，在DeepShip和ShipsEar数据集上取得优异性能


<details>
  <summary>Details</summary>
Motivation: 水下声学目标识别面临标注数据有限和海洋环境复杂的挑战，研究探索预训练语音大模型能否有效迁移到水下声学领域

Method: 提出UATR-SLM框架：复用语音特征提取流程，将语音大模型适配为声学编码器，并添加轻量级分类器

Result: 在DeepShip和ShipsEar基准测试中，UATR-SLM达到超过99%的域内准确率，对不同信号长度保持强鲁棒性，跨域评估准确率最高达96.67%

Conclusion: 语音大模型在水下声学目标识别中表现出强大的可迁移性，为利用语音基础模型处理水下声学问题建立了有前景的范式

Abstract: Underwater acoustic target recognition (UATR) plays a vital role in marine applications but remains challenging due to limited labeled data and the complexity of ocean environments. This paper explores a central question: can speech large models (SLMs), trained on massive human speech corpora, be effectively transferred to underwater acoustics? To investigate this, we propose UATR-SLM, a simple framework that reuses the speech feature pipeline, adapts the SLM as an acoustic encoder, and adds a lightweight classifier.Experiments on the DeepShip and ShipsEar benchmarks show that UATR-SLM achieves over 99% in-domain accuracy, maintains strong robustness across variable signal lengths, and reaches up to 96.67% accuracy in cross-domain evaluation. These results highlight the strong transferability of SLMs to UATR, establishing a promising paradigm for leveraging speech foundation models in underwater acoustics.

</details>


### [51] [VIBEVOICE-ASR Technical Report](https://arxiv.org/abs/2601.18184)
*Zhiliang Peng,Jianwei Yu,Yaoyao Chang,Zilong Wang,Li Dong,Yingbo Hao,Yujie Tu,Chenyu Yang,Wenhui Wang,Songchen Xu,Yutao Sun,Hangbo Bao,Weijiang Xu,Yi Zhu,Zehua Wang,Ting Song,Yan Xia,Zewen Chi,Shaohan Huang,Liang Wang,Chuang Ding,Shuai Wang,Xie Chen,Furu Wei*

Main category: cs.SD

TL;DR: VibeVoice-ASR是一个通用语音理解框架，支持长达60分钟的音频单次处理，统一了语音识别、说话人分离和时间戳标注，支持50+语言和代码切换，并引入了基于提示的上下文注入机制。


<details>
  <summary>Details</summary>
Motivation: 解决长音频（如会议、播客）处理中的上下文碎片化和多说话人复杂性问题，传统基于音频分块的方法存在局限性，需要更统一的端到端解决方案。

Method: 基于VibeVoice构建，支持单次处理长达60分钟音频，将语音识别、说话人分离和时间戳标注统一为单一端到端生成任务，引入基于提示的上下文注入机制。

Result: 支持50多种语言，无需显式语言设置，原生处理代码切换，通过上下文注入显著提高领域特定术语和多音字消歧的准确性。

Conclusion: VibeVoice-ASR提供了一个强大的通用语音理解框架，有效解决了长音频处理中的关键挑战，通过统一的端到端方法和上下文注入机制提升了整体性能。

Abstract: This report presents VibeVoice-ASR, a general-purpose speech understanding framework built upon VibeVoice, designed to address the persistent challenges of context fragmentation and multi-speaker complexity in long-form audio (e.g., meetings, podcasts) that remain despite recent advancements in short-form speech recognition. Unlike traditional pipelined approaches that rely on audio chunking, VibeVoice-ASRsupports single-pass processing for up to 60 minutes of audio. It unifies Automatic Speech Recognition, Speaker Diarization, and Timestamping into a single end-to-end generation task. In addition, VibeVoice-ASR supports over 50 languages, requires no explicit language setting, and natively handles code-switching within and across utterances. Furthermore, we introduce a prompt-based context injection mechanism that allows users to supply customized conetxt, significantly improving accuracy on domain-specific terminology and polyphonic character disambiguation.

</details>


### [52] [LLM-ForcedAligner: A Non-Autoregressive and Accurate LLM-Based Forced Aligner for Multilingual and Long-Form Speech](https://arxiv.org/abs/2601.18220)
*Bingshen Mu,Xian Shi,Xiong Wang,Hexin Liu,Jin Xu,Lei Xie*

Main category: cs.SD

TL;DR: 提出LLM-ForcedAligner，将强制对齐重新定义为槽填充任务，利用语音大语言模型处理多语言和长序列语音，显著减少时间累积偏移


<details>
  <summary>Details</summary>
Motivation: 现有强制对齐方法存在语言特定性和时间累积偏移问题，而语音大语言模型在多语言理解和长序列处理方面具有优势，但直接应用会产生幻觉和推理速度慢的问题

Method: 将强制对齐重新定义为槽填充范式：将时间戳视为离散索引，在文本中插入特殊时间戳槽作为占位符；基于语音嵌入和带槽的文本，SLLM直接预测槽位置的时间索引；使用因果注意力掩码和非移位输入序列进行训练

Result: 在多语言、跨语言和长语音场景下，LLM-ForcedAligner相比先前方法实现了69%~78%的相对累积平均偏移减少

Conclusion: LLM-ForcedAligner通过槽填充范式有效解决了传统强制对齐方法的局限性，在减少时间偏移的同时支持非自回归推理，避免了幻觉问题并提高了速度

Abstract: Forced alignment (FA) predicts start and end timestamps for words or characters in speech, but existing methods are language-specific and prone to cumulative temporal shifts. The multilingual speech understanding and long-sequence processing abilities of speech large language models (SLLMs) make them promising for FA in multilingual, crosslingual, and long-form speech settings. However, directly applying the next-token prediction paradigm of SLLMs to FA results in hallucinations and slow inference. To bridge the gap, we propose LLM-ForcedAligner, reformulating FA as a slot-filling paradigm: timestamps are treated as discrete indices, and special timestamp tokens are inserted as slots into the transcript. Conditioned on the speech embeddings and the transcript with slots, the SLLM directly predicts the time indices at slots. During training, causal attention masking with non-shifted input and label sequences allows each slot to predict its own timestamp index based on itself and preceding context, with loss computed only at slot positions. Dynamic slot insertion enables FA at arbitrary positions. Moreover, non-autoregressive inference is supported, avoiding hallucinations and improving speed. Experiments across multilingual, crosslingual, and long-form speech scenarios show that LLM-ForcedAligner achieves a 69%~78% relative reduction in accumulated averaging shift compared with prior methods. The checkpoint and inference code will be released later.

</details>


### [53] [Analytic Incremental Learning For Sound Source Localization With Imbalance Rectification](https://arxiv.org/abs/2601.18335)
*Zexia Fan,Yu Chen,Qiquan Zhang,Kainan Chen,Xinyuan Qian*

Main category: cs.SD

TL;DR: 提出一个统一框架解决声源定位中的双重不平衡问题：通过GCC-PHAT数据增强缓解任务内不平衡，通过分析动态不平衡校正器解决任务间不平衡，在SSLR基准上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 声源定位在受控环境下表现良好，但在实际部署中面临双重不平衡挑战：1）任务内不平衡（长尾DoA分布）；2）任务间不平衡（跨任务偏差和重叠）。这些问题导致灾难性遗忘，显著降低定位精度。

Method: 提出统一框架包含两个关键创新：1）GCC-PHAT数据增强方法，利用峰值特征缓解任务内分布偏差；2）分析动态不平衡校正器，具有任务自适应正则化，能够适应任务间动态的分析更新。

Result: 在SSLR基准上达到最先进结果：89.0%准确率、5.3°平均绝对误差和1.6后向迁移，在不存储示例的情况下对演化不平衡表现出鲁棒性。

Conclusion: 该框架有效解决了声源定位中的双重不平衡问题，通过数据增强和动态不平衡校正器的结合，显著提升了在实际部署中的性能，避免了灾难性遗忘。

Abstract: Sound source localization (SSL) demonstrates remarkable results in controlled settings but struggles in real-world deployment due to dual imbalance challenges: intra-task imbalance arising from long-tailed direction-of-arrival (DoA) distributions, and inter-task imbalance induced by cross-task skews and overlaps. These often lead to catastrophic forgetting, significantly degrading the localization accuracy. To mitigate these issues, we propose a unified framework with two key innovations. Specifically, we design a GCC-PHAT-based data augmentation (GDA) method that leverages peak characteristics to alleviate intra-task distribution skews. We also propose an Analytic dynamic imbalance rectifier (ADIR) with task-adaption regularization, which enables analytic updates that adapt to inter-task dynamics. On the SSLR benchmark, our proposal achieves state-of-the-art (SoTA) results of 89.0% accuracy, 5.3° mean absolute error, and 1.6 backward transfer, demonstrating robustness to evolving imbalances without exemplar storage.

</details>


### [54] [A Dataset for Automatic Vocal Mode Classification](https://arxiv.org/abs/2601.18339)
*Reemt Hinrichs,Sonja Stephan,Alexander Lange,Jörn Ostermann*

Main category: cs.SD

TL;DR: 本文介绍了首个完整的声乐模式数据集，用于自动分类CVT声乐模式，包含4位歌手的3752个样本，使用4个麦克风录制，提供基线分类结果（ResNet18达到81.3%平衡准确率）。


<details>
  <summary>Details</summary>
Motivation: 自动分类声乐模式对技术辅助声乐教学很重要，但之前的研究因数据不足而效果不佳，因此需要创建专门的声乐模式数据集。

Method: 录制了包含4位歌手（3位专业CVT歌手）持续元音的新数据集，覆盖完整音域，使用4个麦克风进行自然数据增强，由3位CVT专家分别标注，提供合并标注和个体标注。

Result: 数据集包含3752个独特样本，通过多麦克风录制扩展至13000+样本。基线分类中，ResNet18在5折交叉验证中达到81.3%的平衡准确率。

Conclusion: 该数据集填补了声乐模式分类研究的空白，为技术辅助声乐教学提供了重要资源，基线结果显示了自动分类的可行性。

Abstract: The Complete Vocal Technique (CVT) is a school of singing developed in the past decades by Cathrin Sadolin et al.. CVT groups the use of the voice into so called vocal modes, namely Neutral, Curbing, Overdrive and Edge. Knowledge of the desired vocal mode can be helpful for singing students. Automatic classification of vocal modes can thus be important for technology-assisted singing teaching. Previously, automatic classification of vocal modes has been attempted without major success, potentially due to a lack of data. Therefore, we recorded a novel vocal mode dataset consisting of sustained vowels recorded from four singers, three of which professional singers with more than five years of CVT-experience. The dataset covers the entire vocal range of the subjects, totaling 3,752 unique samples. By using four microphones, thereby offering a natural data augmentation, the dataset consists of more than 13,000 samples combined. An annotation was created using three CVT-experienced annotators, each providing an individual annotation. The merged annotation as well as the three individual annotations come with the published dataset. Additionally, we provide some baseline classification results. The best balanced accuracy across a 5-fold cross validation of 81.3\,\% was achieved with a ResNet18. The dataset can be downloaded under https://zenodo.org/records/14276415.

</details>


### [55] [OCR-Enhanced Multimodal ASR Can Read While Listening](https://arxiv.org/abs/2601.18393)
*Junli Chen,Changli Tang,Yixuan Li,Guangzhi Sun,Chao Zhang*

Main category: cs.SD

TL;DR: Donut-Whisper是一个音频-视觉ASR模型，通过双编码器利用视觉信息（如电影字幕）提升英中双语语音识别性能，结合线性与Q-Former模态对齐结构，并提出轻量级知识蒸馏方法。


<details>
  <summary>Details</summary>
Motivation: 视觉信息（如电影字幕）通常有助于自动语音识别，但现有方法未能充分利用音频-视觉多模态信息来提升英中双语语音识别性能。

Method: 1) 提出Donut-Whisper音频-视觉ASR模型，采用双编码器架构；2) 通过交叉注意力模块结合线性与Q-Former模态对齐结构的优势；3) 提出轻量级知识蒸馏方案，用音频-视觉模型指导纯音频模型；4) 构建基于电影片段的多语言音频-视觉语音识别数据集，包含中英文部分。

Result: Donut-Whisper在英中数据集上均显著优于Donut和Whisper large V3基线。相比Whisper ASR基线，在英文集上实现5.75%绝对WER降低，在中文集上实现16.5%绝对CER降低。

Conclusion: Donut-Whisper通过有效结合音频和视觉信息，显著提升了英中双语语音识别性能，证明了多模态方法在ASR任务中的价值，同时提出的知识蒸馏方案展示了音频-视觉模型指导纯音频模型的潜力。

Abstract: Visual information, such as subtitles in a movie, often helps automatic speech recognition. In this paper, we propose Donut-Whisper, an audio-visual ASR model with dual encoder to leverage visual information to improve speech recognition performance in both English and Chinese. Donut-Whisper combines the advantage of the linear and the Q-Former-based modality alignment structures via a cross-attention module, generating more powerful audio-visual features. Meanwhile, we propose a lightweight knowledge distillation scheme showcasing the potential of using audio-visual models to teach audio-only models to achieve better performance. Moreover, we propose a new multilingual audio-visual speech recognition dataset based on movie clips containing both Chinese and English partitions. As a result, Donut-Whisper achieved significantly better performance on both English and Chinese partition of the dataset compared to both Donut and Whisper large V3 baselines. In particular, an absolute 5.75% WER reduction and a 16.5% absolute CER reduction were achieved on the English and Chinese sets respectively compared to the Whisper ASR baseline.

</details>


### [56] [UrgentMOS: Unified Multi-Metric and Preference Learning for Robust Speech Quality Assessment](https://arxiv.org/abs/2601.18438)
*Wei Wang,Wangyou Zhang,Chenda Li,Jiahe Wang,Samuele Cornell,Marvin Sach,Kohei Saijo,Yihui Fu,Zhaoheng Ni,Bing Han,Xun Gong,Mengxiao Bi,Tim Fingscheidt,Shinji Watanabe,Yanmin Qian*

Main category: cs.SD

TL;DR: UrgentMOS：一个统一的语音质量评估框架，通过联合学习多样化的客观和感知质量指标，容忍训练期间任意指标子集的缺失，在绝对和比较评估设置中均实现最先进性能。


<details>
  <summary>Details</summary>
Motivation: 自动语音质量评估变得越来越重要，因为现代语音生成系统不断进步，而人工听力测试仍然成本高、耗时且难以扩展。现有基于学习的评估模型主要依赖稀缺的人工标注平均意见分数（MOS）数据，这限制了鲁棒性和泛化能力，特别是在跨异构数据集训练时。

Method: 提出UrgentMOS框架，联合学习多样化的客观和感知质量指标，明确容忍训练期间任意指标子集的缺失。除了绝对分数预测外，还通过直接预测比较MOS（CMOS）来显式建模成对质量偏好。

Result: 在广泛的语音质量数据集（包括模拟失真、语音增强和语音合成）上的大量实验表明，UrgentMOS在绝对和比较评估设置中均一致实现最先进的性能。

Conclusion: UrgentMOS通过利用异构监督下的互补质量方面，实现了部分标注数据的有效利用，并在大规模多源数据集训练时提高了鲁棒性，特别适合系统基准测试中常用的基于偏好的评估场景。

Abstract: Automatic speech quality assessment has become increasingly important as modern speech generation systems continue to advance, while human listening tests remain costly, time-consuming, and difficult to scale. Most existing learning-based assessment models rely primarily on scarce human-annotated mean opinion score (MOS) data, which limits robustness and generalization, especially when training across heterogeneous datasets. In this work, we propose UrgentMOS, a unified speech quality assessment framework that jointly learns from diverse objective and perceptual quality metrics, while explicitly tolerating the absence of arbitrary subsets of metrics during training. By leveraging complementary quality facets under heterogeneous supervision, UrgentMOS enables effective utilization of partially annotated data and improves robustness when trained on large-scale, multi-source datasets. Beyond absolute score prediction, UrgentMOS explicitly models pairwise quality preferences by directly predicting comparative MOS (CMOS), making it well suited for preference-based evaluation scenarios commonly adopted in system benchmarking. Extensive experiments across a wide range of speech quality datasets, including simulated distortions, speech enhancement, and speech synthesis, demonstrate that UrgentMOS consistently achieves state-of-the-art performance in both absolute and comparative evaluation settings.

</details>


### [57] [Geneses: Unified Generative Speech Enhancement and Separation](https://arxiv.org/abs/2601.18456)
*Kohei Asai,Wataru Nakata,Yuki Saito,Hiroshi Saruwatari*

Main category: cs.SD

TL;DR: Geneses是一个生成式框架，通过潜在流匹配和多模态扩散Transformer，在复杂退化条件下实现高质量的统一语音增强与分离。


<details>
  <summary>Details</summary>
Motivation: 现实世界的音频录音通常包含多个说话者和各种退化，这限制了构建最先进语音处理模型的数据数量和质量。传统的SE-SS方法在处理超出加性噪声的复杂退化时效果有限。

Method: 提出Geneses生成式框架，利用潜在流匹配来估计每个说话者的干净语音特征，使用多模态扩散Transformer，并以来自噪声混合的自监督学习表示作为条件。

Result: 在LibriTTS-R双说话者混合数据集上的实验表明，Geneses在加性噪声和复杂退化两种条件下，在各种客观指标上显著优于传统的基于掩码的SE-SS方法，对复杂退化具有高鲁棒性。

Conclusion: Geneses是一个有效的生成式框架，能够在复杂退化条件下实现高质量的统一语音增强与分离，为现实世界多说话者音频处理提供了有前景的解决方案。

Abstract: Real-world audio recordings often contain multiple speakers and various degradations, which limit both the quantity and quality of speech data available for building state-of-the-art speech processing models. Although end-to-end approaches that concatenate speech enhancement (SE) and speech separation (SS) to obtain a clean speech signal for each speaker are promising, conventional SE-SS methods suffer from complex degradations beyond additive noise. To this end, we propose \textbf{Geneses}, a generative framework to achieve unified, high-quality SE--SS. Our Geneses leverages latent flow matching to estimate each speaker's clean speech features using multi-modal diffusion Transformer conditioned on self-supervised learning representation from noisy mixture. We conduct experimental evaluation using two-speaker mixtures from LibriTTS-R under two conditions: additive-noise-only and complex degradations. The results demonstrate that Geneses significantly outperforms a conventional mask-based SE--SS method across various objective metrics with high robustness against complex degradations. Audio samples are available in our demo page.

</details>


### [58] [Neural Multi-Speaker Voice Cloning for Nepali in Low-Resource Settings](https://arxiv.org/abs/2601.18694)
*Aayush M. Shrestha,Aditya Bajracharya,Projan Shakya,Dinesh B. Kshatri*

Main category: cs.SD

TL;DR: 开发了一个尼泊尔语少样本语音克隆系统，使用少量数据就能从梵文文本合成特定说话人的语音。


<details>
  <summary>Details</summary>
Motivation: 尼泊尔语作为低资源语言，语音克隆研究几乎空白，需要探索在数据有限的情况下实现个性化语音合成的方法。

Method: 构建两个数据集：无转录音频训练说话人编码器，配对文本-音频数据训练Tacotron2合成器。说话人编码器使用生成式端到端损失优化，生成表征说话人声音特征的嵌入向量，与Tacotron2的文本嵌入融合生成梅尔频谱图，最后通过WaveRNN声码器转换为音频。

Result: 系统能够有效克隆说话人特征，即使对于未见过的声音也能实现，证明了尼泊尔语少样本语音克隆的可行性。

Conclusion: 该研究为低资源语言的个性化语音合成奠定了基础，展示了在数据有限情况下实现高质量语音克隆的可能性。

Abstract: This research presents a few-shot voice cloning system for Nepali speakers, designed to synthesize speech in a specific speaker's voice from Devanagari text using minimal data. Voice cloning in Nepali remains largely unexplored due to its low-resource nature. To address this, we constructed separate datasets: untranscribed audio for training a speaker encoder and paired text-audio data for training a Tacotron2-based synthesizer. The speaker encoder, optimized with Generative End2End loss, generates embeddings that capture the speaker's vocal identity, validated through Uniform Manifold Approximation and Projection (UMAP) for dimension reduction visualizations. These embeddings are fused with Tacotron2's text embeddings to produce mel-spectrograms, which are then converted into audio using a WaveRNN vocoder. Audio data were collected from various sources, including self-recordings, and underwent thorough preprocessing for quality and alignment. Training was performed using mel and gate loss functions under multiple hyperparameter settings. The system effectively clones speaker characteristics even for unseen voices, demonstrating the feasibility of few-shot voice cloning for the Nepali language and establishing a foundation for personalized speech synthesis in low-resource scenarios.

</details>

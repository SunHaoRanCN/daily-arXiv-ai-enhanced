{"id": "2601.18927", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.18927", "abs": "https://arxiv.org/abs/2601.18927", "authors": ["Yuanwei Liu", "Hao Jiang", "Xu Gan", "Xiaoxia Xu", "Jia Guo", "Zhaolin Wang", "Chongjun Ouyang", "Xidong Mu", "Zhiguo Ding", "Arumugam Nallanathan", "Octavia A. Dobre", "George K. Karagiannidis", "Robert Schober"], "title": "A Survey of Pinching-Antenna Systems (PASS)", "comment": "Submitted to IEEE journal", "summary": "The pinching-antenna system (PASS), recently proposed as a flexible-antenna technology, has been regarded as a promising solution for several challenges in next-generation wireless networks. It provides large-scale antenna reconfiguration, establishes stable line-of-sight links, mitigates signal blockage, and exploits near-field advantages through its distinctive architecture. This article aims to present a comprehensive overview of the state of the art in PASS. The fundamental principles of PASS are first discussed, including its hardware architecture, circuit and physical models, and signal models. Several emerging PASS designs, such as segmented PASS (S-PASS), center-fed PASS (C-PASS), and multi-mode PASS (M-PASS), are subsequently introduced, and their design features are discussed. In addition, the properties and promising applications of PASS for wireless sensing are reviewed. On this basis, recent progress in the performance analysis of PASS for both communications and sensing is surveyed, and the performance gains achieved by PASS are highlighted. Existing research contributions in optimization and machine learning are also summarized, with the practical challenges of beamforming and resource allocation being identified in relation to the unique transmission structure and propagation characteristics of PASS. Finally, several variants of PASS are presented, and key implementation challenges that remain open for future study are discussed."}
{"id": "2601.19173", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.19173", "abs": "https://arxiv.org/abs/2601.19173", "authors": ["Yingzhe Mao", "Chao Zou", "Yanqun Tang"], "title": "SynthRM: A Synthetic Data Platform for Vision-Aided Mobile System Simulation", "comment": "conference", "summary": "Vision-aided wireless sensing is emerging as a cornerstone of 6G mobile computing. While data-driven approaches have advanced rapidly, establishing a precise geometric correspondence between ego-centric visual data and radio propagation remains a challenge. Existing paradigms typically either associate 2D topology maps and auxiliary information with radio maps, or provide 3D perspective views limited by sparse radio data. This spatial representation flattens the complex vertical interactions such as occlusion and diffraction that govern signal behavior in urban environments, rendering the task of cross-view signal inference mathematically ill-posed. To resolve this geometric ambiguity, we introduce SynthRM, a scalable synthetic data platform. SynthRM implements a Visible-Aligned-Surface simulation strategy: rather than probing a global volumetric grid, it performs ray-tracing directly onto the geometry exposed to the sensor. This approach ensures pixel-level consistency between visual semantics and electromagnetic response, transforming the learning objective into a physically well-posed problem. We demonstrate the platform's capabilities by presenting a diverse, city-scale dataset derived from procedurally generated environments. By combining efficient procedural synthesis with high-fidelity electromagnetic modeling, SynthRM provides a transparent, accessible foundation for developing next-generation mobile systems for environment-aware sensing and communication."}
{"id": "2601.19248", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.19248", "abs": "https://arxiv.org/abs/2601.19248", "authors": ["Lina Zhu", "Lin Zhou"], "title": "Exponentially Consistent Low-Complexity Outlier Hypothesis Testing for Continuous Sequences", "comment": null, "summary": "In this work, we revisit outlier hypothesis testing and propose exponentially consistent, low-complexity fixed-length tests that achieve a better tradeoff between detection performance and computational complexity than existing exhaustive-search methods. In this setting, the goal is to identify outlying sequences from a set of observed sequences, where most sequences are i.i.d. from a nominal distribution and outliers are i.i.d. from a different anomalous distribution. While prior work has primarily focused on discrete-valued sequences, we extend the results of Bu et al. (TSP 2019) to continuous-valued sequences and develop a distribution-free test based on the MMD metric. Our framework handles both known and unknown numbers of outliers. In the unknown-count case, we bound the detection performance and characterize the tradeoff among the exponential decay rates of three types of error probabilities. Finally, we quantify the performance penalty incurred when the number of outliers is unknown."}
{"id": "2601.19313", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.19313", "abs": "https://arxiv.org/abs/2601.19313", "authors": ["Hetong Wang", "Yashuai Cao", "Tiejun Lv", "Jintao Wang", "Ni Wei", "Jiancheng An", "Chau Yuen"], "title": "Stacked Intelligent Metasurfaces-Based Electromagnetic Wave Domain Interference-Free Precoding", "comment": "16 pages, 13 figures, IEEE Transactions on Wirelesss Communications, Accepted", "summary": "This paper introduces an interference-free multi-stream transmission architecture leveraging stacked intelligent metasurfaces (SIMs), from a new perspective of interference exploitation. Unlike traditional interference exploitation precoding (IEP) which relies on computational hardware circuitry, we perform the precoding operations within the analog wave domain provided by SIMs. However, the benefits of SIM-enabled IEP are limited by the nonlinear distortion (NLD) caused by power amplifiers. A hardware-efficient interference-free transmitter architecture is developed to exploit SIM's high and flexible degree of freedom (DoF), where the NLD on modulated symbols can be directly compensated in the wave domain. Moreover, we design a frame-level SIM configuration scheme and formulate a maxmin problem on the safety margin function. With respect to the optimization of SIM phase shifts, we propose a recursive oblique manifold (ROM) algorithm to tackle the complex coupling among phase shifts across multiple layers. A flexible DoF-driven antenna selection (AS) scheme is explored in the SIM-enabled IEP system. Using an ROM-based alternating optimization (ROM-AO) framework, our approach jointly optimizes transmit AS, SIM phase shift design, and power allocation (PA), and develops a greedy safety margin-based AS algorithm. Simulations show that the proposed SIM-enabled frame-level IEP scheme significantly outperforms benchmarks. Specifically, the strategy with AS and PA can achieve a 20 dB performance gain compared to the case without any strategy under the 12 dB signal-to-noise ratio, which confirms the superiority of the NLD-aware IEP scheme and the effectiveness of the proposed algorithm."}
{"id": "2601.18904", "categories": ["cs.SD", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.18904", "abs": "https://arxiv.org/abs/2601.18904", "authors": ["Haolong Zheng", "Siyin Wang", "Zengrui Jin", "Mark Hasegawa-Johnson"], "title": "SICL-AT: Another way to adapt Auditory LLM to low-resource task", "comment": null, "summary": "Auditory Large Language Models (LLMs) have demonstrated strong performance across a wide range of speech and audio understanding tasks. Nevertheless, they often struggle when applied to low-resource or unfamiliar tasks. In case of labeled in-domain data is scarce or mismatched to the true test distribution, direct fine-tuning can be brittle. In-Context Learning (ICL) provides a training-free, inference-time solution by adapting auditory LLMs through conditioning on a few in-domain demonstrations. In this work, we first show that \\emph{Vanilla ICL}, improves zero-shot performance across diverse speech and audio tasks for selected models which suggest this ICL adaptation capability can be generalized to multimodal setting. Building on this, we propose \\textbf{Speech In-Context Learning Adaptation Training (SICL-AT)}, a post-training recipe utilizes only high resource speech data intending to strengthen model's in-context learning capability. The enhancement can generalize to audio understanding/reasoning task. Experiments indicate our proposed method consistently outperforms direct fine-tuning in low-resource scenario."}
{"id": "2601.19130", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2601.19130", "abs": "https://arxiv.org/abs/2601.19130", "authors": ["Zexu Pan", "Xinyuan Qian", "Shengkui Zhao", "Kun Zhou", "Bin Ma"], "title": "Beyond Lips: Integrating Gesture and Lip Cues for Robust Audio-visual Speaker Extraction", "comment": "ICASSP 2026", "summary": "Most audio-visual speaker extraction methods rely on synchronized lip recording to isolate the speech of a target speaker from a multi-talker mixture. However, in natural human communication, co-speech gestures are also temporally aligned with speech, often emphasizing specific words or syllables. These gestures provide complementary visual cues that can be especially valuable when facial or lip regions are occluded or distant. In this work, we move beyond lip-centric approaches and propose SeLG, a model that integrates both lip and upper-body gesture information for robust speaker extraction. SeLG features a cross-attention-based fusion mechanism that enables each visual modality to query and selectively attend to relevant speech features in the mixture. To improve the alignment of gesture representations with speech dynamics, SeLG also employs a contrastive InfoNCE loss that encourages gesture embeddings to align more closely with corresponding lip embeddings, which are more strongly correlated with speech. Experimental results on the YGD dataset, containing TED talks, demonstrate that the proposed contrastive learning strategy significantly improves gesture-based speaker extraction, and that our proposed SeLG model, by effectively fusing lip and gesture cues with an attention mechanism and InfoNCE loss, achieves superior performance compared to baselines, across both complete and partial (i.e., missing-modality) conditions."}
{"id": "2601.19366", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.19366", "abs": "https://arxiv.org/abs/2601.19366", "authors": ["Weijie Xiong", "Jingran Lin", "Di Jiang", "Yuhan Zhang", "Kai Zhong", "Qiang Li"], "title": "Cooperative Double IRS aided Secure Communication for MIMO-OFDM Systems", "comment": null, "summary": "Cooperative double intelligent reflecting surface (double-IRS) has emerged as a promising approach for enhancing physical layer security (PLS) in MIMO systems. However, existing studies are limited to narrowband scenarios and fail to address wideband MIMO-OFDM. In this regime, frequency-flat IRS phases and cascaded IRS links cause severe coupling, rendering narrowband designs inapplicable. To overcome this challenge, we introduce cooperative double-IRS-assisted wideband MIMO-OFDM and propose an efficient manifold-based solution. By regarding the power and constant modulus constraints as Riemannian manifolds, we reformulate the non-convex secrecy sum rate maximization as an unconstrained optimization on a product manifold. Building on this formulation, we further develop a product Riemannian gradient descent (PRGD) algorithm with guaranteed stationary convergence. Simulation results demonstrate that the proposed scheme effectively resolves the OFDM coupling issue and achieves significant secrecy rate gains, outperforming single-IRS and distributed multi-IRS benchmarks by 32.0% and 22.3%, respectively."}
{"id": "2601.18908", "categories": ["cs.SD", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.18908", "abs": "https://arxiv.org/abs/2601.18908", "authors": ["Marouane El Hizabri", "Abdelfattah Bezzaz", "Ismail Hayoukane", "Youssef Taki"], "title": "Enhancing Speech Emotion Recognition using Dynamic Spectral Features and Kalman Smoothing", "comment": null, "summary": "Speech Emotion Recognition systems often use static features like Mel-Frequency Cepstral Coefficients (MFCCs), Zero Crossing Rate (ZCR), and Root Mean Square Energy (RMSE). Because of this, they can misclassify emotions when there is acoustic noise in vocal signals. To address this, we added dynamic features using Dynamic Spectral features (Deltas and Delta-Deltas) along with the Kalman Smoothing algorithm. This approach reduces noise and improves emotion classification. Since emotion changes over time, the Kalman Smoothing filter also helped make the classifier outputs more stable. Tests on the RAVDESS dataset showed that this method achieved a state-of-the-art accuracy of 87\\% and reduced misclassification between emotions with similar acoustic features"}
{"id": "2601.19153", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2601.19153", "abs": "https://arxiv.org/abs/2601.19153", "authors": ["Zexu Pan", "Shengkui Zhao", "Yukun Ma", "Haoxu Wang", "Yiheng Jiang", "Biao Tian", "Bin Ma"], "title": "LuSeeL: Language-queried Binaural Universal Sound Event Extraction and Localization", "comment": "ICASSP 2026", "summary": "Most universal sound extraction algorithms focus on isolating a target sound event from single-channel audio mixtures. However, the real world is three-dimensional, and binaural audio, which mimics human hearing, can capture richer spatial information, including sound source location. This spatial context is crucial for understanding and modeling complex auditory scenes, as it inherently informs sound detection and extraction. In this work, we propose a language-driven universal sound extraction network that isolates text-described sound events from binaural mixtures by effectively leveraging the spatial cues present in binaural signals. Additionally, we jointly predict the direction of arrival (DoA) of the target sound using spatial features from the extraction network. This dual-task approach exploits complementary location information to improve extraction performance while enabling accurate DoA estimation. Experimental results on the in-the-wild AudioCaps dataset show that our proposed LuSeeL model significantly outperforms single-channel and uni-task baselines."}
{"id": "2601.19372", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.19372", "abs": "https://arxiv.org/abs/2601.19372", "authors": ["Hao Fang", "Xiao Li", "Chongtao Guo", "Le Liang", "Shi Jin"], "title": "AoI-Driven Queue Management and Power Control in V2V Networks: A GNN-Enhanced MARL Approach", "comment": null, "summary": "Queue management and resource allocation play a critical role in enabling cooperative status awareness in vehicular networks. This paper investigates the problem of age of information (AoI)-aware status updates in vehicle-to-vehicle (V2V) communication, where each vehicle's status is represented by multiple interdependent packets. To enable fine-grained queue management at the packet level under resource constraints, we formulate a joint optimization problem that simultaneously learns active packet dropping and transmit power control strategies. A hybrid action space is designed to support both discrete dropping decisions and continuous power control. To exploit the graph-structured interference inherent in V2V topology, a graph neural network (GNN) is introduced to aggregate slowly varying large-scale fading, allowing agents to capture topological dependencies implicitly without frequent message exchange. The overall framework is built upon multi-agent proximal policy optimization (MAPPO), with centralized training and decentralized execution (CTDE). Simulations demonstrate that the proposed method significantly reduces average AoI across a wide range of network densities, channel conditions, and traffic loads, consistently outperforming several baselines."}
{"id": "2601.19017", "categories": ["cs.SD", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.19017", "abs": "https://arxiv.org/abs/2601.19017", "authors": ["Alexander Buck", "Georgina Cosma", "Iain Phillips", "Paul Conway", "Patrick Baker"], "title": "A Framework for Evaluating Faithfulness in Explainable AI for Machine Anomalous Sound Detection Using Frequency-Band Perturbation", "comment": "16 pages, 24 figures", "summary": "Explainable AI (XAI) is commonly applied to anomalous sound detection (ASD) models to identify which time-frequency regions of an audio signal contribute to an anomaly decision. However, most audio explanations rely on qualitative inspection of saliency maps, leaving open the question of whether these attributions accurately reflect the spectral cues the model uses. In this work, we introduce a new quantitative framework for evaluating XAI faithfulness in machine-sound analysis by directly linking attribution relevance to model behaviour through systematic frequency-band removal. This approach provides an objective measure of whether an XAI method for machine ASD correctly identifies frequency regions that influence an ASD model's predictions. By using four widely adopted methods, namely Integrated Gradients, Occlusion, Grad-CAM and SmoothGrad, we show that XAI techniques differ in reliability, with Occlusion demonstrating the strongest alignment with true model sensitivity and gradient-+based methods often failing to accurately capture spectral dependencies. The proposed framework offers a reproducible way to benchmark audio explanations and enables more trustworthy interpretation of spectrogram-based ASD systems."}
{"id": "2601.19194", "categories": ["eess.AS", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.19194", "abs": "https://arxiv.org/abs/2601.19194", "authors": ["Alexander Polok", "Dominik Klement", "Samuele Cornell", "Matthew Wiesner", "Jan Černocký", "Sanjeev Khudanpur", "Lukáš Burget"], "title": "SE-DiCoW: Self-Enrolled Diarization-Conditioned Whisper", "comment": "Accepted to ICASSP 2026", "summary": "Speaker-attributed automatic speech recognition (ASR) in multi-speaker environments remains a major challenge. While some approaches achieve strong performance when fine-tuned on specific domains, few systems generalize well across out-of-domain datasets. Our prior work, Diarization-Conditioned Whisper (DiCoW), leverages speaker diarization outputs as conditioning information and, with minimal fine-tuning, demonstrated strong multilingual and multi-domain performance. In this paper, we address a key limitation of DiCoW: ambiguity in Silence-Target-Non-target-Overlap (STNO) masks, where two or more fully overlapping speakers may have nearly identical conditioning despite differing transcriptions. We introduce SE-DiCoW (Self-Enrolled Diarization-Conditioned Whisper), which uses diarization output to locate an enrollment segment anywhere in the conversation where the target speaker is most active. This enrollment segment is used as fixed conditioning via cross-attention at each encoder layer. We further refine DiCoW with improved data segmentation, model initialization, and augmentation. Together, these advances yield substantial gains: SE-DiCoW reduces macro-averaged tcpWER by 52.4% relative to the original DiCoW on the EMMA MT-ASR benchmark."}
{"id": "2601.19457", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.19457", "abs": "https://arxiv.org/abs/2601.19457", "authors": ["Dario Cellini", "Stella Civelli", "Marco Secondini"], "title": "ML-Enhanced Digital Backpropagation for Long-Reach Single-Span Systems", "comment": null, "summary": "We propose a digital backpropagation method that employs machine-learning-aided joint optimization of dispersion step lengths and nonlinear phase rotation filters within an FFT-based enhanced split-step Fourier structure, achieving improved accuracy at low computational complexity."}
{"id": "2601.19029", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.19029", "abs": "https://arxiv.org/abs/2601.19029", "authors": ["Jai Dhiman"], "title": "Audio Foundation Models Outperform Symbolic Representations for Piano Performance Evaluation", "comment": "6 pages, 4 figures, 2 tables. Code available at https://github.com/Jai-Dhiman/crescendai", "summary": "Automated piano performance evaluation traditionally relies on symbolic (MIDI) representations, which capture note-level information but miss the acoustic nuances that characterize expressive playing. I propose using pre-trained audio foundation models, specifically MuQ and MERT, to predict 19 perceptual dimensions of piano performance quality. Using synthesized audio from PercePiano MIDI files (rendered via Pianoteq), I compare audio and symbolic approaches under controlled conditions where both derive from identical source data. The best model, MuQ layers 9-12 with Pianoteq soundfont augmentation, achieves R^2 = 0.537 (95% CI: [0.465, 0.575]), representing a 55% improvement over the symbolic baseline (R^2 = 0.347). Statistical analysis confirms significance (p < 10^-25) with audio outperforming symbolic on all 19 dimensions. I validate the approach through cross-soundfont generalization (R^2 = 0.534 +/- 0.075), difficulty correlation with an external dataset (rho = 0.623), and multi-performer consistency analysis. Analysis of audio-symbolic fusion reveals high error correlation (r = 0.738), explaining why fusion provides minimal benefit: audio representations alone are sufficient. I release the complete training pipeline, pretrained models, and inference code."}
{"id": "2601.19491", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2601.19491", "abs": "https://arxiv.org/abs/2601.19491", "authors": ["Xingyu Chen", "Sipei Zhao", "Fei Ma", "Eva Cheng", "Ian S. Burnett"], "title": "Permutation-Invariant Physics-Informed Neural Network for Region-to-Region Sound Field Reconstruction", "comment": "Accepted to the 31st International Congress on Sound and Vibration (ICSV 2025)", "summary": "Most existing sound field reconstruction methods target point-to-region reconstruction, interpolating the Acoustic Transfer Functions (ATFs) between a fixed-position sound source and a receiver region. The applicability of these methods is limited because real-world ATFs tend to varying continuously with respect to the positions of sound sources and receiver regions. This paper presents a permutation-invariant physics-informed neural network for region-to-region sound field reconstruction, which aims to interpolate the ATFs across continuously varying sound sources and measurement regions. The proposed method employs a deep set architecture to process the receiver and sound source positions as an unordered set, preserving acoustic reciprocity. Furthermore, it incorporates the Helmholtz equation as a physical constraint to guide network training, ensuring physically consistent predictions."}
{"id": "2601.19518", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.19518", "abs": "https://arxiv.org/abs/2601.19518", "authors": ["Andreas Angelou", "Pourya Behmandpoor", "Marc Moonen"], "title": "Master-Assisted Distributed Uplink Operation for Cell-Free Massive MIMO Networks", "comment": "This paper has been accepted for publication in the IEEE ICASSP 2026", "summary": "Cell-free massive multiple-input-multiple-output is considered a promising technology for the next generation of wireless communication networks. The main idea is to distribute a large number of access points (APs) in a geographical region to serve the user equipments (UEs) cooperatively. In the uplink, one of two types of operations is often adopted: centralized or distributed. In centralized operation, channel estimation and data decoding are performed at the central processing unit (CPU), whereas in distributed operation, channel estimation occurs at the APs and data detection at the CPU. In this paper, we propose a novel uplink operation, termed Master-Assisted Distributed Uplink Operation (MADUO), where each UE is assigned a master AP, which receives soft data estimates from the other APs and decodes the data using its local signals and the received data estimates. Numerical experiments demonstrate that the proposed operation performs comparably to the centralized operation and balances fronthaul signaling and computational complexity."}
{"id": "2601.19109", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2601.19109", "abs": "https://arxiv.org/abs/2601.19109", "authors": ["Arhan Vohra", "Taketo Akama"], "title": "Interpretable and Perceptually-Aligned Music Similarity with Pretrained Embeddings", "comment": null, "summary": "Perceptual similarity representations enable music retrieval systems to determine which songs sound most similar to listeners. State-of-the-art approaches based on task-specific training via self-supervised metric learning show promising alignment with human judgment, but are difficult to interpret or generalize due to limited dataset availability. We show that pretrained text-audio embeddings (CLAP and MuQ-MuLan) offer comparable perceptual alignment on similarity tasks without any additional fine-tuning. To surpass this baseline, we introduce a novel method to perceptually align pretrained embeddings with source separation and linear optimization on ABX preference data from listening tests. Our model provides interpretable and controllable instrument-wise weights, allowing music producers to retrieve stem-level loops and samples based on mixed reference songs."}
{"id": "2601.19573", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2601.19573", "abs": "https://arxiv.org/abs/2601.19573", "authors": ["Haohan Shi", "Xiyu Shi", "Safak Dogan", "Tianjin Huang", "Yunxiao Zhang"], "title": "Audio Deepfake Detection at the First Greeting: \"Hi!\"", "comment": "Accepted at ICASSP 2026. Copyright 2026 IEEE. The final published version will be available via IEEE Xplore", "summary": "This paper focuses on audio deepfake detection under real-world communication degradations, with an emphasis on ultra-short inputs (0.5-2.0s), targeting the capability to detect synthetic speech at a conversation opening, e.g., when a scammer says \"Hi.\" We propose Short-MGAA (S-MGAA), a novel lightweight extension of Multi-Granularity Adaptive Time-Frequency Attention, designed to enhance discriminative representation learning for short, degraded inputs subjected to communication processing and perturbations. The S-MGAA integrates two tailored modules: a Pixel-Channel Enhanced Module (PCEM) that amplifies fine-grained time-frequency saliency, and a Frequency Compensation Enhanced Module (FCEM) to supplement limited temporal evidence via multi-scale frequency modeling and adaptive frequency-temporal interaction. Extensive experiments demonstrate that S-MGAA consistently surpasses nine state-of-the-art baselines while achieving strong robustness to degradations and favorable efficiency-accuracy trade-offs, including low RTF, competitive GFLOPs, compact parameters, and reduced training cost, highlighting its strong potential for real-time deployment in communication systems and edge devices."}
{"id": "2601.19523", "categories": ["eess.SP", "cs.IT"], "pdf": "https://arxiv.org/pdf/2601.19523", "abs": "https://arxiv.org/abs/2601.19523", "authors": ["Sergi Liesegang", "Antonio Pascual-Iserte", "Olga Muñoz", "Alessio Zappone"], "title": "Design of RIS-aided mMTC+ Networks for Rate Maximization under the Finite Blocklength Regime with Imperfect Channel Knowledge", "comment": "This work has been accepted for publication in IEEE Communications Letters. The final published version is available via IEEE Xplore", "summary": "Within the context of massive machine-type communications+, reconfigurable intelligent surfaces (RISs) represent a promising technology to boost system performance in scenarios with poor channel conditions. Considering single-antenna sensors transmitting short data packets to a multiple-antenna collector node, we introduce and design an RIS to maximize the weighted sum rate (WSR) of the system working in the finite blocklength regime. Due to the large number of reflecting elements and their passive nature, channel estimation errors may occur. In this letter, we then propose a robust RIS optimization to combat such a detrimental issue. Based on concave bounds and approximations, the nonconvex WSR problem for the RIS response is addressed via successive convex optimization (SCO). Numerical experiments validate the performance and complexity of the SCO solutions."}
{"id": "2601.19113", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.19113", "abs": "https://arxiv.org/abs/2601.19113", "authors": ["Yinghao Liu", "Chengwei Liu", "Xiaotao Liang", "Haoyin Yan", "Shaofei Xue", "Zheng Xue"], "title": "A Hybrid Discriminative and Generative System for Universal Speech Enhancement", "comment": "Accepted by ICASSP 2026.This work was submitted to the ICASSP 2026 URGENT Challenge (Track 1)", "summary": "Universal speech enhancement aims at handling inputs with various speech distortions and recording conditions. In this work, we propose a novel hybrid architecture that synergizes the signal fidelity of discriminative modeling with the reconstruction capabilities of generative modeling. Our system utilizes the discriminative TF-GridNet model with the Sampling-Frequency-Independent strategy to handle variable sampling rates universally. In parallel, an autoregressive model combined with spectral mapping modeling generates detail-rich speech while effectively suppressing generative artifacts. Finally, a fusion network learns adaptive weights of the two outputs under the optimization of signal-level losses and the comprehensive Speech Quality Assessment (SQA) loss. Our proposed system is evaluated in the ICASSP 2026 URGENT Challenge (Track 1) and ranks the third place."}
{"id": "2601.19702", "categories": ["eess.AS", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.19702", "abs": "https://arxiv.org/abs/2601.19702", "authors": ["Helin Wang", "Bowen Shi", "Andros Tjandra", "John Hoffman", "Yi-Chiao Wu", "Apoorv Vyas", "Najim Dehak", "Ann Lee", "Wei-Ning Hsu"], "title": "SAM Audio Judge: A Unified Multimodal Framework for Perceptual Evaluation of Audio Separation", "comment": null, "summary": "The performance evaluation remains a complex challenge in audio separation, and existing evaluation metrics are often misaligned with human perception, course-grained, relying on ground truth signals. On the other hand, subjective listening tests remain the gold standard for real-world evaluation, but they are expensive, time-consuming, and difficult to scale. This paper addresses the growing need for automated systems capable of evaluating audio separation without human intervention. The proposed evaluation metric, SAM Audio Judge (SAJ), is a multimodal fine-grained reference-free objective metric, which shows highly alignment with human perceptions. SAJ supports three audio domains (speech, music and general sound events) and three prompt inputs (text, visual and span), covering four different dimensions of evaluation (recall, percision, faithfulness, and overall). SAM Audio Judge also shows potential applications in data filtering, pseudo-labeling large datasets and reranking in audio separation models. We release our code and pre-trained models at: https://github.com/facebookresearch/sam-audio."}
{"id": "2601.19539", "categories": ["eess.SP", "cs.IT"], "pdf": "https://arxiv.org/pdf/2601.19539", "abs": "https://arxiv.org/abs/2601.19539", "authors": ["Heedong Do", "Angel Lozano"], "title": "Cramer-Rao Bound for Arbitrarily Constrained Sets", "comment": null, "summary": "This paper presents a Cramer-Rao bound (CRB) for the estimation of parameters confined to an arbitrary set. Unlike existing results that rely on equality or inequality constraints, manifold structures, or the nonsingularity of the Fisher information matrix, the derived CRB applies to any constrained set and holds for any estimation bias and any Fisher information matrix. The key geometric object governing the new CRB is the tangent cone to the constraint set, whose span determines how the constraints affect the estimation accuracy. This CRB subsumes, unifies, and generalizes known special cases, offering an intuitive and broadly applicable framework to characterize the minimum mean-square error of constrained estimators."}
{"id": "2601.19297", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.19297", "abs": "https://arxiv.org/abs/2601.19297", "authors": ["Karl Schrader", "Shoichi Koyama", "Tomohiko Nakamura", "Mirco Pezzoli"], "title": "Phase-Retrieval-Based Physics-Informed Neural Networks For Acoustic Magnitude Field Reconstruction", "comment": "Accepted to International Conference on Acoustics, Speech and Signal Processing (ICASSP) 2026", "summary": "We propose a method for estimating the magnitude distribution of an acoustic field from spatially sparse magnitude measurements. Such a method is useful when phase measurements are unreliable or inaccessible. Physics-informed neural networks (PINNs) have shown promise for sound field estimation by incorporating constraints derived from governing partial differential equations (PDEs) into neural networks. However, they do not extend to settings where phase measurements are unavailable, as the loss function based on the governing PDE relies on phase information. To remedy this, we propose a phase-retrieval-based PINN for magnitude field estimation. By representing the magnitude and phase distributions with separate networks, the PDE loss can be computed based on the reconstructed complex amplitude. We demonstrate the effectiveness of our phase-retrieval-based PINN through experimental evaluation."}
{"id": "2601.19786", "categories": ["eess.AS", "cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2601.19786", "abs": "https://arxiv.org/abs/2601.19786", "authors": ["Jinzuomu Zhong", "Yi Wang", "Korin Richmond", "Peter Bell"], "title": "Rethinking Discrete Speech Representation Tokens for Accent Generation", "comment": null, "summary": "Discrete Speech Representation Tokens (DSRTs) have become a foundational component in speech generation. While prior work has extensively studied phonetic and speaker information in DSRTs, how accent information is encoded in DSRTs remains largely unexplored. In this paper, we present the first systematic investigation of accent information in DSRTs. We propose a unified evaluation framework that measures both accessibility of accent information via a novel Accent ABX task and recoverability via cross-accent Voice Conversion (VC) resynthesis. Using this framework, we analyse DSRTs derived from a variety of speech encoders. Our results reveal that accent information is substantially reduced when ASR supervision is used to fine-tune the encoder, but cannot be effectively disentangled from phonetic and speaker information through naive codebook size reduction. Based on these findings, we propose new content-only and content-accent DSRTs that significantly outperform existing designs in controllable accent generation. Our work highlights the importance of accent-aware evaluation and provides practical guidance for designing DSRTs for accent-controlled speech generation."}
{"id": "2601.19587", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.19587", "abs": "https://arxiv.org/abs/2601.19587", "authors": ["Zihan Zhou", "Ang Chen", "Yunfei Chen", "Weidong Wang", "Li Chen"], "title": "Exposure-Aware Beamforming for mmWave Systems: From EM Theory to Thermal Compliance", "comment": "13 pages, 8 figures", "summary": "Electromagnetic (EM) exposure compliance has long been recognized as a crucial aspect of communications terminal designs. However, accurately assessing the impact of EM exposure for proper design strategies remains challenging. In this paper, we develop a long-term thermal EM exposure constraint model and propose a novel adaptive exposure-aware beamforming design for an mmWave uplink system. Specifically, we first establish an equivalent channel model based on Maxwell's radiation equations, which accurately captures the EM physical effects. Then, we derive a closed-form thermal impulse response model from the Pennes bioheat transfer equation (BHTE), characterizing the thermal inertia of tissue. Inspired by this model, we formulate a beamforming optimization problem that translates rigid instantaneous exposure limits into a flexible long-term thermal budget constraint. Furthermore, we develop a low-complexity online beamforming algorithm based on Lyapunov optimization theory, obtaining a closed-form near-optimal solution. Simulation results demonstrate that the proposed algorithm effectively stabilizes tissue temperature near a predefined safety threshold and significantly outperforms the conventional scheme with instantaneous exposure constraints."}
{"id": "2601.19399", "categories": ["cs.SD", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.19399", "abs": "https://arxiv.org/abs/2601.19399", "authors": ["Samir Sadok", "Stéphane Lathuilière", "Xavier Alameda-Pineda"], "title": "Residual Tokens Enhance Masked Autoencoders for Speech Modeling", "comment": "Submitted to ICASSP 2026 (accepted)", "summary": "Recent speech modeling relies on explicit attributes such as pitch, content, and speaker identity, but these alone cannot capture the full richness of natural speech. We introduce RT-MAE, a novel masked autoencoder framework that augments the supervised attributes-based modeling with unsupervised residual trainable tokens, designed to encode the information not explained by explicit labeled factors (e.g., timbre variations, noise, emotion etc). Experiments show that RT-MAE improves reconstruction quality, preserving content and speaker similarity while enhancing expressivity. We further demonstrate its applicability to speech enhancement, removing noise at inference while maintaining controllability and naturalness."}
{"id": "2601.18908", "categories": ["cs.SD", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.18908", "abs": "https://arxiv.org/abs/2601.18908", "authors": ["Marouane El Hizabri", "Abdelfattah Bezzaz", "Ismail Hayoukane", "Youssef Taki"], "title": "Enhancing Speech Emotion Recognition using Dynamic Spectral Features and Kalman Smoothing", "comment": null, "summary": "Speech Emotion Recognition systems often use static features like Mel-Frequency Cepstral Coefficients (MFCCs), Zero Crossing Rate (ZCR), and Root Mean Square Energy (RMSE). Because of this, they can misclassify emotions when there is acoustic noise in vocal signals. To address this, we added dynamic features using Dynamic Spectral features (Deltas and Delta-Deltas) along with the Kalman Smoothing algorithm. This approach reduces noise and improves emotion classification. Since emotion changes over time, the Kalman Smoothing filter also helped make the classifier outputs more stable. Tests on the RAVDESS dataset showed that this method achieved a state-of-the-art accuracy of 87\\% and reduced misclassification between emotions with similar acoustic features"}
{"id": "2601.19590", "categories": ["eess.SP", "cs.IT"], "pdf": "https://arxiv.org/pdf/2601.19590", "abs": "https://arxiv.org/abs/2601.19590", "authors": ["Sergi Liesegang", "Antonio Pascual-Iserte", "Olga Muñoz"], "title": "Robust Design of Reconfigurable Intelligent Surfaces for Parameter Estimation in MTC", "comment": "This work has been accepted for publication in EURASIP Journal on Wireless Communications and Networking. The final published version is available via Springer Nature Link", "summary": "This paper introduces a reconfigurable intelligent surface (RIS) to support parameter estimation in machine-type communications (MTC). We focus on a network where single-antenna sensors transmit spatially correlated measurements to a multiple-antenna collector node (CN) via non-orthogonal multiple access. We propose an estimation scheme based on the minimum mean square error (MMSE) criterion. We also integrate successive interference cancelation (SIC) at the receiver to mitigate communication failures in noisy and interference-prone channels under the finite blocklength (FBL) regime. Moreover, recognizing the importance of channel state information (CSI), we explore various methodologies for its acquisition at the CN. We statistically design the RIS configuration and SIC decoding order to minimize estimation error while accounting for channel temporal variations and short packet lengths. To mirror practical systems, we incorporate the detrimental effects of FBL communication and imperfect CSI errors in our analysis. Simulations demonstrate that larger reflecting surfaces lead to smaller MSEs and underscore the importance of selecting an appropriate decoding order for accuracy and ultimate performance."}
{"id": "2601.19472", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2601.19472", "abs": "https://arxiv.org/abs/2601.19472", "authors": ["Zhen Liao", "Gaole Dai", "Mengqiao Chen", "Wenqing Cheng", "Wei Xu"], "title": "Dual-Strategy-Enhanced ConBiMamba for Neural Speaker Diarization", "comment": "Accepted at ICASSP 2026", "summary": "Conformer and Mamba have achieved strong performance in speech modeling but face limitations in speaker diarization. Mamba is efficient but struggles with local details and nonlinear patterns. Conformer's self-attention incurs high memory overhead for long speech sequences and may cause instability in long-range dependency modeling. These limitations are critical for diarization, which requires both precise modeling of local variations and robust speaker consistency over extended spans. To address these challenges, we first apply ConBiMamba for speaker diarization. We follow the Pyannote pipeline and propose the Dual-Strategy-Enhanced ConBiMamba neural speaker diarization system. ConBiMamba integrates the strengths of Conformer and Mamba, where Conformer's convolutional and feed-forward structures are utilized to improve local feature extraction. By replacing Conformer's self-attention with ExtBiMamba, ConBiMamba efficiently handles long audio sequences while alleviating the high memory cost of self-attention. Furthermore, to address the problem of the higher DER around speaker change points, we introduce the Boundary-Enhanced Transition Loss to enhance the detection of speaker change points. We also propose Layer-wise Feature Aggregation to enhance the utilization of multi-layer representations. The system is evaluated on six diarization datasets and achieves state-of-the-art performance on four of them. The source code of our study is available at https://github.com/lz-hust/DSE-CBM."}
{"id": "2601.19029", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.19029", "abs": "https://arxiv.org/abs/2601.19029", "authors": ["Jai Dhiman"], "title": "Audio Foundation Models Outperform Symbolic Representations for Piano Performance Evaluation", "comment": "6 pages, 4 figures, 2 tables. Code available at https://github.com/Jai-Dhiman/crescendai", "summary": "Automated piano performance evaluation traditionally relies on symbolic (MIDI) representations, which capture note-level information but miss the acoustic nuances that characterize expressive playing. I propose using pre-trained audio foundation models, specifically MuQ and MERT, to predict 19 perceptual dimensions of piano performance quality. Using synthesized audio from PercePiano MIDI files (rendered via Pianoteq), I compare audio and symbolic approaches under controlled conditions where both derive from identical source data. The best model, MuQ layers 9-12 with Pianoteq soundfont augmentation, achieves R^2 = 0.537 (95% CI: [0.465, 0.575]), representing a 55% improvement over the symbolic baseline (R^2 = 0.347). Statistical analysis confirms significance (p < 10^-25) with audio outperforming symbolic on all 19 dimensions. I validate the approach through cross-soundfont generalization (R^2 = 0.534 +/- 0.075), difficulty correlation with an external dataset (rho = 0.623), and multi-performer consistency analysis. Analysis of audio-symbolic fusion reveals high error correlation (r = 0.738), explaining why fusion provides minimal benefit: audio representations alone are sufficient. I release the complete training pipeline, pretrained models, and inference code."}
{"id": "2601.19602", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.19602", "abs": "https://arxiv.org/abs/2601.19602", "authors": ["Sergio Micó-Rosa", "Concepcion Garcia-Pardo", "Matteo Frasson", "Narcis Cardona", "Vicente Pons-Beltrán", "Pedro López-Muñoz"], "title": "Initial Characterization of Healthy and Malignant in vivo and ex vivo Human Colon Tissues under Surgery Procedures", "comment": "6 pages, 8 figures, 3 tables. Published at the 2024 IEEE 35th International Symposium on Personal, Indoor and Mobile Radio Communications (PIMRC)", "summary": "The dielectric characterization of human tissues can play a crucial role in the development of new medical diagnostic tools. In particular, the characterization of healthy and pathological tissues can provide vital information for diagnosis. In this paper, preliminary results from a small-scale measurement campaign conducted in 0.5-26.5GHz during real surgeries on healthy and malignant human colon tissues are presented. Those measurements were carried out externally to the colon, without direct contact to the tumor growing inside the colon. Furthermore, different tumor stages are taken into account. Initial findings reveal that advanced tumor stages are related with increased higher values of dielectric properties in malignant tumor tissues compared to the healthy ones."}
{"id": "2601.19533", "categories": ["cs.SD", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.19533", "abs": "https://arxiv.org/abs/2601.19533", "authors": ["Tianhua Li", "Chenda Li", "Wei Wang", "Xin Zhou", "Xihui Chen", "Jianqing Gao", "Yanmin Qian"], "title": "SLM-SS: Speech Language Model for Generative Speech Separation", "comment": null, "summary": "Speech separation (SS) has advanced significantly with neural network-based methods, showing improved performance on signal-level metrics. However, these methods often struggle to maintain speech intelligibility in the separated signals, which can negatively affect the performance of downstream tasks such as speech recognition. In this work, we propose SLM-SS, a novel approach that applies speech language models to SS, aiming to enhance the intelligibility and coherence of the separated signals. We frame SS as discrete multi-codebook sequence generation, using Encoder-Decoder models to map quantized speech mixtures to target tokens. In addition to the autoregressive modeling strategy, we introduce a non-autoregressive model to improve decoding efficiency for residual tokens. Experimental results on the LibriMix dataset demonstrate that our approach shows significantly better preservation of speech intelligibility, leading to improved linguistic consistency in a variety of downstream tasks compared to existing approaches."}
{"id": "2601.19113", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.19113", "abs": "https://arxiv.org/abs/2601.19113", "authors": ["Yinghao Liu", "Chengwei Liu", "Xiaotao Liang", "Haoyin Yan", "Shaofei Xue", "Zheng Xue"], "title": "A Hybrid Discriminative and Generative System for Universal Speech Enhancement", "comment": "Accepted by ICASSP 2026.This work was submitted to the ICASSP 2026 URGENT Challenge (Track 1)", "summary": "Universal speech enhancement aims at handling inputs with various speech distortions and recording conditions. In this work, we propose a novel hybrid architecture that synergizes the signal fidelity of discriminative modeling with the reconstruction capabilities of generative modeling. Our system utilizes the discriminative TF-GridNet model with the Sampling-Frequency-Independent strategy to handle variable sampling rates universally. In parallel, an autoregressive model combined with spectral mapping modeling generates detail-rich speech while effectively suppressing generative artifacts. Finally, a fusion network learns adaptive weights of the two outputs under the optimization of signal-level losses and the comprehensive Speech Quality Assessment (SQA) loss. Our proposed system is evaluated in the ICASSP 2026 URGENT Challenge (Track 1) and ranks the third place."}
{"id": "2601.19623", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.19623", "abs": "https://arxiv.org/abs/2601.19623", "authors": ["Chenyang Yan", "Geert Leus", "Mats Bengtsson"], "title": "Robust Covariance-Based DoA Estimation under Weather-Induced Distortion", "comment": null, "summary": "We investigate robust direction-of-arrival (DoA) estimation for sensor arrays operating in adverse weather conditions, where weather-induced distortions degrade estimation accuracy. Building on a physics-based $S$-matrix model established in prior work, we adopt a statistical characterization of random phase and amplitude distortions caused by multiple scattering in rain. Based on this model, we develop a measurement framework for uniform linear arrays (ULAs) that explicitly incorporates such distortions. To mitigate their impact, we exploit the Hermitian Toeplitz (HT) structure of the covariance matrix to reduce the number of parameters to be estimated. We then apply a generalized least squares (GLS) approach for calibration. Simulation results show that the proposed method effectively suppresses rain-induced distortions, improves DoA estimation accuracy, and enhances radar sensing performance in challenging weather conditions."}
{"id": "2601.19673", "categories": ["cs.SD", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.19673", "abs": "https://arxiv.org/abs/2601.19673", "authors": ["Iwona Christop", "Mateusz Czyżnikiewicz", "Paweł Skórzewski", "Łukasz Bondaruk", "Jakub Kubiak", "Marcin Lewandowski", "Marek Kubis"], "title": "A Benchmark for Audio Reasoning Capabilities of Multimodal Large Language Models", "comment": "31 pages, 2 figures, accepted to EACL 2026", "summary": "The present benchmarks for testing the audio modality of multimodal large language models concentrate on testing various audio tasks such as speaker diarization or gender identification in isolation. Whether a multimodal model can answer the questions that require reasoning skills to combine audio tasks of different categories, cannot be verified with their use. To address this issue, we propose Audio Reasoning Tasks (ART), a new benchmark for assessing the ability of multimodal models to solve problems that require reasoning over audio signal."}
{"id": "2601.19297", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.19297", "abs": "https://arxiv.org/abs/2601.19297", "authors": ["Karl Schrader", "Shoichi Koyama", "Tomohiko Nakamura", "Mirco Pezzoli"], "title": "Phase-Retrieval-Based Physics-Informed Neural Networks For Acoustic Magnitude Field Reconstruction", "comment": "Accepted to International Conference on Acoustics, Speech and Signal Processing (ICASSP) 2026", "summary": "We propose a method for estimating the magnitude distribution of an acoustic field from spatially sparse magnitude measurements. Such a method is useful when phase measurements are unreliable or inaccessible. Physics-informed neural networks (PINNs) have shown promise for sound field estimation by incorporating constraints derived from governing partial differential equations (PDEs) into neural networks. However, they do not extend to settings where phase measurements are unavailable, as the loss function based on the governing PDE relies on phase information. To remedy this, we propose a phase-retrieval-based PINN for magnitude field estimation. By representing the magnitude and phase distributions with separate networks, the PDE loss can be computed based on the reconstructed complex amplitude. We demonstrate the effectiveness of our phase-retrieval-based PINN through experimental evaluation."}
{"id": "2601.19656", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.19656", "abs": "https://arxiv.org/abs/2601.19656", "authors": ["Parisa Ramezani", "Emil Björnson"], "title": "Cell-Free MIMO in Space: Cooperative Satellite Transmission with Multi-Antenna Ground Users", "comment": null, "summary": "This paper develops a multi-user downlink communication framework for distributed low Earth orbit satellite networks serving ground users equipped with multiple antennas. Building upon the concept of cell-free multiple-input multiple-output in terrestrial networks, we propose a coordinated transmission scheme where multiple satellites jointly transmit spatially multiplexed data streams to each user. Using a new approximate achievable rate expression, we formulate a sum rate maximization problem under per-satellite and per-antenna power constraints and use the classical equivalence between sum rate maximization and mean square error minimization to optimize the satellites' precoding matrices using statistical channel state information. We numerically examine the performance of the proposed scheme in different settings and validate its effectiveness by comparing it against traditional precoding designs."}
{"id": "2601.19709", "categories": ["cs.SD", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.19709", "abs": "https://arxiv.org/abs/2601.19709", "authors": ["Zhihua Fang", "Liang He"], "title": "Hyperbolic Additive Margin Softmax with Hierarchical Information for Speaker Verification", "comment": "5 pages, 3 figures, Accepted at ICASSP 2026", "summary": "Speaker embedding learning based on Euclidean space has achieved significant progress, but it is still insufficient in modeling hierarchical information within speaker features. Hyperbolic space, with its negative curvature geometric properties, can efficiently represent hierarchical information within a finite volume, making it more suitable for the feature distribution of speaker embeddings. In this paper, we propose Hyperbolic Softmax (H-Softmax) and Hyperbolic Additive Margin Softmax (HAM-Softmax) based on hyperbolic space. H-Softmax incorporates hierarchical information into speaker embeddings by projecting embeddings and speaker centers into hyperbolic space and computing hyperbolic distances. HAM-Softmax further enhances inter-class separability by introducing margin constraint on this basis. Experimental results show that H-Softmax and HAM-Softmax achieve average relative EER reductions of 27.84% and 14.23% compared with standard Softmax and AM-Softmax, respectively, demonstrating that the proposed methods effectively improve speaker verification performance and at the same time preserve the capability of hierarchical structure modeling. The code will be released at https://github.com/PunkMale/HAM-Softmax."}
{"id": "2601.19660", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.19660", "abs": "https://arxiv.org/abs/2601.19660", "authors": ["Parisa Ramezani", "Alva Kosasih", "Emil Björnson"], "title": "Maximum A Posteriori Probability Channel Tracking with an Intelligent Transmitting Surface", "comment": null, "summary": "This paper considers an intelligent transmitting surface (ITS) integrated into a base station and develops a low-overhead maximum a posteriori (MAP) probability channel tracking method for the dominant line-of-sight link between the ITS and the user equipment. We cast the per-block channel as a three-parameter model consisting of the channel amplitude, channel phase, and angle-of-arrival at the ITS. We exploit temporal correlation by updating the priors using the estimates from the previous block. Using only two pilots per coherence block alongside a targeted beam alignment strategy, the proposed method achieves precise channel tracking and attains spectral efficiency close to that achievable under perfect channel knowledge."}
{"id": "2601.19712", "categories": ["cs.SD", "cs.MM"], "pdf": "https://arxiv.org/pdf/2601.19712", "abs": "https://arxiv.org/abs/2601.19712", "authors": ["Congyi Fan", "Jian Guan", "Youtian Lin", "Dongli Xu", "Tong Ye", "Qiaoxi Zhu", "Pengming Feng", "Wenwu Wang"], "title": "Physics-Aware Novel-View Acoustic Synthesis with Vision-Language Priors and 3D Acoustic Environment Modeling", "comment": "ICASSP 2026 Accept, Project page: https://physnvas.github.io/", "summary": "Spatial audio is essential for immersive experiences, yet novel-view acoustic synthesis (NVAS) remains challenging due to complex physical phenomena such as reflection, diffraction, and material absorption. Existing methods based on single-view or panoramic inputs improve spatial fidelity but fail to capture global geometry and semantic cues such as object layout and material properties. To address this, we propose Phys-NVAS, the first physics-aware NVAS framework that integrates spatial geometry modeling with vision-language semantic priors. A global 3D acoustic environment is reconstructed from multi-view images and depth maps to estimate room size and shape, enhancing spatial awareness of sound propagation. Meanwhile, a vision-language model extracts physics-aware priors of objects, layouts, and materials, capturing absorption and reflection beyond geometry. An acoustic feature fusion adapter unifies these cues into a physics-aware representation for binaural generation. Experiments on RWAVS demonstrate that Phys-NVAS yields binaural audio with improved realism and physical consistency."}
{"id": "2601.19784", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.19784", "abs": "https://arxiv.org/abs/2601.19784", "authors": ["Danilo Lelin Li", "Ramtin Rabiee", "Arman Farhang"], "title": "Channel Estimation using 5G Sounding Reference Signals: A Delay-Doppler Domain Approach", "comment": null, "summary": "Delay-Doppler multicarrier modulation (DDMC) techniques have been among the central topics of research for high-Doppler channels. However, a complete transition to DDMC-based waveforms is not yet practically feasible. This is because 5G NR based waveforms, orthogonal frequency division multiplexing (OFDM) and discrete Fourier transform-spread OFDM (DFT-s-OFDM), remain as the modulation schemes for the sixth-generation radio (6GR). Hence, in this paper, we demonstrate how we can still benefit from DD-domain processing in high-mobility scenarios using 5G NR sounding reference signals (SRSs). By considering a DFT-s-OFDM receiver, we transform each received OFDM symbol into the delay-Doppler (DD) domain, where the channel is then estimated. With this approach, we estimate the DD channel parameters, allowing us to predict the aged channel over OFDM symbols without pilots. To improve channel prediction, we propose a linear joint channel estimation and equalization technique, where we use the detected data in each OFDM symbol to sequentially update our channel estimates. Our simulation results show that the proposed technique significantly outperforms the conventional frequency-domain estimation technique in terms of bit error rate (BER) and normalized mean squared error (NMSE). Furthermore, we show that using only two slots with SRS for initial channel estimation, our method supports pilot-free detection for more than 25 subsequent OFDM symbols."}
{"id": "2601.19767", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2601.19767", "abs": "https://arxiv.org/abs/2601.19767", "authors": ["Kentaro Onda", "Satoru Fukayama", "Daisuke Saito", "Nobuaki Minematsu"], "title": "Advanced Modeling of Interlanguage Speech Intelligibility Benefit with L1-L2 Multi-Task Learning Using Differentiable K-Means for Accent-Robust Discrete Token-Based ASR", "comment": "Accepted to ICASSP 2026", "summary": "Building ASR systems robust to foreign-accented speech is an important challenge in today's globalized world. A prior study explored the way to enhance the performance of phonetic token-based ASR on accented speech by reproducing the phenomenon known as interlanguage speech intelligibility benefit (ISIB), where foreign-accented speech is more intelligible to listeners sharing the speaker's native language than to native listeners. ISIB was technically implemented by using the speaker's L1 to learn k-means cluster centroids in an SSL feature space to obtain phonetic tokens. In this study, we propose a more advanced modeling of ISIB. By employing differentiable k-means and optimizing the entire module for both L1 and L2 ASR, the proposed method outperformed the baselines, both when using only native speech and when additionally incorporating a limited amount of accented speech. Notably, in the latter scenario, our method achieved approximately a 20% relative improvement in recognition accuracy."}
{"id": "2601.19853", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.19853", "abs": "https://arxiv.org/abs/2601.19853", "authors": ["Huy Trinh"], "title": "Generative Latent Alignment for Interpretable Radar Based Occupancy Detection in Ambient Assisted Living", "comment": null, "summary": "In this work, we study how to make mmWave radar presence detection more interpretable for Ambient Assisted Living (AAL) settings, where camera-based sensing raises privacy concerns. We propose a Generative Latent Alignment (GLA) framework that combines a lightweight convolutional variational autoencoder with a frozen CLIP text encoder to learn a low-dimensional latent representation of radar Range-Angle (RA) heatmaps. The latent space is softly aligned with two semantic anchors corresponding to \"empty room\" and \"person present\", and Grad-CAM is applied in this aligned latent space to visualize which spatial regions support each presence decision. On our mmWave radar dataset, we qualitatively observe that the \"person present\" class produces compact Grad-CAM blobs that coincide with strong RA returns, whereas \"empty room\" samples yield diffuse or no evidence. We also conduct an ablation study using unrelated text prompts, which degrades both reconstruction and localization, suggesting that radar-specific anchors are important for meaningful explanations in this setting."}
{"id": "2601.19781", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2601.19781", "abs": "https://arxiv.org/abs/2601.19781", "authors": ["Kentaro Onda", "Hayato Futami", "Yosuke Kashiwagi", "Emiru Tsunoo", "Shinji Watanabe"], "title": "Phonological Tokenizer: Prosody-Aware Phonetic Token via Multi-Objective Fine-Tuning with Differentiable K-Means", "comment": "Accepted to ICASSP 2026", "summary": "In recent years, there has been growing interest in representing speech with discrete tokens, which serve as pseudo-text for speech language models (speechLMs) and as efficient intermediate representations for downstream tasks. These tokens are typically categorized as acoustic and phonetic tokens: the former holds detailed acoustic information for reconstruction while the latter mainly captures linguistic content. In human speech communication, however, unnecessary acoustic details such as speaker information are abstracted, while both linguistic and prosodic information are utilized for speech comprehension and production. Given this, neither type of token seems an ideal representation for tasks sensitive to prosody, such as speechLMs. In this study, we propose the Phonological Tokenizer, a method that fine-tunes phonetic tokens via differentiable k-means with a multi-task objective of ASR and speech resynthesis. Experimental validation on diverse tasks confirms that our tokens retain phonological (both linguistic and prosodic) information while appropriately discarding speaker identity."}
{"id": "2601.19786", "categories": ["eess.AS", "cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2601.19786", "abs": "https://arxiv.org/abs/2601.19786", "authors": ["Jinzuomu Zhong", "Yi Wang", "Korin Richmond", "Peter Bell"], "title": "Rethinking Discrete Speech Representation Tokens for Accent Generation", "comment": null, "summary": "Discrete Speech Representation Tokens (DSRTs) have become a foundational component in speech generation. While prior work has extensively studied phonetic and speaker information in DSRTs, how accent information is encoded in DSRTs remains largely unexplored. In this paper, we present the first systematic investigation of accent information in DSRTs. We propose a unified evaluation framework that measures both accessibility of accent information via a novel Accent ABX task and recoverability via cross-accent Voice Conversion (VC) resynthesis. Using this framework, we analyse DSRTs derived from a variety of speech encoders. Our results reveal that accent information is substantially reduced when ASR supervision is used to fine-tune the encoder, but cannot be effectively disentangled from phonetic and speaker information through naive codebook size reduction. Based on these findings, we propose new content-only and content-accent DSRTs that significantly outperform existing designs in controllable accent generation. Our work highlights the importance of accent-aware evaluation and provides practical guidance for designing DSRTs for accent-controlled speech generation."}

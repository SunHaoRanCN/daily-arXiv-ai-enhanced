{"id": "2601.19946", "categories": ["eess.AS", "cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2601.19946", "abs": "https://arxiv.org/abs/2601.19946", "authors": ["Nikhil Raghav", "Avisek Gupta", "Swagatam Das", "Md Sahidullah"], "title": "MK-SGC-SC: Multiple Kernel guided Sparse Graph Construction in Spectral Clustering for Unsupervised Speaker Diarization", "comment": "5 pages", "summary": "Speaker diarization aims to segment audio recordings into regions corresponding to individual speakers. Although unsupervised speaker diarization is inherently challenging, the prospect of identifying speaker regions without pretraining or weak supervision motivates research on clustering techniques. In this work, we share the notable observation that measuring multiple kernel similarities of speaker embeddings to thereafter craft a sparse graph for spectral clustering in a principled manner is sufficient to achieve state-of-the-art performances in a fully unsupervised setting. Specifically, we consider four polynomial kernels and a degree one arccosine kernel to measure similarities in speaker embeddings, using which sparse graphs are constructed in a principled manner to emphasize local similarities. Experiments show the proposed approach excels in unsupervised speaker diarization over a variety of challenging environments in the DIHARD-III, AMI, and VoxConverse corpora. To encourage further research, our implementations are available at https://github.com/nikhilraghav29/MK-SGC-SC."}
{"id": "2601.19949", "categories": ["eess.AS", "cs.CL", "cs.SD", "eess.SP"], "pdf": "https://arxiv.org/pdf/2601.19949", "abs": "https://arxiv.org/abs/2601.19949", "authors": ["Mandip Goswami"], "title": "RIR-Mega-Speech: A Reverberant Speech Corpus with Comprehensive Acoustic Metadata and Reproducible Evaluation", "comment": null, "summary": "Despite decades of research on reverberant speech, comparing methods remains difficult because most corpora lack per-file acoustic annotations or provide limited documentation for reproduction. We present RIR-Mega-Speech, a corpus of approximately 117.5 hours created by convolving LibriSpeech utterances with roughly 5,000 simulated room impulse responses from the RIR-Mega collection. Every file includes RT60, direct-to-reverberant ratio (DRR), and clarity index ($C_{50}$) computed from the source RIR using clearly defined, reproducible procedures. We also provide scripts to rebuild the dataset and reproduce all evaluation results.\n  Using Whisper small on 1,500 paired utterances, we measure 5.20% WER (95% CI: 4.69--5.78) on clean speech and 7.70% (7.04--8.35) on reverberant versions, corresponding to a paired increase of 2.50 percentage points (2.06--2.98). This represents a 48% relative degradation. WER increases monotonically with RT60 and decreases with DRR, consistent with prior perceptual studies. While the core finding that reverberation harms recognition is well established, we aim to provide the community with a standardized resource where acoustic conditions are transparent and results can be verified independently. The repository includes one-command rebuild instructions for both Windows and Linux environments."}
{"id": "2601.19956", "categories": ["eess.AS", "cs.AI", "cs.SD"], "pdf": "https://arxiv.org/pdf/2601.19956", "abs": "https://arxiv.org/abs/2601.19956", "authors": ["Yuxiang Wang", "Hongyu Liu", "Dekun Chen", "Xueyao Zhang", "Zhizheng Wu"], "title": "VoxPrivacy: A Benchmark for Evaluating Interactional Privacy of Speech Language Models", "comment": null, "summary": "As Speech Language Models (SLMs) transition from personal devices to shared, multi-user environments such as smart homes, a new challenge emerges: the model is expected to distinguish between users to manage information flow appropriately. Without this capability, an SLM could reveal one user's confidential schedule to another, a privacy failure we term interactional privacy. Thus, the ability to generate speaker-aware responses becomes essential for SLM safe deployment. Current SLM benchmarks test dialogue ability but overlook speaker identity. Multi-speaker benchmarks check who said what without assessing whether SLMs adapt their responses. Privacy benchmarks focus on globally sensitive data (e.g., bank passwords) while neglecting contextual privacy-sensitive information (e.g., a user's private appointment). To address this gap, we introduce VoxPrivacy, the first benchmark designed to evaluate interactional privacy in SLMs. VoxPrivacy spans three tiers of increasing difficulty, from following direct secrecy commands to proactively protecting privacy. Our evaluation of nine SLMs on a 32-hour bilingual dataset reveals a widespread vulnerability: most open-source models perform close to random chance (around 50% accuracy) on conditional privacy decisions, while even strong closed-source systems fall short on proactive privacy inference. We further validate these findings on Real-VoxPrivacy, a human-recorded subset, confirming that failures observed on synthetic data persist in real speech. Finally, we demonstrate a viable path forward: by fine-tuning on a new 4,000-hour training set, we improve privacy-preserving abilities while maintaining robustness. To support future work, we release the VoxPrivacy benchmark, the large-scale training set, and the fine-tuned model to foster the development of safer and more context-aware SLMs."}
{"id": "2601.19960", "categories": ["eess.AS", "cs.AI", "cs.SD"], "pdf": "https://arxiv.org/pdf/2601.19960", "abs": "https://arxiv.org/abs/2601.19960", "authors": ["Youness Dkhissi", "Valentin Vielzeuf", "Elys Allesiardo", "Anthony Larcher"], "title": "Do we really need Self-Attention for Streaming Automatic Speech Recognition?", "comment": null, "summary": "Transformer-based architectures are the most used architectures in many deep learning fields like Natural Language Processing, Computer Vision or Speech processing. It may encourage the direct use of Transformers in the constrained tasks, without questioning whether it will yield the same benefits as in standard tasks.  Given specific constraints, it is essential to evaluate the relevance of transformer models. This work questions the suitability of transformers for specific domains. We argue that the high computational requirements and latency issues associated with these models do not align well with streaming applications. Our study promotes the search for alternative strategies to improve efficiency without sacrificing performance.  In light of this observation, our paper critically examines the usefulness of transformer architecture in such constrained environments. As a first attempt, we show that the computational cost for Streaming Automatic Speech Recognition (ASR) can be reduced using deformable convolution instead of Self-Attention. Furthermore, we show that Self-Attention mechanisms can be entirely removed and not replaced, without observing significant degradation in the Word Error Rate."}
{"id": "2601.19951", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.19951", "abs": "https://arxiv.org/abs/2601.19951", "authors": ["Lekai Qian", "Haoyu Gu", "Dehan Li", "Boyu Cao", "Qi Liu"], "title": "Pianoroll-Event: A Novel Score Representation for Symbolic Music", "comment": null, "summary": "Symbolic music representation is a fundamental challenge in computational musicology. While grid-based representations effectively preserve pitch-time spatial correspondence, their inherent data sparsity leads to low encoding efficiency. Discrete-event representations achieve compact encoding but fail to adequately capture structural invariance and spatial locality. To address these complementary limitations, we propose Pianoroll-Event, a novel encoding scheme that describes pianoroll representations through events, combining structural properties with encoding efficiency while maintaining temporal dependencies and local spatial patterns. Specifically, we design four complementary event types: Frame Events for temporal boundaries, Gap Events for sparse regions, Pattern Events for note patterns, and Musical Structure Events for musical metadata. Pianoroll-Event strikes an effective balance between sequence length and vocabulary size, improving encoding efficiency by 1.36\\times to 7.16\\times over representative discrete sequence methods. Experiments across multiple autoregressive architectures show models using our representation consistently outperform baselines in both quantitative and human evaluations."}
{"id": "2601.20017", "categories": ["eess.SP", "physics.app-ph"], "pdf": "https://arxiv.org/pdf/2601.20017", "abs": "https://arxiv.org/abs/2601.20017", "authors": ["Albert Salmi", "Ville Viikari", "Philipp del Hougne"], "title": "Electromagnetically Consistent Bounds on Information Transfer in Real-World RIS-Parametrized Wireless Channels", "comment": "13 pages with 3 figures", "summary": "A reconfigurable intelligent surface (RIS) endows a wireless channel with programmability that can be leveraged to optimize wireless information transfer. While many works study algorithms for optimizing such a programmable channel, relatively little is known about fundamental bounds on the achievable information transfer. In particular, non-trivial bounds that are both electromagnetically consistent (e.g., aware of mutual coupling) and in line with realistic hardware constraints (e.g., few-bit-programmable, potentially lossy loads) are missing. Here, based on a rigorous multiport network model of a single-input single-output (SISO) channel parametrized by 1-bit-programmable RIS elements, we apply a semidefinite relaxation (SDR) to derive a fundamental bound on the achievable SISO channel gain enhancement. A bound on the maximum achievable rate of information transfer at a given noise level follows directly from Shannon's theorem. We apply our bound to several numerical and experimental examples of different RIS-parametrized radio environments. Compared to electromagnetically consistent benchmark bounding strategies (a norm-inequality bound and, where applicable, a relaxation to an idealized beyond-diagonal load network for which a global solution exists), we consistently observe that our SDR-based bound is notably tighter. We reach at least 64 % (but often 100 %) of our SDR-based bound with standard discrete optimization techniques. The applicability of our bound to concrete experimental systems makes it valuable to inform wireless practitioners, e.g., to evaluate RIS hardware design choices and algorithms to optimize the RIS configuration. Our work contributes to the development of an electromagnetic information theory for RIS-parametrized channels as well as other programmable wave systems such as dynamic metasurface antennas or real-life beyond-diagonal RISs."}
{"id": "2601.20094", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2601.20094", "abs": "https://arxiv.org/abs/2601.20094", "authors": ["Haibin Wu", "Bach Viet Do", "Naveen Suda", "Julian Chan", "Madhavan C R", "Gene-Ping Yang", "Yi-Chiao Wu", "Naoyuki Kanda", "Yossef Adi", "Xin Lei", "Yue Liu", "Florian Metze", "Yuzong Liu"], "title": "T-Mimi: A Transformer-based Mimi Decoder for Real-Time On-Phone TTS", "comment": "Accepted by ICASSP 2026", "summary": "Neural audio codecs provide promising acoustic features for speech synthesis, with representative streaming codecs like Mimi providing high-quality acoustic features for real-time Text-to-Speech (TTS) applications. However, Mimi's decoder, which employs a hybrid transformer and convolution architecture, introduces significant latency bottlenecks on edge devices due to the the compute intensive nature of deconvolution layers which are not friendly for mobile-CPUs, such as the most representative framework XNNPACK. This paper introduces T-Mimi, a novel modification of the Mimi codec decoder that replaces its convolutional components with a purely transformer-based decoder, inspired by the TS3-Codec architecture. This change dramatically reduces on-device TTS latency from 42.1ms to just 4.4ms. Furthermore, we conduct quantization aware training and derive a crucial finding: the final two transformer layers and the concluding linear layers of the decoder, which are close to the waveform, are highly sensitive to quantization and must be preserved at full precision to maintain audio quality."}
{"id": "2601.19952", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.19952", "abs": "https://arxiv.org/abs/2601.19952", "authors": ["Wenhao Zou", "Yuwei Miao", "Zhanyu Ma", "Jun Xu", "Jiuchong Gao", "Jinghua Hao", "Renqing He", "Jingwen Xu"], "title": "LTS-VoiceAgent: A Listen-Think-Speak Framework for Efficient Streaming Voice Interaction via Semantic Triggering and Incremental Reasoning", "comment": null, "summary": "Real-time voice agents face a dilemma: end-to-end models often lack deep reasoning, while cascaded pipelines incur high latency by executing ASR, LLM reasoning, and TTS strictly in sequence, unlike human conversation where listeners often start thinking before the speaker finishes. Since cascaded architectures remain the dominant choice for complex tasks, existing cascaded streaming strategies attempt to reduce this latency via mechanical segmentation (e.g., fixed chunks, VAD-based splitting) or speculative generation, but they frequently either break semantic units or waste computation on predictions that must be rolled back. To address these challenges, we propose LTS-VoiceAgent, a Listen-Think-Speak framework that explicitly separates when to think from how to reason incrementally. It features a Dynamic Semantic Trigger to detect meaningful prefixes, and a Dual-Role Stream Orchestrator that coordinates a background Thinker (for state maintenance) and a foreground Speaker (for speculative solving). This parallel design enables \"thinking while speaking\" without blocking responses. We also introduce a Pause-and-Repair benchmark containing natural disfluencies to stress-test streaming robustness. Experiments across VERA, Spoken-MQA, BigBenchAudio, and our benchmark show that LTS-VoiceAgent achieves a stronger accuracy-latency-efficiency trade-off than serial cascaded baselines and existing streaming strategies."}
{"id": "2601.20124", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.20124", "abs": "https://arxiv.org/abs/2601.20124", "authors": ["Domenico Ciuonzo", "Alessio Zappone", "Marco Di Renzo", "Ciro D'Elia"], "title": "Holographic & Channel-Aware Distributed Detection of a Non-cooperative Target", "comment": "accepted for publication in IEEE Transactions on Aerospace and Electronic Systems", "summary": "This work investigates Distributed Detection (DD) in Wireless Sensor Networks (WSNs), where spatially distributed sensors transmit binary decisions over a shared flat-fading channel. To enhance fusion efficiency, a reconfigurable metasurface is positioned in the near-field of a few receive antennas, enabling a holographic architecture that harnesses large-aperture gains with minimal RF hardware. A generalized likelihood ratio test is derived for fixed metasurface settings, and two low-complexity joint design strategies are proposed to optimize both fusion and metasurface configuration. These suboptimal schemes achieve a balance between performance, complexity, and system knowledge. The goal is to ensure reliable detection of a localized phenomenon at the fusion center, under energy-efficient constraints aligned with IoT requirements. Simulation results validate the effectiveness of the proposed holographic fusion, even under simplified designs."}
{"id": "2601.20319", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2601.20319", "abs": "https://arxiv.org/abs/2601.20319", "authors": ["Ya-Tse Wu", "Chi-Chun Lee"], "title": "ASR for Affective Speech: Investigating Impact of Emotion and Speech Generative Strategy", "comment": "Accepted for publication at IEEE Automatic Speech Recognition and Understanding Workshop (ASRU) 2025", "summary": "This work investigates how emotional speech and generative strategies affect ASR performance. We analyze speech synthesized from three emotional TTS models and find that substitution errors dominate, with emotional expressiveness varying across models. Based on these insights, we introduce two generative strategies: one using transcription correctness and another using emotional salience, to construct fine-tuning subsets. Results show consistent WER improvements on real emotional datasets without noticeable degradation on clean LibriSpeech utterances. The combined strategy achieves the strongest gains, particularly for expressive speech. These findings highlight the importance of targeted augmentation for building emotion-aware ASR systems."}
{"id": "2601.20362", "categories": ["cs.SD", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.20362", "abs": "https://arxiv.org/abs/2601.20362", "authors": ["Xiangbo Wang", "Wenbin Jiang", "Jin Wang", "Yubo You", "Sheng Fang", "Fei Wen"], "title": "Switchcodec: Adaptive residual-expert sparse quantization for high-fidelity neural audio coding", "comment": "4page,3figure,Accepted by ICASSP 2026,We would like to express our sincere gratitude to Senior Fellow Jing Wang for his continuous support and assistance. He has made an indelible and significant contribution to this work", "summary": "Recent neural audio compression models often rely on residual vector quantization for high-fidelity coding, but using a fixed number of per-frame codebooks is suboptimal for the wide variability of audio content-especially for signals that are either very simple or highly complex. To address this limitation, we propose SwitchCodec, a neural audio codec based on Residual Experts Vector Quantization (REVQ). REVQ combines a shared quantizer with dynamically routed expert quantizers that are activated according to the input audio, decoupling bitrate from codebook capacity and improving compression efficiency. This design ensures full training and utilization of each quantizer. In addition, a variable-bitrate mechanism adjusts the number of active expert quantizers at inference, enabling multi-bitrate operation without retraining. Experiments demonstrate that SwitchCodec surpasses existing baselines on both objective metrics and subjective listening tests."}
{"id": "2601.20178", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.20178", "abs": "https://arxiv.org/abs/2601.20178", "authors": ["Gaoze Mu", "Yanzhao Hou", "Mingjie Chen", "Yuanyu Hu", "Yongan Zheng", "Qimei Cui", "Xiaofeng Tao"], "title": "Coverage Performance Analysis of FAS-enhanced LoRa Wide Area Networks under both Co-SF and Inter-SF Interference", "comment": "6 pages, 3 figures", "summary": "This paper presents an analytical framework for evaluating the coverage performance of the fluid antenna system (FAS)-enhanced LoRa wide-area networks (LoRaWANs). We investigate the effects of large-scale pathloss in LoRaWAN, small-scale fading characterized by FAS, and dense interference (i.e., collision in an ALOHA-based mechanism) arising from randomly deployed end devices (EDs). Both co-spreading factor (co-SF) interference (with the same SF) and inter-SF interference (with different SFs) are introduced into the network, and their differences in physical characteristics are also considered in the analysis. Additionally, simple yet accurate statistical approximations of the FAS channel envelope and power are derived using the extreme-value theorem. Based on the approximated channel expression, the theoretical coverage probability of the proposed FAS-enhanced LoRaWAN is derived. Numerical results validate our analytical approximations by exhibiting close agreement with the exact correlation model. Notably, it is revealed that a FAS with a normalized aperture of 1 times 1 can greatly enhance network performance, in terms of both ED numbers and coverage range."}
{"id": "2601.20481", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2601.20481", "abs": "https://arxiv.org/abs/2601.20481", "authors": ["Myungjin Lee", "Eunji Shin", "Jiyoung Lee"], "title": "Erasing Your Voice Before It's Heard: Training-free Speaker Unlearning for Zero-shot Text-to-Speech", "comment": "ICASSP'2026", "summary": "Modern zero-shot text-to-speech (TTS) models offer unprecedented expressivity but also pose serious crime risks, as they can synthesize voices of individuals who never consented. In this context, speaker unlearning aims to prevent the generation of specific speaker identities upon request. Existing approaches, reliant on retraining, are costly and limited to speakers seen in the training set. We present TruS, a training-free speaker unlearning framework that shifts the paradigm from data deletion to inference-time control. TruS steers identity-specific hidden activations to suppress target speakers while preserving other attributes (e.g., prosody and emotion). Experimental results show that TruS effectively prevents voice generation on both seen and unseen opt-out speakers, establishing a scalable safeguard for speech synthesis. The demo and code are available on http://mmai.ewha.ac.kr/trus."}
{"id": "2601.20426", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2601.20426", "abs": "https://arxiv.org/abs/2601.20426", "authors": ["Annie Chu", "Hugo Flores García", "Oriol Nieto", "Justin Salamon", "Bryan Pardo", "Prem Seetharaman"], "title": "Mix2Morph: Learning Sound Morphing from Noisy Mixes", "comment": "Accepted into ICASSP 2026", "summary": "We introduce Mix2Morph, a text-to-audio diffusion model fine-tuned to perform sound morphing without a dedicated dataset of morphs. By finetuning on noisy surrogate mixes at higher diffusion timesteps, Mix2Morph yields stable, perceptually coherent morphs that convincingly integrate qualities of both sources. We specifically target sound infusions, a practically and perceptually motivated subclass of morphing in which one sound acts as the dominant primary source, providing overall temporal and structural behavior, while a secondary sound is infused throughout, enriching its timbral and textural qualities. Objective evaluations and listening tests show that Mix2Morph outperforms prior baselines and produces high-quality sound infusions across diverse categories, representing a step toward more controllable and concept-driven tools for sound design. Sound examples are available at https://anniejchu.github.io/mix2morph ."}
{"id": "2601.20190", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.20190", "abs": "https://arxiv.org/abs/2601.20190", "authors": ["Viet Chu", "Omar Mashaal", "Hatem Abou-Zeid"], "title": "WirelessJEPA: A Multi-Antenna Foundation Model using Spatio-temporal Wireless Latent Predictions", "comment": null, "summary": "We propose WirelessJEPA, a novel wireless foundation model (WFM) that uses the Joint Embedding Predictive Architecture (JEPA). WirelessJEPA learns general-purpose representations directly from real-world multi-antenna IQ data by predicting latent representations of masked signal regions. This enables multiple diverse downstream tasks without reliance on carefully engineered contrastive augmentations. To adapt JEPA to wireless signals, we introduce a 2D antenna time representation that reshapes multi-antenna IQ streams into structured grids, allowing convolutional processing with block masking and efficient sparse computation over unmasked patches. Building on this representation, we propose novel spatio temporal mask geometries that encode inductive biases across antennas and time. We evaluate WirelessJEPA across six downstream tasks and demonstrate it's robust performance and strong task generalization. Our results establish that JEPA-based learning as a promising direction for building generalizable WFMs."}
{"id": "2601.20542", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2601.20542", "abs": "https://arxiv.org/abs/2601.20542", "authors": ["Yayun Liang", "Yuanming Zhang", "Fei Chen", "Jing Lu", "Zhibin Lin"], "title": "Decoding Speech Envelopes from Electroencephalogram with a Contrastive Pearson Correlation Coefficient Loss", "comment": null, "summary": "Recent advances in reconstructing speech envelopes from Electroencephalogram (EEG) signals have enabled continuous auditory attention decoding (AAD) in multi-speaker environments. Most Deep Neural Network (DNN)-based envelope reconstruction models are trained to maximize the Pearson correlation coefficients (PCC) between the attended envelope and the reconstructed envelope (attended PCC). While the difference between the attended PCC and the unattended PCC plays an essential role in auditory attention decoding, existing methods often focus on maximizing the attended PCC. We therefore propose a contrastive PCC loss which represents the difference between the attended PCC and the unattended PCC. The proposed approach is evaluated on three public EEG AAD datasets using four DNN architectures. Across many settings, the proposed objective improves envelope separability and AAD accuracy, while also revealing dataset- and architecture-dependent failure cases."}
{"id": "2601.20432", "categories": ["cs.SD", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.20432", "abs": "https://arxiv.org/abs/2601.20432", "authors": ["Yigitcan Özer", "Wanying Ge", "Zhe Zhang", "Xin Wang", "Junichi Yamagishi"], "title": "Self Voice Conversion as an Attack against Neural Audio Watermarking", "comment": "7 pages; 2 figures; 2 tables; accepted at IEICE, SP/SLP 2026", "summary": "Audio watermarking embeds auxiliary information into speech while maintaining speaker identity, linguistic content, and perceptual quality. Although recent advances in neural and digital signal processing-based watermarking methods have improved imperceptibility and embedding capacity, robustness is still primarily assessed against conventional distortions such as compression, additive noise, and resampling. However, the rise of deep learning-based attacks introduces novel and significant threats to watermark security. In this work, we investigate self voice conversion as a universal, content-preserving attack against audio watermarking systems. Self voice conversion remaps a speaker's voice to the same identity while altering acoustic characteristics through a voice conversion model. We demonstrate that this attack severely degrades the reliability of state-of-the-art watermarking approaches and highlight its implications for the security of modern audio watermarking techniques."}
{"id": "2601.20501", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.20501", "abs": "https://arxiv.org/abs/2601.20501", "authors": ["Ruizhi Zhang", "Yuchen Zhang", "Ying Zhang"], "title": "User Localization via Active Sensing with Electromagnetically Reconfigurable Antennas", "comment": null, "summary": "This paper presents an end-to-end deep learning framework for electromagnetically reconfigurable antenna (ERA)-aided user localization with active sensing, where ERAs provide additional electromagnetic reconfigurability to diversify the received measurements and enhance localization informativeness.\n  To balance sensing flexibility and overhead, we adopt a two-timescale design: the digital combiner is updated at each stage, while the ERA patterns are reconfigured at each substage via a spherical-harmonic representation. The proposed mechanism integrates attention-based feature extraction and LSTM-based temporal learning, enabling the system to learn an optimized sensing strategy and progressively refine the UE position estimate from sequential observations. Simulation results show that the proposed approach consistently outperforms conventional digital beamforming-only and single-stage sensing baselines in terms of localization accuracy. These results highlight the effectiveness of ERA-enabled active sensing for user localization in future wireless systems."}
{"id": "2601.19951", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.19951", "abs": "https://arxiv.org/abs/2601.19951", "authors": ["Lekai Qian", "Haoyu Gu", "Dehan Li", "Boyu Cao", "Qi Liu"], "title": "Pianoroll-Event: A Novel Score Representation for Symbolic Music", "comment": null, "summary": "Symbolic music representation is a fundamental challenge in computational musicology. While grid-based representations effectively preserve pitch-time spatial correspondence, their inherent data sparsity leads to low encoding efficiency. Discrete-event representations achieve compact encoding but fail to adequately capture structural invariance and spatial locality. To address these complementary limitations, we propose Pianoroll-Event, a novel encoding scheme that describes pianoroll representations through events, combining structural properties with encoding efficiency while maintaining temporal dependencies and local spatial patterns. Specifically, we design four complementary event types: Frame Events for temporal boundaries, Gap Events for sparse regions, Pattern Events for note patterns, and Musical Structure Events for musical metadata. Pianoroll-Event strikes an effective balance between sequence length and vocabulary size, improving encoding efficiency by 1.36\\times to 7.16\\times over representative discrete sequence methods. Experiments across multiple autoregressive architectures show models using our representation consistently outperform baselines in both quantitative and human evaluations."}
{"id": "2601.20478", "categories": ["cs.SD", "cs.IR"], "pdf": "https://arxiv.org/pdf/2601.20478", "abs": "https://arxiv.org/abs/2601.20478", "authors": ["Adam Štefunko", "Carlos Eduardo Cancino-Chacón", "Jan Hajič"], "title": "On Every Note a Griff: Looking for a Useful Representation of Basso Continuo Performance Style", "comment": "6 pages, 5 figures, accepted to the Music Encoding Conference (MEC) 2026", "summary": "Basso continuo is a baroque improvisatory accompaniment style which involves improvising multiple parts above a given bass line in a musical score on a harpsichord or organ. Basso continuo is not merely a matter of history; moreover, it is a historically inspired living practice, and The Aligned Continuo Dataset (ACoRD) records the first sample of modern-day basso continuo playing in the symbolic domain. This dataset, containing 175 MIDI recordings of 5 basso continuo scores performed by 7 players, allows us to start observing and analyzing the variety that basso continuo improvisation brings. A recently proposed basso continuo performance-to-score alignment system provides a way of mapping improvised performance notes to score notes. In order to study aligned basso continuo performances, we need an appropriate feature representation. We propose griff, a representation inspired by historical basso continuo treatises. It enables us to encode both pitch content and structure of a basso continuo realization in a transposition-invariant way. Griffs are directly extracted from aligned basso continuo performances by grouping together performance notes aligned to the same score note in a onset-time ordered way, and they provide meaningful tokens that form a feature space in which we can analyze basso continuo performance styles. We statistically describe griffs extracted from the ACoRD dataset recordings, and show in two experiments how griffs can be used for statistical analysis of individuality of different players' basso continuo performance styles. We finally present an argument why it is desirable to preserve the structure of a basso continuo improvisation in order to conduct a refined analysis of personal performance styles of individual basso continuo practitioners, and why griffs can provide a meaningful historically informed feature space worthy of a more robust empirical validation."}
{"id": "2601.20547", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.20547", "abs": "https://arxiv.org/abs/2601.20547", "authors": ["Sharief Saleh", "Satyam Dwivedi", "Russ Whiton", "Peter Hammarberg", "Musa Furkan Keskin", "Julia Equi", "Hui Chen", "Florent Munier", "Olof Eriksson", "Fredrik Gunnarsson", "Fredrik Tufvesson", "Henk Wymeersch"], "title": "Vehicular Wireless Positioning -- A Survey", "comment": "Under review at IEEE Communications Surveys & Tutorials", "summary": "The rapid advancement of connected and autonomous vehicles has driven a growing demand for precise and reliable positioning systems capable of operating in complex environments. Meeting these demands requires an integrated approach that combines multiple positioning technologies, including wireless-based systems, perception-based technologies, and motion-based sensors. This paper presents a comprehensive survey of wireless-based positioning for vehicular applications, with a focus on satellite-based positioning (such as global navigation satellite systems (GNSS) and low-Earth-orbit (LEO) satellites), cellular-based positioning (5G and beyond), and IEEE-based technologies (including Wi-Fi, ultrawideband (UWB), Bluetooth, and vehicle-to-vehicle (V2V) communications). First, the survey reviews a wide range of vehicular positioning use cases, outlining their specific performance requirements. Next, it explores the historical development, standardization, and evolution of each wireless positioning technology, providing an in-depth categorization of existing positioning solutions and algorithms, and identifying open challenges and contemporary trends. Finally, the paper examines sensor fusion techniques that integrate these wireless systems with onboard perception and motion sensors to enhance positioning accuracy and resilience in real-world conditions. This survey thus offers a holistic perspective on the historical foundations, current advancements, and future directions of wireless-based positioning for vehicular applications, addressing a critical gap in the literature."}
{"id": "2601.19952", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.19952", "abs": "https://arxiv.org/abs/2601.19952", "authors": ["Wenhao Zou", "Yuwei Miao", "Zhanyu Ma", "Jun Xu", "Jiuchong Gao", "Jinghua Hao", "Renqing He", "Jingwen Xu"], "title": "LTS-VoiceAgent: A Listen-Think-Speak Framework for Efficient Streaming Voice Interaction via Semantic Triggering and Incremental Reasoning", "comment": null, "summary": "Real-time voice agents face a dilemma: end-to-end models often lack deep reasoning, while cascaded pipelines incur high latency by executing ASR, LLM reasoning, and TTS strictly in sequence, unlike human conversation where listeners often start thinking before the speaker finishes. Since cascaded architectures remain the dominant choice for complex tasks, existing cascaded streaming strategies attempt to reduce this latency via mechanical segmentation (e.g., fixed chunks, VAD-based splitting) or speculative generation, but they frequently either break semantic units or waste computation on predictions that must be rolled back. To address these challenges, we propose LTS-VoiceAgent, a Listen-Think-Speak framework that explicitly separates when to think from how to reason incrementally. It features a Dynamic Semantic Trigger to detect meaningful prefixes, and a Dual-Role Stream Orchestrator that coordinates a background Thinker (for state maintenance) and a foreground Speaker (for speculative solving). This parallel design enables \"thinking while speaking\" without blocking responses. We also introduce a Pause-and-Repair benchmark containing natural disfluencies to stress-test streaming robustness. Experiments across VERA, Spoken-MQA, BigBenchAudio, and our benchmark show that LTS-VoiceAgent achieves a stronger accuracy-latency-efficiency trade-off than serial cascaded baselines and existing streaming strategies."}
{"id": "2601.20510", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.20510", "abs": "https://arxiv.org/abs/2601.20510", "authors": ["Robin Singh", "Aditya Yogesh Nair", "Fabio Palumbo", "Florian Barbaro", "Anna Dyka", "Lohith Rachakonda"], "title": "Audio Deepfake Detection in the Age of Advanced Text-to-Speech models", "comment": "This work was performed using HPC resources from GENCI-IDRIS (Grant 2025- AD011016076)", "summary": "Recent advances in Text-to-Speech (TTS) systems have substantially increased the realism of synthetic speech, raising new challenges for audio deepfake detection. This work presents a comparative evaluation of three state-of-the-art TTS models--Dia2, Maya1, and MeloTTS--representing streaming, LLM-based, and non-autoregressive architectures. A corpus of 12,000 synthetic audio samples was generated using the Daily-Dialog dataset and evaluated against four detection frameworks, including semantic, structural, and signal-level approaches. The results reveal significant variability in detector performance across generative mechanisms: models effective against one TTS architecture may fail against others, particularly LLM-based synthesis. In contrast, a multi-view detection approach combining complementary analysis levels demonstrates robust performance across all evaluated models. These findings highlight the limitations of single-paradigm detectors and emphasize the necessity of integrated detection strategies to address the evolving landscape of audio deepfake threats."}
{"id": "2601.20647", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.20647", "abs": "https://arxiv.org/abs/2601.20647", "authors": ["Charlotte Muth", "Shrinivas Chimmalgi", "Laurent Schmalen"], "title": "Precoding Design for Multi-User MIMO Joint Communications and Sensing", "comment": "Accepted at ICC26", "summary": "We investigate precoding for multi-user (MU) multiple-input multiple-output (MIMO) joint communications and sensing (JCAS) systems, taking into account the potential interference between sensing and communication channels. We derive indicators for the sensing and communication performance, i.e., the detection probability and the communication signal-to-interference-and-noise ratio (SINR) for general input signals. Our results show that the use of the communication signal for sensing can prevent a loss in communication performance if channel interference occurs, while the kurtosis of the transmit alphabet of the communication signal limits the sensing performance. We present simulation results of example setups."}
{"id": "2601.20510", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.20510", "abs": "https://arxiv.org/abs/2601.20510", "authors": ["Robin Singh", "Aditya Yogesh Nair", "Fabio Palumbo", "Florian Barbaro", "Anna Dyka", "Lohith Rachakonda"], "title": "Audio Deepfake Detection in the Age of Advanced Text-to-Speech models", "comment": "This work was performed using HPC resources from GENCI-IDRIS (Grant 2025- AD011016076)", "summary": "Recent advances in Text-to-Speech (TTS) systems have substantially increased the realism of synthetic speech, raising new challenges for audio deepfake detection. This work presents a comparative evaluation of three state-of-the-art TTS models--Dia2, Maya1, and MeloTTS--representing streaming, LLM-based, and non-autoregressive architectures. A corpus of 12,000 synthetic audio samples was generated using the Daily-Dialog dataset and evaluated against four detection frameworks, including semantic, structural, and signal-level approaches. The results reveal significant variability in detector performance across generative mechanisms: models effective against one TTS architecture may fail against others, particularly LLM-based synthesis. In contrast, a multi-view detection approach combining complementary analysis levels demonstrates robust performance across all evaluated models. These findings highlight the limitations of single-paradigm detectors and emphasize the necessity of integrated detection strategies to address the evolving landscape of audio deepfake threats."}
{"id": "2601.20573", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2601.20573", "abs": "https://arxiv.org/abs/2601.20573", "authors": ["Taihui Wang", "Jinzheng Zhao", "Rilin Chen", "Tong Lei", "Wenwu Wang", "Dong Yu"], "title": "Gen-SER: When the generative model meets speech emotion recognition", "comment": "Accepted to IEEE ICASSP 2026", "summary": "Speech emotion recognition (SER) is crucial in speech understanding and generation. Most approaches are based on either classification models or large language models. Different from previous methods, we propose Gen-SER, a novel approach that reformulates SER as a distribution shift problem via generative models. We propose to project discrete class labels into a continuous space, and obtain the terminal distribution via sinusoidal taxonomy encoding. The target-matching-based generative model is adopted to transform the initial distribution into the terminal distribution efficiently. The classification is achieved by calculating the similarity of the generated terminal distribution and ground truth terminal distribution. The experimental results confirm the efficacy of the proposed method, demonstrating its extensibility to various speech-understanding tasks and suggesting its potential applicability to a broader range of classification tasks."}
{"id": "2601.20654", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.20654", "abs": "https://arxiv.org/abs/2601.20654", "authors": ["Qian Gao", "Ruikang Zhong", "Yue Liu", "Hyundong Shin", "Yuanwei Liu"], "title": "RL based Beamforming Optimization for 3D Pinching Antenna assisted ISAC Systems", "comment": null, "summary": "In this paper, a three-dimensional (3D) deployment scheme of pinching antenna array is proposed, aiming to enhances the performance of integrated sensing and communication (ISAC) systems. To fully realize the potential of 3D deployment, a joint antenna positioning, time allocation and transmit power optimization problem is formulated to maximize the sum communication rate with the constraints of target sensing rates and system energy. To solve the sum rate maximization problem, we propose a heterogeneous graph neural network based reinforcement learning (HGRL) algorithm. Simulation results prove that 3D deployment of pinching antenna array outperforms 1D and 2D counterparts in ISAC systems. Moreover, the proposed HGRL algorithm surpasses other baselines in both performance and convergence speed due to the advanced observation construction of the environment."}
{"id": "2601.19946", "categories": ["eess.AS", "cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2601.19946", "abs": "https://arxiv.org/abs/2601.19946", "authors": ["Nikhil Raghav", "Avisek Gupta", "Swagatam Das", "Md Sahidullah"], "title": "MK-SGC-SC: Multiple Kernel guided Sparse Graph Construction in Spectral Clustering for Unsupervised Speaker Diarization", "comment": "5 pages", "summary": "Speaker diarization aims to segment audio recordings into regions corresponding to individual speakers. Although unsupervised speaker diarization is inherently challenging, the prospect of identifying speaker regions without pretraining or weak supervision motivates research on clustering techniques. In this work, we share the notable observation that measuring multiple kernel similarities of speaker embeddings to thereafter craft a sparse graph for spectral clustering in a principled manner is sufficient to achieve state-of-the-art performances in a fully unsupervised setting. Specifically, we consider four polynomial kernels and a degree one arccosine kernel to measure similarities in speaker embeddings, using which sparse graphs are constructed in a principled manner to emphasize local similarities. Experiments show the proposed approach excels in unsupervised speaker diarization over a variety of challenging environments in the DIHARD-III, AMI, and VoxConverse corpora. To encourage further research, our implementations are available at https://github.com/nikhilraghav29/MK-SGC-SC."}
{"id": "2601.20658", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.20658", "abs": "https://arxiv.org/abs/2601.20658", "authors": ["Qian Gao", "Ruikang Zhong", "Hyundong Shin", "Yuanwei Liu"], "title": "Integrated Sensing and Communication for Segmented Waveguide-Enabled Pinching Antenna Systems", "comment": null, "summary": "In this paper, an integrated sensing and communication (ISAC) design for segmented waveguide-enabled pinching-antenna array (SWAN) systems is proposed to improve the performance of systems by leveraging the low in-waveguide propagation loss of segmented waveguides. The hybrid segment selection and multiplexing (HSSM) protocol is implemented to provide favorable performance with less hardware cost. To achieve this, a joint transmit beamforming optimization, segment selection, and pinching antenna positioning problem is formulated to maximize the sum communication rate with the constraints of sensing performance. To solve the maximization problem, we propose a segment hysteresis based reinforcement learning (SHRL) algorithm to learn segment selection and pinching antenna positions in different progress to explore better strategies. Simulation results demonstrate that 1) the proposed SWAN-ISAC scheme outperforms the other baseline schemes, and 2) the proposed HARL algorithm achieves better performance compared to conventional RL algorithms."}
{"id": "2601.19949", "categories": ["eess.AS", "cs.CL", "cs.SD", "eess.SP"], "pdf": "https://arxiv.org/pdf/2601.19949", "abs": "https://arxiv.org/abs/2601.19949", "authors": ["Mandip Goswami"], "title": "RIR-Mega-Speech: A Reverberant Speech Corpus with Comprehensive Acoustic Metadata and Reproducible Evaluation", "comment": null, "summary": "Despite decades of research on reverberant speech, comparing methods remains difficult because most corpora lack per-file acoustic annotations or provide limited documentation for reproduction. We present RIR-Mega-Speech, a corpus of approximately 117.5 hours created by convolving LibriSpeech utterances with roughly 5,000 simulated room impulse responses from the RIR-Mega collection. Every file includes RT60, direct-to-reverberant ratio (DRR), and clarity index ($C_{50}$) computed from the source RIR using clearly defined, reproducible procedures. We also provide scripts to rebuild the dataset and reproduce all evaluation results.\n  Using Whisper small on 1,500 paired utterances, we measure 5.20% WER (95% CI: 4.69--5.78) on clean speech and 7.70% (7.04--8.35) on reverberant versions, corresponding to a paired increase of 2.50 percentage points (2.06--2.98). This represents a 48% relative degradation. WER increases monotonically with RT60 and decreases with DRR, consistent with prior perceptual studies. While the core finding that reverberation harms recognition is well established, we aim to provide the community with a standardized resource where acoustic conditions are transparent and results can be verified independently. The repository includes one-command rebuild instructions for both Windows and Linux environments."}
{"id": "2601.20667", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.20667", "abs": "https://arxiv.org/abs/2601.20667", "authors": ["Qian Gao", "Ruikang Zhong", "Yuanwei Liu"], "title": "Deep Learning based Three-stage Solution for ISAC Beamforming Optimization", "comment": null, "summary": "In this paper, a general ISAC system where the base station (BS) communicates with multiple users and performs target detection is considered. Then, a sum communication rate maximization problem is formulated, subjected to the constraints of transmit power and the minimum sensing rates of users. To solve this problem, we develop a framework that leverages deep learning algorithms to provide a three-stage solution for ISAC beamforming. The three-stage beamforming optimization solution includes three modules: 1) an unsupervised learning based feature extraction algorithm is proposed to extract fixed-size latent features while keeping its essential information from the variable channel state information (CSI); 2) a reinforcement learning (RL) based beampattern optimization algorithm is proposed to search the desired beampattern according to the extracted features; 3) a supervised learning based beamforming reconstruction algorithm is proposed to reconstruct the beamforming vector from beampattern given by the RL agent. Simulation results demonstrate that the proposed three-stage solution outperforms the baseline RL algorithm by optimizing the intuitional beampattern rather than beamforming."}
{"id": "2601.19956", "categories": ["eess.AS", "cs.AI", "cs.SD"], "pdf": "https://arxiv.org/pdf/2601.19956", "abs": "https://arxiv.org/abs/2601.19956", "authors": ["Yuxiang Wang", "Hongyu Liu", "Dekun Chen", "Xueyao Zhang", "Zhizheng Wu"], "title": "VoxPrivacy: A Benchmark for Evaluating Interactional Privacy of Speech Language Models", "comment": null, "summary": "As Speech Language Models (SLMs) transition from personal devices to shared, multi-user environments such as smart homes, a new challenge emerges: the model is expected to distinguish between users to manage information flow appropriately. Without this capability, an SLM could reveal one user's confidential schedule to another, a privacy failure we term interactional privacy. Thus, the ability to generate speaker-aware responses becomes essential for SLM safe deployment. Current SLM benchmarks test dialogue ability but overlook speaker identity. Multi-speaker benchmarks check who said what without assessing whether SLMs adapt their responses. Privacy benchmarks focus on globally sensitive data (e.g., bank passwords) while neglecting contextual privacy-sensitive information (e.g., a user's private appointment). To address this gap, we introduce VoxPrivacy, the first benchmark designed to evaluate interactional privacy in SLMs. VoxPrivacy spans three tiers of increasing difficulty, from following direct secrecy commands to proactively protecting privacy. Our evaluation of nine SLMs on a 32-hour bilingual dataset reveals a widespread vulnerability: most open-source models perform close to random chance (around 50% accuracy) on conditional privacy decisions, while even strong closed-source systems fall short on proactive privacy inference. We further validate these findings on Real-VoxPrivacy, a human-recorded subset, confirming that failures observed on synthetic data persist in real speech. Finally, we demonstrate a viable path forward: by fine-tuning on a new 4,000-hour training set, we improve privacy-preserving abilities while maintaining robustness. To support future work, we release the VoxPrivacy benchmark, the large-scale training set, and the fine-tuned model to foster the development of safer and more context-aware SLMs."}
{"id": "2601.20688", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.20688", "abs": "https://arxiv.org/abs/2601.20688", "authors": ["Ruining Fan", "Xingyu Huang", "Mouli Chakraborty", "Avishek Nag", "Anshu Mukherjee"], "title": "Grover's Search-Inspired Quantum Reinforcement Learning for Massive MIMO User Scheduling", "comment": null, "summary": "The efficient user scheduling policy in the massive Multiple Input Multiple Output (mMIMO) system remains a significant challenge in the field of 5G and Beyond 5G (B5G) due to its high computational complexity, scalability, and Channel State Information (CSI) overhead. This paper proposes a novel Grover's search-inspired Quantum Reinforcement Learning (QRL) framework for mMIMO user scheduling. The QRL agent can explore the exponentially large scheduling space effectively by applying Grover's search to the reinforcement learning process. The model is implemented using our designed quantum-gate-based circuit, which imitates the layered architecture of reinforcement learning, where quantum operations act as policy updates and decision-making units. Moreover, the simulation results demonstrate that the proposed method achieves proper convergence and significantly outperforms classical Convolutional Neural Networks (CNN) and Quantum Deep Learning (QDL) benchmarks."}
{"id": "2601.19960", "categories": ["eess.AS", "cs.AI", "cs.SD"], "pdf": "https://arxiv.org/pdf/2601.19960", "abs": "https://arxiv.org/abs/2601.19960", "authors": ["Youness Dkhissi", "Valentin Vielzeuf", "Elys Allesiardo", "Anthony Larcher"], "title": "Do we really need Self-Attention for Streaming Automatic Speech Recognition?", "comment": null, "summary": "Transformer-based architectures are the most used architectures in many deep learning fields like Natural Language Processing, Computer Vision or Speech processing. It may encourage the direct use of Transformers in the constrained tasks, without questioning whether it will yield the same benefits as in standard tasks.  Given specific constraints, it is essential to evaluate the relevance of transformer models. This work questions the suitability of transformers for specific domains. We argue that the high computational requirements and latency issues associated with these models do not align well with streaming applications. Our study promotes the search for alternative strategies to improve efficiency without sacrificing performance.  In light of this observation, our paper critically examines the usefulness of transformer architecture in such constrained environments. As a first attempt, we show that the computational cost for Streaming Automatic Speech Recognition (ASR) can be reduced using deformable convolution instead of Self-Attention. Furthermore, we show that Self-Attention mechanisms can be entirely removed and not replaced, without observing significant degradation in the Word Error Rate."}
{"id": "2601.20721", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.20721", "abs": "https://arxiv.org/abs/2601.20721", "authors": ["Vida Ranjbar", "Robbert Beerten", "Marc Moonen", "Sofie Pollin"], "title": "Sequential Processing Strategies in Fronthaul Constrained Cell-Free Massive MIMO Networks", "comment": "Accepted to be published in IEEE Wireless Communications Letters", "summary": "In a cell-free massive MIMO (CFmMIMO) network with a daisy-chain fronthaul, the amount of information that each access point (AP) needs to communicate with the next AP in the chain is determined by the location of the AP in the sequential fronthaul. Therefore, we propose two sequential processing strategies to combat the adverse effect of fronthaul compression on the sum of users' spectral efficiency (SE): 1) linearly increasing fronthaul capacity allocation among APs and 2) Two-Path users' signal estimation. The two strategies show superior performance in terms of sum SE compared to the equal fronthaul capacity allocation and Single-Path sequential signal estimation."}
{"id": "2601.20481", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2601.20481", "abs": "https://arxiv.org/abs/2601.20481", "authors": ["Myungjin Lee", "Eunji Shin", "Jiyoung Lee"], "title": "Erasing Your Voice Before It's Heard: Training-free Speaker Unlearning for Zero-shot Text-to-Speech", "comment": "ICASSP'2026", "summary": "Modern zero-shot text-to-speech (TTS) models offer unprecedented expressivity but also pose serious crime risks, as they can synthesize voices of individuals who never consented. In this context, speaker unlearning aims to prevent the generation of specific speaker identities upon request. Existing approaches, reliant on retraining, are costly and limited to speakers seen in the training set. We present TruS, a training-free speaker unlearning framework that shifts the paradigm from data deletion to inference-time control. TruS steers identity-specific hidden activations to suppress target speakers while preserving other attributes (e.g., prosody and emotion). Experimental results show that TruS effectively prevents voice generation on both seen and unseen opt-out speakers, establishing a scalable safeguard for speech synthesis. The demo and code are available on http://mmai.ewha.ac.kr/trus."}
{"id": "2601.20780", "categories": ["eess.SP", "eess.SY"], "pdf": "https://arxiv.org/pdf/2601.20780", "abs": "https://arxiv.org/abs/2601.20780", "authors": ["Xiaoxia Xu", "Xidong Mu", "Yuanwei Liu", "Arumugam Nallanathan"], "title": "Multi-Mode Pinching Antenna Systems Enabled Multi-User Communications", "comment": "13 pages, 10 figures. Submitted to IEEE", "summary": "This paper proposes a novel multi-mode pinching-antenna systems (PASS) framework. Multiple data streams can be transmitted within a single waveguide through multiple guided modes, thus facilitating efficient multi-user communications through the mode-domain multiplexing. A physic model is derived, which reveals the mode-selective power radiation feature of pinching antennas (PAs). A two-mode PASS enabled two-user downlink communication system is investigated. Considering the mode selectivity of PA power radiation, a practical PA grouping scheme is proposed, where each PA group matches with one specific guided mode and mainly radiates its signal sequentially. Depending on whether the guided mode leaks power to unmatched PAs or not, the proposed PA grouping scheme operates in either the non-leakage or weak-leakage regime. Based on this, the baseband beamforming and PA locations are jointly optimized for sum rate maximization, subject to each user's minimum rate requirement. 1) A simple two-PA case in non-leakage regime is first considered. To solve the formulated problem, a channel orthogonality based solution is proposed. The channel orthogonality is ensured by large-scale and wavelength-scale equality constraints on PA locations. Thus, the optimal beamforming reduces to maximum-ratio transmission (MRT). Moreover, the optimal PA locations are obtained via a Newton-based one-dimension search algorithm that enforces two-scale PA-location constraints by Newton's method. 2) A general multi-PA case in both non-leakage and weak-leakage regimes is further considered. A low-complexity particle-swarm optimization with zero-forcing beamforming (PSO-ZF) algorithm is developed, thus effectively tackling the high-oscillatory and strong-coupled problem. Simulation results demonstrate the superiority of the proposed multi-mode PASS over conventional single-mode PASS and fixed-antenna structures."}
{"id": "2601.20795", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.20795", "abs": "https://arxiv.org/abs/2601.20795", "authors": ["Ivan Iudice", "Giacinto Gelli", "Donatella Darsena"], "title": "AI-Driven Design of Stacked Intelligent Metasurfaces for Software-Defined Radio Applications", "comment": "8 pages, 3 figures, accepted for publication in the proceedings of 2026 IEEE Aerospace Conference", "summary": "The integration of reconfigurable intelligent surfaces (RIS) into future wireless communication systems offers promising capabilities in dynamic environment shaping and spectrum efficiency. In this work, we present a consistent implementation of a stacked intelligent metasurface (SIM) model within the NVIDIA's AI-native framework Sionna for 6G physical layer research. Our implementation allows simulation and learning-based optimization of SIM-assisted communication channels in fully differentiable and GPU-accelerated environments, enabling end-to-end training for cognitive and software-defined radio (SDR) applications. We describe the architecture of the SIM model, including its integration into the TensorFlow-based pipeline, and showcase its use in closed-loop learning scenarios involving adaptive beamforming and dynamic reconfiguration. Benchmarking results are provided for various deployment scenarios, highlighting the model's effectiveness in enabling intelligent control and signal enhancement in non-terrestrial-network (NTN) propagation environments. This work demonstrates a scalable, modular approach for incorporating intelligent metasurfaces into modern AI-accelerated SDR systems and paves the way for future hardware-in-the-loop experiments."}
{"id": "2601.20817", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.20817", "abs": "https://arxiv.org/abs/2601.20817", "authors": ["Mats Viberg", "Daniele Gerosa", "Tomas McKelvey", "Thomas Eriksson"], "title": "Statistical Properties of Target Localization Using Passive Radar Systems", "comment": null, "summary": "Passive Radar Systems have received tremendous attention during the past few decades, due to their low cost and ability to remain covert during operation. Such systems do not transmit any energy themselves, but rely on a so-called Illuminator-of-Opportunity (IO), for example a commercial TV station. A network of Receiving Nodes (RN) receive the direct signal as well as reflections from possible targets. The RNs transmit information to a Central Node (CN), that performs the final target detection, localization and tracking. A large number of methods and algorithms for target detection and localization have been proposed in the literature. In the present contribution, the focus is on the seminal Extended Cancelation Algorithm (ECA), in which each RN estimates target parameters after canceling interference from the direct-path as well as clutter from unwanted stationary objects. This is done by exploiting a separate Reference Channel (RC), which captures the IO signal without interference apart from receiver noise. We derive the statistical properties of the ECA parameter estimates under the assumption of a high Signal-to-Noise Ratio (SNR), and we give a sufficient condition for the SNR in the RC to enable statistically efficient estimates. The theoretical results are corroborated through computer simulations, which show that the theory agrees well with empirical results above a certain SNR threshold. The results can be used to predict the performance of passive radar systems in given scenarios, which is useful for feasibility studies as well as system design."}
{"id": "2601.19949", "categories": ["eess.AS", "cs.CL", "cs.SD", "eess.SP"], "pdf": "https://arxiv.org/pdf/2601.19949", "abs": "https://arxiv.org/abs/2601.19949", "authors": ["Mandip Goswami"], "title": "RIR-Mega-Speech: A Reverberant Speech Corpus with Comprehensive Acoustic Metadata and Reproducible Evaluation", "comment": null, "summary": "Despite decades of research on reverberant speech, comparing methods remains difficult because most corpora lack per-file acoustic annotations or provide limited documentation for reproduction. We present RIR-Mega-Speech, a corpus of approximately 117.5 hours created by convolving LibriSpeech utterances with roughly 5,000 simulated room impulse responses from the RIR-Mega collection. Every file includes RT60, direct-to-reverberant ratio (DRR), and clarity index ($C_{50}$) computed from the source RIR using clearly defined, reproducible procedures. We also provide scripts to rebuild the dataset and reproduce all evaluation results.\n  Using Whisper small on 1,500 paired utterances, we measure 5.20% WER (95% CI: 4.69--5.78) on clean speech and 7.70% (7.04--8.35) on reverberant versions, corresponding to a paired increase of 2.50 percentage points (2.06--2.98). This represents a 48% relative degradation. WER increases monotonically with RT60 and decreases with DRR, consistent with prior perceptual studies. While the core finding that reverberation harms recognition is well established, we aim to provide the community with a standardized resource where acoustic conditions are transparent and results can be verified independently. The repository includes one-command rebuild instructions for both Windows and Linux environments."}

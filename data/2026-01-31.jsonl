{"id": "2601.21009", "categories": ["eess.SP", "cs.IT"], "pdf": "https://arxiv.org/pdf/2601.21009", "abs": "https://arxiv.org/abs/2601.21009", "authors": ["Joe Asano", "Yuto Hama", "Hiroki Iimori", "Chandan Pradhan", "Szabolcs Malomsoky", "Naoki Ishikawa"], "title": "Sparse Grassmannian Design for Noncoherent Codes via Schubert Cell Decomposition", "comment": "13 pages, 8 figures", "summary": "In this paper, we propose a method for designing sparse Grassmannian codes for noncoherent multiple-input multiple-output systems. Conventional pairwise error probability formulations under uncorrelated Rayleigh fading channels fail to account for rank deficiency induced by sparse configurations. We revise these formulations to handle such cases in a unified manner. Furthermore, we derive a closed-form metric that effectively maximizes the noncoherent average mutual information (AMI) at a given signal-to-noise ratio. We focus on the fact that the Schubert cell decomposition of the Grassmann manifold provides a mathematically sparse property, and establish design criteria for sparse noncoherent codes based on our analyses. In numerical results, the proposed sparse noncoherent codes outperform conventional methods in terms of both symbol error rate and AMI, and asymptotically approach the performance of the optimal Grassmannian constellations in the high-signal-to-noise ratio regime. Moreover, they reduce the time and space complexity, which does not scale with the number of transmit antennas."}
{"id": "2601.21303", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.21303", "abs": "https://arxiv.org/abs/2601.21303", "authors": ["Zhifeng Tang", "Nan Yang", "Xiangyun Zhou", "Salman Durrani", "Markku Juntti", "Josep Miquel Jornet"], "title": "Impact of Pointing Error on Coverage Performance of 3D Indoor Terahertz Communication Systems", "comment": "Published in IEEE Globecom 2025", "summary": "In this paper, we develop a tractable analytical framework for a three-dimensional (3D) indoor terahertz (THz) communication system to theoretically assess the impact of the pointing error on its coverage performance. Specifically, we model the locations of access points (APs) using a Poisson point process, human blockages as random cylinder processes, and wall blockages through a Boolean straight line process. A pointing error refers to beamforming gain and direction mismatch between the transmitter and receiver. We characterize it based on the inaccuracy of location estimate. We then analyze the impact of this pointing error on the received signal power and derive a tractable expression for the coverage probability, incorporating the multi-cluster fluctuating two-ray distribution to accurately model small-scale fading in THz communications. Aided by simulation results, we corroborate our analysis and demonstrate that the pointing error has a pronounced impact on the coverage probability. Specifically, we find that merely increasing the antenna array size is insufficient to improve the coverage probability and mitigate the detrimental impact of the pointing error, highlighting the necessity of advanced estimation techniques in THz communication systems."}
{"id": "2601.21308", "categories": ["eess.SP", "eess.SY"], "pdf": "https://arxiv.org/pdf/2601.21308", "abs": "https://arxiv.org/abs/2601.21308", "authors": ["Richard Zeng", "Anthony Chan Carusone", "Xilin Liu"], "title": "A Time-Domain Dual-Edge Asynchronous Pipelined SAR ADC Featuring Reset-Free Quantization at Multi-GS/s", "comment": "12 pages, 16 figures. This work has been submitted to IEEE", "summary": "Time-domain ADCs are attractive for high-speed wireline receivers, as time resolution scales favorably with advanced CMOS technologies, enabling multi-GS/s single-channel sampling rates. However, conventional time-domain ADCs require explicit reset of voltage-to-time and time-domain signal paths between samples, introducing dead time that fundamentally limits resolution, speed, and energy efficiency. This paper introduces a dual-edge reset-free quantization concept for asynchronous pipelined SAR time-domain ADCs, in which both rising and falling signal edges are exploited to enable reset-free quantization within a single conversion period. By eliminating explicit reset phases, the proposed approach expands the effective conversion window and relaxes the resolution-speed tradeoff at high sampling rates. An 8-bit dual-edge asynchronous pipelined SAR time-domain ADC is implemented in 22-nm FD-SOI, incorporating a linearity-compensated dual-edge voltage-to-time converter and a dual-edge time-to-digital converter with independently tunable rising- and falling-edge delays. The prototype occupies a core area of 0.0089 mm^2 and achieves continuous single-channel operation at 3.5 GS/s, with architectural scalability demonstrated through intermittent operation at 10.5 GS/s and higher. At 3.5 GS/s, the ADC achieves 21.6 dB SNDR and 32.2 dB SFDR. The measured performance is primarily limited by identifiable implementation-level factors rather than by architectural constraints, demonstrating the feasibility of dual-edge reset-free quantization for high-speed time-domain ADCs."}
{"id": "2601.21397", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.21397", "abs": "https://arxiv.org/abs/2601.21397", "authors": ["Jian Liu", "Wei Zhao", "Jianting Zhao", "Shisong Li"], "title": "A Linearization of DFT Spectrum for Precision Power Measurement in Presence of Interharmonics", "comment": "15 pages, 12 figures", "summary": "The presence of interharmonics in power systems can lead to asynchronous sampling, a phenomenon further aggravated by shifts in the fundamental frequency, which significantly degrades the accuracy of power measurements. Under such asynchronous conditions, interharmonics lose orthogonality with the fundamental and harmonic components, giving rise to additional power components. To address these challenges, this paper introduces a linearization algorithm based on DFT spectrum analysis for precise power measurement in systems containing interharmonics. The proposed approach constructs a system of linear equations from the DFT spectrum and solves it through efficient matrix operations, enabling accurate extraction of interharmonic components near the fundamental and harmonic frequencies (with a frequency interval $\\geq$1 Hz). This allows for precise measurement of power across the fundamental, harmonic, interharmonic, and cross-power bands, as well as total power. Test results demonstrate that the proposed method accurately computes various power components under diverse conditions--including varying interharmonic/fundamental/harmonic intervals, fundamental frequency deviations, and noise. Compared to existing methods such as fast Fourier transform (FFT), Windowed interpolation FFT, and Matrix pencil-Singular value decomposition, the proposed technique reduces estimation error by several times to multiple folds and exhibits improved robustness, while maintaining a computational time of only 7 ms for processing 10-power-line-cycle (200 ms) data."}
{"id": "2601.20898", "categories": ["eess.AS", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.20898", "abs": "https://arxiv.org/abs/2601.20898", "authors": ["Sergio Burdisso", "Esaú Villatoro-Tello", "Shashi Kumar", "Srikanth Madikeri", "Andrés Carofilis", "Pradeep Rangappa", "Manjunath K E", "Kadri Hacioglu", "Petr Motlicek", "Andreas Stolcke"], "title": "Reducing Prompt Sensitivity in LLM-based Speech Recognition Through Learnable Projection", "comment": "Paper accepted at ICASSP 2026", "summary": "LLM-based automatic speech recognition (ASR), a well-established approach, connects speech foundation models to large language models (LLMs) through a speech-to-LLM projector, yielding promising results. A common design choice in these architectures is the use of a fixed, manually defined prompt during both training and inference. This setup not only enables applicability across a range of practical scenarios, but also helps maximize model performance. However, the impact of prompt design remains underexplored. This paper presents a comprehensive analysis of commonly used prompts across diverse datasets, showing that prompt choice significantly affects ASR performance and introduces instability, with no single prompt performing best across all cases. Inspired by the speech-to-LLM projector, we propose a prompt projector module, a simple, model-agnostic extension that learns to project prompt embeddings to more effective regions of the LLM input space, without modifying the underlying LLM-based ASR model. Experiments on four datasets show that the addition of a prompt projector consistently improves performance, reduces variability, and outperforms the best manually selected prompts."}
{"id": "2601.20867", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.20867", "abs": "https://arxiv.org/abs/2601.20867", "authors": ["Jaehyuk Jang", "Wonjun Lee", "Kangwook Ko", "Changick Kim"], "title": "Generalizable Prompt Tuning for Audio-Language Models via Semantic Expansion", "comment": null, "summary": "Prompt tuning has achieved remarkable progress in vision-language models (VLMs) and is recently being adopted for audio-language models (ALMs). However, its generalization ability in ALMs remains largely underexplored. We observe that conventional prompt tuning for ALMs also suffers from the Base-New Tradeoff, and we identify that this issue stems from the disrupted semantic structure of the embedding space. To address this issue, we propose Semantically Expanded Prompt Tuning (SEPT)-a plug-and-play framework that explicitly regularizes the prompt embedding space by incorporating semantic neighbors generated by large language models. SEPT introduces a novel semantic expansion loss with margin constraints that promote intra-class compactness and inter-class separability, thereby enhancing the semantic structure of the prompt embedding space. For comprehensive evaluation, we establish the first benchmark setup for prompt generalization in ALMs, covering both base-to-new generalization and cross-dataset transferability. Extensive experiments demonstrate that SEPT consistently improves generalization performance across multiple prompt tuning baselines, while maintaining computational cost during inference. Codes are available in https://github.com/jhyukjang/SEPT."}
{"id": "2601.21429", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.21429", "abs": "https://arxiv.org/abs/2601.21429", "authors": ["Laurits Randers", "Martin Voigt Vejling", "Petar Popovski"], "title": "Interference Detection and Exploitation for Multi-User Radar Sensing", "comment": null, "summary": "Integrated sensing and communication is a key feature in next-generation wireless networks, enabling joint data transmission and environmental radar sensing on shared spectrum. In multi-user scenarios, simultaneous transmissions cause mutual interference on overlapping frequencies, leading to spurious target detections and degraded sensing accuracy. This paper proposes an interference detection and exploitation algorithm for sensing using spectrally interleaved orthogonal frequency division multiplexing. A statistically rigorous procedure is introduced to detect interference while controlling the familywise error rate. We propose an algorithm that estimates the angle by exploiting interference, while estimating the delay by avoiding the interference. Numerical experiments demonstrate that the proposed method reliably detects interference, and that the delay and angle estimation error approaches the Cramér-Rao lower bound."}
{"id": "2601.21110", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2601.21110", "abs": "https://arxiv.org/abs/2601.21110", "authors": ["Jaden Pieper", "Stephen D. Voran"], "title": "Unseen but not Unknown: Using Dataset Concealment to Robustly Evaluate Speech Quality Estimation Models", "comment": "To be appear in Proc. ICASSP 2026", "summary": "We introduce Dataset Concealment (DSC), a rigorous new procedure for evaluating and interpreting objective speech quality estimation models. DSC quantifies and decomposes the performance gap between research results and real-world application requirements, while offering context and additional insights into model behavior and dataset characteristics. We also show the benefits of addressing the corpus effect by using the dataset Aligner from AlignNet when training models with multiple datasets. We demonstrate DSC and the improvements from the Aligner using nine training datasets and nine unseen datasets with three well-studied models: MOSNet, NISQA, and a Wav2Vec2.0-based model. DSC provides interpretable views of the generalization capabilities and limitations of models, while allowing all available data to be used at training. An additional result is that adding the 1000 parameter dataset Aligner to the 94 million parameter Wav2Vec model during training does significantly improve the resulting model's ability to estimate speech quality for unseen data."}
{"id": "2601.20883", "categories": ["cs.SD", "cs.CR", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.20883", "abs": "https://arxiv.org/abs/2601.20883", "authors": ["Bharath Krishnamurthy", "Ajita Rattani"], "title": "VoxMorph: Scalable Zero-shot Voice Identity Morphing via Disentangled Embeddings", "comment": "Accepted to IEEE ICASSP 2026 (51st International Conference on Acoustics, Speech, and Signal Processing, ICASSP 2026). 5 pages, 1 figure, 3 tables. Project page: https://vcbsl.github.io/VoxMorph/", "summary": "Morphing techniques generate artificial biometric samples that combine features from multiple individuals, allowing each contributor to be verified against a single enrolled template. While extensively studied in face recognition, this vulnerability remains largely unexplored in voice biometrics. Prior work on voice morphing is computationally expensive, non-scalable, and limited to acoustically similar identity pairs, constraining practical deployment. Moreover, existing sound-morphing methods target audio textures, music, or environmental sounds and are not transferable to voice identity manipulation. We propose VoxMorph, a zero-shot framework that produces high-fidelity voice morphs from as little as five seconds of audio per subject without model retraining. Our method disentangles vocal traits into prosody and timbre embeddings, enabling fine-grained interpolation of speaking style and identity. These embeddings are fused via Spherical Linear Interpolation (Slerp) and synthesized using an autoregressive language model coupled with a Conditional Flow Matching network. VoxMorph achieves state-of-the-art performance, delivering a 2.6x gain in audio quality, a 73% reduction in intelligibility errors, and a 67.8% morphing attack success rate on automated speaker verification systems under strict security thresholds. This work establishes a practical and scalable paradigm for voice morphing with significant implications for biometric security. The code and dataset are available on our project page: https://vcbsl.github.io/VoxMorph/"}
{"id": "2601.21481", "categories": ["eess.SP", "cs.ET"], "pdf": "https://arxiv.org/pdf/2601.21481", "abs": "https://arxiv.org/abs/2601.21481", "authors": ["Sai Pavan Deram", "Jacopo Pegoraro", "Javier Lorca Hernando", "Jesus O. Lacruz", "Joerg Widmer"], "title": "Compressed Sensing-Driven Near-Field Localization Exploiting Array of Subarrays", "comment": "Accepted in IEEE International Conference on Communications 2026 for Signal Processing for Communications Track", "summary": "Near-field localization for ISAC requires large-aperture arrays, making fully-digital implementations prohibitively complex and costly. While sparse subarray architectures can reduce cost, they introduce severe estimation ambiguity from grating lobes. To address both issues, we propose SHARE (Sparse Hierarchical Angle-Range Estimation), a novel two-stage sparse recovery algorithm. SHARE operates in two stages. It first performs coarse, unambiguous angle estimation using individual subarrays to resolve the grating lobe ambiguity. It then leverages the full sparse aperture to perform a localized joint angle-range search. This hierarchical approach avoids an exhaustive and computationally intensive two-dimensional grid search while preserving the high resolution of the large aperture. Simulation results show that SHARE significantly outperforms conventional one-shot sparse recovery methods, such as Orthogonal Matching Pursuit (OMP), in both localization accuracy and robustness. Furthermore, we show that SHARE's overall localization accuracy is comparable to or even surpasses that of the fully-digital 2D-MUSIC algorithm, despite MUSIC having access to the complete, uncompressed data from every antenna element. SHARE therefore provides a practical path for high-resolution near-field ISAC systems."}
{"id": "2601.21114", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2601.21114", "abs": "https://arxiv.org/abs/2601.21114", "authors": ["Henri Gode", "Simon Doclo"], "title": "DNN-Based Online Source Counting Based on Spatial Generalized Magnitude Squared Coherence", "comment": "in Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) 2026, Barcelona, Spain", "summary": "The number of active sound sources is a key parameter in many acoustic signal processing tasks, such as source localization, source separation, and multi-microphone speech enhancement. This paper proposes a novel method for online source counting by detecting changes in the number of active sources based on spatial coherence. The proposed method exploits the fact that a single coherent source in spatially white background noise yields high spatial coherence, whereas only noise results in low spatial coherence. By applying a spatial whitening operation, the source counting problem is reformulated as a change detection task, aiming to identify the time frames when the number of active sources changes. The method leverages the generalized magnitude-squared coherence as a measure to quantify spatial coherence, providing features for a compact neural network trained to detect source count changes framewise. Simulation results with binaural hearing aids in reverberant acoustic scenes with up to 4 speakers and background noise demonstrate the effectiveness of the proposed method for online source counting."}
{"id": "2601.20890", "categories": ["cs.SD", "cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.20890", "abs": "https://arxiv.org/abs/2601.20890", "authors": ["Manali Sharma", "Riya Naik", "Buvaneshwari G"], "title": "SW-ASR: A Context-Aware Hybrid ASR Pipeline for Robust Single Word Speech Recognition", "comment": null, "summary": "Single-word Automatic Speech Recognition (ASR) is a challenging task due to the lack of linguistic context and sensitivity to noise, pronunciation variation, and channel artifacts, especially in low-resource, communication-critical domains such as healthcare and emergency response. This paper reviews recent deep learning approaches and proposes a modular framework for robust single-word detection. The system combines denoising and normalization with a hybrid ASR front end (Whisper + Vosk) and a verification layer designed to handle out-of-vocabulary words and degraded audio. The verification layer supports multiple matching strategies, including embedding similarity, edit distance, and LLM-based matching with optional contextual guidance. We evaluate the framework on the Google Speech Commands dataset and a curated real-world dataset collected from telephony and messaging platforms under bandwidth-limited conditions. Results show that while the hybrid ASR front end performs well on clean audio, the verification layer significantly improves accuracy on noisy and compressed channels. Context-guided and LLM-based matching yield the largest gains, demonstrating that lightweight verification and context mechanisms can substantially improve single-word ASR robustness without sacrificing latency required for real-time telephony applications."}
{"id": "2601.21524", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.21524", "abs": "https://arxiv.org/abs/2601.21524", "authors": ["Yuan Gao", "Xinyi Wu", "Jiang Jun", "Zitian Zhang", "Zhaohui Yang", "Shugong Xu", "Cheng-Xiang Wang", "Zhu Han"], "title": "Channel Extrapolation for MIMO Systems with the Assistance of Multi-path Information Induced from Channel State Information", "comment": null, "summary": "Acquiring channel state information (CSI) through traditional methods, such as channel estimation, is increasingly challenging for the emerging sixth generation (6G) mobile networks due to high overhead. To address this issue, channel extrapolation techniques have been proposed to acquire complete CSI from a limited number of known CSIs. To improve extrapolation accuracy, environmental information, such as visual images or radar data, has been utilized, which poses challenges including additional hardware, privacy and multi-modal alignment concerns. To this end, this paper proposes a novel channel extrapolation framework by leveraging environment-related multi-path characteristics induced directly from CSI without integrating additional modalities. Specifically, we propose utilizing the multi-path characteristics in the form of power-delay profile (PDP), which is acquired using a CSI-to-PDP module. CSI-to-PDP module is trained in an AE-based framework by reconstructing the PDPs and constraining the latent low-dimensional features to represent the CSI. We further extract the total power & power-weighted delay of all the identified paths in PDP as the multi-path information. Building on this, we proposed a MAE architecture trained in a self-supervised manner to perform channel extrapolation. Unlike standard MAE approaches, our method employs separate encoders to extract features from the masked CSI and the multi-path information, which are then fused by a cross-attention module. Extensive simulations demonstrate that this framework improves extrapolation performance dramatically, with a minor increase in inference time (around 0.1 ms). Furthermore, our model shows strong generalization capabilities, particularly when only a small portion of the CSI is known, outperforming existing benchmarks."}
{"id": "2601.21347", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2601.21347", "abs": "https://arxiv.org/abs/2601.21347", "authors": ["Xiuwen Zheng", "Sixun Dong", "Bornali Phukon", "Mark Hasegawa-Johnson", "Chang D. Yoo"], "title": "Towards Robust Dysarthric Speech Recognition: LLM-Agent Post-ASR Correction Beyond WER", "comment": "Accepted to ICASSP 2026", "summary": "While Automatic Speech Recognition (ASR) is typically benchmarked by word error rate (WER), real-world applications ultimately hinge on semantic fidelity. This mismatch is particularly problematic for dysarthric speech, where articulatory imprecision and disfluencies can cause severe semantic distortions. To bridge this gap, we introduce a Large Language Model (LLM)-based agent for post-ASR correction: a Judge-Editor over the top-k ASR hypotheses that keeps high-confidence spans, rewrites uncertain segments, and operates in both zero-shot and fine-tuned modes. In parallel, we release SAP-Hypo5, the largest benchmark for dysarthric speech correction, to enable reproducibility and future exploration. Under multi-perspective evaluation, our agent achieves a 14.51% WER reduction alongside substantial semantic gains, including a +7.59 pp improvement in MENLI and +7.66 pp in Slot Micro F1 on challenging samples. Our analysis further reveals that WER is highly sensitive to domain shift, whereas semantic metrics correlate more closely with downstream task performance."}
{"id": "2601.20896", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.20896", "abs": "https://arxiv.org/abs/2601.20896", "authors": ["Ryan Whetten", "Titouan Parcollet", "Marco Dinarelli", "Yannick Estève"], "title": "A Study of Data Selection Strategies for Pre-training Self-Supervised Speech Models", "comment": "Accepted for publication in the 2026 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP 2026)", "summary": "Self-supervised learning (SSL) has transformed speech processing, yet its reliance on massive pre-training datasets remains a bottleneck. While robustness is often attributed to scale and diversity, the role of the data distribution is less understood. We systematically examine how curated subsets of pre-training data influence Automatic Speech Recognition (ASR) performance. Surprisingly, optimizing for acoustic, speaker, or linguistic diversity yields no clear improvements over random sampling. Instead, we find that prioritizing the longest utterances achieves superior ASR results while using only half the original dataset, reducing pre-training time by 24% on a large corpora. These findings suggest that for pre-training speech SSL models, data length is a more critical factor than either data diversity or overall data quantity for performance and efficiency, offering a new perspective for data selection strategies in SSL speech processing."}
{"id": "2601.21550", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.21550", "abs": "https://arxiv.org/abs/2601.21550", "authors": ["Yuan Gao", "Xinyu Guo", "Han Li", "Jianbo Du", "Shugong Xu"], "title": "Near-Field Positioning for XL-MIMO Uniform Circular Arrays: An Attention-Enhanced Deep Learning Approach", "comment": null, "summary": "In the evolving landscape of sixth-generation (6G) mobile communication, multiple-input multiple-output (MIMO) systems are incorporating an unprecedented number of antenna elements, advancing towards Extremely large-scale multiple-input-multiple-output (XL-MIMO) systems. This enhancement significantly increases the spatial degrees of freedom, offering substantial benefits for wireless positioning. However, the expansion of the near-field range in XL-MIMO challenges the traditional far-field assumptions used in previous MIMO models. Among various configurations, uniform circular arrays (UCAs) demonstrate superior performance by maintaining constant angular resolution, unlike linear planar arrays. Addressing how to leverage the expanded aperture and harness the near-field effects in XL-MIMO systems remains an area requiring further investigation. In this paper, we introduce an attention-enhanced deep learning approach for precise positioning. We employ a dual-path channel attention mechanism and a spatial attention mechanism to effectively integrate channel-level and spatial-level features. Our comprehensive simulations show that this model surpasses existing benchmarks such as attention-based positioning networks (ABPN), near-field positioning networks (NFLnet), convolutional neural networks (CNN), and multilayer perceptrons (MLP). The proposed model achieves superior positioning accuracy by utilizing covariance metrics of the input signal. Also, simulation results reveal that covariance metric is advantageous for positioning over channel state information (CSI) in terms of positioning accuracy and model efficiency."}
{"id": "2601.21402", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2601.21402", "abs": "https://arxiv.org/abs/2601.21402", "authors": ["Zheqi Dai", "Guangyan Zhang", "Haolin He", "Xiquan Li", "Jingyu Li", "Chunyat Wu", "Yiwen Guo", "Qiuqiang Kong"], "title": "SemanticAudio: Audio Generation and Editing in Semantic Space", "comment": null, "summary": "In recent years, Text-to-Audio Generation has achieved remarkable progress, offering sound creators powerful tools to transform textual inspirations into vivid audio. However, existing models predominantly operate directly in the acoustic latent space of a Variational Autoencoder (VAE), often leading to suboptimal alignment between generated audio and textual descriptions. In this paper, we introduce SemanticAudio, a novel framework that conducts both audio generation and editing directly in a high-level semantic space. We define this semantic space as a compact representation capturing the global identity and temporal sequence of sound events, distinct from fine-grained acoustic details. SemanticAudio employs a two-stage Flow Matching architecture: the Semantic Planner first generates these compact semantic features to sketch the global semantic layout, and the Acoustic Synthesizer subsequently produces high-fidelity acoustic latents conditioned on this semantic plan. Leveraging this decoupled design, we further introduce a training-free text-guided editing mechanism that enables precise attribute-level modifications on general audio without retraining. Specifically, this is achieved by steering the semantic generation trajectory via the difference of velocity fields derived from source and target text prompts. Extensive experiments demonstrate that SemanticAudio surpasses existing mainstream approaches in semantic alignment. Demo available at: https://semanticaudio1.github.io/"}
{"id": "2601.20900", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.20900", "abs": "https://arxiv.org/abs/2601.20900", "authors": ["Sergio Burdisso", "Esaú Villatoro-Tello", "Andrés Carofilis", "Shashi Kumar", "Kadri Hacioglu", "Srikanth Madikeri", "Pradeep Rangappa", "Manjunath K E", "Petr Motlicek", "Shankar Venkatesan", "Andreas Stolcke"], "title": "Text-only adaptation in LLM-based ASR through text denoising", "comment": "Paper accepted at ICASSP 2026", "summary": "Adapting automatic speech recognition (ASR) systems based on large language models (LLMs) to new domains using text-only data is a significant yet underexplored challenge. Standard fine-tuning of the LLM on target-domain text often disrupts the critical alignment between speech and text modalities learned by the projector, degrading performance. We introduce a novel text-only adaptation method that emulates the audio projection task by treating it as a text denoising task. Our approach thus trains the LLM to recover clean transcripts from noisy inputs. This process effectively adapts the model to a target domain while preserving cross-modal alignment. Our solution is lightweight, requiring no architectural changes or additional parameters. Extensive evaluation on two datasets demonstrates up to 22.1% relative improvement, outperforming recent state-of-the-art text-only adaptation methods."}
{"id": "2601.21887", "categories": ["eess.SP", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.21887", "abs": "https://arxiv.org/abs/2601.21887", "authors": ["Gustav Norén", "Anubhab Ghosh", "Fredrik Cumlin", "Saikat Chatterjee"], "title": "VSE: Variational state estimation of complex model-free process", "comment": "The article is accepted at ICASSP 2026", "summary": "We design a variational state estimation (VSE) method that provides a closed-form Gaussian posterior of an underlying complex dynamical process from (noisy) nonlinear measurements. The complex process is model-free. That is, we do not have a suitable physics-based model characterizing the temporal evolution of the process state. The closed-form Gaussian posterior is provided by a recurrent neural network (RNN). The use of RNN is computationally simple in the inference phase. For learning the RNN, an additional RNN is used in the learning phase. Both RNNs help each other learn better based on variational inference principles. The VSE is demonstrated for a tracking application - state estimation of a stochastic Lorenz system (a benchmark process) using a 2-D camera measurement model. The VSE is shown to be competitive against a particle filter that knows the Lorenz system model and a recently proposed data-driven state estimation method that does not know the Lorenz system model."}
{"id": "2601.21612", "categories": ["eess.AS", "cs.AI", "cs.SD"], "pdf": "https://arxiv.org/pdf/2601.21612", "abs": "https://arxiv.org/abs/2601.21612", "authors": ["Bing Han", "Chushu Zhou", "Yifan Yang", "Wei Wang", "Chenda Li", "Wangyou Zhang", "Yanmin Qian"], "title": "Representation-Regularized Convolutional Audio Transformer for Audio Understanding", "comment": "12 pages, 3 figures", "summary": "Bootstrap-based Self-Supervised Learning (SSL) has achieved remarkable progress in audio understanding. However, existing methods typically operate at a single level of granularity, limiting their ability to model the diverse temporal and spectral structures inherent in complex audio signals. Furthermore, bootstrapping representations from scratch is computationally expensive, often requiring extensive training to converge. In this work, we propose the Convolutional Audio Transformer (CAT), a unified framework designed to address these challenges. First, to capture hierarchical audio features, CAT incorporates a Multi-resolution Block that aggregates information across varying granularities. Second, to enhance training efficiency, we introduce a Representation Regularization objective. Drawing inspiration from generative modeling, this auxiliary task guides the student model by aligning its predictions with high-quality semantic representations from frozen, pre-trained external encoders. Experimental results demonstrate that CAT significantly outperforms baselines on audio understanding benchmarks. Notably, it achieves competitive performance on the AudioSet 20k dataset with 5 times faster convergence than existing methods. Codes and checkpoints will be released soon at https://github.com/realzhouchushu/CAT."}
{"id": "2601.21124", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.21124", "abs": "https://arxiv.org/abs/2601.21124", "authors": ["Artem Dementyev", "Wazeer Zulfikar", "Sinan Hersek", "Pascal Getreuer", "Anurag Kumar", "Vivek Kumar"], "title": "PhaseCoder: Microphone Geometry-Agnostic Spatial Audio Understanding for Multimodal LLMs", "comment": null, "summary": "Current multimodal LLMs process audio as a mono stream, ignoring the rich spatial information essential for embodied AI. Existing spatial audio models, conversely, are constrained to fixed microphone geometries, preventing deployment across diverse devices. We present PhaseCoder, a transformer-only spatial audio encoder that is agnostic to microphone geometry. PhaseCoder takes raw multichannel audio and microphone coordinates as inputs to perform localization and produces robust spatial embeddings. We demonstrate that Gemma 3n LLM can be fine-tuned to reason over \"Spatial Audio Tokens\" produced by PhaseCoder. We show our encoder achieves state-of-the-art results on microphone-invariant localization benchmarks and, for the first time, enables an LLM to perform complex spatial reasoning and targeted transcription tasks from an arbitrary microphone array."}
{"id": "2601.21914", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.21914", "abs": "https://arxiv.org/abs/2601.21914", "authors": ["Zhouyou Gu", "Jihong Park", "Jinho Choi"], "title": "Joint Laser Inter-Satellite Link Matching and Traffic Flow Routing in LEO Mega-Constellations via Lagrangian Duality", "comment": "This work has been submitted to an IEEE journal for possible publication", "summary": "Low Earth orbit (LEO) mega-constellations greatly extend the coverage and resilience of future wireless systems. Within the mega-constellations, laser inter-satellite links (LISLs) enable high-capacity, long-range connectivity. Existing LISL schemes often overlook mechanical limitations of laser communication terminals (LCTs) and non-uniform global traffic profiles caused by uneven user and gateway distributions, leading to suboptimal throughput and underused LCTs/LISLs -- especially when each satellite carries only a few LCTs. This paper investigates the joint optimization of LCT connections and traffic routing to maximize the constellation throughput, considering the realistic LCT mechanics and the global traffic profile. The problem is formulated as an NP-hard mixed-integer program coupling LCT connections with flow-rate variables under link capacity constraints. Due to its intractability, we resort to relaxing the coupling constraints via Lagrangian duality, decomposing the problem into a weighted graph-matching for LCT connections, weighted shortest-path routing tasks, and a linear program for rate allocation. Here, Lagrange multipliers reflect congestion weights between satellites, jointly guiding the matching, routing, and rate allocation. Subgradient descent optimizes the multipliers, with provable convergence. Simulations using real-world constellation and terrestrial data show that our methods substantially improve network throughput by up to $35\\%$--$145\\%$ over existing non-joint approaches."}
{"id": "2601.21886", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2601.21886", "abs": "https://arxiv.org/abs/2601.21886", "authors": ["Michael Kuhlmann", "Alexander Werning", "Thilo von Neumann", "Reinhold Haeb-Umbach"], "title": "Speech Quality-Based Localization of Low-Quality Speech and Text-to-Speech Synthesis Artefacts", "comment": "Accepted at ICASSP 2026", "summary": "A large number of works view the automatic assessment of speech from an utterance- or system-level perspective. While such approaches are good in judging overall quality, they cannot adequately explain why a certain score was assigned to an utterance. frame-level scores can provide better interpretability, but models predicting them are harder to tune and regularize since no strong targets are available during training. In this work, we show that utterance-level speech quality predictors can be regularized with a segment-based consistency constraint which notably reduces frame-level stochasticity. We then demonstrate two applications involving frame-level scores: The partial spoof scenario and the detection of synthesis artefacts in two state-of-the-art text-to-speech systems. For the latter, we perform listening tests and confirm that listeners rate segments to be of poor quality more often in the set defined by low frame-level scores than in a random control set."}
{"id": "2601.21260", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.21260", "abs": "https://arxiv.org/abs/2601.21260", "authors": ["Seonghyeon Go", "Yumin Kim"], "title": "Music Plagiarism Detection: Problem Formulation and a Segment-based Solution", "comment": null, "summary": "Recently, the problem of music plagiarism has emerged as an even more pressing social issue. As music information retrieval research advances, there is a growing effort to address issues related to music plagiarism. However, many studies, including our previous work, have conducted research without clearly defining what the music plagiarism detection task actually involves. This lack of a clear definition has slowed research progress and made it hard to apply results to real-world scenarios. To fix this situation, we defined how Music Plagiarism Detection is different from other MIR tasks and explained what problems need to be solved. We introduce the Similar Music Pair dataset to support this newly defined task. In addition, we propose a method based on segment transcription as one way to solve the task. Our demo and dataset are available at https://github.com/Mippia/ICASSP2026-MPD."}
{"id": "2601.21921", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.21921", "abs": "https://arxiv.org/abs/2601.21921", "authors": ["Zhouyou Gu", "Jinho Choi", "Tony Q. S. Quek", "Jihong Park"], "title": "Duality-Guided Graph Learning for Real-Time Joint Connectivity and Routing in LEO Mega-Constellations", "comment": "This work has been submitted to an IEEE journal for possible publication", "summary": "Laser inter-satellite links (LISLs) of low Earth orbit (LEO) mega-constellations enable high-capacity backbone connectivity in non-terrestrial networks, but their management is challenged by limited laser communication terminals, mechanical pointing constraints, and rapidly time-varying network topologies. This paper studies the joint problem of LISL connection establishment, traffic routing, and flow-rate allocation under heterogeneous global traffic demand and gateway availability. We formulate the problem as a mixed-integer optimization over large-scale, time-varying constellation graphs and develop a Lagrangian dual decomposition that interprets per-link dual variables as congestion prices coordinating connectivity and routing decisions. To overcome the prohibitive latency of iterative dual updates, we propose DeepLaDu, a Lagrangian duality-guided deep learning framework that trains a graph neural network (GNN) to directly infer per-link (edge-level) congestion prices from the constellation state in a single forward pass. We enable scalable and stable training using a subgradient-based edge-level loss in DeepLaDu. We analyze the convergence and computational complexity of the proposed approach and evaluate it using realistic Starlink-like constellations with optical and traffic constraints. Simulation results show that DeepLaDu achieves up to 20\\% higher network throughput than non-joint or heuristic baselines, while matching the performance of iterative dual optimization with orders-of-magnitude lower computation time, suitable for real-time operation in dynamic LEO networks."}
{"id": "2601.21940", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2601.21940", "abs": "https://arxiv.org/abs/2601.21940", "authors": ["Yihui Fu", "Tim Fingscheidt"], "title": "DisContSE: Single-Step Diffusion Speech Enhancement Based on Joint Discrete and Continuous Embeddings", "comment": "Accepted by IEEE ICASSP 2026", "summary": "Diffusion speech enhancement on discrete audio codec features gain immense attention due to their improved speech component reconstruction capability. However, they usually suffer from high inference computational complexity due to multiple reverse process iterations. Furthermore, they generally achieve promising results on non-intrusive metrics but show poor performance on intrusive metrics, as they may struggle in reconstructing the correct phones. In this paper, we propose DisContSE, an efficient diffusion-based speech enhancement model on joint discrete codec tokens and continuous embeddings. Our contributions are three-fold. First, we formulate both a discrete and a continuous enhancement module operating on discrete audio codec tokens and continuous embeddings, respectively, to achieve improved fidelity and intelligibility simultaneously. Second, a semantic enhancement module is further adopted to achieve optimal phonetic accuracy. Third, we achieve a single-step efficient reverse process in inference with a novel quantization error mask initialization strategy, which, according to our knowledge, is the first successful single-step diffusion speech enhancement based on an audio codec. Trained and evaluated on URGENT 2024 Speech Enhancement Challenge data splits, the proposed DisContSE excels top-reported time- and frequency-domain diffusion baseline methods in PESQ, POLQA, UTMOS, and in a subjective ITU-T P.808 listening test, clearly achieving an overall top rank."}
{"id": "2601.21386", "categories": ["cs.SD", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21386", "abs": "https://arxiv.org/abs/2601.21386", "authors": ["June-Woo Kim", "Dhruv Agarwal", "Federica Cerina"], "title": "Understanding Frechet Speech Distance for Synthetic Speech Quality Evaluation", "comment": "accepted to ICASSP 2026", "summary": "Objective evaluation of synthetic speech quality remains a critical challenge. Human listening tests are the gold standard, but costly and impractical at scale. Fréchet Distance has emerged as a promising alternative, yet its reliability depends heavily on the choice of embeddings and experimental settings. In this work, we comprehensively evaluate Fréchet Speech Distance (FSD) and its variant Speech Maximum Mean Discrepancy (SMMD) under varied embeddings and conditions. We further incorporate human listening evaluations alongside TTS intelligibility and synthetic-trained ASR WER to validate the perceptual relevance of these metrics. Our findings show that WavLM Base+ features yield the most stable alignment with human ratings. While FSD and SMMD cannot fully replace subjective evaluation, we show that they can serve as complementary, cost-efficient, and reproducible measures, particularly useful when large-scale or direct listening assessments are infeasible. Code is available at https://github.com/kaen2891/FrechetSpeechDistance."}
{"id": "2601.21997", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.21997", "abs": "https://arxiv.org/abs/2601.21997", "authors": ["Lucía Pallarés-Rodríguez", "Angelo Coluccia", "Alessio Fascista", "Musa Furkan Keskin", "Henk Wymeersch", "José A. López-Salcedo", "Gonzalo Seco-Granados"], "title": "Optimal Placement of Movable Antennas for Angle-of-Departure Estimation Under User Location Uncertainty", "comment": "Accepted at IEEE International Conference on Acoustics, Speech, and Signal Processing 2026 (ICASSP 2026)", "summary": "Movable antennas (MA) have gained significant attention in recent years to overcome the limitations of extremely large antenna arrays in terms of cost and power consumption. In this paper, we investigate the use of MA arrays at the base station (BS) for angle-of-departure (AoD) estimation under uncertainty in the user equipment (UE) location. Specifically, we (i) derive the theoretical performance limits through the Cramér-Rao bound (CRB) and (ii) optimize the antenna positions to ensure robust performance within the UE's uncertainty region. Numerical results show that dynamically optimizing antenna placement by explicitly considering the uncertainty region yields superior performance compared to fixed arrays, demonstrating the ability of MA systems to adapt and outperform conventional arrays."}
{"id": "2601.21960", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2601.21960", "abs": "https://arxiv.org/abs/2601.21960", "authors": ["Aref Farhadipour", "Jan Marquenie", "Srikanth Madikeri", "Teodora Vukovic", "Volker Dellwo", "Kathy Reid", "Francis M. Tyers", "Ingo Siegert", "Eleanor Chodroff"], "title": "TidyVoice 2026 Challenge Evaluation Plan", "comment": "https://tidyvoice2026.github.io/", "summary": "The performance of speaker verification systems degrades significantly under language mismatch, a critical challenge exacerbated by the field's reliance on English-centric data. To address this, we propose the TidyVoice Challenge for cross-lingual speaker verification. The challenge leverages the TidyVoiceX dataset from the novel TidyVoice benchmark, a large-scale, multilingual corpus derived from Mozilla Common Voice, and specifically curated to isolate the effect of language switching across approximately 40 languages. Participants will be tasked with building systems robust to this mismatch, with performance primarily evaluated using the Equal Error Rate on cross-language trials. By providing standardized data, open-source baselines, and a rigorous evaluation protocol, this challenge aims to drive research towards fairer, more inclusive, and language-independent speaker recognition technologies, directly aligning with the Interspeech 2026 theme, \"Speaking Together.\""}
{"id": "2601.21463", "categories": ["cs.SD", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21463", "abs": "https://arxiv.org/abs/2601.21463", "authors": ["Jun Xue", "Yi Chai", "Yanzhen Ren", "Jinshen He", "Zhiqiang Tang", "Zhuolin Yi", "Yihuan Huang", "Yuankun Xie", "Yujie Chen"], "title": "Unifying Speech Editing Detection and Content Localization via Prior-Enhanced Audio LLMs", "comment": null, "summary": "Speech editing achieves semantic inversion by performing fine-grained segment-level manipulation on original utterances, while preserving global perceptual naturalness. Existing detection studies mainly focus on manually edited speech with explicit splicing artifacts, and therefore struggle to cope with emerging end-to-end neural speech editing techniques that generate seamless acoustic transitions. To address this challenge, we first construct a large-scale bilingual dataset, AiEdit, which leverages large language models to drive precise semantic tampering logic and employs multiple advanced neural speech editing methods for data synthesis, thereby filling the gap of high-quality speech editing datasets. Building upon this foundation, we propose PELM (Prior-Enhanced Audio Large Language Model), the first large-model framework that unifies speech editing detection and content localization by formulating them as an audio question answering task. To mitigate the inherent forgery bias and semantic-priority bias observed in existing audio large models, PELM incorporates word-level probability priors to provide explicit acoustic cues, and further designs a centroid-aggregation-based acoustic consistency perception loss to explicitly enforce the modeling of subtle local distribution anomalies. Extensive experimental results demonstrate that PELM significantly outperforms state-of-the-art methods on both the HumanEdit and AiEdit datasets, achieving equal error rates (EER) of 0.57\\% and 9.28\\% (localization), respectively."}
{"id": "2601.22109", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.22109", "abs": "https://arxiv.org/abs/2601.22109", "authors": ["Ali Reda", "Tamer Mekkawy", "Theodoros A. Tsiftsis", "Chan-Byoung Chae", "Kai-Kit Wong"], "title": "Towards Joint Optimization for UAV-Integrated RIS-Assisted Fluid Antenna Systems", "comment": "11 pages, 8 figures", "summary": "Unmanned aerial vehicles (UAVs) integrated into cellular networks face significant challenges from air-to-ground interference. To address this, we propose a downlink UAV communication system that leverages a fluid antenna system (FAS)- assisted reconfigurable intelligent surface (RIS) to enhance signal quality. By jointly optimizing the FAS port positions and RIS phase shifts, we maximize the achievable rate. The resulting nonconvex optimization problem is solved using successive convex approximation (SCA) based on second-order cone programming (SOCP), which reformulates the constraints into a tractable form. Simulation results show that the proposed algorithm significantly improves both outage probability and achievable rate over conventional fixed-position antenna (FPA) schemes, with particularly large gains in large-scale RIS configurations. Moreover, the algorithm converges rapidly, making it suitable for real-time applications"}
{"id": "2601.20867", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.20867", "abs": "https://arxiv.org/abs/2601.20867", "authors": ["Jaehyuk Jang", "Wonjun Lee", "Kangwook Ko", "Changick Kim"], "title": "Generalizable Prompt Tuning for Audio-Language Models via Semantic Expansion", "comment": null, "summary": "Prompt tuning has achieved remarkable progress in vision-language models (VLMs) and is recently being adopted for audio-language models (ALMs). However, its generalization ability in ALMs remains largely underexplored. We observe that conventional prompt tuning for ALMs also suffers from the Base-New Tradeoff, and we identify that this issue stems from the disrupted semantic structure of the embedding space. To address this issue, we propose Semantically Expanded Prompt Tuning (SEPT)-a plug-and-play framework that explicitly regularizes the prompt embedding space by incorporating semantic neighbors generated by large language models. SEPT introduces a novel semantic expansion loss with margin constraints that promote intra-class compactness and inter-class separability, thereby enhancing the semantic structure of the prompt embedding space. For comprehensive evaluation, we establish the first benchmark setup for prompt generalization in ALMs, covering both base-to-new generalization and cross-dataset transferability. Extensive experiments demonstrate that SEPT consistently improves generalization performance across multiple prompt tuning baselines, while maintaining computational cost during inference. Codes are available in https://github.com/jhyukjang/SEPT."}
{"id": "2601.21925", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2601.21925", "abs": "https://arxiv.org/abs/2601.21925", "authors": ["Yuchen Mao", "Wen Huang", "Yanmin Qian"], "title": "Localizing Speech Deepfakes Beyond Transitions via Segment-Aware Learning", "comment": null, "summary": "Localizing partial deepfake audio, where only segments of speech are manipulated, remains challenging due to the subtle and scattered nature of these modifications. Existing approaches typically rely on frame-level predictions to identify spoofed segments, and some recent methods improve performance by concentrating on the transitions between real and fake audio. However, we observe that these models tend to over-rely on boundary artifacts while neglecting the manipulated content that follows. We argue that effective localization requires understanding the entire segments beyond just detecting transitions. Thus, we propose Segment-Aware Learning (SAL), a framework that encourages models to focus on the internal structure of segments. SAL introduces two core techniques: Segment Positional Labeling, which provides fine-grained frame supervision based on relative position within a segment; and Cross-Segment Mixing, a data augmentation method that generates diverse segment patterns. Experiments across multiple deepfake localization datasets show that SAL consistently achieves strong performance in both in-domain and out-of-domain settings, with notable gains in non-boundary regions and reduced reliance on transition artifacts. The code is available at https://github.com/SentryMao/SAL."}
{"id": "2601.20883", "categories": ["cs.SD", "cs.CR", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.20883", "abs": "https://arxiv.org/abs/2601.20883", "authors": ["Bharath Krishnamurthy", "Ajita Rattani"], "title": "VoxMorph: Scalable Zero-shot Voice Identity Morphing via Disentangled Embeddings", "comment": "Accepted to IEEE ICASSP 2026 (51st International Conference on Acoustics, Speech, and Signal Processing, ICASSP 2026). 5 pages, 1 figure, 3 tables. Project page: https://vcbsl.github.io/VoxMorph/", "summary": "Morphing techniques generate artificial biometric samples that combine features from multiple individuals, allowing each contributor to be verified against a single enrolled template. While extensively studied in face recognition, this vulnerability remains largely unexplored in voice biometrics. Prior work on voice morphing is computationally expensive, non-scalable, and limited to acoustically similar identity pairs, constraining practical deployment. Moreover, existing sound-morphing methods target audio textures, music, or environmental sounds and are not transferable to voice identity manipulation. We propose VoxMorph, a zero-shot framework that produces high-fidelity voice morphs from as little as five seconds of audio per subject without model retraining. Our method disentangles vocal traits into prosody and timbre embeddings, enabling fine-grained interpolation of speaking style and identity. These embeddings are fused via Spherical Linear Interpolation (Slerp) and synthesized using an autoregressive language model coupled with a Conditional Flow Matching network. VoxMorph achieves state-of-the-art performance, delivering a 2.6x gain in audio quality, a 73% reduction in intelligibility errors, and a 67.8% morphing attack success rate on automated speaker verification systems under strict security thresholds. This work establishes a practical and scalable paradigm for voice morphing with significant implications for biometric security. The code and dataset are available on our project page: https://vcbsl.github.io/VoxMorph/"}
{"id": "2601.21110", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2601.21110", "abs": "https://arxiv.org/abs/2601.21110", "authors": ["Jaden Pieper", "Stephen D. Voran"], "title": "Unseen but not Unknown: Using Dataset Concealment to Robustly Evaluate Speech Quality Estimation Models", "comment": "To be appear in Proc. ICASSP 2026", "summary": "We introduce Dataset Concealment (DSC), a rigorous new procedure for evaluating and interpreting objective speech quality estimation models. DSC quantifies and decomposes the performance gap between research results and real-world application requirements, while offering context and additional insights into model behavior and dataset characteristics. We also show the benefits of addressing the corpus effect by using the dataset Aligner from AlignNet when training models with multiple datasets. We demonstrate DSC and the improvements from the Aligner using nine training datasets and nine unseen datasets with three well-studied models: MOSNet, NISQA, and a Wav2Vec2.0-based model. DSC provides interpretable views of the generalization capabilities and limitations of models, while allowing all available data to be used at training. An additional result is that adding the 1000 parameter dataset Aligner to the 94 million parameter Wav2Vec model during training does significantly improve the resulting model's ability to estimate speech quality for unseen data."}
{"id": "2601.20890", "categories": ["cs.SD", "cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.20890", "abs": "https://arxiv.org/abs/2601.20890", "authors": ["Manali Sharma", "Riya Naik", "Buvaneshwari G"], "title": "SW-ASR: A Context-Aware Hybrid ASR Pipeline for Robust Single Word Speech Recognition", "comment": null, "summary": "Single-word Automatic Speech Recognition (ASR) is a challenging task due to the lack of linguistic context and sensitivity to noise, pronunciation variation, and channel artifacts, especially in low-resource, communication-critical domains such as healthcare and emergency response. This paper reviews recent deep learning approaches and proposes a modular framework for robust single-word detection. The system combines denoising and normalization with a hybrid ASR front end (Whisper + Vosk) and a verification layer designed to handle out-of-vocabulary words and degraded audio. The verification layer supports multiple matching strategies, including embedding similarity, edit distance, and LLM-based matching with optional contextual guidance. We evaluate the framework on the Google Speech Commands dataset and a curated real-world dataset collected from telephony and messaging platforms under bandwidth-limited conditions. Results show that while the hybrid ASR front end performs well on clean audio, the verification layer significantly improves accuracy on noisy and compressed channels. Context-guided and LLM-based matching yield the largest gains, demonstrating that lightweight verification and context mechanisms can substantially improve single-word ASR robustness without sacrificing latency required for real-time telephony applications."}
{"id": "2601.21114", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2601.21114", "abs": "https://arxiv.org/abs/2601.21114", "authors": ["Henri Gode", "Simon Doclo"], "title": "DNN-Based Online Source Counting Based on Spatial Generalized Magnitude Squared Coherence", "comment": "in Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) 2026, Barcelona, Spain", "summary": "The number of active sound sources is a key parameter in many acoustic signal processing tasks, such as source localization, source separation, and multi-microphone speech enhancement. This paper proposes a novel method for online source counting by detecting changes in the number of active sources based on spatial coherence. The proposed method exploits the fact that a single coherent source in spatially white background noise yields high spatial coherence, whereas only noise results in low spatial coherence. By applying a spatial whitening operation, the source counting problem is reformulated as a change detection task, aiming to identify the time frames when the number of active sources changes. The method leverages the generalized magnitude-squared coherence as a measure to quantify spatial coherence, providing features for a compact neural network trained to detect source count changes framewise. Simulation results with binaural hearing aids in reverberant acoustic scenes with up to 4 speakers and background noise demonstrate the effectiveness of the proposed method for online source counting."}
{"id": "2601.20896", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.20896", "abs": "https://arxiv.org/abs/2601.20896", "authors": ["Ryan Whetten", "Titouan Parcollet", "Marco Dinarelli", "Yannick Estève"], "title": "A Study of Data Selection Strategies for Pre-training Self-Supervised Speech Models", "comment": "Accepted for publication in the 2026 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP 2026)", "summary": "Self-supervised learning (SSL) has transformed speech processing, yet its reliance on massive pre-training datasets remains a bottleneck. While robustness is often attributed to scale and diversity, the role of the data distribution is less understood. We systematically examine how curated subsets of pre-training data influence Automatic Speech Recognition (ASR) performance. Surprisingly, optimizing for acoustic, speaker, or linguistic diversity yields no clear improvements over random sampling. Instead, we find that prioritizing the longest utterances achieves superior ASR results while using only half the original dataset, reducing pre-training time by 24% on a large corpora. These findings suggest that for pre-training speech SSL models, data length is a more critical factor than either data diversity or overall data quantity for performance and efficiency, offering a new perspective for data selection strategies in SSL speech processing."}
{"id": "2601.21347", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2601.21347", "abs": "https://arxiv.org/abs/2601.21347", "authors": ["Xiuwen Zheng", "Sixun Dong", "Bornali Phukon", "Mark Hasegawa-Johnson", "Chang D. Yoo"], "title": "Towards Robust Dysarthric Speech Recognition: LLM-Agent Post-ASR Correction Beyond WER", "comment": "Accepted to ICASSP 2026", "summary": "While Automatic Speech Recognition (ASR) is typically benchmarked by word error rate (WER), real-world applications ultimately hinge on semantic fidelity. This mismatch is particularly problematic for dysarthric speech, where articulatory imprecision and disfluencies can cause severe semantic distortions. To bridge this gap, we introduce a Large Language Model (LLM)-based agent for post-ASR correction: a Judge-Editor over the top-k ASR hypotheses that keeps high-confidence spans, rewrites uncertain segments, and operates in both zero-shot and fine-tuned modes. In parallel, we release SAP-Hypo5, the largest benchmark for dysarthric speech correction, to enable reproducibility and future exploration. Under multi-perspective evaluation, our agent achieves a 14.51% WER reduction alongside substantial semantic gains, including a +7.59 pp improvement in MENLI and +7.66 pp in Slot Micro F1 on challenging samples. Our analysis further reveals that WER is highly sensitive to domain shift, whereas semantic metrics correlate more closely with downstream task performance."}
{"id": "2601.20900", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.20900", "abs": "https://arxiv.org/abs/2601.20900", "authors": ["Sergio Burdisso", "Esaú Villatoro-Tello", "Andrés Carofilis", "Shashi Kumar", "Kadri Hacioglu", "Srikanth Madikeri", "Pradeep Rangappa", "Manjunath K E", "Petr Motlicek", "Shankar Venkatesan", "Andreas Stolcke"], "title": "Text-only adaptation in LLM-based ASR through text denoising", "comment": "Paper accepted at ICASSP 2026", "summary": "Adapting automatic speech recognition (ASR) systems based on large language models (LLMs) to new domains using text-only data is a significant yet underexplored challenge. Standard fine-tuning of the LLM on target-domain text often disrupts the critical alignment between speech and text modalities learned by the projector, degrading performance. We introduce a novel text-only adaptation method that emulates the audio projection task by treating it as a text denoising task. Our approach thus trains the LLM to recover clean transcripts from noisy inputs. This process effectively adapts the model to a target domain while preserving cross-modal alignment. Our solution is lightweight, requiring no architectural changes or additional parameters. Extensive evaluation on two datasets demonstrates up to 22.1% relative improvement, outperforming recent state-of-the-art text-only adaptation methods."}
{"id": "2601.21402", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2601.21402", "abs": "https://arxiv.org/abs/2601.21402", "authors": ["Zheqi Dai", "Guangyan Zhang", "Haolin He", "Xiquan Li", "Jingyu Li", "Chunyat Wu", "Yiwen Guo", "Qiuqiang Kong"], "title": "SemanticAudio: Audio Generation and Editing in Semantic Space", "comment": null, "summary": "In recent years, Text-to-Audio Generation has achieved remarkable progress, offering sound creators powerful tools to transform textual inspirations into vivid audio. However, existing models predominantly operate directly in the acoustic latent space of a Variational Autoencoder (VAE), often leading to suboptimal alignment between generated audio and textual descriptions. In this paper, we introduce SemanticAudio, a novel framework that conducts both audio generation and editing directly in a high-level semantic space. We define this semantic space as a compact representation capturing the global identity and temporal sequence of sound events, distinct from fine-grained acoustic details. SemanticAudio employs a two-stage Flow Matching architecture: the Semantic Planner first generates these compact semantic features to sketch the global semantic layout, and the Acoustic Synthesizer subsequently produces high-fidelity acoustic latents conditioned on this semantic plan. Leveraging this decoupled design, we further introduce a training-free text-guided editing mechanism that enables precise attribute-level modifications on general audio without retraining. Specifically, this is achieved by steering the semantic generation trajectory via the difference of velocity fields derived from source and target text prompts. Extensive experiments demonstrate that SemanticAudio surpasses existing mainstream approaches in semantic alignment. Demo available at: https://semanticaudio1.github.io/"}
{"id": "2601.21124", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.21124", "abs": "https://arxiv.org/abs/2601.21124", "authors": ["Artem Dementyev", "Wazeer Zulfikar", "Sinan Hersek", "Pascal Getreuer", "Anurag Kumar", "Vivek Kumar"], "title": "PhaseCoder: Microphone Geometry-Agnostic Spatial Audio Understanding for Multimodal LLMs", "comment": null, "summary": "Current multimodal LLMs process audio as a mono stream, ignoring the rich spatial information essential for embodied AI. Existing spatial audio models, conversely, are constrained to fixed microphone geometries, preventing deployment across diverse devices. We present PhaseCoder, a transformer-only spatial audio encoder that is agnostic to microphone geometry. PhaseCoder takes raw multichannel audio and microphone coordinates as inputs to perform localization and produces robust spatial embeddings. We demonstrate that Gemma 3n LLM can be fine-tuned to reason over \"Spatial Audio Tokens\" produced by PhaseCoder. We show our encoder achieves state-of-the-art results on microphone-invariant localization benchmarks and, for the first time, enables an LLM to perform complex spatial reasoning and targeted transcription tasks from an arbitrary microphone array."}
{"id": "2601.21612", "categories": ["eess.AS", "cs.AI", "cs.SD"], "pdf": "https://arxiv.org/pdf/2601.21612", "abs": "https://arxiv.org/abs/2601.21612", "authors": ["Bing Han", "Chushu Zhou", "Yifan Yang", "Wei Wang", "Chenda Li", "Wangyou Zhang", "Yanmin Qian"], "title": "Representation-Regularized Convolutional Audio Transformer for Audio Understanding", "comment": "12 pages, 3 figures", "summary": "Bootstrap-based Self-Supervised Learning (SSL) has achieved remarkable progress in audio understanding. However, existing methods typically operate at a single level of granularity, limiting their ability to model the diverse temporal and spectral structures inherent in complex audio signals. Furthermore, bootstrapping representations from scratch is computationally expensive, often requiring extensive training to converge. In this work, we propose the Convolutional Audio Transformer (CAT), a unified framework designed to address these challenges. First, to capture hierarchical audio features, CAT incorporates a Multi-resolution Block that aggregates information across varying granularities. Second, to enhance training efficiency, we introduce a Representation Regularization objective. Drawing inspiration from generative modeling, this auxiliary task guides the student model by aligning its predictions with high-quality semantic representations from frozen, pre-trained external encoders. Experimental results demonstrate that CAT significantly outperforms baselines on audio understanding benchmarks. Notably, it achieves competitive performance on the AudioSet 20k dataset with 5 times faster convergence than existing methods. Codes and checkpoints will be released soon at https://github.com/realzhouchushu/CAT."}
{"id": "2601.21260", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.21260", "abs": "https://arxiv.org/abs/2601.21260", "authors": ["Seonghyeon Go", "Yumin Kim"], "title": "Music Plagiarism Detection: Problem Formulation and a Segment-based Solution", "comment": null, "summary": "Recently, the problem of music plagiarism has emerged as an even more pressing social issue. As music information retrieval research advances, there is a growing effort to address issues related to music plagiarism. However, many studies, including our previous work, have conducted research without clearly defining what the music plagiarism detection task actually involves. This lack of a clear definition has slowed research progress and made it hard to apply results to real-world scenarios. To fix this situation, we defined how Music Plagiarism Detection is different from other MIR tasks and explained what problems need to be solved. We introduce the Similar Music Pair dataset to support this newly defined task. In addition, we propose a method based on segment transcription as one way to solve the task. Our demo and dataset are available at https://github.com/Mippia/ICASSP2026-MPD."}
{"id": "2601.21960", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2601.21960", "abs": "https://arxiv.org/abs/2601.21960", "authors": ["Aref Farhadipour", "Jan Marquenie", "Srikanth Madikeri", "Teodora Vukovic", "Volker Dellwo", "Kathy Reid", "Francis M. Tyers", "Ingo Siegert", "Eleanor Chodroff"], "title": "TidyVoice 2026 Challenge Evaluation Plan", "comment": "https://tidyvoice2026.github.io/", "summary": "The performance of speaker verification systems degrades significantly under language mismatch, a critical challenge exacerbated by the field's reliance on English-centric data. To address this, we propose the TidyVoice Challenge for cross-lingual speaker verification. The challenge leverages the TidyVoiceX dataset from the novel TidyVoice benchmark, a large-scale, multilingual corpus derived from Mozilla Common Voice, and specifically curated to isolate the effect of language switching across approximately 40 languages. Participants will be tasked with building systems robust to this mismatch, with performance primarily evaluated using the Equal Error Rate on cross-language trials. By providing standardized data, open-source baselines, and a rigorous evaluation protocol, this challenge aims to drive research towards fairer, more inclusive, and language-independent speaker recognition technologies, directly aligning with the Interspeech 2026 theme, \"Speaking Together.\""}

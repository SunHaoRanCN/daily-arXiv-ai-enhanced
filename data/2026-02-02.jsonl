{"id": "2601.22260", "categories": ["eess.AS", "eess.SP"], "pdf": "https://arxiv.org/pdf/2601.22260", "abs": "https://arxiv.org/abs/2601.22260", "authors": ["Tom Gajecki", "Jonas Althoff", "Waldo Nogueira"], "title": "Brain-Informed Speech Separation for Cochlear Implants", "comment": null, "summary": "We propose a brain-informed speech separation method for cochlear implants (CIs) that uses electroencephalography (EEG)-derived attention cues to guide enhancement toward the attended speaker. An attention-guided network fuses audio mixtures with EEG features through a lightweight fusion layer, producing attended-source electrodograms for CI stimulation while resolving the label-permutation ambiguity of audio-only separators. Robustness to degraded attention cues is improved with a mixed curriculum that varies cue quality during training, yielding stable gains even when EEG-speech correlation is moderate. In multi-talker conditions, the model achieves higher signal-to-interference ratio improvements than an audio-only electrodogram baseline while remaining slightly smaller (167k vs. 171k parameters). With 2 ms algorithmic latency and comparable cost, the approach highlights the promise of coupling auditory and neural cues for cognitively adaptive CI processing."}
{"id": "2601.22306", "categories": ["eess.AS", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.22306", "abs": "https://arxiv.org/abs/2601.22306", "authors": ["Cheol Jun Cho", "Nicholas Lee", "Alan W Black", "Gopala K. Anumanchipalli"], "title": "Sylber 2.0: A Universal Syllable Embedding", "comment": null, "summary": "Scaling spoken language modeling requires speech tokens that are both efficient and universal. Recent work has proposed syllables as promising speech tokens at low temporal resolution, but existing models are constrained to English and fail to capture sufficient acoustic detail. To address this gap, we present Sylber 2.0, a self-supervised framework for coding speech at the syllable level that enables efficient temporal compression and high-fidelity reconstruction. Sylber 2.0 achieves a very low token frequency around 5 Hz, while retaining both linguistic and acoustic detail across multiple languages and expressive styles. Experiments show that it performs on par with previous models operating on high-frequency baselines. Furthermore, Sylber 2.0 enables efficient TTS modeling which can generate speech with competitive intelligibility and quality with SOTA models using only 72M parameters. Moreover, the universality of Sylber 2.0 provides more effective features for low resource ASR than previous speech coding frameworks. In sum, we establish an effective syllable-level abstraction for general spoken language."}
{"id": "2601.22319", "categories": ["eess.AS", "eess.SP"], "pdf": "https://arxiv.org/pdf/2601.22319", "abs": "https://arxiv.org/abs/2601.22319", "authors": ["Weixin Liu", "Bowen Qu", "Matthew Pontell", "Maria Powell", "Bradley Malin", "Zhijun Yin"], "title": "Optimizing Domain-Adaptive Self-Supervised Learning for Clinical Voice-Based Disease Classification", "comment": "Accepted at IEEE ICASSP 2026", "summary": "The human voice is a promising non-invasive digital biomarker, yet deep learning for voice-based health analysis is hindered by data scarcity and domain mismatch, where models pre-trained on general audio fail to capture the subtle pathological features characteristic of clinical voice data. To address these challenges, we investigate domain-adaptive self-supervised learning (SSL) with Masked Autoencoders (MAE) and demonstrate that standard configurations are suboptimal for health-related audio. Using the Bridge2AI-Voice dataset, a multi-institutional collection of pathological voices, we systematically examine three performance-critical factors: reconstruction loss (Mean Absolute Error vs. Mean Squared Error), normalization (patch-wise vs. global), and masking (random vs. content-aware). Our optimized design, which combines Mean Absolute Error (MA-Error) loss, patch-wise normalization, and content-aware masking, achieves a Macro F1 of $0.688 \\pm 0.009$ (over 10 fine-tuning runs), outperforming a strong out-of-domain SSL baseline pre-trained on large-scale general audio, which has a Macro F1 of $0.663 \\pm 0.011$. The results show that MA-Error loss improves robustness and content-aware masking boosts performance by emphasizing information-rich regions. These findings highlight the importance of component-level optimization in data-constrained medical applications that rely on audio data."}
{"id": "2601.22504", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2601.22504", "abs": "https://arxiv.org/abs/2601.22504", "authors": ["Binh Thien Nguyen", "Masahiro Yasuda", "Daiki Takeuchi", "Daisuke Niizumi", "Noboru Harada"], "title": "Class-Aware Permutation-Invariant Signal-to-Distortion Ratio for Semantic Segmentation of Sound Scene with Same-Class Sources", "comment": "Accepted by ICASSP 2026", "summary": "To advance immersive communication, the Detection and Classification of Acoustic Scenes and Events (DCASE) 2025 Challenge recently introduced Task 4 on Spatial Semantic Segmentation of Sound Scenes (S5). An S5 system takes a multi-channel audio mixture as input and outputs single-channel dry sources along with their corresponding class labels. Although the DCASE 2025 Challenge simplifies the task by constraining class labels in each mixture to be mutually exclusive, real-world mixtures frequently contain multiple sources from the same class. The presence of duplicated labels can significantly degrade the performance of the label-queried source separation (LQSS) model, which is the key component of many existing S5 systems, and can also limit the validity of the official evaluation metric of DCASE 2025 Task 4. To address these issues, we propose a class-aware permutation-invariant loss function that enables the LQSS model to handle queries involving duplicated labels. In addition, we redesign the S5 evaluation metric to eliminate ambiguities caused by these same-class sources. To evaluate the proposed method within the S5 system, we extend the label prediction model to support same-class labels. Experimental results demonstrate the effectiveness of the proposed methods and the robustness of the new metric on mixtures both with and without same-class sources."}
{"id": "2601.22390", "categories": ["cs.SD", "cs.CR", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.22390", "abs": "https://arxiv.org/abs/2601.22390", "authors": ["Chanwoo Park", "Chanwoo Kim"], "title": "An Effective Energy Mask-based Adversarial Evasion Attacks against Misclassification in Speaker Recognition Systems", "comment": null, "summary": "Evasion attacks pose significant threats to AI systems, exploiting vulnerabilities in machine learning models to bypass detection mechanisms. The widespread use of voice data, including deepfakes, in promising future industries is currently hindered by insufficient legal frameworks. Adversarial attack methods have emerged as the most effective countermeasure against the indiscriminate use of such data. This research introduces masked energy perturbation (MEP), a novel approach using power spectrum for energy masking of original voice data. MEP applies masking to small energy regions in the frequency domain before generating adversarial perturbations, targeting areas less noticeable to the human auditory model. The study primarily employs advanced speaker recognition models, including ECAPA-TDNN and ResNet34, which have shown remarkable performance in speaker verification tasks. The proposed MEP method demonstrated strong performance in both audio quality and evasion effectiveness. The energy masking approach effectively minimizes the perceptual evaluation of speech quality (PESQ) degradation, indicating that minimal perceptual distortion occurs to the human listener despite the adversarial perturbations. Specifically, in the PESQ evaluation, the relative performance of the MEP method was 26.68% when compared to the fast gradient sign method (FGSM) and iterative FGSM."}
{"id": "2601.22243", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.22243", "abs": "https://arxiv.org/abs/2601.22243", "authors": ["Zijun Wang", "Maria Nivetha A", "Ye Hu", "Rui Zhang"], "title": "Compressive Beam-Pattern-Aware Near-field Beam Training via Total Variation Denoising", "comment": null, "summary": "Extremely large antenna arrays envisioned for 6G incurs near-field effect, where steering vector depends on angles and range simultaneously. Polar-domain near-field codebooks can focus energy accurately but incur extra two-dimensional sweeping overhead; compressed-sensing (CS) approaches with Gaussian-masked DFT sensing offer a lower-overhead alternative. This letter revisits near-field beam training using conventional DFT codebooks. Unlike far-field responses that concentrate energy on a few isolated DFT beams, near-field responses produce contiguous, plateau-like energy segments with sharp transitions in the DFT beamspace. Pure LASSO denoising, therefore, tends to over-shrink magnitudes and fragment plateaus. We propose a beam-pattern-preserving beam training scheme for multiple-path scenarios that combines LASSO with a lightweight denoising pipeline: LASSO to suppress small-amplitude noise, followed by total variation (TV) to maintain plateau levels and edge sharpness. The two proximal steps require no near-field codebook design. Simulations with Gaussian pilots show consistent NMSE and cosine-similarity gains over least squares and LASSO at the same pilot budget."}
{"id": "2601.22779", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2601.22779", "abs": "https://arxiv.org/abs/2601.22779", "authors": ["Genshun Wan", "Wenhui Zhang", "Jing-Xuan Zhang", "Shifu Xiong", "Jianqing Gao", "Zhongfu Ye"], "title": "Streaming Speech Recognition with Decoder-Only Large Language Models and Latency Optimization", "comment": "accepted to ICASSP 2026", "summary": "Recent advances have demonstrated the potential of decoderonly large language models (LLMs) for automatic speech recognition (ASR). However, enabling streaming recognition within this framework remains a challenge. In this work, we propose a novel streaming ASR approach that integrates a read/write policy network with monotonic chunkwise attention (MoChA) to dynamically segment speech embeddings. These segments are interleaved with label sequences during training, enabling seamless integration with the LLM. During inference, the audio stream is buffered until the MoChA module triggers a read signal, at which point the buffered segment together with the previous token is fed into the LLM for the next token prediction. We also introduce a minimal-latency training objective to guide the policy network toward accurate segmentation boundaries. Furthermore, we adopt a joint training strategy in which a non-streaming LLM-ASR model and our streaming model share parameters. Experiments on the AISHELL-1 and AISHELL-2 Mandarin benchmarks demonstrate that our method consistently outperforms recent streaming ASR baselines, achieving character error rates of 5.1% and 5.5%, respectively. The latency optimization results in a 62.5% reduction in average token generation delay with negligible impact on recognition accuracy"}
{"id": "2601.22480", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.22480", "abs": "https://arxiv.org/abs/2601.22480", "authors": ["Seungu Han", "Sungho Lee", "Kyogu Lee"], "title": "Rethinking Speech Representation Aggregation in Speech Enhancement: A Phonetic Mutual Information Perspective", "comment": "Accepted to ICASSP 2026", "summary": "Recent speech enhancement (SE) models increasingly leverage self-supervised learning (SSL) representations for their rich semantic information. Typically, intermediate features are aggregated into a single representation via a lightweight adaptation module. However, most SSL models are not trained for noise robustness, which can lead to corrupted semantic representations. Moreover, the adaptation module is trained jointly with the SE model, potentially prioritizing acoustic details over semantic information, contradicting the original purpose. To address this issue, we first analyze the behavior of SSL models on noisy speech from an information-theoretic perspective. Specifically, we measure the mutual information (MI) between the corrupted SSL representations and the corresponding phoneme labels, focusing on preservation of linguistic contents. Building upon this analysis, we introduce the linguistic aggregation layer, which is pre-trained to maximize MI with phoneme labels (with optional dynamic aggregation) and then frozen during SE training. Experiments show that this decoupled approach improves Word Error Rate (WER) over jointly optimized baselines, demonstrating the benefit of explicitly aligning the adaptation module with linguistic contents."}
{"id": "2601.22270", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.22270", "abs": "https://arxiv.org/abs/2601.22270", "authors": ["Zulqarnain Bin Ashraf", "Triantafyllos Mavrovoltsos", "Constantinos Psomas", "Ioannis Krikidis", "Besma Smida"], "title": "Dual-Diode Unified SWIPT for High Data Rates with Adaptive Detection", "comment": "Accepted for presentation at IEEE International Conference on Communications (ICC) 2026", "summary": "Due to their low-complexity and energy-efficiency, unified simultaneous wireless information and power transfer (U-SWIPT) receivers are especially suitable for low-power Internet of Things (IoT) applications. Towards accurately modeling practical operating conditions, in this study, we provide a unified transient framework for a dual-diode U-SWIPT that jointly accounts for diode nonlinearity and capacitor-induced memory effects. The proposed model accurately describes the inherent time dependence of the rectifier, highlighting its fundamental impact on both energy harvesting (EH) and information decoding (ID) processes. Based on the provided memory-aware model, we design a low-complexity adaptive detector that learns the nonlinear state transition dynamics and performs decision-directed detection with linear complexity. The proposed detection scheme approaches maximum likelihood sequence detection (MLSD) performance in memory-dominated regimes, while avoiding the exponential search required by classical sequence detection. Overall, these results demonstrate that properly exploiting rectifier memory provides a better tradeoff between data rate and reliability for U-SWIPT receivers."}
{"id": "2601.22792", "categories": ["eess.AS", "cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2601.22792", "abs": "https://arxiv.org/abs/2601.22792", "authors": ["Muhammad Shakeel", "Yosuke Fukumoto", "Chikara Maeda", "Chyi-Jiunn Lin", "Shinji Watanabe"], "title": "CALM: Joint Contextual Acoustic-Linguistic Modeling for Personalization of Multi-Speaker ASR", "comment": "Accepted to IEEE ICASSP 2026", "summary": "We present CALM, a joint Contextual Acoustic-Linguistic Modeling framework for multi-speaker automatic speech recognition (ASR). In personalized AI scenarios, the joint availability of acoustic and linguistic cues naturally motivates the integration of target-speaker conditioning with contextual biasing in overlapping conversations. CALM implements this integration in an end-to-end framework through speaker embedding-driven target-speaker extraction and dynamic vocabulary-based contextual biasing. We evaluate CALM on simulated English (LibriSpeechMix) and Japanese (Corpus of Spontaneous Japanese mixtures, CSJMix). On two-speaker mixtures, CALM reduces biased word error rate (B-WER) from 12.7 to 4.7 on LibriSpeech2Mix and biased character error rate (B-CER) from 16.6 to 8.4 on CSJMix2 (eval3), demonstrating the effectiveness of joint acoustic-linguistic modeling across languages. We additionally report results on the AMI corpus (IHM-mix condition) to validate performance on standardized speech mixtures."}
{"id": "2601.22599", "categories": ["cs.SD", "cs.HC"], "pdf": "https://arxiv.org/pdf/2601.22599", "abs": "https://arxiv.org/abs/2601.22599", "authors": ["Kai Li", "Jintao Cheng", "Chang Zeng", "Zijun Yan", "Helin Wang", "Zixiong Su", "Bo Zheng", "Xiaolin Hu"], "title": "A Semantically Consistent Dataset for Data-Efficient Query-Based Universal Sound Separation", "comment": "Technical Report", "summary": "Query-based universal sound separation is fundamental to intelligent auditory systems, aiming to isolate specific sources from mixtures. Despite recent advances, existing methods continue to suffer from residual interference in complex acoustic scenes. This performance limitation stems largely from a data bottleneck: in-the-wild datasets contain weak labels and severe co-occurrence of events. These flaws induce models to learn spurious correlations between background noise and target categories instead of robust acoustic features. To address this, we propose an automated pipeline that eliminates co-occurrence of events by mining high-purity single-event segments from in-the-wild datasets via a semantically consistent synthesis protocol. Utilizing this pipeline, we constructed Hive, a high-quality synthetic dataset comprising 2.4k hours of raw audio. Experimental results demonstrate that, compared with the state-of-the-art model SAM-Audio which was trained on a huge dataset $\\sim$500 times larger than Hive, certain open-source models trained on Hive achieve competitive separation accuracy and perceptual quality. Moreover, these models exhibited remarkable zero-shot generalization on out-of-distribution evaluation benchmarks. These findings highlight that prioritizing purity of supervised signals enables significant data efficiency, offering a new paradigm for training robust auditory foundation models with reduced computational costs. Code and dataset are available at https://shandaai.github.io/Hive."}
{"id": "2601.22415", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.22415", "abs": "https://arxiv.org/abs/2601.22415", "authors": ["Sadaf Syed", "Wolfgang Utschick", "Michael Joham"], "title": "On the Optimality of Rate Balancing for Max-Min Fair Multicasting", "comment": null, "summary": "The max-min fair (MMF) multicasting problem is known to be NP-hard. In this work, we analytically derive the optimal solution to this NP-hard problem and establish the equivalence between rate balancing and the optimal MMF multicasting solution under certain conditions. Based on this theoretical insight, we propose a low-complexity algorithm for MMF multicasting that yields closed-form solutions. Simulation results validate our analysis and demonstrate that the proposed algorithm outperforms the state-of-the-art methods while being computationally more efficient."}
{"id": "2601.22873", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2601.22873", "abs": "https://arxiv.org/abs/2601.22873", "authors": ["Li Zhou", "Hao Jiang", "Junjie Li", "Tianrui Wang", "Haizhou Li"], "title": "EmoShift: Lightweight Activation Steering for Enhanced Emotion-Aware Speech Synthesis", "comment": "Activation Steering; Emotion-Aware TTS; Speech Synthesis; Accepted by ICASSP 2026", "summary": "Achieving precise and controllable emotional expression is crucial for producing natural and context-appropriate speech in text-to-speech (TTS) synthesis. However, many emotion-aware TTS systems, including large language model (LLM)-based designs, rely on scaling fixed emotion embeddings or external guidance, limiting their ability to model emotion-specific latent characteristics. To address this gap, we present EmoShift, a lightweight activation-steering framework incorporating a EmoSteer layer, which learns a steering vector for each target emotion in the output embedding space to capture its latent offset and maintain stable, appropriate expression across utterances and categories. With only 10M trainable parameters,less than 1/30 of full fine-tuning, EmoShift outperforms zero-shot and fully fine-tuned baselines in objective and subjective evaluations, enhancing emotional expressiveness while preserving naturalness and speaker similarity. Further analysis confirms the proposed EmoSteer layer's effectiveness and reveals its potential for controllable emotional intensity in speech synthesis."}
{"id": "2601.22661", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2601.22661", "abs": "https://arxiv.org/abs/2601.22661", "authors": ["Yong Ren", "Jingbei Li", "Haiyang Sun", "Yujie Chen", "Cheng Yi", "Yechang Huang", "Hao Gu", "Ye Bai", "Xuerui Yang"], "title": "Evaluating and Rewarding LALMs for Expressive Role-Play TTS via Mean Continuation Log-Probability", "comment": null, "summary": "Recent advances in Large Audio Language Models (LALMs) have extended Text-to-Speech (TTS) to interactive role-play scenarios, which demand high expressiveness and strict adherence to role-play instructions. However, existing models struggle to maintain stylistic consistency with character profiles and scene descriptions across multi-turn dialogues. A critical bottleneck is the lack of objective metrics for quantifying speaking style. To bridge this gap, we propose Mean Continuation Log-Probability (MCLP) as both an evaluation metric and a reward signal, validated on LALM-based Role-Play TTS (RP-TTS) tasks. Critically, we leverage the In-Context Learning capability of pre-trained LALMs to formulate MCLP via a continuation log-probability prediction. This metric quantifies stylistic consistency by measuring the likelihood of the ground-truth speech conditioned on the generated speech. Furthermore, we employ MCLP as a reinforcement learning reward to enhance the style alignment between generated speech and Role-Play instructions. To facilitate evaluation, we construct an RP-TTS dataset with rich scene and character annotations. Experimental results demonstrate that our method significantly outperforms strong LALM baselines on both objective and subjective metrics."}
{"id": "2601.22523", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.22523", "abs": "https://arxiv.org/abs/2601.22523", "authors": ["Yushi Lei", "Yusha Liu", "Guanghui Liu", "Lei Wan", "Kun Yang"], "title": "Superimposed-Pilot OTFS Under Fractional Doppler: Modular End-to-End Learning", "comment": "13 pages, 12 figures", "summary": "Orthogonal time frequency space (OTFS) modulation has emerged as a promising candidate to overcome the performance degradation of orthogonal frequency division multiplexing (OFDM), which are commonly encountered in high-mobility wireless communication scenarios. However, conventional OTFS transceivers rely on multiple separately designed signal-processing modules, whose isolated optimization often limits global optimal performance. To overcome limitations, this paper proposes a modular deep learning (DL) based end-to-end OTFS transceiver framework that consists of trainable and interchangeable neural network (NN) modules, including constellation mapping/demapping, superimposed pilot placement, inverse Zak (IZak)/Zak transforms, and a U-Net-enhanced NN tailored for joint channel estimation and detection (JCED), while explicitly accounting for the impact of the cyclic prefix. This physics-informed modular architecture provides flexibility for integration with conventional OTFS systems and adaptability to different communication configurations. Simulations demonstrate that the proposed design significantly outperforms baseline methods in terms of both normalized mean squared error (NMSE) and detection reliability, maintaining robustness under integer and fractional Doppler conditions. The results highlight the potential of DL-based end-to-end optimization to enable practical and high-performance OTFS transceivers for next-generation high-mobility networks."}
{"id": "2601.23004", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2601.23004", "abs": "https://arxiv.org/abs/2601.23004", "authors": ["Krystof Novotny", "Laureano Moro-Vel√°zquez", "Jiri Mekyska"], "title": "Layer-Aware Early Fusion of Acoustic and Linguistic Embeddings for Cognitive Status Classification", "comment": "5 pages, 3 figures, paper accepted for ICASSP 2026 conference", "summary": "Speech contains both acoustic and linguistic patterns that reflect cognitive decline, and therefore models describing only one domain cannot fully capture such complexity. This study investigates how early fusion (EF) of speech and its corresponding transcription text embeddings, with attention to encoder layer depth, can improve cognitive status classification. Using a DementiaBank-derived collection of recordings (1,629 speakers; cognitively normal controls$\\unicode{x2013}$CN, Mild Cognitive Impairment$\\unicode{x2013}$MCI, and Alzheimer's Disease and Related Dementias$\\unicode{x2013}$ADRD), we extracted frame-aligned embeddings from different internal layers of wav2vec 2.0 or Whisper combined with DistilBERT or RoBERTa. Unimodal, EF and late fusion (LF) models were trained with a transformer classifier, optimized, and then evaluated across 10 seeds. Performance consistently peaked in mid encoder layers ($\\sim$8$\\unicode{x2013}$10), with the single best F1 at Whisper + RoBERTa layer 9 and the best log loss at Whisper + DistilBERT layer 10. Acoustic-only models consistently outperformed text-only variants. EF boosts discrimination for genuinely acoustic embeddings, whereas LF improves probability calibration. Layer choice critically shapes clinical multimodal synergy."}
{"id": "2601.22764", "categories": ["cs.SD", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.22764", "abs": "https://arxiv.org/abs/2601.22764", "authors": ["Deepak Kumar", "Emmanouil Karystinaios", "Gerhard Widmer", "Markus Schedl"], "title": "How Far Can Pretrained LLMs Go in Symbolic Music? Controlled Comparisons of Supervised and Preference-based Adaptation", "comment": "Accepted at NLP4MusA 2026", "summary": "Music often shares notable parallels with language, motivating the use of pretrained large language models (LLMs) for symbolic music understanding and generation. Despite growing interest, the practical effectiveness of adapting instruction-tuned LLMs to symbolic music remains insufficiently characterized. We present a controlled comparative study of finetuning strategies for ABC-based generation and understanding, comparing an off-the-shelf instruction-tuned backbone to domain-adapted variants and a music-specialized LLM baseline. Across multiple symbolic music corpora and evaluation signals, we provide some insights into adaptation choices for symbolic music applications. We highlight the domain adaptation vs.~preserving prior information tradeoff as well as the distinct behaviour of metrics used to measure the domain adaptation for symbolic music."}
{"id": "2601.22724", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.22724", "abs": "https://arxiv.org/abs/2601.22724", "authors": ["Evangelos Koutsonas", "Alexandros-Apostolos A. Boulogeorgos", "Stylianos E. Trevlakis", "George C. Alexandropoulos", "Theodoros A. Tsiftsis", "Rui Zhang"], "title": "SORIS: A Self-Organized Reconfigurable Intelligent Surface Architecture for Wireless Communications", "comment": "12 figures, 1 table, journal", "summary": "In this paper, a new reconfigurable intelligent surface (RIS) hardware architecture, called self-organized RIS (SORIS), is proposed. The architecture incorporates a microcontroller connected to a single-antenna receiver operating at the same frequency as the RIS unit elements, operating either in transmission or reflection mode. The transmitting RIS elements enable the low latency estimation of both the incoming and outcoming channels at the microcontroller's side. In addition, a machine learning approach for estimating the incoming and outcoming channels involving the remaining RIS elements operating in reflection mode is devised. Specifically, by appropriately selecting a small number of elements in transmission mode, and based on the channel reciprocity principle, the respective channel coefficients are first estimated, which are then fed to a low-complexity neural network that, leveraging spatial channel correlation over RIS elements, returns predictions of the channel coefficients referring to the rest of elements. In this way, the SORIS microcontroller acquires channel state information, and accordingly reconfigures the panel's metamaterials to assist data communication between a transmitter and a receiver, without the need for separate connections with them. Moreover, the impact of channel estimation on the proposed solution, and a detailed complexity analysis for the used model, as well as a wiring density and control signaling analysis, is performed. The feasibility and efficacy of the proposed self-organized RIS design and operation are verified by Monte Carlo simulations, providing useful guidelines on the selection of the RIS elements for operating in transmission mode for initial channel estimation."}
{"id": "2601.23196", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2601.23196", "abs": "https://arxiv.org/abs/2601.23196", "authors": ["Mikko Heikkinen", "Archontis Politis", "Konstantinos Drossos", "Tuomas Virtanen"], "title": "Beyond Omnidirectional: Neural Ambisonics Encoding for Arbitrary Microphone Directivity Patterns using Cross-Attention", "comment": "Accepted to ICASSP 2026", "summary": "We present a deep neural network approach for encoding microphone array signals into Ambisonics that generalizes to arbitrary microphone array configurations with fixed microphone count but varying locations and frequency-dependent directional characteristics. Unlike previous methods that rely only on array geometry as metadata, our approach uses directional array transfer functions, enabling accurate characterization of real-world arrays. The proposed architecture employs separate encoders for audio and directional responses, combining them through cross-attention mechanisms to generate array-independent spatial audio representations. We evaluate the method on simulated data in two settings: a mobile phone with complex body scattering, and a free-field condition, both with varying numbers of sound sources in reverberant environments. Evaluations demonstrate that our approach outperforms both conventional digital signal processing-based methods and existing deep neural network solutions. Furthermore, using array transfer functions instead of geometry as metadata input improves accuracy on realistic arrays."}
{"id": "2601.23066", "categories": ["cs.SD", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.23066", "abs": "https://arxiv.org/abs/2601.23066", "authors": ["Xiaoxuan Guo", "Yuankun Xie", "Haonan Cheng", "Jiayi Zhou", "Jian Liu", "Hengyan Huang", "Long Ye", "Qin Zhang"], "title": "Towards Explicit Acoustic Evidence Perception in Audio LLMs for Speech Deepfake Detection", "comment": "9 pages, 4 figures", "summary": "Speech deepfake detection (SDD) focuses on identifying whether a given speech signal is genuine or has been synthetically generated. Existing audio large language model (LLM)-based methods excel in content understanding; however, their predictions are often biased toward semantically correlated cues, which results in fine-grained acoustic artifacts being overlooked during the decisionmaking process. Consequently, fake speech with natural semantics can bypass detectors despite harboring subtle acoustic anomalies; this suggests that the challenge stems not from the absence of acoustic data, but from its inadequate accessibility when semantic-dominant reasoning prevails. To address this issue, we investigate SDD within the audio LLM paradigm and introduce SDD with Auditory Perception-enhanced Audio Large Language Model (SDD-APALLM), an acoustically enhanced framework designed to explicitly expose fine-grained time-frequency evidence as accessible acoustic cues. By combining raw audio with structured spectrograms, the proposed framework empowers audio LLMs to more effectively capture subtle acoustic inconsistencies without compromising their semantic understanding. Experimental results indicate consistent gains in detection accuracy and robustness, especially in cases where semantic cues are misleading. Further analysis reveals that these improvements stem from a coordinated utilization of semantic and acoustic information, as opposed to simple modality aggregation."}
{"id": "2601.22765", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.22765", "abs": "https://arxiv.org/abs/2601.22765", "authors": ["Rohit Varma Chiluvuri", "Santosh Nannuru"], "title": "Bayesian Matrix Completion Under Geometric Constraints", "comment": "4 pages, 3 figures, Accepted to ICASSP 2026", "summary": "The completion of a Euclidean distance matrix (EDM) from sparse and noisy observations is a fundamental challenge in signal processing, with applications in sensor network localization, acoustic room reconstruction, molecular conformation, and manifold learning. Traditional approaches, such as rank-constrained optimization and semidefinite programming, enforce geometric constraints but often struggle under sparse or noisy conditions. This paper introduces a hierarchical Bayesian framework that places structured priors directly on the latent point set generating the EDM, naturally embedding geometric constraints. By incorporating a hierarchical prior on latent point set, the model enables automatic regularization and robust noise handling. Posterior inference is performed using a Metropolis-Hastings within Gibbs sampler to handle coupled latent point posterior. Experiments on synthetic data demonstrate improved reconstruction accuracy compared to deterministic baselines in sparse regimes."}
{"id": "2601.22390", "categories": ["cs.SD", "cs.CR", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.22390", "abs": "https://arxiv.org/abs/2601.22390", "authors": ["Chanwoo Park", "Chanwoo Kim"], "title": "An Effective Energy Mask-based Adversarial Evasion Attacks against Misclassification in Speaker Recognition Systems", "comment": null, "summary": "Evasion attacks pose significant threats to AI systems, exploiting vulnerabilities in machine learning models to bypass detection mechanisms. The widespread use of voice data, including deepfakes, in promising future industries is currently hindered by insufficient legal frameworks. Adversarial attack methods have emerged as the most effective countermeasure against the indiscriminate use of such data. This research introduces masked energy perturbation (MEP), a novel approach using power spectrum for energy masking of original voice data. MEP applies masking to small energy regions in the frequency domain before generating adversarial perturbations, targeting areas less noticeable to the human auditory model. The study primarily employs advanced speaker recognition models, including ECAPA-TDNN and ResNet34, which have shown remarkable performance in speaker verification tasks. The proposed MEP method demonstrated strong performance in both audio quality and evasion effectiveness. The energy masking approach effectively minimizes the perceptual evaluation of speech quality (PESQ) degradation, indicating that minimal perceptual distortion occurs to the human listener despite the adversarial perturbations. Specifically, in the PESQ evaluation, the relative performance of the MEP method was 26.68% when compared to the fast gradient sign method (FGSM) and iterative FGSM."}
{"id": "2601.23149", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2601.23149", "abs": "https://arxiv.org/abs/2601.23149", "authors": ["Junchi Yao", "Lokranjan Lakshmikanthan", "Annie Zhao", "Danielle Zhao", "Shu Yang", "Zikang Ding", "Di Wang", "Lijie Hu"], "title": "Hearing is Believing? Evaluating and Analyzing Audio Language Model Sycophancy with SYAUDIO", "comment": null, "summary": "Audio Language Models (ALMs) have recently shown strong capabilities in unified reasoning over speech, sound, and natural language; yet they inherit behavioral issues observed in Large Language Models, including sycophancy--the tendency to agree with user assertions even when they contradict objective evidence. While sycophancy has been extensively studied in text and vision-language models, its manifestation in audio-conditioned reasoning remains largely unexplored, despite the need for ALMs to rely on auditory cues such as acoustic events, speaker characteristics, and speech rate. To address this gap, we introduce SYAUDIO, the first benchmark dedicated to evaluating sycophancy in ALMs, consisting of 4,319 audio questions spanning Audio Perception, Audio Reasoning, Audio Math, and Audio Ethics. Built upon established audio benchmarks and augmented with TTS-generated arithmetic and moral reasoning tasks, SYAUDIO enables systematic evaluation across multiple domains and sycophancy types with carefully verified data quality. Furthermore, we analyze audio-specific sycophancy under realistic conditions involving noise and rate, and demonstrate that supervised fine-tuning with chain-of-thought data is an effective mitigation strategy for reducing sycophantic behavior in ALMs."}
{"id": "2601.22915", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.22915", "abs": "https://arxiv.org/abs/2601.22915", "authors": ["Fatih Merdan", "Ozgur B. Akan"], "title": "Intrinsic MIMO Particle Communication Channel with Random Advection", "comment": "6 pages, 5 figures", "summary": "In this work, receiver diversity in advection-dominated diffusion-advection channels is investigated. Strong directed flow fundamentally alters the communication-theoretic properties of molecular communication systems (MC). Specifically, advection preserves the temporal ordering and shape of transmitted pulses, enabling pulse-based and higher-order modulation schemes that are typically infeasible in purely diffusive environments. Focusing on a single transmitter and a single type of information molecule, it is demonstrated that spatially distributed receivers can observe distinct realizations of the same transmitted signal, giving rise to diversity gain. Several receiver combining strategies are evaluated and shown to improve detection performance compared to single-receiver operation, particularly in low-to-moderate signal-to-noise ratio (SNR) regimes. The results provide a structured framework for understanding receiver-side diversity in molecular communication, highlighting the role of advection as a key enabler for reliable pulse-based signaling. This perspective establishes a foundation for future studies on advanced modulation, joint equalization and detection, and multi-molecule MIMO extensions that can further enhance the performance and physical applicability of MC systems."}
{"id": "2601.22480", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.22480", "abs": "https://arxiv.org/abs/2601.22480", "authors": ["Seungu Han", "Sungho Lee", "Kyogu Lee"], "title": "Rethinking Speech Representation Aggregation in Speech Enhancement: A Phonetic Mutual Information Perspective", "comment": "Accepted to ICASSP 2026", "summary": "Recent speech enhancement (SE) models increasingly leverage self-supervised learning (SSL) representations for their rich semantic information. Typically, intermediate features are aggregated into a single representation via a lightweight adaptation module. However, most SSL models are not trained for noise robustness, which can lead to corrupted semantic representations. Moreover, the adaptation module is trained jointly with the SE model, potentially prioritizing acoustic details over semantic information, contradicting the original purpose. To address this issue, we first analyze the behavior of SSL models on noisy speech from an information-theoretic perspective. Specifically, we measure the mutual information (MI) between the corrupted SSL representations and the corresponding phoneme labels, focusing on preservation of linguistic contents. Building upon this analysis, we introduce the linguistic aggregation layer, which is pre-trained to maximize MI with phoneme labels (with optional dynamic aggregation) and then frozen during SE training. Experiments show that this decoupled approach improves Word Error Rate (WER) over jointly optimized baselines, demonstrating the benefit of explicitly aligning the adaptation module with linguistic contents."}
{"id": "2601.23161", "categories": ["cs.SD", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.23161", "abs": "https://arxiv.org/abs/2601.23161", "authors": ["Jiaming Zhou", "Xuxin Cheng", "Shiwan Zhao", "Yuhang Jia", "Cao Liu", "Ke Zeng", "Xunliang Cai", "Yong Qin"], "title": "DIFFA-2: A Practical Diffusion Large Language Model for General Audio Understanding", "comment": null, "summary": "Autoregressive (AR) large audio language models (LALMs) such as Qwen-2.5-Omni have achieved strong performance on audio understanding and interaction, but scaling them remains costly in data and computation, and strictly sequential decoding limits inference efficiency. Diffusion large language models (dLLMs) have recently been shown to make effective use of limited training data, and prior work on DIFFA indicates that replacing an AR backbone with a diffusion counterpart can substantially improve audio understanding under matched settings, albeit at a proof-of-concept scale without large-scale instruction tuning, preference alignment, or practical decoding schemes. We introduce DIFFA-2, a practical diffusion-based LALM for general audio understanding. DIFFA-2 upgrades the speech encoder, employs dual semantic and acoustic adapters, and is trained with a four-stage curriculum that combines semantic and acoustic alignment, large-scale supervised fine-tuning, and variance-reduced preference optimization, using only fully open-source corpora. Experiments on MMSU, MMAU, and MMAR show that DIFFA-2 consistently improves over DIFFA and is competitive to strong AR LALMs under practical training budgets, supporting diffusion-based modeling is a viable backbone for large-scale audio understanding. Our code is available at https://github.com/NKU-HLT/DIFFA.git."}
{"id": "2601.22989", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.22989", "abs": "https://arxiv.org/abs/2601.22989", "authors": ["Saeid Pakravan", "Mohsen Ahmadzadeh", "Ming Zeng", "Wessam Ajib", "Ji Wang", "Xingwang Li"], "title": "Fluid Antenna Systems under Channel Uncertainty and Hardware Impairments: Trends, Challenges, and Future Research Directions", "comment": "12 pages", "summary": "Fluid antenna systems (FAS) have recently emerged as a promising paradigm for achieving spatially reconfigurable, compact, and energy-efficient wireless communications in beyond fifth-generation (B5G) and sixth-generation (6G) networks. By dynamically repositioning a liquid-based radiating element within a confined physical structure, FAS can exploit spatial diversity without relying on multiple fixed antenna elements. This spatial mobility provides a new degree of freedom for mitigating channel fading and interference, while maintaining low hardware complexity and power consumption. However, the performance of FAS in realistic deployments is strongly affected by channel uncertainty, hardware nonidealities, and mechanical constraints, all of which can substantially deviate from idealized analytical assumptions. This paper presents a comprehensive survey of the operation and design of FAS under such practical considerations. Key aspects include the characterization of spatio-temporal channel uncertainty, analysis of hardware and mechanical impairments such as RF nonlinearity, port coupling, and fluid response delay, as well as the exploration of robust design and learning-based control strategies to enhance system reliability. Finally, open research directions are identified, aiming to guide future developments toward robust, adaptive, and cross-domain FAS design for next-generation wireless networks."}
{"id": "2601.22779", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2601.22779", "abs": "https://arxiv.org/abs/2601.22779", "authors": ["Genshun Wan", "Wenhui Zhang", "Jing-Xuan Zhang", "Shifu Xiong", "Jianqing Gao", "Zhongfu Ye"], "title": "Streaming Speech Recognition with Decoder-Only Large Language Models and Latency Optimization", "comment": "accepted to ICASSP 2026", "summary": "Recent advances have demonstrated the potential of decoderonly large language models (LLMs) for automatic speech recognition (ASR). However, enabling streaming recognition within this framework remains a challenge. In this work, we propose a novel streaming ASR approach that integrates a read/write policy network with monotonic chunkwise attention (MoChA) to dynamically segment speech embeddings. These segments are interleaved with label sequences during training, enabling seamless integration with the LLM. During inference, the audio stream is buffered until the MoChA module triggers a read signal, at which point the buffered segment together with the previous token is fed into the LLM for the next token prediction. We also introduce a minimal-latency training objective to guide the policy network toward accurate segmentation boundaries. Furthermore, we adopt a joint training strategy in which a non-streaming LLM-ASR model and our streaming model share parameters. Experiments on the AISHELL-1 and AISHELL-2 Mandarin benchmarks demonstrate that our method consistently outperforms recent streaming ASR baselines, achieving character error rates of 5.1% and 5.5%, respectively. The latency optimization results in a 62.5% reduction in average token generation delay with negligible impact on recognition accuracy"}
{"id": "2601.23076", "categories": ["eess.SP", "eess.SY"], "pdf": "https://arxiv.org/pdf/2601.23076", "abs": "https://arxiv.org/abs/2601.23076", "authors": ["Jayadev Joy", "Sundeep Rangan"], "title": "Learning-Based Signal Recovery in Nonlinear Systems with Spectrally Separated Interference", "comment": null, "summary": "Upper Mid-Band (FR3, 7-24 GHz) receivers for 6G must operate over wide bandwidths in dense spectral environments, making them particularly vulnerable to strong adjacent-band interference and front-end nonlinearities. While conventional linear receivers can suppress spectrally separated interferers under ideal hardware assumptions, receiver saturation and finite-resolution quantization cause nonlinear spectral leakage that severely degrades performance in practical wideband radios. We study the recovery of a desired signal from nonlinear receiver observations corrupted by a high-power out-of-band interferer. The receiver front-end is modeled as a smooth, memoryless nonlinearity followed by additive noise and optional quantization. To mitigate these nonlinear and quantization-induced distortions, we propose a learned multi-layer Vector Approximate Message Passing (LMLVAMP) algorithm that incorporates spectral priors with neural network based denoising. Simulation results demonstrate significant performance gains over conventional methods, particularly in high-interference regimes representative of FR3 coexistence scenarios."}
{"id": "2601.22792", "categories": ["eess.AS", "cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2601.22792", "abs": "https://arxiv.org/abs/2601.22792", "authors": ["Muhammad Shakeel", "Yosuke Fukumoto", "Chikara Maeda", "Chyi-Jiunn Lin", "Shinji Watanabe"], "title": "CALM: Joint Contextual Acoustic-Linguistic Modeling for Personalization of Multi-Speaker ASR", "comment": "Accepted to IEEE ICASSP 2026", "summary": "We present CALM, a joint Contextual Acoustic-Linguistic Modeling framework for multi-speaker automatic speech recognition (ASR). In personalized AI scenarios, the joint availability of acoustic and linguistic cues naturally motivates the integration of target-speaker conditioning with contextual biasing in overlapping conversations. CALM implements this integration in an end-to-end framework through speaker embedding-driven target-speaker extraction and dynamic vocabulary-based contextual biasing. We evaluate CALM on simulated English (LibriSpeechMix) and Japanese (Corpus of Spontaneous Japanese mixtures, CSJMix). On two-speaker mixtures, CALM reduces biased word error rate (B-WER) from 12.7 to 4.7 on LibriSpeech2Mix and biased character error rate (B-CER) from 16.6 to 8.4 on CSJMix2 (eval3), demonstrating the effectiveness of joint acoustic-linguistic modeling across languages. We additionally report results on the AMI corpus (IHM-mix condition) to validate performance on standardized speech mixtures."}
{"id": "2601.23119", "categories": ["eess.SP", "eess.SY"], "pdf": "https://arxiv.org/pdf/2601.23119", "abs": "https://arxiv.org/abs/2601.23119", "authors": ["Ruibin Chen", "Jayadev Joy", "Yaqi Hu", "Mingsheng Yin", "Marco Mezzavilla", "Sundeep Rangan"], "title": "Interpolation Techniques for Fast Channel Estimation in Ray Tracing", "comment": "This is the authors accepted version of a paper published in the Proceedings of the 2024 58th Asilomar Conference on Signals, Systems, and Computers", "summary": "Ray tracing is increasingly utilized in wireless system simulations to estimate channel paths. In large-scale simulations with complex environments, ray tracing at high resolution can be computationally demanding. To reduce the computation, this paper presents a novel method for conducting ray tracing at a coarse set of reference points and interpolating the channels at other locations. The key insight is to interpolate the images of reflected points. In addition to the computational savings, the method directly captures the spherical nature of each wavefront enabling fast and accurate computation of channels using line-of-sight MIMO and other wide aperture techniques. Through empirical validation and comparison with exhaustive ray tracing, we demonstrate the efficacy and practicality of our approach in achieving high-fidelity channel predictions with reduced computational resources."}
{"id": "2601.22873", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2601.22873", "abs": "https://arxiv.org/abs/2601.22873", "authors": ["Li Zhou", "Hao Jiang", "Junjie Li", "Tianrui Wang", "Haizhou Li"], "title": "EmoShift: Lightweight Activation Steering for Enhanced Emotion-Aware Speech Synthesis", "comment": "Activation Steering; Emotion-Aware TTS; Speech Synthesis; Accepted by ICASSP 2026", "summary": "Achieving precise and controllable emotional expression is crucial for producing natural and context-appropriate speech in text-to-speech (TTS) synthesis. However, many emotion-aware TTS systems, including large language model (LLM)-based designs, rely on scaling fixed emotion embeddings or external guidance, limiting their ability to model emotion-specific latent characteristics. To address this gap, we present EmoShift, a lightweight activation-steering framework incorporating a EmoSteer layer, which learns a steering vector for each target emotion in the output embedding space to capture its latent offset and maintain stable, appropriate expression across utterances and categories. With only 10M trainable parameters,less than 1/30 of full fine-tuning, EmoShift outperforms zero-shot and fully fine-tuned baselines in objective and subjective evaluations, enhancing emotional expressiveness while preserving naturalness and speaker similarity. Further analysis confirms the proposed EmoSteer layer's effectiveness and reveals its potential for controllable emotional intensity in speech synthesis."}
{"id": "2601.22260", "categories": ["eess.AS", "eess.SP"], "pdf": "https://arxiv.org/pdf/2601.22260", "abs": "https://arxiv.org/abs/2601.22260", "authors": ["Tom Gajecki", "Jonas Althoff", "Waldo Nogueira"], "title": "Brain-Informed Speech Separation for Cochlear Implants", "comment": null, "summary": "We propose a brain-informed speech separation method for cochlear implants (CIs) that uses electroencephalography (EEG)-derived attention cues to guide enhancement toward the attended speaker. An attention-guided network fuses audio mixtures with EEG features through a lightweight fusion layer, producing attended-source electrodograms for CI stimulation while resolving the label-permutation ambiguity of audio-only separators. Robustness to degraded attention cues is improved with a mixed curriculum that varies cue quality during training, yielding stable gains even when EEG-speech correlation is moderate. In multi-talker conditions, the model achieves higher signal-to-interference ratio improvements than an audio-only electrodogram baseline while remaining slightly smaller (167k vs. 171k parameters). With 2 ms algorithmic latency and comparable cost, the approach highlights the promise of coupling auditory and neural cues for cognitively adaptive CI processing."}
{"id": "2601.22319", "categories": ["eess.AS", "eess.SP"], "pdf": "https://arxiv.org/pdf/2601.22319", "abs": "https://arxiv.org/abs/2601.22319", "authors": ["Weixin Liu", "Bowen Qu", "Matthew Pontell", "Maria Powell", "Bradley Malin", "Zhijun Yin"], "title": "Optimizing Domain-Adaptive Self-Supervised Learning for Clinical Voice-Based Disease Classification", "comment": "Accepted at IEEE ICASSP 2026", "summary": "The human voice is a promising non-invasive digital biomarker, yet deep learning for voice-based health analysis is hindered by data scarcity and domain mismatch, where models pre-trained on general audio fail to capture the subtle pathological features characteristic of clinical voice data. To address these challenges, we investigate domain-adaptive self-supervised learning (SSL) with Masked Autoencoders (MAE) and demonstrate that standard configurations are suboptimal for health-related audio. Using the Bridge2AI-Voice dataset, a multi-institutional collection of pathological voices, we systematically examine three performance-critical factors: reconstruction loss (Mean Absolute Error vs. Mean Squared Error), normalization (patch-wise vs. global), and masking (random vs. content-aware). Our optimized design, which combines Mean Absolute Error (MA-Error) loss, patch-wise normalization, and content-aware masking, achieves a Macro F1 of $0.688 \\pm 0.009$ (over 10 fine-tuning runs), outperforming a strong out-of-domain SSL baseline pre-trained on large-scale general audio, which has a Macro F1 of $0.663 \\pm 0.011$. The results show that MA-Error loss improves robustness and content-aware masking boosts performance by emphasizing information-rich regions. These findings highlight the importance of component-level optimization in data-constrained medical applications that rely on audio data."}

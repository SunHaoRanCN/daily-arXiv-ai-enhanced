<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 10]
- [eess.AS](#eess.AS) [Total: 9]
- [cs.SD](#cs.SD) [Total: 8]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [Compressive Beam-Pattern-Aware Near-field Beam Training via Total Variation Denoising](https://arxiv.org/abs/2601.22243)
*Zijun Wang,Maria Nivetha A,Ye Hu,Rui Zhang*

Main category: eess.SP

TL;DR: 提出一种用于多径近场波束训练的波束模式保持方案，结合LASSO和总变差去噪，无需近场码本设计，在相同导频预算下优于最小二乘和LASSO方法。


<details>
  <summary>Details</summary>
Motivation: 6G超大天线阵列引入近场效应，传统DFT码本在近场响应中产生连续的平台状能量分布而非孤立波束，纯LASSO去噪会过度收缩幅度并破坏平台结构。

Method: 提出波束模式保持的波束训练方案：先用LASSO抑制小幅度噪声，再用总变差(TV)保持平台水平和边缘锐度，两个近端步骤无需近场码本设计。

Result: 使用高斯导频的仿真显示，在相同导频预算下，该方法在NMSE和余弦相似度方面持续优于最小二乘和LASSO方法。

Conclusion: 提出的LASSO+TV组合方案能有效处理近场波束训练中的平台状能量分布问题，无需复杂近场码本设计，在性能上优于传统方法。

Abstract: Extremely large antenna arrays envisioned for 6G incurs near-field effect, where steering vector depends on angles and range simultaneously. Polar-domain near-field codebooks can focus energy accurately but incur extra two-dimensional sweeping overhead; compressed-sensing (CS) approaches with Gaussian-masked DFT sensing offer a lower-overhead alternative. This letter revisits near-field beam training using conventional DFT codebooks. Unlike far-field responses that concentrate energy on a few isolated DFT beams, near-field responses produce contiguous, plateau-like energy segments with sharp transitions in the DFT beamspace. Pure LASSO denoising, therefore, tends to over-shrink magnitudes and fragment plateaus. We propose a beam-pattern-preserving beam training scheme for multiple-path scenarios that combines LASSO with a lightweight denoising pipeline: LASSO to suppress small-amplitude noise, followed by total variation (TV) to maintain plateau levels and edge sharpness. The two proximal steps require no near-field codebook design. Simulations with Gaussian pilots show consistent NMSE and cosine-similarity gains over least squares and LASSO at the same pilot budget.

</details>


### [2] [Dual-Diode Unified SWIPT for High Data Rates with Adaptive Detection](https://arxiv.org/abs/2601.22270)
*Zulqarnain Bin Ashraf,Triantafyllos Mavrovoltsos,Constantinos Psomas,Ioannis Krikidis,Besma Smida*

Main category: eess.SP

TL;DR: 提出一个考虑二极管非线性和电容器记忆效应的U-SWIPT接收器统一瞬态模型，并设计低复杂度自适应检测器，在记忆主导场景下接近MLSD性能


<details>
  <summary>Details</summary>
Motivation: U-SWIPT接收器具有低复杂度和高能效，适合低功耗物联网应用。现有模型未能充分考虑实际工作条件中的二极管非线性和电容器引起的记忆效应，需要更准确的建模框架

Method: 提出双二极管U-SWIPT的统一瞬态框架，联合考虑二极管非线性和电容器记忆效应。基于该记忆感知模型，设计低复杂度自适应检测器，学习非线性状态转移动态，执行决策导向检测，具有线性复杂度

Result: 提出的模型准确描述了整流器固有的时间依赖性，揭示了其对能量收集和信息解码过程的基本影响。自适应检测器在记忆主导场景下接近最大似然序列检测性能，避免了经典序列检测的指数搜索

Conclusion: 正确利用整流器记忆效应可以为U-SWIPT接收器提供更好的数据速率和可靠性之间的权衡，展示了记忆感知模型和检测方案的实际价值

Abstract: Due to their low-complexity and energy-efficiency, unified simultaneous wireless information and power transfer (U-SWIPT) receivers are especially suitable for low-power Internet of Things (IoT) applications. Towards accurately modeling practical operating conditions, in this study, we provide a unified transient framework for a dual-diode U-SWIPT that jointly accounts for diode nonlinearity and capacitor-induced memory effects. The proposed model accurately describes the inherent time dependence of the rectifier, highlighting its fundamental impact on both energy harvesting (EH) and information decoding (ID) processes. Based on the provided memory-aware model, we design a low-complexity adaptive detector that learns the nonlinear state transition dynamics and performs decision-directed detection with linear complexity. The proposed detection scheme approaches maximum likelihood sequence detection (MLSD) performance in memory-dominated regimes, while avoiding the exponential search required by classical sequence detection. Overall, these results demonstrate that properly exploiting rectifier memory provides a better tradeoff between data rate and reliability for U-SWIPT receivers.

</details>


### [3] [On the Optimality of Rate Balancing for Max-Min Fair Multicasting](https://arxiv.org/abs/2601.22415)
*Sadaf Syed,Wolfgang Utschick,Michael Joham*

Main category: eess.SP

TL;DR: 本文针对NP难的MMF多播问题，推导出最优解的闭式表达式，提出低复杂度算法，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 最大最小公平（MMF）多播问题已知是NP难的，现有方法计算复杂度高，需要寻找更高效的解决方案。

Method: 1. 理论分析推导MMF多播问题的最优解；2. 建立速率平衡与最优MMF多播解之间的等价关系；3. 基于理论洞察提出低复杂度闭式解算法。

Result: 仿真验证了理论分析的正确性，提出的算法在性能上优于现有最先进方法，同时计算效率更高。

Conclusion: 本文成功解决了MMF多播这一NP难问题，通过理论分析找到了最优解的闭式表达式，提出的低复杂度算法在性能和效率上都表现出色。

Abstract: The max-min fair (MMF) multicasting problem is known to be NP-hard. In this work, we analytically derive the optimal solution to this NP-hard problem and establish the equivalence between rate balancing and the optimal MMF multicasting solution under certain conditions. Based on this theoretical insight, we propose a low-complexity algorithm for MMF multicasting that yields closed-form solutions. Simulation results validate our analysis and demonstrate that the proposed algorithm outperforms the state-of-the-art methods while being computationally more efficient.

</details>


### [4] [Superimposed-Pilot OTFS Under Fractional Doppler: Modular End-to-End Learning](https://arxiv.org/abs/2601.22523)
*Yushi Lei,Yusha Liu,Guanghui Liu,Lei Wan,Kun Yang*

Main category: eess.SP

TL;DR: 提出基于模块化深度学习的端到端OTFS收发器框架，通过可训练的神经网络模块实现联合优化，显著提升高移动性场景下的通信性能。


<details>
  <summary>Details</summary>
Motivation: 传统OTFS收发器采用多个独立设计的信号处理模块，孤立优化限制了全局最优性能。需要一种能够联合优化各模块的端到端解决方案，以克服OFDM在高移动性场景下的性能下降问题。

Method: 提出模块化深度学习框架，包含可训练和可互换的神经网络模块：星座映射/解映射、叠加导频放置、逆Zak/Zak变换、以及用于联合信道估计和检测的U-Net增强神经网络，并明确考虑循环前缀的影响。

Result: 仿真显示，所提设计在归一化均方误差和检测可靠性方面显著优于基线方法，在整数和分数多普勒条件下均保持鲁棒性。

Conclusion: 基于深度学习的端到端优化有望实现实用且高性能的OTFS收发器，为下一代高移动性网络提供解决方案。

Abstract: Orthogonal time frequency space (OTFS) modulation has emerged as a promising candidate to overcome the performance degradation of orthogonal frequency division multiplexing (OFDM), which are commonly encountered in high-mobility wireless communication scenarios. However, conventional OTFS transceivers rely on multiple separately designed signal-processing modules, whose isolated optimization often limits global optimal performance. To overcome limitations, this paper proposes a modular deep learning (DL) based end-to-end OTFS transceiver framework that consists of trainable and interchangeable neural network (NN) modules, including constellation mapping/demapping, superimposed pilot placement, inverse Zak (IZak)/Zak transforms, and a U-Net-enhanced NN tailored for joint channel estimation and detection (JCED), while explicitly accounting for the impact of the cyclic prefix. This physics-informed modular architecture provides flexibility for integration with conventional OTFS systems and adaptability to different communication configurations. Simulations demonstrate that the proposed design significantly outperforms baseline methods in terms of both normalized mean squared error (NMSE) and detection reliability, maintaining robustness under integer and fractional Doppler conditions. The results highlight the potential of DL-based end-to-end optimization to enable practical and high-performance OTFS transceivers for next-generation high-mobility networks.

</details>


### [5] [SORIS: A Self-Organized Reconfigurable Intelligent Surface Architecture for Wireless Communications](https://arxiv.org/abs/2601.22724)
*Evangelos Koutsonas,Alexandros-Apostolos A. Boulogeorgos,Stylianos E. Trevlakis,George C. Alexandropoulos,Theodoros A. Tsiftsis,Rui Zhang*

Main category: eess.SP

TL;DR: 提出一种自组织可重构智能表面(SORIS)架构，通过少量发射模式元件实现低延迟信道估计，结合机器学习预测其余反射元件的信道系数，无需与收发端单独连接即可获取信道状态信息并重新配置面板。


<details>
  <summary>Details</summary>
Motivation: 传统RIS需要与收发端单独连接获取信道状态信息，这增加了系统复杂性和延迟。本文旨在设计一种自组织的RIS架构，能够自主获取信道信息并重新配置，降低系统复杂度。

Method: 提出SORIS架构，包含连接单天线接收机的微控制器。选择少量RIS元件工作在发射模式，利用信道互易性估计信道系数；将这些估计值输入低复杂度神经网络，利用RIS元件间的空间信道相关性预测其余反射元件的信道系数。

Result: 通过蒙特卡洛仿真验证了SORIS设计的可行性和有效性，提供了选择发射模式RIS元件进行初始信道估计的实用指导。分析了信道估计影响、模型复杂度、布线密度和控制信令。

Conclusion: SORIS架构能够自主获取信道状态信息并重新配置RIS面板，无需与收发端单独连接，降低了系统复杂性和延迟，为自组织RIS设计提供了有效解决方案。

Abstract: In this paper, a new reconfigurable intelligent surface (RIS) hardware architecture, called self-organized RIS (SORIS), is proposed. The architecture incorporates a microcontroller connected to a single-antenna receiver operating at the same frequency as the RIS unit elements, operating either in transmission or reflection mode. The transmitting RIS elements enable the low latency estimation of both the incoming and outcoming channels at the microcontroller's side. In addition, a machine learning approach for estimating the incoming and outcoming channels involving the remaining RIS elements operating in reflection mode is devised. Specifically, by appropriately selecting a small number of elements in transmission mode, and based on the channel reciprocity principle, the respective channel coefficients are first estimated, which are then fed to a low-complexity neural network that, leveraging spatial channel correlation over RIS elements, returns predictions of the channel coefficients referring to the rest of elements. In this way, the SORIS microcontroller acquires channel state information, and accordingly reconfigures the panel's metamaterials to assist data communication between a transmitter and a receiver, without the need for separate connections with them. Moreover, the impact of channel estimation on the proposed solution, and a detailed complexity analysis for the used model, as well as a wiring density and control signaling analysis, is performed. The feasibility and efficacy of the proposed self-organized RIS design and operation are verified by Monte Carlo simulations, providing useful guidelines on the selection of the RIS elements for operating in transmission mode for initial channel estimation.

</details>


### [6] [Bayesian Matrix Completion Under Geometric Constraints](https://arxiv.org/abs/2601.22765)
*Rohit Varma Chiluvuri,Santosh Nannuru*

Main category: eess.SP

TL;DR: 提出了一种用于稀疏噪声欧几里得距离矩阵补全的分层贝叶斯框架，通过结构化先验直接建模潜在点集，相比传统方法在稀疏条件下表现更好。


<details>
  <summary>Details</summary>
Motivation: 欧几里得距离矩阵（EDM）从稀疏噪声观测中补全是一个基本挑战，传统方法（如秩约束优化和半定规划）虽然强制执行几何约束，但在稀疏或噪声条件下往往表现不佳。

Method: 引入分层贝叶斯框架，直接在生成EDM的潜在点集上放置结构化先验，自然嵌入几何约束。通过分层先验实现自动正则化和鲁棒噪声处理，使用Metropolis-Hastings within Gibbs采样器进行后验推断。

Result: 在合成数据实验中，相比确定性基线方法，在稀疏机制下显示出改进的重建精度。

Conclusion: 分层贝叶斯方法为稀疏噪声EDM补全提供了有效的解决方案，通过结构化先验自然嵌入几何约束，在稀疏条件下优于传统确定性方法。

Abstract: The completion of a Euclidean distance matrix (EDM) from sparse and noisy observations is a fundamental challenge in signal processing, with applications in sensor network localization, acoustic room reconstruction, molecular conformation, and manifold learning. Traditional approaches, such as rank-constrained optimization and semidefinite programming, enforce geometric constraints but often struggle under sparse or noisy conditions. This paper introduces a hierarchical Bayesian framework that places structured priors directly on the latent point set generating the EDM, naturally embedding geometric constraints. By incorporating a hierarchical prior on latent point set, the model enables automatic regularization and robust noise handling. Posterior inference is performed using a Metropolis-Hastings within Gibbs sampler to handle coupled latent point posterior. Experiments on synthetic data demonstrate improved reconstruction accuracy compared to deterministic baselines in sparse regimes.

</details>


### [7] [Intrinsic MIMO Particle Communication Channel with Random Advection](https://arxiv.org/abs/2601.22915)
*Fatih Merdan,Ozgur B. Akan*

Main category: eess.SP

TL;DR: 该论文研究了在对流主导的扩散-对流通道中的接收器分集，发现强定向流改变了分子通信系统的特性，使脉冲调制成为可能，并通过空间分布接收器实现分集增益。


<details>
  <summary>Details</summary>
Motivation: 研究对流主导环境中分子通信系统的接收器分集，因为强定向流改变了通信特性，使脉冲调制方案在分子通信中变得可行，而这是纯扩散环境中难以实现的。

Method: 研究单发射器和单类型信息分子的系统，分析空间分布接收器如何观测同一传输信号的不同实现，评估多种接收器组合策略，比较它们与单接收器操作的检测性能差异。

Result: 研究表明空间分布接收器能够获得分集增益，特别是在低到中等信噪比条件下，接收器组合策略显著改善了检测性能。对流成为实现可靠脉冲信号传输的关键因素。

Conclusion: 该研究为理解分子通信中的接收端分集提供了结构化框架，强调了对流在实现可靠脉冲信号传输中的关键作用，为未来研究高级调制、联合均衡检测和多分子MIMO扩展奠定了基础。

Abstract: In this work, receiver diversity in advection-dominated diffusion-advection channels is investigated. Strong directed flow fundamentally alters the communication-theoretic properties of molecular communication systems (MC). Specifically, advection preserves the temporal ordering and shape of transmitted pulses, enabling pulse-based and higher-order modulation schemes that are typically infeasible in purely diffusive environments. Focusing on a single transmitter and a single type of information molecule, it is demonstrated that spatially distributed receivers can observe distinct realizations of the same transmitted signal, giving rise to diversity gain. Several receiver combining strategies are evaluated and shown to improve detection performance compared to single-receiver operation, particularly in low-to-moderate signal-to-noise ratio (SNR) regimes. The results provide a structured framework for understanding receiver-side diversity in molecular communication, highlighting the role of advection as a key enabler for reliable pulse-based signaling. This perspective establishes a foundation for future studies on advanced modulation, joint equalization and detection, and multi-molecule MIMO extensions that can further enhance the performance and physical applicability of MC systems.

</details>


### [8] [Fluid Antenna Systems under Channel Uncertainty and Hardware Impairments: Trends, Challenges, and Future Research Directions](https://arxiv.org/abs/2601.22989)
*Saeid Pakravan,Mohsen Ahmadzadeh,Ming Zeng,Wessam Ajib,Ji Wang,Xingwang Li*

Main category: eess.SP

TL;DR: 本文对流体天线系统(FAS)在实际部署中的操作与设计进行了全面综述，重点分析了信道不确定性、硬件非理想性和机械约束等实际问题，并探讨了鲁棒设计和基于学习的控制策略。


<details>
  <summary>Details</summary>
Motivation: FAS作为B5G/6G网络中有前景的通信范式，能够在保持低硬件复杂度和功耗的同时提供空间分集增益。然而，实际部署中的信道不确定性、硬件非理想性和机械约束会显著偏离理想化分析假设，影响系统性能。

Method: 通过全面调查FAS在实际考虑下的操作与设计，包括：1）时空信道不确定性的表征；2）硬件和机械损伤分析（如RF非线性、端口耦合、流体响应延迟）；3）鲁棒设计和基于学习的控制策略探索。

Result: 系统分析了FAS在实际部署中面临的关键挑战，提出了增强系统可靠性的方法框架，为下一代无线网络中鲁棒、自适应和跨域FAS设计提供了理论基础。

Conclusion: FAS在B5G/6G网络中具有重要应用潜力，但需要解决实际部署中的挑战。通过鲁棒设计和智能控制策略，可以实现更可靠的FAS系统。本文为未来研究方向提供了指导，推动下一代无线网络的发展。

Abstract: Fluid antenna systems (FAS) have recently emerged as a promising paradigm for achieving spatially reconfigurable, compact, and energy-efficient wireless communications in beyond fifth-generation (B5G) and sixth-generation (6G) networks. By dynamically repositioning a liquid-based radiating element within a confined physical structure, FAS can exploit spatial diversity without relying on multiple fixed antenna elements. This spatial mobility provides a new degree of freedom for mitigating channel fading and interference, while maintaining low hardware complexity and power consumption. However, the performance of FAS in realistic deployments is strongly affected by channel uncertainty, hardware nonidealities, and mechanical constraints, all of which can substantially deviate from idealized analytical assumptions. This paper presents a comprehensive survey of the operation and design of FAS under such practical considerations. Key aspects include the characterization of spatio-temporal channel uncertainty, analysis of hardware and mechanical impairments such as RF nonlinearity, port coupling, and fluid response delay, as well as the exploration of robust design and learning-based control strategies to enhance system reliability. Finally, open research directions are identified, aiming to guide future developments toward robust, adaptive, and cross-domain FAS design for next-generation wireless networks.

</details>


### [9] [Learning-Based Signal Recovery in Nonlinear Systems with Spectrally Separated Interference](https://arxiv.org/abs/2601.23076)
*Jayadev Joy,Sundeep Rangan*

Main category: eess.SP

TL;DR: 提出LMLVAMP算法，利用神经网络去噪和频谱先验，解决6G FR3频段接收机中强邻带干扰和非线性失真问题


<details>
  <summary>Details</summary>
Motivation: 6G FR3频段（7-24 GHz）接收机在密集频谱环境中工作，易受强邻带干扰和前端非线性影响。传统线性接收机在理想硬件下能抑制频谱分离的干扰，但实际宽带无线电中接收机饱和和有限分辨率量化会导致非线性频谱泄漏，严重降低性能。

Method: 提出学习型多层向量近似消息传递（LMLVAMP）算法，将接收机前端建模为平滑无记忆非线性，后接加性噪声和可选量化。该算法结合频谱先验和基于神经网络的去噪，以减轻非线性和量化引起的失真。

Result: 仿真结果显示，在代表FR3共存场景的高干扰环境下，相比传统方法有显著性能提升。

Conclusion: LMLVAMP算法能有效处理6G FR3频段接收机中的非线性失真和强干扰问题，为实际宽带无线电系统提供了有前景的解决方案。

Abstract: Upper Mid-Band (FR3, 7-24 GHz) receivers for 6G must operate over wide bandwidths in dense spectral environments, making them particularly vulnerable to strong adjacent-band interference and front-end nonlinearities. While conventional linear receivers can suppress spectrally separated interferers under ideal hardware assumptions, receiver saturation and finite-resolution quantization cause nonlinear spectral leakage that severely degrades performance in practical wideband radios. We study the recovery of a desired signal from nonlinear receiver observations corrupted by a high-power out-of-band interferer. The receiver front-end is modeled as a smooth, memoryless nonlinearity followed by additive noise and optional quantization. To mitigate these nonlinear and quantization-induced distortions, we propose a learned multi-layer Vector Approximate Message Passing (LMLVAMP) algorithm that incorporates spectral priors with neural network based denoising. Simulation results demonstrate significant performance gains over conventional methods, particularly in high-interference regimes representative of FR3 coexistence scenarios.

</details>


### [10] [Interpolation Techniques for Fast Channel Estimation in Ray Tracing](https://arxiv.org/abs/2601.23119)
*Ruibin Chen,Jayadev Joy,Yaqi Hu,Mingsheng Yin,Marco Mezzavilla,Sundeep Rangan*

Main category: eess.SP

TL;DR: 提出一种基于参考点粗粒度射线追踪和反射点图像插值的方法，降低无线信道仿真的计算复杂度


<details>
  <summary>Details</summary>
Motivation: 在复杂环境的大规模无线系统仿真中，高分辨率射线追踪计算成本过高，需要降低计算复杂度

Method: 在粗粒度参考点进行射线追踪，通过插值反射点图像来估计其他位置的信道

Result: 通过经验验证和与穷举射线追踪对比，证明该方法能以较少计算资源实现高保真信道预测

Conclusion: 该方法不仅节省计算资源，还能直接捕捉波前的球面特性，支持视距MIMO等宽孔径技术的快速准确信道计算

Abstract: Ray tracing is increasingly utilized in wireless system simulations to estimate channel paths. In large-scale simulations with complex environments, ray tracing at high resolution can be computationally demanding. To reduce the computation, this paper presents a novel method for conducting ray tracing at a coarse set of reference points and interpolating the channels at other locations. The key insight is to interpolate the images of reflected points. In addition to the computational savings, the method directly captures the spherical nature of each wavefront enabling fast and accurate computation of channels using line-of-sight MIMO and other wide aperture techniques. Through empirical validation and comparison with exhaustive ray tracing, we demonstrate the efficacy and practicality of our approach in achieving high-fidelity channel predictions with reduced computational resources.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [11] [Brain-Informed Speech Separation for Cochlear Implants](https://arxiv.org/abs/2601.22260)
*Tom Gajecki,Jonas Althoff,Waldo Nogueira*

Main category: eess.AS

TL;DR: 提出了一种用于人工耳蜗的脑信息语音分离方法，利用EEG注意力线索引导增强朝向被注意的说话者，解决了音频分离器的标签置换歧义问题。


<details>
  <summary>Details</summary>
Motivation: 人工耳蜗在多人说话环境中性能受限，需要更智能的语音分离方法。结合脑电图注意力线索可以引导系统专注于用户关注的说话者，提高语音理解能力。

Method: 使用注意力引导网络，通过轻量级融合层将音频混合信号与EEG特征融合，生成被注意源的电极图用于CI刺激。采用混合课程训练策略提高对退化注意力线索的鲁棒性。

Result: 在多人说话条件下，相比纯音频电极图基线，模型获得了更高的信干比改进（167k vs. 171k参数），具有2ms算法延迟和可比较的计算成本。

Conclusion: 该方法展示了结合听觉和神经线索进行认知自适应CI处理的前景，能够在EEG-语音相关性中等的情况下仍保持稳定增益。

Abstract: We propose a brain-informed speech separation method for cochlear implants (CIs) that uses electroencephalography (EEG)-derived attention cues to guide enhancement toward the attended speaker. An attention-guided network fuses audio mixtures with EEG features through a lightweight fusion layer, producing attended-source electrodograms for CI stimulation while resolving the label-permutation ambiguity of audio-only separators. Robustness to degraded attention cues is improved with a mixed curriculum that varies cue quality during training, yielding stable gains even when EEG-speech correlation is moderate. In multi-talker conditions, the model achieves higher signal-to-interference ratio improvements than an audio-only electrodogram baseline while remaining slightly smaller (167k vs. 171k parameters). With 2 ms algorithmic latency and comparable cost, the approach highlights the promise of coupling auditory and neural cues for cognitively adaptive CI processing.

</details>


### [12] [Sylber 2.0: A Universal Syllable Embedding](https://arxiv.org/abs/2601.22306)
*Cheol Jun Cho,Nicholas Lee,Alan W Black,Gopala K. Anumanchipalli*

Main category: eess.AS

TL;DR: Sylber 2.0是一个自监督的音节级语音编码框架，实现了约5Hz的低token频率，在多语言和表达风格下保持语言学和声学细节，性能与高频基线模型相当。


<details>
  <summary>Details</summary>
Motivation: 现有音节级语音模型局限于英语且无法捕捉足够的声学细节，需要既高效又通用的语音token来扩展口语建模。

Method: 提出了Sylber 2.0自监督框架，在音节级别编码语音，实现高效时间压缩和高保真重建。

Result: 达到约5Hz的低token频率，在多语言和表达风格下保持语言学和声学细节，性能与高频基线模型相当；仅用7200万参数就能生成具有竞争力的TTS；为低资源ASR提供更有效的特征。

Conclusion: 为通用口语建立了有效的音节级抽象表示。

Abstract: Scaling spoken language modeling requires speech tokens that are both efficient and universal. Recent work has proposed syllables as promising speech tokens at low temporal resolution, but existing models are constrained to English and fail to capture sufficient acoustic detail. To address this gap, we present Sylber 2.0, a self-supervised framework for coding speech at the syllable level that enables efficient temporal compression and high-fidelity reconstruction. Sylber 2.0 achieves a very low token frequency around 5 Hz, while retaining both linguistic and acoustic detail across multiple languages and expressive styles. Experiments show that it performs on par with previous models operating on high-frequency baselines. Furthermore, Sylber 2.0 enables efficient TTS modeling which can generate speech with competitive intelligibility and quality with SOTA models using only 72M parameters. Moreover, the universality of Sylber 2.0 provides more effective features for low resource ASR than previous speech coding frameworks. In sum, we establish an effective syllable-level abstraction for general spoken language.

</details>


### [13] [Optimizing Domain-Adaptive Self-Supervised Learning for Clinical Voice-Based Disease Classification](https://arxiv.org/abs/2601.22319)
*Weixin Liu,Bowen Qu,Matthew Pontell,Maria Powell,Bradley Malin,Zhijun Yin*

Main category: eess.AS

TL;DR: 本文针对语音健康分析中数据稀缺和领域不匹配问题，研究了基于MAE的自监督学习领域自适应方法，通过优化重建损失、归一化和掩码策略，在病理语音数据集上取得了优于通用音频预训练基线的性能。


<details>
  <summary>Details</summary>
Motivation: 人类语音是一种有前景的非侵入式数字生物标志物，但深度学习在语音健康分析中面临数据稀缺和领域不匹配的挑战。通用音频预训练模型难以捕捉临床语音数据中细微的病理特征，需要专门的自监督学习领域自适应方法。

Method: 使用Bridge2AI-Voice病理语音数据集，系统研究三个关键因素：重建损失（MAE vs. MSE）、归一化（patch-wise vs. global）和掩码策略（随机 vs. 内容感知）。提出优化设计：结合MA-Error损失、patch-wise归一化和内容感知掩码的MAE自监督学习方法。

Result: 优化设计在10次微调运行中达到Macro F1为0.688±0.009，优于在大型通用音频上预训练的强基线（Macro F1为0.663±0.011）。MA-Error损失提高了鲁棒性，内容感知掩码通过强调信息丰富区域提升了性能。

Conclusion: 研究结果表明，在数据受限的医疗音频应用中，组件级优化至关重要。优化的MAE自监督学习方法能有效解决语音健康分析中的领域不匹配问题，为病理语音分析提供了有效的解决方案。

Abstract: The human voice is a promising non-invasive digital biomarker, yet deep learning for voice-based health analysis is hindered by data scarcity and domain mismatch, where models pre-trained on general audio fail to capture the subtle pathological features characteristic of clinical voice data. To address these challenges, we investigate domain-adaptive self-supervised learning (SSL) with Masked Autoencoders (MAE) and demonstrate that standard configurations are suboptimal for health-related audio. Using the Bridge2AI-Voice dataset, a multi-institutional collection of pathological voices, we systematically examine three performance-critical factors: reconstruction loss (Mean Absolute Error vs. Mean Squared Error), normalization (patch-wise vs. global), and masking (random vs. content-aware). Our optimized design, which combines Mean Absolute Error (MA-Error) loss, patch-wise normalization, and content-aware masking, achieves a Macro F1 of $0.688 \pm 0.009$ (over 10 fine-tuning runs), outperforming a strong out-of-domain SSL baseline pre-trained on large-scale general audio, which has a Macro F1 of $0.663 \pm 0.011$. The results show that MA-Error loss improves robustness and content-aware masking boosts performance by emphasizing information-rich regions. These findings highlight the importance of component-level optimization in data-constrained medical applications that rely on audio data.

</details>


### [14] [Class-Aware Permutation-Invariant Signal-to-Distortion Ratio for Semantic Segmentation of Sound Scene with Same-Class Sources](https://arxiv.org/abs/2601.22504)
*Binh Thien Nguyen,Masahiro Yasuda,Daiki Takeuchi,Daisuke Niizumi,Noboru Harada*

Main category: eess.AS

TL;DR: 本文针对DCASE 2025挑战赛Task 4中的空间语义分割任务，解决了现实场景中同类别声源重复出现的问题，提出了处理重复标签的类感知置换不变损失函数，并重新设计了评估指标。


<details>
  <summary>Details</summary>
Motivation: DCASE 2025挑战赛Task 4简化了任务，假设每个混合音频中的类别标签是互斥的，但现实世界中的音频混合经常包含来自同一类别的多个声源。重复标签的存在会显著降低标签查询源分离模型的性能，并限制官方评估指标的有效性。

Method: 1. 提出了类感知置换不变损失函数，使标签查询源分离模型能够处理涉及重复标签的查询；2. 重新设计了S5评估指标，消除同类别声源引起的歧义；3. 扩展了标签预测模型以支持同类别标签。

Result: 实验结果表明，所提方法在处理包含和不包含同类别声源的混合音频时都有效，新评估指标在这些场景下具有鲁棒性。

Conclusion: 本文解决了现实音频场景中同类别声源重复出现的关键问题，提出的方法增强了S5系统的实用性和评估的准确性，为沉浸式通信中的空间语义分割任务提供了更可靠的解决方案。

Abstract: To advance immersive communication, the Detection and Classification of Acoustic Scenes and Events (DCASE) 2025 Challenge recently introduced Task 4 on Spatial Semantic Segmentation of Sound Scenes (S5). An S5 system takes a multi-channel audio mixture as input and outputs single-channel dry sources along with their corresponding class labels. Although the DCASE 2025 Challenge simplifies the task by constraining class labels in each mixture to be mutually exclusive, real-world mixtures frequently contain multiple sources from the same class. The presence of duplicated labels can significantly degrade the performance of the label-queried source separation (LQSS) model, which is the key component of many existing S5 systems, and can also limit the validity of the official evaluation metric of DCASE 2025 Task 4. To address these issues, we propose a class-aware permutation-invariant loss function that enables the LQSS model to handle queries involving duplicated labels. In addition, we redesign the S5 evaluation metric to eliminate ambiguities caused by these same-class sources. To evaluate the proposed method within the S5 system, we extend the label prediction model to support same-class labels. Experimental results demonstrate the effectiveness of the proposed methods and the robustness of the new metric on mixtures both with and without same-class sources.

</details>


### [15] [Streaming Speech Recognition with Decoder-Only Large Language Models and Latency Optimization](https://arxiv.org/abs/2601.22779)
*Genshun Wan,Wenhui Zhang,Jing-Xuan Zhang,Shifu Xiong,Jianqing Gao,Zhongfu Ye*

Main category: eess.AS

TL;DR: 提出了一种基于解码器大语言模型的流式语音识别方法，通过读写策略网络和单调分块注意力动态分割语音嵌入，实现低延迟的流式识别。


<details>
  <summary>Details</summary>
Motivation: 尽管解码器大语言模型在语音识别方面展现出潜力，但实现流式识别仍然是一个挑战。现有方法难以在保持识别准确性的同时实现低延迟的流式处理。

Method: 1. 集成读写策略网络和单调分块注意力动态分割语音嵌入；2. 训练时将语音段与标签序列交错排列；3. 推理时音频流缓冲直到MoChA触发读取信号；4. 引入最小延迟训练目标指导策略网络；5. 采用联合训练策略，非流式和流式模型共享参数。

Result: 在AISHELL-1和AISHELL-2普通话基准测试中，分别达到5.1%和5.5%的字错误率，优于现有流式ASR基线。延迟优化使平均token生成延迟减少62.5%，对识别准确率影响可忽略。

Conclusion: 该方法成功实现了基于大语言模型的高效流式语音识别，在保持识别准确性的同时显著降低了延迟，为LLM在实时语音识别应用中的部署提供了有效解决方案。

Abstract: Recent advances have demonstrated the potential of decoderonly large language models (LLMs) for automatic speech recognition (ASR). However, enabling streaming recognition within this framework remains a challenge. In this work, we propose a novel streaming ASR approach that integrates a read/write policy network with monotonic chunkwise attention (MoChA) to dynamically segment speech embeddings. These segments are interleaved with label sequences during training, enabling seamless integration with the LLM. During inference, the audio stream is buffered until the MoChA module triggers a read signal, at which point the buffered segment together with the previous token is fed into the LLM for the next token prediction. We also introduce a minimal-latency training objective to guide the policy network toward accurate segmentation boundaries. Furthermore, we adopt a joint training strategy in which a non-streaming LLM-ASR model and our streaming model share parameters. Experiments on the AISHELL-1 and AISHELL-2 Mandarin benchmarks demonstrate that our method consistently outperforms recent streaming ASR baselines, achieving character error rates of 5.1% and 5.5%, respectively. The latency optimization results in a 62.5% reduction in average token generation delay with negligible impact on recognition accuracy

</details>


### [16] [CALM: Joint Contextual Acoustic-Linguistic Modeling for Personalization of Multi-Speaker ASR](https://arxiv.org/abs/2601.22792)
*Muhammad Shakeel,Yosuke Fukumoto,Chikara Maeda,Chyi-Jiunn Lin,Shinji Watanabe*

Main category: eess.AS

TL;DR: CALM是一个联合上下文声学-语言建模框架，用于多说话人自动语音识别，通过说话人嵌入驱动的目标说话人提取和基于动态词汇的上下文偏置，显著降低重叠对话中的偏置词错误率。


<details>
  <summary>Details</summary>
Motivation: 在个性化AI场景中，声学和语言线索的联合可用性自然促使将目标说话人条件化与重叠对话中的上下文偏置相结合，以提高多说话人ASR性能。

Method: CALM采用端到端框架，通过说话人嵌入驱动的目标说话人提取和动态词汇基础的上下文偏置，实现声学-语言联合建模。

Result: 在双说话人混合语音上，CALM将LibriSpeech2Mix的偏置词错误率从12.7降至4.7，将CSJMix2的偏置字符错误率从16.6降至8.4，在AMI语料库上也验证了标准化语音混合的性能。

Conclusion: CALM框架通过联合声学-语言建模，在多说话人ASR中有效整合目标说话人条件化和上下文偏置，在不同语言上均表现出显著性能提升。

Abstract: We present CALM, a joint Contextual Acoustic-Linguistic Modeling framework for multi-speaker automatic speech recognition (ASR). In personalized AI scenarios, the joint availability of acoustic and linguistic cues naturally motivates the integration of target-speaker conditioning with contextual biasing in overlapping conversations. CALM implements this integration in an end-to-end framework through speaker embedding-driven target-speaker extraction and dynamic vocabulary-based contextual biasing. We evaluate CALM on simulated English (LibriSpeechMix) and Japanese (Corpus of Spontaneous Japanese mixtures, CSJMix). On two-speaker mixtures, CALM reduces biased word error rate (B-WER) from 12.7 to 4.7 on LibriSpeech2Mix and biased character error rate (B-CER) from 16.6 to 8.4 on CSJMix2 (eval3), demonstrating the effectiveness of joint acoustic-linguistic modeling across languages. We additionally report results on the AMI corpus (IHM-mix condition) to validate performance on standardized speech mixtures.

</details>


### [17] [EmoShift: Lightweight Activation Steering for Enhanced Emotion-Aware Speech Synthesis](https://arxiv.org/abs/2601.22873)
*Li Zhou,Hao Jiang,Junjie Li,Tianrui Wang,Haizhou Li*

Main category: eess.AS

TL;DR: EmoShift：轻量级激活导向框架，通过EmoSteer层学习情感导向向量，在TTS中实现精确可控的情感表达，仅需1000万可训练参数。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的情感感知TTS系统依赖固定情感嵌入或外部指导，难以建模情感特定的潜在特征，限制了精确可控的情感表达能力。

Method: 提出EmoShift框架，包含EmoSteer层，在输出嵌入空间中为每个目标情感学习导向向量，捕捉其潜在偏移，保持跨话语和类别的稳定适当表达。

Result: 仅1000万可训练参数（少于全微调的1/30），在客观和主观评估中优于零样本和全微调基线，增强情感表现力同时保持自然度和说话人相似性。

Conclusion: EmoShift框架有效解决了情感TTS中的潜在特征建模问题，EmoSteer层展现出可控情感强度的潜力，为轻量级情感可控语音合成提供了新方向。

Abstract: Achieving precise and controllable emotional expression is crucial for producing natural and context-appropriate speech in text-to-speech (TTS) synthesis. However, many emotion-aware TTS systems, including large language model (LLM)-based designs, rely on scaling fixed emotion embeddings or external guidance, limiting their ability to model emotion-specific latent characteristics. To address this gap, we present EmoShift, a lightweight activation-steering framework incorporating a EmoSteer layer, which learns a steering vector for each target emotion in the output embedding space to capture its latent offset and maintain stable, appropriate expression across utterances and categories. With only 10M trainable parameters,less than 1/30 of full fine-tuning, EmoShift outperforms zero-shot and fully fine-tuned baselines in objective and subjective evaluations, enhancing emotional expressiveness while preserving naturalness and speaker similarity. Further analysis confirms the proposed EmoSteer layer's effectiveness and reveals its potential for controllable emotional intensity in speech synthesis.

</details>


### [18] [Layer-Aware Early Fusion of Acoustic and Linguistic Embeddings for Cognitive Status Classification](https://arxiv.org/abs/2601.23004)
*Krystof Novotny,Laureano Moro-Velázquez,Jiri Mekyska*

Main category: eess.AS

TL;DR: 研究探索了语音和文本嵌入的早期融合（EF）如何通过关注编码器层深度来改善认知状态分类，发现中层编码器层（约8-10层）性能最佳，声学模型优于文本模型，EF提升声学嵌入的区分能力，LF改善概率校准。


<details>
  <summary>Details</summary>
Motivation: 语音包含反映认知衰退的声学和语言模式，仅描述单一领域的模型无法完全捕捉这种复杂性。本研究旨在探索语音及其对应转录文本嵌入的早期融合，通过关注编码器层深度来改善认知状态分类。

Method: 使用DementiaBank数据集（1,629名说话者，包括认知正常对照组、轻度认知障碍和阿尔茨海默病及相关痴呆症），从wav2vec 2.0或Whisper的不同内部层提取帧对齐嵌入，与DistilBERT或RoBERTa结合。训练单模态、早期融合和晚期融合模型，使用transformer分类器进行优化，并在10个随机种子下评估。

Result: 性能在中层编码器层（约8-10层）达到峰值，最佳F1分数在Whisper + RoBERTa第9层，最佳对数损失在Whisper + DistilBERT第10层。声学模型始终优于文本模型。早期融合提升声学嵌入的区分能力，而晚期融合改善概率校准。

Conclusion: 编码器层选择对临床多模态协同效应至关重要。早期融合能有效提升声学嵌入的认知状态分类性能，而晚期融合则能改善模型的概率校准能力，为认知衰退评估提供了有效的多模态融合策略。

Abstract: Speech contains both acoustic and linguistic patterns that reflect cognitive decline, and therefore models describing only one domain cannot fully capture such complexity. This study investigates how early fusion (EF) of speech and its corresponding transcription text embeddings, with attention to encoder layer depth, can improve cognitive status classification. Using a DementiaBank-derived collection of recordings (1,629 speakers; cognitively normal controls$\unicode{x2013}$CN, Mild Cognitive Impairment$\unicode{x2013}$MCI, and Alzheimer's Disease and Related Dementias$\unicode{x2013}$ADRD), we extracted frame-aligned embeddings from different internal layers of wav2vec 2.0 or Whisper combined with DistilBERT or RoBERTa. Unimodal, EF and late fusion (LF) models were trained with a transformer classifier, optimized, and then evaluated across 10 seeds. Performance consistently peaked in mid encoder layers ($\sim$8$\unicode{x2013}$10), with the single best F1 at Whisper + RoBERTa layer 9 and the best log loss at Whisper + DistilBERT layer 10. Acoustic-only models consistently outperformed text-only variants. EF boosts discrimination for genuinely acoustic embeddings, whereas LF improves probability calibration. Layer choice critically shapes clinical multimodal synergy.

</details>


### [19] [Beyond Omnidirectional: Neural Ambisonics Encoding for Arbitrary Microphone Directivity Patterns using Cross-Attention](https://arxiv.org/abs/2601.23196)
*Mikko Heikkinen,Archontis Politis,Konstantinos Drossos,Tuomas Virtanen*

Main category: eess.AS

TL;DR: 提出一种深度神经网络方法，将任意麦克风阵列信号编码为Ambisonics，适用于固定麦克风数量但位置和频率相关指向性特征可变的阵列配置。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅依赖阵列几何作为元数据，无法准确表征真实世界阵列的复杂特性。需要一种能处理任意阵列配置、考虑实际阵列频率相关指向性特征的方法。

Method: 使用方向性阵列传递函数作为元数据，而非仅用几何信息。架构包含音频编码器和方向响应编码器，通过交叉注意力机制结合，生成与阵列无关的空间音频表示。

Result: 在两种模拟设置（具有复杂机身散射的手机阵列和自由场条件）中评估，在混响环境中处理不同数量的声源。方法优于传统DSP方法和现有深度神经网络方案，且使用阵列传递函数比几何元数据在真实阵列上精度更高。

Conclusion: 提出的深度神经网络方法能有效处理任意麦克风阵列配置，使用阵列传递函数作为元数据能更准确表征真实阵列特性，在空间音频编码任务中表现优异。

Abstract: We present a deep neural network approach for encoding microphone array signals into Ambisonics that generalizes to arbitrary microphone array configurations with fixed microphone count but varying locations and frequency-dependent directional characteristics. Unlike previous methods that rely only on array geometry as metadata, our approach uses directional array transfer functions, enabling accurate characterization of real-world arrays. The proposed architecture employs separate encoders for audio and directional responses, combining them through cross-attention mechanisms to generate array-independent spatial audio representations. We evaluate the method on simulated data in two settings: a mobile phone with complex body scattering, and a free-field condition, both with varying numbers of sound sources in reverberant environments. Evaluations demonstrate that our approach outperforms both conventional digital signal processing-based methods and existing deep neural network solutions. Furthermore, using array transfer functions instead of geometry as metadata input improves accuracy on realistic arrays.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [20] [An Effective Energy Mask-based Adversarial Evasion Attacks against Misclassification in Speaker Recognition Systems](https://arxiv.org/abs/2601.22390)
*Chanwoo Park,Chanwoo Kim*

Main category: cs.SD

TL;DR: 提出MEP方法，通过频域能量掩蔽生成对抗扰动，在保持音频质量的同时有效规避说话人识别系统


<details>
  <summary>Details</summary>
Motivation: 语音数据（包括深度伪造）的滥用缺乏法律监管，对抗攻击成为有效对抗手段。现有方法在音频质量和规避效果之间存在权衡，需要更优方案。

Method: 提出掩蔽能量扰动（MEP）方法：在频域中对原始语音数据的低能量区域进行掩蔽，然后生成对抗扰动，针对人类听觉模型不敏感的区域进行攻击。

Result: MEP方法在音频质量和规避效果上表现优异，PESQ评估中相对性能达到26.68%（相比FGSM和迭代FGSM），表明对抗扰动引起的人类感知失真最小。

Conclusion: MEP方法通过频域能量掩蔽策略，在保持良好音频质量的同时实现了有效的对抗攻击，为对抗语音滥用提供了新的技术方案。

Abstract: Evasion attacks pose significant threats to AI systems, exploiting vulnerabilities in machine learning models to bypass detection mechanisms. The widespread use of voice data, including deepfakes, in promising future industries is currently hindered by insufficient legal frameworks. Adversarial attack methods have emerged as the most effective countermeasure against the indiscriminate use of such data. This research introduces masked energy perturbation (MEP), a novel approach using power spectrum for energy masking of original voice data. MEP applies masking to small energy regions in the frequency domain before generating adversarial perturbations, targeting areas less noticeable to the human auditory model. The study primarily employs advanced speaker recognition models, including ECAPA-TDNN and ResNet34, which have shown remarkable performance in speaker verification tasks. The proposed MEP method demonstrated strong performance in both audio quality and evasion effectiveness. The energy masking approach effectively minimizes the perceptual evaluation of speech quality (PESQ) degradation, indicating that minimal perceptual distortion occurs to the human listener despite the adversarial perturbations. Specifically, in the PESQ evaluation, the relative performance of the MEP method was 26.68% when compared to the fast gradient sign method (FGSM) and iterative FGSM.

</details>


### [21] [Rethinking Speech Representation Aggregation in Speech Enhancement: A Phonetic Mutual Information Perspective](https://arxiv.org/abs/2601.22480)
*Seungu Han,Sungho Lee,Kyogu Lee*

Main category: cs.SD

TL;DR: 提出一种解耦的语音增强方法，通过预训练语言聚合层来最大化与音素标签的互信息，然后在SE训练中冻结该层，以更好地保留语义信息。


<details>
  <summary>Details</summary>
Motivation: 当前语音增强模型使用自监督学习表示，但这些表示在噪声环境下可能被破坏。同时，适应模块与SE模型联合训练时可能优先考虑声学细节而非语义信息，违背了使用SSL表示的初衷。

Method: 1. 从信息论角度分析SSL模型在噪声语音上的行为，测量SSL表示与音素标签的互信息；2. 提出语言聚合层，预训练以最大化与音素标签的互信息（可选动态聚合）；3. 在SE训练期间冻结该聚合层。

Result: 实验表明，这种解耦方法相比联合优化的基线模型，在词错误率（WER）上有所改善，证明了显式对齐适应模块与语言内容的好处。

Conclusion: 通过预训练语言聚合层并冻结使用，可以更好地保留SSL表示中的语义信息，从而提升语音增强系统的性能。

Abstract: Recent speech enhancement (SE) models increasingly leverage self-supervised learning (SSL) representations for their rich semantic information. Typically, intermediate features are aggregated into a single representation via a lightweight adaptation module. However, most SSL models are not trained for noise robustness, which can lead to corrupted semantic representations. Moreover, the adaptation module is trained jointly with the SE model, potentially prioritizing acoustic details over semantic information, contradicting the original purpose. To address this issue, we first analyze the behavior of SSL models on noisy speech from an information-theoretic perspective. Specifically, we measure the mutual information (MI) between the corrupted SSL representations and the corresponding phoneme labels, focusing on preservation of linguistic contents. Building upon this analysis, we introduce the linguistic aggregation layer, which is pre-trained to maximize MI with phoneme labels (with optional dynamic aggregation) and then frozen during SE training. Experiments show that this decoupled approach improves Word Error Rate (WER) over jointly optimized baselines, demonstrating the benefit of explicitly aligning the adaptation module with linguistic contents.

</details>


### [22] [A Semantically Consistent Dataset for Data-Efficient Query-Based Universal Sound Separation](https://arxiv.org/abs/2601.22599)
*Kai Li,Jintao Cheng,Chang Zeng,Zijun Yan,Helin Wang,Zixiong Su,Bo Zheng,Xiaolin Hu*

Main category: cs.SD

TL;DR: 提出Hive数据集，通过自动化流程从野外数据中挖掘高纯度单事件音频段，解决现有方法因数据瓶颈导致的残留干扰问题，显著提升数据效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于查询的通用声音分离方法在复杂声学场景中存在残留干扰问题，主要源于数据瓶颈：野外数据集包含弱标签和严重的事件共现，导致模型学习背景噪声与目标类别之间的虚假相关性而非鲁棒的声学特征。

Method: 提出自动化流程，通过语义一致的合成协议从野外数据集中挖掘高纯度单事件音频段，构建Hive高质量合成数据集（2.4k小时原始音频）。

Result: 在Hive上训练的某些开源模型，相比在比Hive大~500倍数据集上训练的SOTA模型SAM-Audio，实现了竞争性的分离准确性和感知质量，并在分布外评估基准上表现出显著的零样本泛化能力。

Conclusion: 优先考虑监督信号的纯度能够实现显著的数据效率，为以更低计算成本训练鲁棒的听觉基础模型提供了新范式。

Abstract: Query-based universal sound separation is fundamental to intelligent auditory systems, aiming to isolate specific sources from mixtures. Despite recent advances, existing methods continue to suffer from residual interference in complex acoustic scenes. This performance limitation stems largely from a data bottleneck: in-the-wild datasets contain weak labels and severe co-occurrence of events. These flaws induce models to learn spurious correlations between background noise and target categories instead of robust acoustic features. To address this, we propose an automated pipeline that eliminates co-occurrence of events by mining high-purity single-event segments from in-the-wild datasets via a semantically consistent synthesis protocol. Utilizing this pipeline, we constructed Hive, a high-quality synthetic dataset comprising 2.4k hours of raw audio. Experimental results demonstrate that, compared with the state-of-the-art model SAM-Audio which was trained on a huge dataset $\sim$500 times larger than Hive, certain open-source models trained on Hive achieve competitive separation accuracy and perceptual quality. Moreover, these models exhibited remarkable zero-shot generalization on out-of-distribution evaluation benchmarks. These findings highlight that prioritizing purity of supervised signals enables significant data efficiency, offering a new paradigm for training robust auditory foundation models with reduced computational costs. Code and dataset are available at https://shandaai.github.io/Hive.

</details>


### [23] [Evaluating and Rewarding LALMs for Expressive Role-Play TTS via Mean Continuation Log-Probability](https://arxiv.org/abs/2601.22661)
*Yong Ren,Jingbei Li,Haiyang Sun,Yujie Chen,Cheng Yi,Yechang Huang,Hao Gu,Ye Bai,Xuerui Yang*

Main category: cs.SD

TL;DR: 提出MCLP作为评估指标和奖励信号，用于提升角色扮演TTS中的风格一致性


<details>
  <summary>Details</summary>
Motivation: 现有大音频语言模型在角色扮演TTS中难以保持多轮对话的风格一致性，缺乏量化说话风格的客观指标

Method: 提出平均延续对数概率(MCLP)，利用预训练LALM的上下文学习能力，通过延续对数概率预测来量化风格一致性，并将其作为强化学习奖励信号

Result: 方法在客观和主观指标上显著优于强基线模型

Conclusion: MCLP能有效评估和提升角色扮演TTS的风格一致性，为交互式角色扮演场景提供了更好的解决方案

Abstract: Recent advances in Large Audio Language Models (LALMs) have extended Text-to-Speech (TTS) to interactive role-play scenarios, which demand high expressiveness and strict adherence to role-play instructions. However, existing models struggle to maintain stylistic consistency with character profiles and scene descriptions across multi-turn dialogues. A critical bottleneck is the lack of objective metrics for quantifying speaking style. To bridge this gap, we propose Mean Continuation Log-Probability (MCLP) as both an evaluation metric and a reward signal, validated on LALM-based Role-Play TTS (RP-TTS) tasks. Critically, we leverage the In-Context Learning capability of pre-trained LALMs to formulate MCLP via a continuation log-probability prediction. This metric quantifies stylistic consistency by measuring the likelihood of the ground-truth speech conditioned on the generated speech. Furthermore, we employ MCLP as a reinforcement learning reward to enhance the style alignment between generated speech and Role-Play instructions. To facilitate evaluation, we construct an RP-TTS dataset with rich scene and character annotations. Experimental results demonstrate that our method significantly outperforms strong LALM baselines on both objective and subjective metrics.

</details>


### [24] [How Far Can Pretrained LLMs Go in Symbolic Music? Controlled Comparisons of Supervised and Preference-based Adaptation](https://arxiv.org/abs/2601.22764)
*Deepak Kumar,Emmanouil Karystinaios,Gerhard Widmer,Markus Schedl*

Main category: cs.SD

TL;DR: 比较不同微调策略在符号音乐理解和生成任务上的效果，分析领域适应与保留预训练知识之间的权衡


<details>
  <summary>Details</summary>
Motivation: 音乐与语言存在显著相似性，促使研究者使用预训练大语言模型处理符号音乐任务，但指令微调LLM在符号音乐上的实际效果尚未充分研究

Method: 采用对照比较研究，对比现成的指令微调骨干模型、领域适应变体和音乐专用LLM基线，在多个符号音乐语料库和评估指标上进行评估

Result: 揭示了领域适应与保留先验信息之间的权衡关系，以及用于测量符号音乐领域适应的不同指标表现出不同的行为特征

Conclusion: 为符号音乐应用提供了关于适应策略选择的见解，强调了在领域适应过程中需要平衡专业化和通用知识保留

Abstract: Music often shares notable parallels with language, motivating the use of pretrained large language models (LLMs) for symbolic music understanding and generation. Despite growing interest, the practical effectiveness of adapting instruction-tuned LLMs to symbolic music remains insufficiently characterized. We present a controlled comparative study of finetuning strategies for ABC-based generation and understanding, comparing an off-the-shelf instruction-tuned backbone to domain-adapted variants and a music-specialized LLM baseline. Across multiple symbolic music corpora and evaluation signals, we provide some insights into adaptation choices for symbolic music applications. We highlight the domain adaptation vs.~preserving prior information tradeoff as well as the distinct behaviour of metrics used to measure the domain adaptation for symbolic music.

</details>


### [25] [Towards Explicit Acoustic Evidence Perception in Audio LLMs for Speech Deepfake Detection](https://arxiv.org/abs/2601.23066)
*Xiaoxuan Guo,Yuankun Xie,Haonan Cheng,Jiayi Zhou,Jian Liu,Hengyan Huang,Long Ye,Qin Zhang*

Main category: cs.SD

TL;DR: 提出SDD-APALLM框架，通过增强听觉感知来改进音频大语言模型在语音深度伪造检测中的性能，使模型能同时利用语义和声学线索，提高检测准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有基于音频大语言模型的语音深度伪造检测方法过于依赖语义线索，容易忽略细微的声学伪影，导致语义自然但包含声学异常的伪造语音能够绕过检测。问题的根源不是缺乏声学数据，而是在语义主导的推理过程中声学信息可访问性不足。

Method: 提出SDD-APALLM框架，结合原始音频和结构化频谱图，显式暴露细粒度时频证据作为可访问的声学线索。该框架增强音频大语言模型的听觉感知能力，使其在不损害语义理解的前提下更有效地捕捉细微声学不一致性。

Result: 实验结果显示检测准确性和鲁棒性持续提升，特别是在语义线索具有误导性的情况下。进一步分析表明，这些改进源于语义和声学信息的协调利用，而非简单的模态聚合。

Conclusion: 通过增强音频大语言模型的听觉感知能力，SDD-APALLM框架能够更有效地检测语音深度伪造，特别是在语义线索具有欺骗性的情况下。这表明协调利用语义和声学信息比简单的多模态融合更有效。

Abstract: Speech deepfake detection (SDD) focuses on identifying whether a given speech signal is genuine or has been synthetically generated. Existing audio large language model (LLM)-based methods excel in content understanding; however, their predictions are often biased toward semantically correlated cues, which results in fine-grained acoustic artifacts being overlooked during the decisionmaking process. Consequently, fake speech with natural semantics can bypass detectors despite harboring subtle acoustic anomalies; this suggests that the challenge stems not from the absence of acoustic data, but from its inadequate accessibility when semantic-dominant reasoning prevails. To address this issue, we investigate SDD within the audio LLM paradigm and introduce SDD with Auditory Perception-enhanced Audio Large Language Model (SDD-APALLM), an acoustically enhanced framework designed to explicitly expose fine-grained time-frequency evidence as accessible acoustic cues. By combining raw audio with structured spectrograms, the proposed framework empowers audio LLMs to more effectively capture subtle acoustic inconsistencies without compromising their semantic understanding. Experimental results indicate consistent gains in detection accuracy and robustness, especially in cases where semantic cues are misleading. Further analysis reveals that these improvements stem from a coordinated utilization of semantic and acoustic information, as opposed to simple modality aggregation.

</details>


### [26] [Hearing is Believing? Evaluating and Analyzing Audio Language Model Sycophancy with SYAUDIO](https://arxiv.org/abs/2601.23149)
*Junchi Yao,Lokranjan Lakshmikanthan,Annie Zhao,Danielle Zhao,Shu Yang,Zikang Ding,Di Wang,Lijie Hu*

Main category: cs.SD

TL;DR: SYAUDIO是首个评估音频语言模型中奉承行为的基准，包含4,319个音频问题，涵盖感知、推理、数学和伦理等领域，并研究了噪声和语速等真实条件下的奉承行为。


<details>
  <summary>Details</summary>
Motivation: 音频语言模型虽然展现出强大的多模态推理能力，但继承了大型语言模型中的奉承行为（倾向于同意用户观点而忽视客观证据）。目前奉承行为主要在文本和视觉语言模型中得到研究，而在音频条件下的表现尚未探索，而音频模型需要依赖声学事件、说话人特征和语速等听觉线索。

Method: 构建SYAUDIO基准，包含4,319个音频问题，涵盖音频感知、音频推理、音频数学和音频伦理四个领域。基于现有音频基准，并通过TTS生成的算术和道德推理任务进行增强。研究噪声和语速等真实条件下的音频特定奉承行为，并使用思维链数据进行监督微调作为缓解策略。

Result: SYAUDIO基准能够系统评估多个领域和奉承类型，数据质量经过仔细验证。研究表明，在涉及噪声和语速的现实条件下，音频特定奉承行为确实存在。使用思维链数据进行监督微调被证明是减少ALMs中奉承行为的有效缓解策略。

Conclusion: 该研究填补了音频语言模型中奉承行为评估的空白，提出的SYAUDIO基准为系统评估提供了工具，并展示了监督微调作为缓解策略的有效性，对开发更可靠、诚实的音频语言模型具有重要意义。

Abstract: Audio Language Models (ALMs) have recently shown strong capabilities in unified reasoning over speech, sound, and natural language; yet they inherit behavioral issues observed in Large Language Models, including sycophancy--the tendency to agree with user assertions even when they contradict objective evidence. While sycophancy has been extensively studied in text and vision-language models, its manifestation in audio-conditioned reasoning remains largely unexplored, despite the need for ALMs to rely on auditory cues such as acoustic events, speaker characteristics, and speech rate. To address this gap, we introduce SYAUDIO, the first benchmark dedicated to evaluating sycophancy in ALMs, consisting of 4,319 audio questions spanning Audio Perception, Audio Reasoning, Audio Math, and Audio Ethics. Built upon established audio benchmarks and augmented with TTS-generated arithmetic and moral reasoning tasks, SYAUDIO enables systematic evaluation across multiple domains and sycophancy types with carefully verified data quality. Furthermore, we analyze audio-specific sycophancy under realistic conditions involving noise and rate, and demonstrate that supervised fine-tuning with chain-of-thought data is an effective mitigation strategy for reducing sycophantic behavior in ALMs.

</details>


### [27] [DIFFA-2: A Practical Diffusion Large Language Model for General Audio Understanding](https://arxiv.org/abs/2601.23161)
*Jiaming Zhou,Xuxin Cheng,Shiwan Zhao,Yuhang Jia,Cao Liu,Ke Zeng,Xunliang Cai,Yong Qin*

Main category: cs.SD

TL;DR: DIFFA-2是一个实用的基于扩散的大型音频语言模型，用于通用音频理解，相比之前的DIFFA模型有显著改进，并在有限训练预算下与自回归模型竞争。


<details>
  <summary>Details</summary>
Motivation: 自回归大型音频语言模型（如Qwen-2.5-Omni）在音频理解和交互方面表现强劲，但扩展成本高（数据和计算），且严格顺序解码限制了推理效率。扩散大语言模型已被证明能有效利用有限训练数据，但之前的DIFFA模型仅停留在概念验证规模，缺乏大规模指令调优、偏好对齐或实用解码方案。

Method: 升级语音编码器，采用双语义和声学适配器，通过四阶段课程训练：结合语义和声学对齐、大规模监督微调、方差减少的偏好优化，仅使用完全开源语料库。

Result: 在MMSU、MMAU和MMAR基准测试中，DIFFA-2相比DIFFA持续改进，在实用训练预算下与强大的自回归音频语言模型竞争，证明基于扩散的建模是大型音频理解的可行骨干。

Conclusion: DIFFA-2展示了基于扩散的建模作为大规模音频理解骨干的可行性，代码已开源。

Abstract: Autoregressive (AR) large audio language models (LALMs) such as Qwen-2.5-Omni have achieved strong performance on audio understanding and interaction, but scaling them remains costly in data and computation, and strictly sequential decoding limits inference efficiency. Diffusion large language models (dLLMs) have recently been shown to make effective use of limited training data, and prior work on DIFFA indicates that replacing an AR backbone with a diffusion counterpart can substantially improve audio understanding under matched settings, albeit at a proof-of-concept scale without large-scale instruction tuning, preference alignment, or practical decoding schemes. We introduce DIFFA-2, a practical diffusion-based LALM for general audio understanding. DIFFA-2 upgrades the speech encoder, employs dual semantic and acoustic adapters, and is trained with a four-stage curriculum that combines semantic and acoustic alignment, large-scale supervised fine-tuning, and variance-reduced preference optimization, using only fully open-source corpora. Experiments on MMSU, MMAU, and MMAR show that DIFFA-2 consistently improves over DIFFA and is competitive to strong AR LALMs under practical training budgets, supporting diffusion-based modeling is a viable backbone for large-scale audio understanding. Our code is available at https://github.com/NKU-HLT/DIFFA.git.

</details>

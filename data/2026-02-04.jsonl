{"id": "2602.02591", "categories": ["cs.SD", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02591", "abs": "https://arxiv.org/abs/2602.02591", "authors": ["Chengyuan Ma", "Jiawei Jin", "Ruijie Xiong", "Chunxiang Jin", "Canxiang Yan", "Wenming Yang"], "title": "VividVoice: A Unified Framework for Scene-Aware Visually-Driven Speech Synthesis", "comment": "Accepted by ICASSP 2026", "summary": "We introduce and define a novel task-Scene-Aware Visually-Driven Speech Synthesis, aimed at addressing the limitations of existing speech generation models in creating immersive auditory experiences that align with the real physical world. To tackle the two core challenges of data scarcity and modality decoupling, we propose VividVoice, a unified generative framework. First, we constructed a large-scale, high-quality hybrid multimodal dataset, Vivid-210K, which, through an innovative programmatic pipeline, establishes a strong correlation between visual scenes, speaker identity, and audio for the first time. Second, we designed a core alignment module, D-MSVA, which leverages a decoupled memory bank architecture and a cross-modal hybrid supervision strategy to achieve fine-grained alignment from visual scenes to timbre and environmental acoustic features. Both subjective and objective experimental results provide strong evidence that VividVoice significantly outperforms existing baseline models in terms of audio fidelity, content clarity, and multimodal consistency. Our demo is available at https://chengyuann.github.io/VividVoice/."}
{"id": "2602.02738", "categories": ["cs.SD", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02738", "abs": "https://arxiv.org/abs/2602.02738", "authors": ["Xiaosha Li", "Chun Liu", "Ziyu Wang"], "title": "When Noise Lowers The Loss: Rethinking Likelihood-Based Evaluation in Music Large Language Models", "comment": "Accepted by IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP) 2026", "summary": "The rise of music large language models (LLMs) demands robust methods of evaluating output quality, especially in distinguishing high-quality compositions from \"garbage music\". Curiously, we observe that the standard cross-entropy loss -- a core training metric -- often decrease when models encounter systematically corrupted music, undermining its validity as a standalone quality indicator. To investigate this paradox, we introduce noise injection experiment, where controlled noise signal of varying lengths are injected into musical contexts. We hypothesize that a model's loss reacting positively to these perturbations, specifically a sharp increase (\"Peak\" area) for short injection, can serve as a proxy for its ability to discern musical integrity. Experiments with MusicGen models in the audio waveform domain confirm that Music LLMs respond more strongly to local, texture-level disruptions than to global semantic corruption. Beyond exposing this bias, our results highlight a new principle: the shape of the loss curve -- rather than its absolute value -- encodes critical information about the quality of the generated content (i.e., model behavior). We envision this profile-based evaluation as a label-free, model-intrinsic framework for assessing musical quality -- opening the door to more principled training objectives and sharper benchmarks."}
{"id": "2602.02955", "categories": ["cs.SD", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02955", "abs": "https://arxiv.org/abs/2602.02955", "authors": ["David McShannon", "Anthony Mella", "Nicholas Dietrich"], "title": "Synthetic Data Augmentation for Medical Audio Classification: A Preliminary Evaluation", "comment": "5 pages, 1 figure", "summary": "Medical audio classification remains challenging due to low signal-to-noise ratios, subtle discriminative features, and substantial intra-class variability, often compounded by class imbalance and limited training data. Synthetic data augmentation has been proposed as a potential strategy to mitigate these constraints; however, prior studies report inconsistent methodological approaches and mixed empirical results. In this preliminary study, we explore the impact of synthetic augmentation on respiratory sound classification using a baseline deep convolutional neural network trained on a moderately imbalanced dataset (73%:27%). Three generative augmentation strategies (variational autoencoders, generative adversarial networks, and diffusion models) were assessed under controlled experimental conditions. The baseline model without augmentation achieved an F1-score of 0.645. Across individual augmentation strategies, performance gains were not observed, with several configurations demonstrating neutral or degraded classification performance. Only an ensemble of augmented models yielded a modest improvement in F1-score (0.664). These findings suggest that, for medical audio classification, synthetic augmentation may not consistently enhance performance when applied to a standard CNN classifier. Future work should focus on delineating task-specific data characteristics, model-augmentation compatibility, and evaluation frameworks necessary for synthetic augmentation to be effective in medical audio applications."}
{"id": "2602.03023", "categories": ["cs.SD", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.03023", "abs": "https://arxiv.org/abs/2602.03023", "authors": ["Irmak Bukey", "Zhepei Wang", "Chris Donahue", "Nicholas J. Bryan"], "title": "Rethinking Music Captioning with Music Metadata LLMs", "comment": "Accepted to ICASSP 2026", "summary": "Music captioning, or the task of generating a natural language description of music, is useful for both music understanding and controllable music generation. Training captioning models, however, typically requires high-quality music caption data which is scarce compared to metadata (e.g., genre, mood, etc.). As a result, it is common to use large language models (LLMs) to synthesize captions from metadata to generate training data for captioning models, though this process imposes a fixed stylization and entangles factual information with natural language style. As a more direct approach, we propose metadata-based captioning. We train a metadata prediction model to infer detailed music metadata from audio and then convert it into expressive captions via pre-trained LLMs at inference time. Compared to a strong end-to-end baseline trained on LLM-generated captions derived from metadata, our method: (1) achieves comparable performance in less training time over end-to-end captioners, (2) offers flexibility to easily change stylization post-training, enabling output captions to be tailored to specific stylistic and quality requirements, and (3) can be prompted with audio and partial metadata to enable powerful metadata imputation or in-filling--a common task for organizing music data."}
{"id": "2602.02503", "categories": ["eess.SP", "cs.AI", "cs.IT"], "pdf": "https://arxiv.org/pdf/2602.02503", "abs": "https://arxiv.org/abs/2602.02503", "authors": ["Jincheng Xie", "Yili Deng", "Jiguang He", "Pengyu Wang", "Miaomiao Dong", "Rui Tang", "Zhongyi Huang"], "title": "Joint single-shot ToA and DoA estimation for VAA-based BLE ranging with phase ambiguity: A deep learning-based approach", "comment": null, "summary": "Conventional direction-of-arrival (DoA) estimation methods rely on multi-antenna arrays, which are costly to implement on size-constrained Bluetooth Low Energy (BLE) devices. Virtual antenna array (VAA) techniques enable DoA estimation with a single antenna, making angle estimation feasible on such devices. However, BLE only provides a single-shot two-way channel frequency response (CFR) with a binary phase ambiguity issue, which hinders the direct application of VAA. To address this challenge, we propose a unified model that combines VAA with BLE two-way CFR, and introduce a neural network based phase recovery framework that employs row / column predictors with a voting mechanism to resolve the ambiguity. The recovered one-way CFR then enables super resolution algorithms such as MUSIC for joint time of arrival (ToA) and DoA estimation. Simulation results demonstrate that the proposed method achieves superior performance under non-uniform VAAs, with mean square errors approaching the Cramer Rao bound at SNR $\\geq$ 5 dB."}
{"id": "2602.02734", "categories": ["eess.AS", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.02734", "abs": "https://arxiv.org/abs/2602.02734", "authors": ["Abdoulaye Diack", "Perry Nelson", "Kwaku Agbesi", "Angela Nakalembe", "MohamedElfatih MohamedKhair", "Vusumuzi Dube", "Tavonga Siyavora", "Subhashini Venugopalan", "Jason Hickey", "Uche Okonkwo", "Abhishek Bapna", "Isaac Wiafe", "Raynard Dodzi Helegah", "Elikem Doe Atsakpo", "Charles Nutrokpor", "Fiifi Baffoe Payin Winful", "Kafui Kwashie Solaga", "Jamal-Deen Abdulai", "Akon Obu Ekpezu", "Audace Niyonkuru", "Samuel Rutunda", "Boris Ishimwe", "Michael Melese", "Engineer Bainomugisha", "Joyce Nakatumba-Nabende", "Andrew Katumba", "Claire Babirye", "Jonathan Mukiibi", "Vincent Kimani", "Samuel Kibacia", "James Maina", "Fridah Emmah", "Ahmed Ibrahim Shekarau", "Ibrahim Shehu Adamu", "Yusuf Abdullahi", "Howard Lakougna", "Bob MacDonald", "Hadar Shemtov", "Aisha Walcott-Bryant", "Moustapha Cisse", "Avinatan Hassidim", "Jeff Dean", "Yossi Matias"], "title": "WAXAL: A Large-Scale Multilingual African Language Speech Corpus", "comment": "Initial dataset release", "summary": "The advancement of speech technology has predominantly favored high-resource languages, creating a significant digital divide for speakers of most Sub-Saharan African languages. To address this gap, we introduce WAXAL, a large-scale, openly accessible speech dataset for 21 languages representing over 100 million speakers. The collection consists of two main components: an Automated Speech Recognition (ASR) dataset containing approximately 1,250 hours of transcribed, natural speech from a diverse range of speakers, and a Text-to-Speech (TTS) dataset with over 180 hours of high-quality, single-speaker recordings reading phonetically balanced scripts. This paper details our methodology for data collection, annotation, and quality control, which involved partnerships with four African academic and community organizations. We provide a detailed statistical overview of the dataset and discuss its potential limitations and ethical considerations. The WAXAL datasets are released at https://huggingface.co/datasets/google/WaxalNLP under the permissive CC-BY-4.0 license to catalyze research, enable the development of inclusive technologies, and serve as a vital resource for the digital preservation of these languages."}
{"id": "2602.03307", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2602.03307", "abs": "https://arxiv.org/abs/2602.03307", "authors": ["Goksenin Yuksel", "Marcel van Gerven", "Kiki van der Heijden"], "title": "GRAM: Spatial general-purpose audio representations for real-world environments", "comment": "Revise with RealSELD", "summary": "Audio foundation models learn general-purpose audio representations that facilitate a wide range of downstream tasks. While the performance of these models has greatly increased for conventional single-channel, dry audio clips, their success in real-world acoustic environments with reverberation and noise is limited. Furthermore, most audio foundation models ignore the spatial dimension of real-world acoustic environments, ruling out tasks involving sound localization. To address these limitations, we propose GRAM: a general-purpose real-world audio model that employs a multi-channel masked autoencoder to efficiently learn spatial audio representations. We evaluated GRAM and other audio foundation models in a standardized manner on high-quality simulations of naturalistic, spatial acoustic environments as well as recordings of real-world environments and release these two complementary benchmark task suites: NatHEAR and RealSELD. Our results demonstrate that GRAM outperforms all state-of-the-art self-supervised audio foundation models on NatHEAR and the clean, single-channel version HEAR, while using only a fraction of the training data. GRAM also shows state-of-the-art localization performance in simulated environments and generalizes efficiently to real-world recordings in RealSELD. Taken together, GRAM presents a significant advance toward robust spatial audio foundation models for real-world environments."}
{"id": "2602.02627", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2602.02627", "abs": "https://arxiv.org/abs/2602.02627", "authors": ["Wenkai Qin", "Mark L. Psiaki", "John R. Bowman", "Todd E. Humphreys"], "title": "Pilots and Other Predictable Elements of the Starlink Ku-Band Downlink", "comment": null, "summary": "We identify and characterize dedicated pilot symbols and other predictable elements embedded within the Starlink Ku-band downlink waveform. Exploitation of these predictable elements enables precise opportunistic positioning, navigation, and timing using compact, low-gain receivers by maximizing the signal processing gain available for signal acquisition and time-of-arrival (TOA) estimation. We develop an acquisition and demodulation framework to decode Starlink frames and disclose the explicit sequences of the edge pilots -- bands of 4QAM symbols located at both edges of each Starlink channel that apparently repeat identically across all frames, beams, channels, and satellites. We further reveal that the great majority of QPSK-modulated symbols do not carry high-entropy user data but instead follow a regular tessellated structure superimposed on a constant reference template. We demonstrate that exploiting frame-level predictable elements yields a processing gain of approximately 48 dB, thereby enabling low-cost, compact receivers to extract precise TOA measurements even from low-SNR Starlink side beams."}
{"id": "2602.02980", "categories": ["eess.AS", "cs.CL", "eess.SP"], "pdf": "https://arxiv.org/pdf/2602.02980", "abs": "https://arxiv.org/abs/2602.02980", "authors": ["Xi Xuan", "Davide Carbone", "Ruchi Pandey", "Wenxin Zhang", "Tomi H. Kinnunen"], "title": "WST-X Series: Wavelet Scattering Transform for Interpretable Speech Deepfake Detection", "comment": "Submitted to IEEE Signal Processing Letters", "summary": "Designing front-ends for speech deepfake detectors primarily focuses on two categories. Hand-crafted filterbank features are transparent but are limited in capturing high-level semantic details, often resulting in performance gaps compared to self-supervised (SSL) features. SSL features, in turn, lack interpretability and may overlook fine-grained spectral anomalies. We propose the WST-X series, a novel family of feature extractors that combines the best of both worlds via the wavelet scattering transform (WST), integrating wavelets with nonlinearities analogous to deep convolutional networks. We investigate 1D and 2D WSTs to extract acoustic details and higher-order structural anomalies, respectively. Experimental results on the recent and challenging Deepfake-Eval-2024 dataset indicate that WST-X outperforms existing front-ends by a wide margin. Our analysis reveals that a small averaging scale ($J$), combined with high-frequency and directional resolutions ($Q, L$), is critical for capturing subtle artifacts. This underscores the value of translation-invariant and deformation-stable features for robust and interpretable speech deepfake detection."}
{"id": "2602.03355", "categories": ["cs.SD", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.03355", "abs": "https://arxiv.org/abs/2602.03355", "authors": ["Chang Li", "Kanglei Zhou", "Liyuan Wang"], "title": "PACE: Pretrained Audio Continual Learning", "comment": "Accepted at ICLR 2026", "summary": "Audio is a fundamental modality for analyzing speech, music, and environmental sounds. Although pretrained audio models have significantly advanced audio understanding, they remain fragile in real-world settings where data distributions shift over time. In this work, we present the first systematic benchmark for audio continual learning (CL) with pretrained models (PTMs), together with a comprehensive analysis of its unique challenges. Unlike in vision, where parameter-efficient fine-tuning (PEFT) has proven effective for CL, directly transferring such strategies to audio leads to poor performance. This stems from a fundamental property of audio backbones: they focus on low-level spectral details rather than structured semantics, causing severe upstream-downstream misalignment. Through extensive empirical study, we identify analytic classifiers with first-session adaptation (FSA) as a promising direction, but also reveal two major limitations: representation saturation in coarse-grained scenarios and representation drift in fine-grained scenarios. To address these challenges, we propose PACE, a novel method that enhances FSA via a regularized analytic classifier and enables multi-session adaptation through adaptive subspace-orthogonal PEFT for improved semantic alignment. In addition, we introduce spectrogram-based boundary-aware perturbations to mitigate representation overlap and improve stability. Experiments on six diverse audio CL benchmarks demonstrate that PACE substantially outperforms state-of-the-art baselines, marking an important step toward robust and scalable audio continual learning with PTMs."}
{"id": "2602.02893", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2602.02893", "abs": "https://arxiv.org/abs/2602.02893", "authors": ["Ziming Liu", "Tao Chen", "Muran Guo", "Francesco Verde"], "title": "STAR-RIS-Assisted Full-Space Angle Estimation via Finite Rate of Innovation", "comment": "13 pages, 7 figures, journal paper", "summary": "Conventional sensor architectures typically restrict angle estimation to the half-space. By enabling simultaneous transmission and reflection, simultaneously transmitting and reflecting reconfigurable intelligent surfaces (STAR-RIS) can support full-space angle detection. This paper develops a fullspace angle estimation framework by leveraging a finite rate of innovation (FRI) model enabled by STAR-RIS. We distinguish two practical STAR-RIS configurations: (i) an element-wise uniform setting, where all metasurface elements share identical energy-splitting (ES) coefficients and phase differences, and (ii) a nonuniform ES setting, where the phase difference is common across elements while the ES coefficients vary element-wise to increase design flexibility. For each regime, we formulate the corresponding FRI-based signal model and derive the Ziv-Zakai bound (ZZB) for angle estimation. To recover the underlying FRI sampling structure, we develop a proximal-gradient algorithm implemented via alternating projections in matrix space and establish its convergence. Exploiting the recovered FRI structure, we construct an annihilating filter whose zeros encode user angles, enabling gridless estimation via polynomial root finding. Numerical results demonstrate that the proposed methods operate reliably across both configuration regimes and achieve improved angle estimation performance with low overhead."}
{"id": "2602.03245", "categories": ["eess.AS", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.03245", "abs": "https://arxiv.org/abs/2602.03245", "authors": ["Nikola Ljubešić", "Peter Rupnik", "Tea Perinčić"], "title": "Mići Princ -- A Little Boy Teaching Speech Technologies the Chakavian Dialect", "comment": "2 figures, 14 pages, accepted and presented at JTDH 2024", "summary": "This paper documents our efforts in releasing the printed and audio book of the translation of the famous novel The Little Prince into the Chakavian dialect, as a computer-readable, AI-ready dataset, with the textual and the audio components of the two releases now aligned on the level of each written and spoken word. Our motivation for working on this release is multiple. The first one is our wish to preserve the highly valuable and specific content beyond the small editions of the printed and the audio book. With the dataset published in the CLARIN.SI repository, this content is from now on at the fingertips of any interested individual. The second motivation is to make the data available for various artificial-intelligence-related usage scenarios, such as the one we follow upon inside this paper already -- adapting the Whisper-large-v3 open automatic speech recognition model, with decent performance on standard Croatian, to Chakavian dialectal speech. We can happily report that with adapting the model, the word error rate on the selected test data has being reduced to a half, while we managed to remove up to two thirds of the error on character level. We envision many more usages of this dataset beyond the set of experiments we have already performed, both on tasks of artificial intelligence research and application, as well as dialectal research. The third motivation for this release is our hope that this, now highly structured dataset, will be transformed into a digital online edition of this work, allowing individuals beyond the research and technology communities to enjoy the beauty of the message of the little boy in the desert, told through the spectacular prism of the Chakavian dialect."}
{"id": "2602.03420", "categories": ["cs.SD", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.03420", "abs": "https://arxiv.org/abs/2602.03420", "authors": ["Siyi Wang", "Shihong Tan", "Siyi Liu", "Hong Jia", "Gongping Huang", "James Bailey", "Ting Dang"], "title": "CoCoEmo: Composable and Controllable Human-Like Emotional TTS via Activation Steering", "comment": null, "summary": "Emotional expression in human speech is nuanced and compositional, often involving multiple, sometimes conflicting, affective cues that may diverge from linguistic content. In contrast, most expressive text-to-speech systems enforce a single utterance-level emotion, collapsing affective diversity and suppressing mixed or text-emotion-misaligned expression. While activation steering via latent direction vectors offers a promising solution, it remains unclear whether emotion representations are linearly steerable in TTS, where steering should be applied within hybrid TTS architectures, and how such complex emotion behaviors should be evaluated. This paper presents the first systematic analysis of activation steering for emotional control in hybrid TTS models, introducing a quantitative, controllable steering framework, and multi-rater evaluation protocols that enable composable mixed-emotion synthesis and reliable text-emotion mismatch synthesis. Our results demonstrate, for the first time, that emotional prosody and expressive variability are primarily synthesized by the TTS language module instead of the flow-matching module, and also provide a lightweight steering approach for generating natural, human-like emotional speech."}
{"id": "2602.03000", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2602.03000", "abs": "https://arxiv.org/abs/2602.03000", "authors": ["Shupei Zhang", "Shuhao Zeng", "Boya Di", "Lingyang Song"], "title": "Tri-Hybrid Holographic Beamforming for Integrated Sensing and Communication", "comment": "13 pages, 8 figures", "summary": "Integrated sensing and communication (ISAC) can perform both communication and sensing tasks using the same frequency band and hardware, making it a key technology for 6G. As a low-cost implementation for large-scale antenna arrays, reconfigurable holographic surfaces (RHSs) can be integrated into ISAC systems to realize the holographic ISAC paradigm, where enlarged radiation apertures achieve significant beamforming gains. In this paper, we investigate the tri-hybrid holographic ISAC framework, where the beamformer comprises digital, analog, and RHS-based electromagnetic (EM) layers. The analog layer employs a small number of phase shifters (PSs) to provide subarray-level phase control for the amplitude-modulated RHSs. Tri-hybrid beamforming provides a pathway for low-cost large-scale holographic ISAC. However, compared to conventional ISAC systems, it is challenging to achieve joint subarray-level phase control via PSs and element-level radiation amplitude control via RHSs for holographic ISAC. To address this, we present a tri-hybrid holographic ISAC scheme that minimizes sensing waveform error while satisfying the minimum user rate requirement. A joint optimization approach for PS phases and RHS amplitude responses is designed to address inter-layer coupling and distinct feasible regions. Theoretical analyses reveal that the optimized amplitude responses cluster near boundary values, i.e., 1-bit amplitude control, to reduce hardware and algorithmic complexity. Simulation results show that the proposed scheme achieves a controllable performance trade-off between communication and sensing tasks. Measured RHS beam gain validates the enhancement of holographic beamforming through subarray-level phase shifting. Moreover, as the number of RHS elements increases, the proposed approach exceeds the performance of conventional hybrid beamforming while significantly reducing the number of PSs."}
{"id": "2602.03398", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2602.03398", "abs": "https://arxiv.org/abs/2602.03398", "authors": ["Shunxi Xu", "Thushara Abhayapala", "Craig T. Jin"], "title": "A Unified SVD-Modal Solution for Sparse Sound Field Reconstruction with Hybrid Spherical-Linear Microphone Arrays", "comment": "Accepted by ICASSP 2026", "summary": "We propose a data-driven sparse recovery framework for hybrid spherical linear microphone arrays using singular value decomposition (SVD) of the transfer operator. The SVD yields orthogonal microphone and field modes, reducing to spherical harmonics (SH) in the SMA-only case, while incorporating LMAs introduces complementary modes beyond SH. Modal analysis reveals consistent divergence from SH across frequency, confirming the improved spatial selectivity. Experiments in reverberant conditions show reduced energy-map mismatch and angular error across frequency, distance, and source count, outperforming SMA-only and direct concatenation. The results demonstrate that SVD-modal processing provides a principled and unified treatment of hybrid arrays for robust sparse sound-field reconstruction."}
{"id": "2602.03523", "categories": ["cs.SD", "cs.AI", "cs.MM"], "pdf": "https://arxiv.org/pdf/2602.03523", "abs": "https://arxiv.org/abs/2602.03523", "authors": ["Eunjin Choi", "Hounsu Kim", "Hayeon Bang", "Taegyun Kwon", "Juhan Nam"], "title": "D3PIA: A Discrete Denoising Diffusion Model for Piano Accompaniment Generation From Lead sheet", "comment": "Accepted at 2026 IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)", "summary": "Generating piano accompaniments in the symbolic music domain is a challenging task that requires producing a complete piece of piano music from given melody and chord constraints, such as those provided by a lead sheet. In this paper, we propose a discrete diffusion-based piano accompaniment generation model, D3PIA, leveraging local alignment between lead sheet and accompaniment in piano-roll representation. D3PIA incorporates Neighborhood Attention (NA) to both encode the lead sheet and condition it for predicting note states in the piano accompaniment. This design enhances local contextual modeling by efficiently attending to nearby melody and chord conditions. We evaluate our model using the POP909 dataset, a widely used benchmark for piano accompaniment generation. Objective evaluation results demonstrate that D3PIA preserves chord conditions more faithfully compared to continuous diffusion-based and Transformer-based baselines. Furthermore, a subjective listening test indicates that D3PIA generates more musically coherent accompaniments than the comparison models."}
{"id": "2602.03055", "categories": ["eess.SP", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.03055", "abs": "https://arxiv.org/abs/2602.03055", "authors": ["Madeline Navarro", "Andrei Buciulea", "Santiago Segarra", "Antonio Marques"], "title": "Stationarity and Spectral Characterization of Random Signals on Simplicial Complexes", "comment": null, "summary": "It is increasingly common for data to possess intricate structure, necessitating new models and analytical tools. Graphs, a prominent type of structure, can encode the relationships between any two entities (nodes). However, graphs neither allow connections that are not dyadic nor permit relationships between sets of nodes. We thus turn to simplicial complexes for connecting more than two nodes as well as modeling relationships between simplices, such as edges and triangles. Our data then consist of signals lying on topological spaces, represented by simplicial complexes. Much recent work explores these topological signals, albeit primarily through deterministic formulations. We propose a probabilistic framework for random signals defined on simplicial complexes. Specifically, we generalize the classical notion of stationarity. By spectral dualities of Hodge and Dirac theory, we define stationary topological signals as the outputs of topological filters given white noise. This definition naturally extends desirable properties of stationarity that hold for both time-series and graph signals. Crucially, we properly define topological power spectral density (PSD) through a clear spectral characterization. We then discuss the advantages of topological stationarity due to spectral properties via the PSD. In addition, we empirically demonstrate the practicality of these benefits through multiple synthetic and real-world simulations."}
{"id": "2602.03762", "categories": ["eess.AS", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.03762", "abs": "https://arxiv.org/abs/2602.03762", "authors": ["Hugo Malard", "Gael Le Lan", "Daniel Wong", "David Lou Alon", "Yi-Chiao Wu", "Sanjeel Parekh"], "title": "Conditional Flow Matching for Visually-Guided Acoustic Highlighting", "comment": null, "summary": "Visually-guided acoustic highlighting seeks to rebalance audio in alignment with the accompanying video, creating a coherent audio-visual experience. While visual saliency and enhancement have been widely studied, acoustic highlighting remains underexplored, often leading to misalignment between visual and auditory focus. Existing approaches use discriminative models, which struggle with the inherent ambiguity in audio remixing, where no natural one-to-one mapping exists between poorly-balanced and well-balanced audio mixes. To address this limitation, we reframe this task as a generative problem and introduce a Conditional Flow Matching (CFM) framework. A key challenge in iterative flow-based generation is that early prediction errors -- in selecting the correct source to enhance -- compound over steps and push trajectories off-manifold. To address this, we introduce a rollout loss that penalizes drift at the final step, encouraging self-correcting trajectories and stabilizing long-range flow integration. We further propose a conditioning module that fuses audio and visual cues before vector field regression, enabling explicit cross-modal source selection. Extensive quantitative and qualitative evaluations show that our method consistently surpasses the previous state-of-the-art discriminative approach, establishing that visually-guided audio remixing is best addressed through generative modeling."}
{"id": "2602.03549", "categories": ["cs.SD", "cs.HC"], "pdf": "https://arxiv.org/pdf/2602.03549", "abs": "https://arxiv.org/abs/2602.03549", "authors": ["Michael Küttner", "Valeria Zitz", "Supraja Ramesh", "Michael Beigl", "Tobias Röddiger"], "title": "EarResp-ANS : Audio-Based On-Device Respiration Rate Estimation on Earphones with Adaptive Noise Suppression", "comment": "31 pages, 11 figures", "summary": "Respiratory rate (RR) is a key vital sign for clinical assessment and mental well-being, yet it is rarely monitored in everyday life due to the lack of unobtrusive sensing technologies. In-ear audio sensing is promising due to its high social acceptance and the amplification of physiological sounds caused by the occlusion effect; however, existing approaches often fail under real-world noise or rely on computationally expensive models. We present EarResp-ANS, the first system enabling fully on-device, real-time RR estimation on commercial earphones. The system employs LMS-based adaptive noise suppression (ANS) to attenuate ambient noise while preserving respiration-related acoustic components, without requiring neural networks or audio streaming, thereby explicitly addressing the energy and privacy constraints of wearable devices. We evaluate EarResp-ANS in a study with 18 participants under realistic acoustic conditions, including music, cafeteria noise, and white noise up to 80 dB SPL. EarResp-ANS achieves robust performance with a global MAE of 0.84 CPM , reduced to 0.47 CPM via automatic outlier rejection, while operating with less than 2% processor load directly on the earphone."}
{"id": "2602.03464", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2602.03464", "abs": "https://arxiv.org/abs/2602.03464", "authors": ["Guanhua Ding", "Tao Huang", "Qinchen Wu", "Jinping Sun", "Yanping Wang", "Bing Zhu", "Guoqiang Mao"], "title": "Multipath Extended Target Tracking with Labeled Random Finite Sets", "comment": null, "summary": "High-resolution radar sensors are critical for autonomous systems but pose significant challenges to traditional tracking algorithms due to the generation of multiple measurements per object and the presence of multipath effects. Existing solutions often rely on the point target assumption or treat multipath measurements as clutter, whereas current extended target trackers often lack the capability to maintain trajectory continuity in complex multipath environments. To address these limitations, this paper proposes the multipath extended target generalized labeled multi-Bernoulli (MPET-GLMB) filter. A unified Bayesian framework based on labeled random finite set theory is derived to jointly model target existence, measurement partitioning, and the association between measurements, targets, and propagation paths. This formulation enables simultaneous trajectory estimation for both targets and reflectors without requiring heuristic post-processing. To enhance computational efficiency, a joint prediction and update implementation based on Gibbs sampling is developed. Furthermore, a measurement-driven adaptive birth model is introduced to initialize tracks without prior knowledge of target positions. Experimental results from simulated scenarios and real-world automotive radar data demonstrate that the proposed filter outperforms state-of-the-art methods, achieving superior state estimation accuracy and robust trajectory maintenance in dynamic multipath environments."}
{"id": "2602.03817", "categories": ["cs.SD", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.03817", "abs": "https://arxiv.org/abs/2602.03817", "authors": ["Oscar Ovanger", "Levi Harris", "Timothy H. Keitt"], "title": "Adaptive Evidence Weighting for Audio-Spatiotemporal Fusion", "comment": null, "summary": "Many machine learning systems have access to multiple sources of evidence for the same prediction target, yet these sources often differ in reliability and informativeness across inputs. In bioacoustic classification, species identity may be inferred both from the acoustic signal and from spatiotemporal context such as location and season; while Bayesian inference motivates multiplicative evidence combination, in practice we typically only have access to discriminative predictors rather than calibrated generative models. We introduce \\textbf{F}usion under \\textbf{IN}dependent \\textbf{C}onditional \\textbf{H}ypotheses (\\textbf{FINCH}), an adaptive log-linear evidence fusion framework that integrates a pre-trained audio classifier with a structured spatiotemporal predictor. FINCH learns a per-sample gating function that estimates the reliability of contextual information from uncertainty and informativeness statistics. The resulting fusion family \\emph{contains} the audio-only classifier as a special case and explicitly bounds the influence of contextual evidence, yielding a risk-contained hypothesis class with an interpretable audio-only fallback. Across benchmarks, FINCH consistently outperforms fixed-weight fusion and audio-only baselines, improving robustness and error trade-offs even when contextual information is weak in isolation. We achieve state-of-the-art performance on CBI and competitive or improved performance on several subsets of BirdSet using a lightweight, interpretable, evidence-based approach. Code is available: \\texttt{\\href{https://anonymous.4open.science/r/birdnoise-85CD/README.md}{anonymous-repository}}"}
{"id": "2602.03524", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2602.03524", "abs": "https://arxiv.org/abs/2602.03524", "authors": ["Tong Hui", "Xiao Tang", "Yichen Wang", "Qinghe Du", "Dusit Niyato", "Zhu Han"], "title": "Channel-Aware Conditional Diffusion Model for Secure MU-MISO Communications", "comment": "Accepted @ IEEE TVT", "summary": "While information securityis a fundamental requirement for wireless communications, conventional optimization based approaches often struggle with real-time implementation, and deep models, typically discriminative in nature, may lack the ability to cope with unforeseen scenarios. To address this challenge, this paper investigates the design of legitimate beamforming and artificial noise (AN) to achieve physical layer security by exploiting the conditional diffusion model. Specifically, we reformulate the security optimization as a conditional generative process, using a diffusion model to learn the inherent distribution of near-optimal joint beamforming and AN strategies. We employ a U-Net architecture with cross-attention to integrate channel state information, as the basis for the generative process. Moreover, we fine-tune the trained model using an objective incorporating the sum secrecy rate such that the security performance is further enhanced. Finally, simulation results validate the learning process convergence and demonstrate that the proposed generative method achieves superior secrecy performance across various scenarios as compared with the baselines."}
{"id": "2602.03624", "categories": ["eess.SP", "cs.SD"], "pdf": "https://arxiv.org/pdf/2602.03624", "abs": "https://arxiv.org/abs/2602.03624", "authors": ["Rien Sonck", "Bernd Accou", "Tom Francart", "Jonas Vanthornhout"], "title": "A Multi-decoder Neural Tracking Method for Accurately Predicting Speech Intelligibility", "comment": null, "summary": "Objective: EEG-based methods can predict speech intelligibility, but their accuracy and robustness lag behind behavioral tests, which typically show test-retest differences under 1 dB. We introduce the multi-decoder method to predict speech reception thresholds (SRTs) from EEG recordings, enabling objective assessment for populations unable to perform behavioral tests; such as those with disorders of consciousness or during hearing aid fitting. Approach: The method aggregates data from hundreds of decoders, each trained on different speech features and EEG preprocessing setups to quantify neural tracking (NT) of speech signals. Using data from 39 participants (ages 18-24), we recorded 29 minutes of EEG per person while they listened to speech at six signal-to-noise ratios and a quiet story. NT values were combined into a high-dimensional feature vector per subject, and a support vector regression model was trained to predict SRTs from these vectors. Main Result: Predictions correlated significantly with behavioral SRTs (r = 0.647, p < 0.001; NRMSE = 0.19), with all differences under 1 dB. SHAP analysis showed theta/delta bands and early lags had slightly greater influence. Using pretrained subject-independent decoders reduced required EEG data collection to 15 minutes (3 minutes of story, 12 minutes across six SNR conditions) without losing accuracy."}
{"id": "2602.03581", "categories": ["eess.SP", "cs.IT"], "pdf": "https://arxiv.org/pdf/2602.03581", "abs": "https://arxiv.org/abs/2602.03581", "authors": ["Zhe Wang", "Jiayi Zhang", "Bokai Xu", "Dusit Niyato", "Bo Ai", "Shiwen Mao", "Zhu Han"], "title": "Low-Complexity Distributed Combining Design for Near-Field Cell-Free XL-MIMO Systems", "comment": "15 pages, 10 figures, to appear in IEEE Transactions on Wireless Communications", "summary": "In this paper, we investigate the low-complexity distributed combining scheme design for near-field cell-free extremely large-scale multiple-input-multiple-output (CF XL-MIMO) systems. Firstly, we construct the uplink spectral efficiency (SE) performance analysis framework for CF XL-MIMO systems over centralized and distributed processing schemes. Notably, we derive the centralized minimum mean-square error (CMMSE) and local minimum mean-square error (LMMSE) combining schemes over arbitrary channel estimators. Then, focusing on the CMMSE and LMMSE combining schemes, we propose five low-complexity distributed combining schemes based on the matrix approximation methodology or the symmetric successive over relaxation (SSOR) algorithm. More specifically, we propose two matrix approximation methodology-aided combining schemes: Global Statistics \\& Local Instantaneous information-based MMSE (GSLI-MMSE) and Statistics matrix Inversion-based LMMSE (SI-LMMSE). These two schemes are derived by approximating the global instantaneous information in the CMMSE combining and the local instantaneous information in the LMMSE combining with the global and local statistics information by asymptotic analysis and matrix expectation approximation, respectively. Moreover, by applying the low-complexity SSOR algorithm to iteratively solve the matrix inversion in the LMMSE combining, we derive three distributed SSOR-based LMMSE combining schemes, distinguished from the applied information and initial values."}
{"id": "2602.03590", "categories": ["eess.SP", "cs.IT"], "pdf": "https://arxiv.org/pdf/2602.03590", "abs": "https://arxiv.org/abs/2602.03590", "authors": ["Zhe Wang", "Emil Björnson", "Jiayi Zhang", "Peng Zhang", "Vitaly Petrov", "Bo Ai"], "title": "Statistics Approximation-Enabled Distributed Beamforming for Cell-Free Massive MIMO", "comment": "6 pages, 3 figures, accepted by IEEE International Conference on Communications (ICC) 2026", "summary": "We study a distributed beamforming approach for cell-free massive multiple-input multiple-output networks, referred to as Global Statistics \\& Local Instantaneous information-based minimum mean-square error (GSLI-MMSE). The scenario with multi-antenna access points (APs) is considered over three different channel models: correlated Rician fading with fixed or random line-of-sight (LoS) phase-shifts, and correlated Rayleigh fading. With the aid of matrix inversion derivations, we can construct the conventional MMSE combining from the perspective of each AP, where global instantaneous information is involved. Then, for an arbitrary AP, we apply the statistics approximation methodology to approximate instantaneous terms related to other APs by channel statistics to construct the distributed combining scheme at each AP with local instantaneous information and global statistics. With the aid of uplink-downlink duality, we derive the respective GSLI-MMSE precoding schemes. Numerical results showcase that the proposed GSLI-MMSE scheme demonstrates performance comparable to the optimal centralized MMSE scheme, under the stable LoS conditions, e.g., with static users having Rician fading with a fixed LoS path."}
{"id": "2602.03624", "categories": ["eess.SP", "cs.SD"], "pdf": "https://arxiv.org/pdf/2602.03624", "abs": "https://arxiv.org/abs/2602.03624", "authors": ["Rien Sonck", "Bernd Accou", "Tom Francart", "Jonas Vanthornhout"], "title": "A Multi-decoder Neural Tracking Method for Accurately Predicting Speech Intelligibility", "comment": null, "summary": "Objective: EEG-based methods can predict speech intelligibility, but their accuracy and robustness lag behind behavioral tests, which typically show test-retest differences under 1 dB. We introduce the multi-decoder method to predict speech reception thresholds (SRTs) from EEG recordings, enabling objective assessment for populations unable to perform behavioral tests; such as those with disorders of consciousness or during hearing aid fitting. Approach: The method aggregates data from hundreds of decoders, each trained on different speech features and EEG preprocessing setups to quantify neural tracking (NT) of speech signals. Using data from 39 participants (ages 18-24), we recorded 29 minutes of EEG per person while they listened to speech at six signal-to-noise ratios and a quiet story. NT values were combined into a high-dimensional feature vector per subject, and a support vector regression model was trained to predict SRTs from these vectors. Main Result: Predictions correlated significantly with behavioral SRTs (r = 0.647, p < 0.001; NRMSE = 0.19), with all differences under 1 dB. SHAP analysis showed theta/delta bands and early lags had slightly greater influence. Using pretrained subject-independent decoders reduced required EEG data collection to 15 minutes (3 minutes of story, 12 minutes across six SNR conditions) without losing accuracy."}
{"id": "2602.03711", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.03711", "abs": "https://arxiv.org/abs/2602.03711", "authors": ["Metehan Karatas", "Subhrakanti Dey", "Christian Rohner", "Jose Mairton Barros da Silva"], "title": "VR-VFL: Joint Rate and Client Selection for Vehicular Federated Learning Under Imperfect CSI", "comment": "This paper has been accepted for presentation at IEEE ICC 2026", "summary": "Federated learning in vehicular edge networks faces major challenges in efficient resource allocation, largely due to high vehicle mobility and the presence of imperfect channel state information. Many existing methods oversimplify these realities, often assuming fixed communication rounds or ideal channel conditions, which limits their effectiveness in real-world scenarios. To address this, we propose variable rate vehicular federated learning (VR-VFL), a novel federated learning method designed specifically for vehicular networks under imperfect channel state information. VR-VFL combines dynamic client selection with adaptive transmission rate selection, while also allowing round times to flex in response to changing wireless conditions. At its core, VR-VFL is built on a bi-objective optimization framework that strikes a balance between improving learning convergence and minimizing the time required to complete each round. By accounting for both the challenges of mobility and realistic wireless constraints, VR-VFL offers a more practical and efficient approach to federated learning in vehicular edge networks. Simulation results show that the proposed VR-VFL scheme achieves convergence approximately 40% faster than other methods in the literature."}
{"id": "2602.03718", "categories": ["eess.SP", "cs.IT"], "pdf": "https://arxiv.org/pdf/2602.03718", "abs": "https://arxiv.org/abs/2602.03718", "authors": ["Nikola Zlatanov"], "title": "A Narrowband Fully-Analog Multi-Antenna Transmitter", "comment": null, "summary": "This paper proposes a narrowband fully-analog $N$-antenna transmitter that emulates the functionality of a narrowband fully-digital $N$-antenna transmitter. Specifically, in symbol interval $m$, the proposed fully-analog transmitter synthesizes an arbitrary complex excitation vector $\\bm x[m]\\in\\mathbb{C}^N$ with prescribed total power $\\|\\bm x[m]\\|_2^2=P$ from a single coherent RF tone, using only tunable phase-control elements embedded in a passive interferometric programmable network. The programmable network is excited through one input port while the remaining $N - 1$ input ports are impedance matched. In the ideal lossless case, the network transfer is unitary and therefore redistributes RF power among antenna ports without dissipative amplitude control.\n  The synthesis task is posed as a unitary state-preparation problem: program a unitary family so that $\\bm V(\\bm\\varphi)\\bm e_1=\\bm c$, where $\\bm c=\\bm x/\\sqrt{P}$ and $\\|\\bm c\\|_2=1$. We provide a constructive realization and a closed-form programming rule: a binary magnitude-splitting tree allocates the desired per-antenna magnitudes $|c_n|$ using $N -1$ tunable split ratios, and a per-antenna output phase bank assigns the target phases using $N$ tunable phase shifts. The resulting architecture uses $2N-1$ real tunable degrees of freedom and admits a deterministic $O(N)$ programming procedure with no iterative optimization, enabling symbol-by-symbol updates when the chosen phase-control technology supports the required tuning speed.\n  Using representative COTS components, we model the RF-front-end DC power of the proposed fully-analog transmitter and compare it against an equivalent COTS fully-digital array. For $N\\le 16$, the comparison indicates significant RF-front-end power savings for the fully-analog architecture.\n  The results in this paper are intended as a proof-of-concept for a narrowband fully-analog transmitter."}
{"id": "2602.03801", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2602.03801", "abs": "https://arxiv.org/abs/2602.03801", "authors": ["Pulok Tarafder", "Zoheb Hassan", "Imtiaz Ahmed", "Danda B. Rawat", "Kamrul Hasan", "Cong Pu"], "title": "Digital-Twin Empowered Deep Reinforcement Learning For Site-Specific Radio Resource Management in NextG Wireless Aerial Corridor", "comment": "Submitted for possible publication to IEEE. Paper currently under review. The contents of this paper may change at any time without notice", "summary": "Joint base station (BS) association and beam selection in multi-UAV aerial corridors constitutes a challenging radio resource management (RRM) problem. It is driven by high-dimensional action spaces, need for substantial overhead to acquire global channel state information (CSI), rapidly varying propagation channels, and stringent latency requirements. Conventional combinatorial optimization methods, while near-optimal, are computationally prohibitive for real-time operation in such dynamic environments. While learning-based approaches can mitigate computational complexity and CSI overhead, the need for extensive site-specific (SS) datasets for model training remains a key challenge. To address these challenges, we develop a Digital Twin (DT)-enabled two-stage optimization framework that couples physics-based beam gain modeling with DRL for scalable online decision-making. In the first stage, a channel twin (CT) is constructed using a high-fidelity ray-tracing solver with geo-spatial contexts, and network information to capture SS propagation characteristics, and dual annealing algorithm is employed to precompute optimal transmission beam directions. In the second stage, a Multi-Head Proximal Policy Optimization (MH-PPO) agent, equipped with a scalable multi-head actor-critic architecture, is trained on the DT-generated channel dataset to directly map complex channel and beam states to jointly execute UAV-BS-beam association decisions. The proposed PPO agent achieves a 44%-121% improvement over DQN and 249%-807% gain over traditional heuristic based optimization schemes in a dense UAV scenario, while reducing inference latency by several orders of magnitude. These results demonstrate that DT-driven training pipelines can deliver high-performance, low-latency RRM policies tailored to SS deployments suitable for real-time resource management in next-generation aerial corridor networks."}
{"id": "2602.02980", "categories": ["eess.AS", "cs.CL", "eess.SP"], "pdf": "https://arxiv.org/pdf/2602.02980", "abs": "https://arxiv.org/abs/2602.02980", "authors": ["Xi Xuan", "Davide Carbone", "Ruchi Pandey", "Wenxin Zhang", "Tomi H. Kinnunen"], "title": "WST-X Series: Wavelet Scattering Transform for Interpretable Speech Deepfake Detection", "comment": "Submitted to IEEE Signal Processing Letters", "summary": "Designing front-ends for speech deepfake detectors primarily focuses on two categories. Hand-crafted filterbank features are transparent but are limited in capturing high-level semantic details, often resulting in performance gaps compared to self-supervised (SSL) features. SSL features, in turn, lack interpretability and may overlook fine-grained spectral anomalies. We propose the WST-X series, a novel family of feature extractors that combines the best of both worlds via the wavelet scattering transform (WST), integrating wavelets with nonlinearities analogous to deep convolutional networks. We investigate 1D and 2D WSTs to extract acoustic details and higher-order structural anomalies, respectively. Experimental results on the recent and challenging Deepfake-Eval-2024 dataset indicate that WST-X outperforms existing front-ends by a wide margin. Our analysis reveals that a small averaging scale ($J$), combined with high-frequency and directional resolutions ($Q, L$), is critical for capturing subtle artifacts. This underscores the value of translation-invariant and deformation-stable features for robust and interpretable speech deepfake detection."}

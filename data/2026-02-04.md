<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 13]
- [eess.AS](#eess.AS) [Total: 5]
- [cs.SD](#cs.SD) [Total: 10]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [Joint single-shot ToA and DoA estimation for VAA-based BLE ranging with phase ambiguity: A deep learning-based approach](https://arxiv.org/abs/2602.02503)
*Jincheng Xie,Yili Deng,Jiguang He,Pengyu Wang,Miaomiao Dong,Rui Tang,Zhongyi Huang*

Main category: eess.SP

TL;DR: 提出基于神经网络的相位恢复框架，解决BLE单天线DoA估计中的相位模糊问题，实现联合ToA和DoA估计


<details>
  <summary>Details</summary>
Motivation: 传统DoA估计需要多天线阵列，在尺寸受限的BLE设备上成本高昂。虚拟天线阵列技术虽能用单天线实现DoA估计，但BLE提供的单次双向信道频率响应存在二进制相位模糊问题，阻碍了VAA的直接应用。

Method: 提出统一模型结合VAA与BLE双向CFR，引入基于神经网络的相位恢复框架，采用行列预测器和投票机制解决相位模糊问题。恢复后的单向CFR使MUSIC等超分辨率算法能够进行联合ToA和DoA估计。

Result: 仿真结果表明，该方法在非均匀VAA下表现优异，当SNR≥5dB时，均方误差接近克拉美罗界。

Conclusion: 该方法成功解决了BLE单天线DoA估计中的相位模糊问题，实现了高效的联合ToA和DoA估计，为尺寸受限的BLE设备提供了可行的角度估计方案。

Abstract: Conventional direction-of-arrival (DoA) estimation methods rely on multi-antenna arrays, which are costly to implement on size-constrained Bluetooth Low Energy (BLE) devices. Virtual antenna array (VAA) techniques enable DoA estimation with a single antenna, making angle estimation feasible on such devices. However, BLE only provides a single-shot two-way channel frequency response (CFR) with a binary phase ambiguity issue, which hinders the direct application of VAA. To address this challenge, we propose a unified model that combines VAA with BLE two-way CFR, and introduce a neural network based phase recovery framework that employs row / column predictors with a voting mechanism to resolve the ambiguity. The recovered one-way CFR then enables super resolution algorithms such as MUSIC for joint time of arrival (ToA) and DoA estimation. Simulation results demonstrate that the proposed method achieves superior performance under non-uniform VAAs, with mean square errors approaching the Cramer Rao bound at SNR $\geq$ 5 dB.

</details>


### [2] [Pilots and Other Predictable Elements of the Starlink Ku-Band Downlink](https://arxiv.org/abs/2602.02627)
*Wenkai Qin,Mark L. Psiaki,John R. Bowman,Todd E. Humphreys*

Main category: eess.SP

TL;DR: 该论文识别并利用Starlink Ku波段下行信号中的可预测元素（如专用导频符号），实现了低成本紧凑接收机的精确机会定位、导航和授时。


<details>
  <summary>Details</summary>
Motivation: Starlink卫星信号通常被认为是加密且不可预测的，但作者发现其中存在可预测元素，这些元素可用于提高信号处理增益，使低成本、低增益接收机能够实现精确的到达时间估计和定位导航。

Method: 开发了Starlink信号的采集和解调框架，解码Starlink帧结构，揭示了边缘导频（位于每个信道边缘的4QAM符号）在所有帧、波束、信道和卫星中重复出现的特性，并发现大多数QPSK调制符号遵循规则镶嵌结构叠加在恒定参考模板上。

Result: 利用帧级可预测元素可获得约48dB的处理增益，使低成本紧凑接收机即使在低信噪比的Starlink侧波束条件下也能提取精确的到达时间测量值。

Conclusion: Starlink信号中嵌入的可预测元素为机会定位、导航和授时提供了新途径，通过最大化信号处理增益，实现了低成本接收机的高精度时间测量能力。

Abstract: We identify and characterize dedicated pilot symbols and other predictable elements embedded within the Starlink Ku-band downlink waveform. Exploitation of these predictable elements enables precise opportunistic positioning, navigation, and timing using compact, low-gain receivers by maximizing the signal processing gain available for signal acquisition and time-of-arrival (TOA) estimation. We develop an acquisition and demodulation framework to decode Starlink frames and disclose the explicit sequences of the edge pilots -- bands of 4QAM symbols located at both edges of each Starlink channel that apparently repeat identically across all frames, beams, channels, and satellites. We further reveal that the great majority of QPSK-modulated symbols do not carry high-entropy user data but instead follow a regular tessellated structure superimposed on a constant reference template. We demonstrate that exploiting frame-level predictable elements yields a processing gain of approximately 48 dB, thereby enabling low-cost, compact receivers to extract precise TOA measurements even from low-SNR Starlink side beams.

</details>


### [3] [STAR-RIS-Assisted Full-Space Angle Estimation via Finite Rate of Innovation](https://arxiv.org/abs/2602.02893)
*Ziming Liu,Tao Chen,Muran Guo,Francesco Verde*

Main category: eess.SP

TL;DR: 提出基于STAR-RIS和有限创新率模型的全空间角度估计框架，支持两种配置模式，通过近端梯度算法恢复FRI结构，实现无网格角度估计


<details>
  <summary>Details</summary>
Motivation: 传统传感器架构通常将角度估计限制在半空间，而STAR-RIS通过同时传输和反射支持全空间角度检测，需要开发相应的全空间角度估计方法

Method: 1) 区分两种STAR-RIS配置：元素均匀设置和非均匀能量分配设置；2) 为每种配置建立基于FRI的信号模型；3) 推导角度估计的Ziv-Zakai界；4) 开发基于矩阵空间交替投影的近端梯度算法恢复FRI采样结构；5) 构建湮灭滤波器，通过多项式求根实现无网格角度估计

Result: 数值结果表明，所提方法在两种配置模式下都能可靠工作，以低开销实现改进的角度估计性能

Conclusion: 基于STAR-RIS和FRI模型的全空间角度估计框架有效解决了传统架构的限制，支持灵活的配置模式，通过近端梯度算法和多项式求根实现了高效的无网格角度估计

Abstract: Conventional sensor architectures typically restrict angle estimation to the half-space. By enabling simultaneous transmission and reflection, simultaneously transmitting and reflecting reconfigurable intelligent surfaces (STAR-RIS) can support full-space angle detection. This paper develops a fullspace angle estimation framework by leveraging a finite rate of innovation (FRI) model enabled by STAR-RIS. We distinguish two practical STAR-RIS configurations: (i) an element-wise uniform setting, where all metasurface elements share identical energy-splitting (ES) coefficients and phase differences, and (ii) a nonuniform ES setting, where the phase difference is common across elements while the ES coefficients vary element-wise to increase design flexibility. For each regime, we formulate the corresponding FRI-based signal model and derive the Ziv-Zakai bound (ZZB) for angle estimation. To recover the underlying FRI sampling structure, we develop a proximal-gradient algorithm implemented via alternating projections in matrix space and establish its convergence. Exploiting the recovered FRI structure, we construct an annihilating filter whose zeros encode user angles, enabling gridless estimation via polynomial root finding. Numerical results demonstrate that the proposed methods operate reliably across both configuration regimes and achieve improved angle estimation performance with low overhead.

</details>


### [4] [Tri-Hybrid Holographic Beamforming for Integrated Sensing and Communication](https://arxiv.org/abs/2602.03000)
*Shupei Zhang,Shuhao Zeng,Boya Di,Lingyang Song*

Main category: eess.SP

TL;DR: 提出三混合全息ISAC框架，结合数字、模拟和RHS层，通过联合优化PS相位和RHS幅度响应，在满足通信速率要求下最小化感知波形误差，实现低成本大规模天线阵列的ISAC系统。


<details>
  <summary>Details</summary>
Motivation: ISAC作为6G关键技术，需要低成本大规模天线阵列实现。传统混合波束成形成本高，而RHS虽然成本低但控制复杂。需要解决PS子阵列级相位控制和RHS单元级幅度控制的联合优化问题。

Method: 提出三混合全息ISAC框架，包含数字层、模拟层（少量PS提供子阵列级相位控制）和RHS层（幅度调制）。设计联合优化方法处理PS相位和RHS幅度响应，解决层间耦合和不同可行域问题。

Result: 理论分析显示优化后的幅度响应聚集在边界值附近（1比特幅度控制），降低硬件和算法复杂度。仿真表明方案能实现通信与感知任务的性能权衡控制，RHS波束增益验证了全息波束成形的增强效果。随着RHS单元数增加，性能超过传统混合波束成形且显著减少PS数量。

Conclusion: 三混合全息ISAC框架为低成本大规模ISAC系统提供了可行路径，通过联合优化实现了通信与感知的权衡，RHS的幅度控制趋向1比特简化了硬件实现，具有实际应用价值。

Abstract: Integrated sensing and communication (ISAC) can perform both communication and sensing tasks using the same frequency band and hardware, making it a key technology for 6G. As a low-cost implementation for large-scale antenna arrays, reconfigurable holographic surfaces (RHSs) can be integrated into ISAC systems to realize the holographic ISAC paradigm, where enlarged radiation apertures achieve significant beamforming gains. In this paper, we investigate the tri-hybrid holographic ISAC framework, where the beamformer comprises digital, analog, and RHS-based electromagnetic (EM) layers. The analog layer employs a small number of phase shifters (PSs) to provide subarray-level phase control for the amplitude-modulated RHSs. Tri-hybrid beamforming provides a pathway for low-cost large-scale holographic ISAC. However, compared to conventional ISAC systems, it is challenging to achieve joint subarray-level phase control via PSs and element-level radiation amplitude control via RHSs for holographic ISAC. To address this, we present a tri-hybrid holographic ISAC scheme that minimizes sensing waveform error while satisfying the minimum user rate requirement. A joint optimization approach for PS phases and RHS amplitude responses is designed to address inter-layer coupling and distinct feasible regions. Theoretical analyses reveal that the optimized amplitude responses cluster near boundary values, i.e., 1-bit amplitude control, to reduce hardware and algorithmic complexity. Simulation results show that the proposed scheme achieves a controllable performance trade-off between communication and sensing tasks. Measured RHS beam gain validates the enhancement of holographic beamforming through subarray-level phase shifting. Moreover, as the number of RHS elements increases, the proposed approach exceeds the performance of conventional hybrid beamforming while significantly reducing the number of PSs.

</details>


### [5] [Stationarity and Spectral Characterization of Random Signals on Simplicial Complexes](https://arxiv.org/abs/2602.03055)
*Madeline Navarro,Andrei Buciulea,Santiago Segarra,Antonio Marques*

Main category: eess.SP

TL;DR: 提出一个概率框架用于定义单纯复形上的随机拓扑信号，推广经典平稳性概念，通过Hodge和Dirac理论的谱对偶性定义平稳拓扑信号，并建立拓扑功率谱密度


<details>
  <summary>Details</summary>
Motivation: 现实数据具有复杂结构，图只能编码二元关系，无法处理非二元连接或节点集之间的关系。单纯复形能连接多个节点并建模单纯形之间的关系，但现有拓扑信号研究主要基于确定性方法，缺乏概率框架

Method: 通过Hodge和Dirac理论的谱对偶性，将平稳性概念推广到拓扑信号：将平稳拓扑信号定义为拓扑滤波器对白噪声的输出。基于此定义拓扑功率谱密度，提供清晰的谱表征

Result: 成功定义了平稳拓扑信号及其功率谱密度，扩展了时间序列和图信号的平稳性性质。通过合成和真实世界模拟验证了拓扑平稳性在谱特性方面的优势

Conclusion: 提出的概率框架为单纯复形上的随机拓扑信号提供了理论基础，定义了拓扑平稳性和功率谱密度，展示了拓扑平稳性在谱分析中的实用价值

Abstract: It is increasingly common for data to possess intricate structure, necessitating new models and analytical tools. Graphs, a prominent type of structure, can encode the relationships between any two entities (nodes). However, graphs neither allow connections that are not dyadic nor permit relationships between sets of nodes. We thus turn to simplicial complexes for connecting more than two nodes as well as modeling relationships between simplices, such as edges and triangles. Our data then consist of signals lying on topological spaces, represented by simplicial complexes. Much recent work explores these topological signals, albeit primarily through deterministic formulations. We propose a probabilistic framework for random signals defined on simplicial complexes. Specifically, we generalize the classical notion of stationarity. By spectral dualities of Hodge and Dirac theory, we define stationary topological signals as the outputs of topological filters given white noise. This definition naturally extends desirable properties of stationarity that hold for both time-series and graph signals. Crucially, we properly define topological power spectral density (PSD) through a clear spectral characterization. We then discuss the advantages of topological stationarity due to spectral properties via the PSD. In addition, we empirically demonstrate the practicality of these benefits through multiple synthetic and real-world simulations.

</details>


### [6] [Multipath Extended Target Tracking with Labeled Random Finite Sets](https://arxiv.org/abs/2602.03464)
*Guanhua Ding,Tao Huang,Qinchen Wu,Jinping Sun,Yanping Wang,Bing Zhu,Guoqiang Mao*

Main category: eess.SP

TL;DR: 提出MPET-GLMB滤波器，用于多径环境下的扩展目标跟踪，通过统一贝叶斯框架联合建模目标存在、测量分区以及测量-目标-传播路径关联，实现目标与反射面的同时轨迹估计。


<details>
  <summary>Details</summary>
Motivation: 高分辨率雷达在自动驾驶系统中至关重要，但传统跟踪算法面临挑战：每个目标产生多个测量值，存在多径效应。现有解决方案要么依赖点目标假设，要么将多径测量视为杂波，而当前扩展目标跟踪器在复杂多径环境中缺乏保持轨迹连续性的能力。

Method: 提出多径扩展目标广义标签多伯努利(MPET-GLMB)滤波器。基于标签随机有限集理论推导统一贝叶斯框架，联合建模目标存在、测量分区以及测量、目标和传播路径之间的关联。采用基于吉布斯采样的联合预测和更新实现以提高计算效率，并引入测量驱动的自适应出生模型来初始化轨迹。

Result: 在模拟场景和真实汽车雷达数据上的实验结果表明，所提出的滤波器优于最先进的方法，在动态多径环境中实现了卓越的状态估计精度和鲁棒的轨迹维护。

Conclusion: MPET-GLMB滤波器有效解决了高分辨率雷达在多径环境下的扩展目标跟踪问题，通过统一框架同时估计目标和反射面的轨迹，无需启发式后处理，在复杂动态环境中表现出优越性能。

Abstract: High-resolution radar sensors are critical for autonomous systems but pose significant challenges to traditional tracking algorithms due to the generation of multiple measurements per object and the presence of multipath effects. Existing solutions often rely on the point target assumption or treat multipath measurements as clutter, whereas current extended target trackers often lack the capability to maintain trajectory continuity in complex multipath environments. To address these limitations, this paper proposes the multipath extended target generalized labeled multi-Bernoulli (MPET-GLMB) filter. A unified Bayesian framework based on labeled random finite set theory is derived to jointly model target existence, measurement partitioning, and the association between measurements, targets, and propagation paths. This formulation enables simultaneous trajectory estimation for both targets and reflectors without requiring heuristic post-processing. To enhance computational efficiency, a joint prediction and update implementation based on Gibbs sampling is developed. Furthermore, a measurement-driven adaptive birth model is introduced to initialize tracks without prior knowledge of target positions. Experimental results from simulated scenarios and real-world automotive radar data demonstrate that the proposed filter outperforms state-of-the-art methods, achieving superior state estimation accuracy and robust trajectory maintenance in dynamic multipath environments.

</details>


### [7] [Channel-Aware Conditional Diffusion Model for Secure MU-MISO Communications](https://arxiv.org/abs/2602.03524)
*Tong Hui,Xiao Tang,Yichen Wang,Qinghe Du,Dusit Niyato,Zhu Han*

Main category: eess.SP

TL;DR: 本文提出了一种基于条件扩散模型的物理层安全方法，用于生成合法的波束成形和人工噪声策略，以提升无线通信的安全性。


<details>
  <summary>Details</summary>
Motivation: 传统优化方法难以实时实现，而深度判别模型可能无法应对未预见场景，因此需要一种能学习最优安全策略分布的新方法。

Method: 将安全优化重构为条件生成过程，使用扩散模型学习近优联合波束成形和人工噪声策略的分布；采用带交叉注意力的U-Net架构整合信道状态信息；通过包含总保密率的目标函数微调模型以增强安全性能。

Result: 仿真结果验证了学习过程的收敛性，并表明所提出的生成方法在各种场景下相比基线方法实现了更优的保密性能。

Conclusion: 基于条件扩散模型的生成方法为物理层安全提供了一种有效的解决方案，能够学习最优安全策略的分布并在多种场景中实现卓越的保密性能。

Abstract: While information securityis a fundamental requirement for wireless communications, conventional optimization based approaches often struggle with real-time implementation, and deep models, typically discriminative in nature, may lack the ability to cope with unforeseen scenarios. To address this challenge, this paper investigates the design of legitimate beamforming and artificial noise (AN) to achieve physical layer security by exploiting the conditional diffusion model. Specifically, we reformulate the security optimization as a conditional generative process, using a diffusion model to learn the inherent distribution of near-optimal joint beamforming and AN strategies. We employ a U-Net architecture with cross-attention to integrate channel state information, as the basis for the generative process. Moreover, we fine-tune the trained model using an objective incorporating the sum secrecy rate such that the security performance is further enhanced. Finally, simulation results validate the learning process convergence and demonstrate that the proposed generative method achieves superior secrecy performance across various scenarios as compared with the baselines.

</details>


### [8] [A Multi-decoder Neural Tracking Method for Accurately Predicting Speech Intelligibility](https://arxiv.org/abs/2602.03624)
*Rien Sonck,Bernd Accou,Tom Francart,Jonas Vanthornhout*

Main category: eess.SP

TL;DR: 提出多解码器方法，通过EEG预测言语接收阈值，准确度接近行为测试（差异<1 dB），可将EEG数据收集时间缩短至15分钟。


<details>
  <summary>Details</summary>
Motivation: 现有EEG方法预测言语可懂度的准确性和鲁棒性不如行为测试（后者测试-重测差异通常<1 dB）。需要一种客观评估方法，适用于无法进行行为测试的人群（如意识障碍患者或助听器验配时）。

Method: 多解码器方法：聚合数百个解码器的数据，每个解码器在不同言语特征和EEG预处理设置下训练，量化神经追踪(NT)言语信号。使用39名参与者数据，记录29分钟EEG/人，听六种信噪比言语和安静故事。NT值组合成高维特征向量，训练支持向量回归模型预测SRT。

Result: 预测与行为SRT显著相关(r=0.647, p<0.001; NRMSE=0.19)，所有差异<1 dB。SHAP分析显示theta/delta波段和早期滞后影响稍大。使用预训练的主体无关解码器可将EEG数据收集时间减少至15分钟（3分钟故事+12分钟六种SNR条件）而不损失准确性。

Conclusion: 多解码器方法能准确预测言语接收阈值，准确度接近行为测试，且大幅减少EEG数据收集时间，为无法进行行为测试的人群提供客观评估工具。

Abstract: Objective: EEG-based methods can predict speech intelligibility, but their accuracy and robustness lag behind behavioral tests, which typically show test-retest differences under 1 dB. We introduce the multi-decoder method to predict speech reception thresholds (SRTs) from EEG recordings, enabling objective assessment for populations unable to perform behavioral tests; such as those with disorders of consciousness or during hearing aid fitting. Approach: The method aggregates data from hundreds of decoders, each trained on different speech features and EEG preprocessing setups to quantify neural tracking (NT) of speech signals. Using data from 39 participants (ages 18-24), we recorded 29 minutes of EEG per person while they listened to speech at six signal-to-noise ratios and a quiet story. NT values were combined into a high-dimensional feature vector per subject, and a support vector regression model was trained to predict SRTs from these vectors. Main Result: Predictions correlated significantly with behavioral SRTs (r = 0.647, p < 0.001; NRMSE = 0.19), with all differences under 1 dB. SHAP analysis showed theta/delta bands and early lags had slightly greater influence. Using pretrained subject-independent decoders reduced required EEG data collection to 15 minutes (3 minutes of story, 12 minutes across six SNR conditions) without losing accuracy.

</details>


### [9] [Low-Complexity Distributed Combining Design for Near-Field Cell-Free XL-MIMO Systems](https://arxiv.org/abs/2602.03581)
*Zhe Wang,Jiayi Zhang,Bokai Xu,Dusit Niyato,Bo Ai,Shiwen Mao,Zhu Han*

Main category: eess.SP

TL;DR: 本文研究了近场无蜂窝超大规模MIMO系统的低复杂度分布式组合方案设计，提出了基于矩阵近似方法和SSOR算法的五种低复杂度分布式组合方案。


<details>
  <summary>Details</summary>
Motivation: 研究近场无蜂窝超大规模MIMO系统中低复杂度分布式组合方案的设计需求，以解决传统集中式处理方案在超大规模系统中面临的高计算复杂度和通信开销问题。

Method: 首先构建了无蜂窝XL-MIMO系统的上行频谱效率分析框架，推导了集中式和分布式处理方案下的CMMSE和LMMSE组合方案。然后基于矩阵近似方法提出了GSLI-MMSE和SI-LMMSE两种方案，以及基于SSOR算法提出了三种分布式SSOR-LMMSE组合方案。

Result: 提出了五种低复杂度分布式组合方案：两种基于矩阵近似方法（GSLI-MMSE和SI-LMMSE）和三种基于SSOR算法。这些方案通过近似全局/局部瞬时信息或迭代求解矩阵逆运算，显著降低了计算复杂度。

Conclusion: 本文为近场无蜂窝XL-MIMO系统提供了有效的低复杂度分布式组合方案设计框架，通过矩阵近似和SSOR算法实现了计算复杂度的显著降低，同时保持了系统性能。

Abstract: In this paper, we investigate the low-complexity distributed combining scheme design for near-field cell-free extremely large-scale multiple-input-multiple-output (CF XL-MIMO) systems. Firstly, we construct the uplink spectral efficiency (SE) performance analysis framework for CF XL-MIMO systems over centralized and distributed processing schemes. Notably, we derive the centralized minimum mean-square error (CMMSE) and local minimum mean-square error (LMMSE) combining schemes over arbitrary channel estimators. Then, focusing on the CMMSE and LMMSE combining schemes, we propose five low-complexity distributed combining schemes based on the matrix approximation methodology or the symmetric successive over relaxation (SSOR) algorithm. More specifically, we propose two matrix approximation methodology-aided combining schemes: Global Statistics \& Local Instantaneous information-based MMSE (GSLI-MMSE) and Statistics matrix Inversion-based LMMSE (SI-LMMSE). These two schemes are derived by approximating the global instantaneous information in the CMMSE combining and the local instantaneous information in the LMMSE combining with the global and local statistics information by asymptotic analysis and matrix expectation approximation, respectively. Moreover, by applying the low-complexity SSOR algorithm to iteratively solve the matrix inversion in the LMMSE combining, we derive three distributed SSOR-based LMMSE combining schemes, distinguished from the applied information and initial values.

</details>


### [10] [Statistics Approximation-Enabled Distributed Beamforming for Cell-Free Massive MIMO](https://arxiv.org/abs/2602.03590)
*Zhe Wang,Emil Björnson,Jiayi Zhang,Peng Zhang,Vitaly Petrov,Bo Ai*

Main category: eess.SP

TL;DR: 提出GSLI-MMSE分布式波束成形方案，用于无蜂窝大规模MIMO网络，在多种信道模型下性能接近集中式MMSE方案


<details>
  <summary>Details</summary>
Motivation: 传统集中式MMSE方案需要全局瞬时信息，计算复杂且通信开销大。需要设计分布式方案，在保持性能的同时减少信息交换需求

Method: 提出GSLI-MMSE（全局统计&局部瞬时信息最小均方误差）方案：1）从每个AP视角构建传统MMSE合并；2）应用统计近似方法，将其他AP的瞬时项用信道统计近似；3）利用上下行对偶性推导相应的预编码方案

Result: 在稳定LoS条件下（如静态用户的固定LoS路径莱斯衰落），GSLI-MMSE方案性能与最优集中式MMSE方案相当

Conclusion: GSLI-MMSE方案通过结合全局统计信息和局部瞬时信息，实现了接近集中式方案的性能，同时减少了分布式网络中的信息交换需求

Abstract: We study a distributed beamforming approach for cell-free massive multiple-input multiple-output networks, referred to as Global Statistics \& Local Instantaneous information-based minimum mean-square error (GSLI-MMSE). The scenario with multi-antenna access points (APs) is considered over three different channel models: correlated Rician fading with fixed or random line-of-sight (LoS) phase-shifts, and correlated Rayleigh fading. With the aid of matrix inversion derivations, we can construct the conventional MMSE combining from the perspective of each AP, where global instantaneous information is involved. Then, for an arbitrary AP, we apply the statistics approximation methodology to approximate instantaneous terms related to other APs by channel statistics to construct the distributed combining scheme at each AP with local instantaneous information and global statistics. With the aid of uplink-downlink duality, we derive the respective GSLI-MMSE precoding schemes. Numerical results showcase that the proposed GSLI-MMSE scheme demonstrates performance comparable to the optimal centralized MMSE scheme, under the stable LoS conditions, e.g., with static users having Rician fading with a fixed LoS path.

</details>


### [11] [VR-VFL: Joint Rate and Client Selection for Vehicular Federated Learning Under Imperfect CSI](https://arxiv.org/abs/2602.03711)
*Metehan Karatas,Subhrakanti Dey,Christian Rohner,Jose Mairton Barros da Silva*

Main category: eess.SP

TL;DR: VR-VFL是一种针对车联网中联邦学习的新型方法，通过动态客户端选择、自适应传输速率选择和灵活轮次时间，在非完美信道状态下实现更快的收敛。


<details>
  <summary>Details</summary>
Motivation: 车联网边缘计算中的联邦学习面临车辆高移动性和非完美信道状态信息的挑战，现有方法通常假设固定通信轮次或理想信道条件，这在现实场景中效果有限。

Method: 提出可变速率车联网联邦学习(VR-VFL)，结合动态客户端选择和自适应传输速率选择，允许轮次时间根据无线条件灵活调整，基于双目标优化框架平衡学习收敛和轮次完成时间。

Result: 仿真结果显示，VR-VFL方案比现有方法收敛速度快约40%。

Conclusion: VR-VFL通过考虑移动性和实际无线约束，为车联网边缘网络中的联邦学习提供了更实用高效的方法。

Abstract: Federated learning in vehicular edge networks faces major challenges in efficient resource allocation, largely due to high vehicle mobility and the presence of imperfect channel state information. Many existing methods oversimplify these realities, often assuming fixed communication rounds or ideal channel conditions, which limits their effectiveness in real-world scenarios. To address this, we propose variable rate vehicular federated learning (VR-VFL), a novel federated learning method designed specifically for vehicular networks under imperfect channel state information. VR-VFL combines dynamic client selection with adaptive transmission rate selection, while also allowing round times to flex in response to changing wireless conditions. At its core, VR-VFL is built on a bi-objective optimization framework that strikes a balance between improving learning convergence and minimizing the time required to complete each round. By accounting for both the challenges of mobility and realistic wireless constraints, VR-VFL offers a more practical and efficient approach to federated learning in vehicular edge networks. Simulation results show that the proposed VR-VFL scheme achieves convergence approximately 40% faster than other methods in the literature.

</details>


### [12] [A Narrowband Fully-Analog Multi-Antenna Transmitter](https://arxiv.org/abs/2602.03718)
*Nikola Zlatanov*

Main category: eess.SP

TL;DR: 提出一种窄带全模拟N天线发射机，通过可编程无源干涉网络和相位控制元件，用单个相干RF信号合成任意复数激励向量，实现全数字发射机功能但功耗更低。


<details>
  <summary>Details</summary>
Motivation: 传统全数字天线阵列需要每个天线配备独立的RF链和DAC，导致高功耗和高成本。本文旨在设计一种全模拟发射机架构，通过无源网络和相位控制实现类似功能，同时显著降低功耗。

Method: 采用可编程无源干涉网络，网络传输是酉矩阵，通过二进制幅度分割树分配天线幅度（N-1个可调分光比），配合每天线输出相位库（N个可调相移），共2N-1个可调自由度。无需迭代优化，支持符号级更新。

Result: 提出的全模拟发射机架构在N≤16时，相比等效的全数字阵列能显著节省RF前端功耗。架构使用确定性O(N)编程过程，无需迭代优化。

Conclusion: 该窄带全模拟发射机概念验证成功，通过无源干涉网络和相位控制实现全数字功能，为低功耗天线阵列设计提供了新思路，特别适合需要快速符号更新的应用场景。

Abstract: This paper proposes a narrowband fully-analog $N$-antenna transmitter that emulates the functionality of a narrowband fully-digital $N$-antenna transmitter. Specifically, in symbol interval $m$, the proposed fully-analog transmitter synthesizes an arbitrary complex excitation vector $\bm x[m]\in\mathbb{C}^N$ with prescribed total power $\|\bm x[m]\|_2^2=P$ from a single coherent RF tone, using only tunable phase-control elements embedded in a passive interferometric programmable network. The programmable network is excited through one input port while the remaining $N - 1$ input ports are impedance matched. In the ideal lossless case, the network transfer is unitary and therefore redistributes RF power among antenna ports without dissipative amplitude control.
  The synthesis task is posed as a unitary state-preparation problem: program a unitary family so that $\bm V(\bm\varphi)\bm e_1=\bm c$, where $\bm c=\bm x/\sqrt{P}$ and $\|\bm c\|_2=1$. We provide a constructive realization and a closed-form programming rule: a binary magnitude-splitting tree allocates the desired per-antenna magnitudes $|c_n|$ using $N -1$ tunable split ratios, and a per-antenna output phase bank assigns the target phases using $N$ tunable phase shifts. The resulting architecture uses $2N-1$ real tunable degrees of freedom and admits a deterministic $O(N)$ programming procedure with no iterative optimization, enabling symbol-by-symbol updates when the chosen phase-control technology supports the required tuning speed.
  Using representative COTS components, we model the RF-front-end DC power of the proposed fully-analog transmitter and compare it against an equivalent COTS fully-digital array. For $N\le 16$, the comparison indicates significant RF-front-end power savings for the fully-analog architecture.
  The results in this paper are intended as a proof-of-concept for a narrowband fully-analog transmitter.

</details>


### [13] [Digital-Twin Empowered Deep Reinforcement Learning For Site-Specific Radio Resource Management in NextG Wireless Aerial Corridor](https://arxiv.org/abs/2602.03801)
*Pulok Tarafder,Zoheb Hassan,Imtiaz Ahmed,Danda B. Rawat,Kamrul Hasan,Cong Pu*

Main category: eess.SP

TL;DR: 提出基于数字孪生的两阶段优化框架，结合物理波束增益建模与深度强化学习，解决多无人机空中走廊基站关联与波束选择问题，实现高性能低延迟资源管理。


<details>
  <summary>Details</summary>
Motivation: 多无人机空中走廊的基站关联与波束选择面临高维动作空间、全局信道状态信息获取开销大、信道快速变化和严格延迟要求等挑战。传统组合优化方法计算复杂，学习型方法需要大量站点特定数据集训练。

Method: 采用数字孪生驱动的两阶段优化框架：第一阶段构建信道孪生，使用高精度射线追踪求解器和地理空间信息捕获站点特定传播特性，采用双退火算法预计算最优传输波束方向；第二阶段训练多头近端策略优化智能体，在数字孪生生成的信道数据集上学习，直接映射复杂信道和波束状态来联合执行无人机-基站-波束关联决策。

Result: 提出的PPO智能体在密集无人机场景中，相比DQN实现44%-121%的性能提升，相比传统启发式优化方案获得249%-807%的性能增益，同时将推理延迟降低数个数量级。

Conclusion: 数字孪生驱动的训练管道能够为站点特定部署提供高性能、低延迟的无线资源管理策略，适用于下一代空中走廊网络的实时资源管理。

Abstract: Joint base station (BS) association and beam selection in multi-UAV aerial corridors constitutes a challenging radio resource management (RRM) problem. It is driven by high-dimensional action spaces, need for substantial overhead to acquire global channel state information (CSI), rapidly varying propagation channels, and stringent latency requirements. Conventional combinatorial optimization methods, while near-optimal, are computationally prohibitive for real-time operation in such dynamic environments. While learning-based approaches can mitigate computational complexity and CSI overhead, the need for extensive site-specific (SS) datasets for model training remains a key challenge. To address these challenges, we develop a Digital Twin (DT)-enabled two-stage optimization framework that couples physics-based beam gain modeling with DRL for scalable online decision-making. In the first stage, a channel twin (CT) is constructed using a high-fidelity ray-tracing solver with geo-spatial contexts, and network information to capture SS propagation characteristics, and dual annealing algorithm is employed to precompute optimal transmission beam directions. In the second stage, a Multi-Head Proximal Policy Optimization (MH-PPO) agent, equipped with a scalable multi-head actor-critic architecture, is trained on the DT-generated channel dataset to directly map complex channel and beam states to jointly execute UAV-BS-beam association decisions. The proposed PPO agent achieves a 44%-121% improvement over DQN and 249%-807% gain over traditional heuristic based optimization schemes in a dense UAV scenario, while reducing inference latency by several orders of magnitude. These results demonstrate that DT-driven training pipelines can deliver high-performance, low-latency RRM policies tailored to SS deployments suitable for real-time resource management in next-generation aerial corridor networks.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [14] [WAXAL: A Large-Scale Multilingual African Language Speech Corpus](https://arxiv.org/abs/2602.02734)
*Abdoulaye Diack,Perry Nelson,Kwaku Agbesi,Angela Nakalembe,MohamedElfatih MohamedKhair,Vusumuzi Dube,Tavonga Siyavora,Subhashini Venugopalan,Jason Hickey,Uche Okonkwo,Abhishek Bapna,Isaac Wiafe,Raynard Dodzi Helegah,Elikem Doe Atsakpo,Charles Nutrokpor,Fiifi Baffoe Payin Winful,Kafui Kwashie Solaga,Jamal-Deen Abdulai,Akon Obu Ekpezu,Audace Niyonkuru,Samuel Rutunda,Boris Ishimwe,Michael Melese,Engineer Bainomugisha,Joyce Nakatumba-Nabende,Andrew Katumba,Claire Babirye,Jonathan Mukiibi,Vincent Kimani,Samuel Kibacia,James Maina,Fridah Emmah,Ahmed Ibrahim Shekarau,Ibrahim Shehu Adamu,Yusuf Abdullahi,Howard Lakougna,Bob MacDonald,Hadar Shemtov,Aisha Walcott-Bryant,Moustapha Cisse,Avinatan Hassidim,Jeff Dean,Yossi Matias*

Main category: eess.AS

TL;DR: WAXAL是一个大规模、开放访问的语音数据集，包含21种撒哈拉以南非洲语言，提供约1250小时的ASR数据和180小时的TTS数据，旨在解决这些语言在语音技术中的资源不足问题。


<details>
  <summary>Details</summary>
Motivation: 语音技术的发展主要偏向高资源语言，导致大多数撒哈拉以南非洲语言面临显著的数字化鸿沟。为了解决这一资源不平等问题，需要为这些语言创建大规模、高质量的语音数据集。

Method: 通过与四家非洲学术和社区组织合作，采用系统化的数据收集、标注和质量控制方法。数据集包含两个主要部分：ASR数据集（约1250小时的自然语音转录）和TTS数据集（180小时高质量单说话者录音，阅读音素平衡脚本）。

Result: 成功创建了WAXAL数据集，涵盖21种语言，代表超过1亿使用者。数据集已在Hugging Face平台发布，采用CC-BY-4.0许可协议，为研究和包容性技术开发提供重要资源。

Conclusion: WAXAL数据集填补了撒哈拉以南非洲语言语音资源的空白，有望促进这些语言的语音技术研究、包容性技术开发以及数字保存，同时作者也讨论了潜在的局限性和伦理考量。

Abstract: The advancement of speech technology has predominantly favored high-resource languages, creating a significant digital divide for speakers of most Sub-Saharan African languages. To address this gap, we introduce WAXAL, a large-scale, openly accessible speech dataset for 21 languages representing over 100 million speakers. The collection consists of two main components: an Automated Speech Recognition (ASR) dataset containing approximately 1,250 hours of transcribed, natural speech from a diverse range of speakers, and a Text-to-Speech (TTS) dataset with over 180 hours of high-quality, single-speaker recordings reading phonetically balanced scripts. This paper details our methodology for data collection, annotation, and quality control, which involved partnerships with four African academic and community organizations. We provide a detailed statistical overview of the dataset and discuss its potential limitations and ethical considerations. The WAXAL datasets are released at https://huggingface.co/datasets/google/WaxalNLP under the permissive CC-BY-4.0 license to catalyze research, enable the development of inclusive technologies, and serve as a vital resource for the digital preservation of these languages.

</details>


### [15] [WST-X Series: Wavelet Scattering Transform for Interpretable Speech Deepfake Detection](https://arxiv.org/abs/2602.02980)
*Xi Xuan,Davide Carbone,Ruchi Pandey,Wenxin Zhang,Tomi H. Kinnunen*

Main category: eess.AS

TL;DR: WST-X系列特征提取器结合了小波散射变换与深度卷积网络的非线性特性，在语音深度伪造检测中取得了优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有语音深度伪造检测前端存在局限性：手工设计的滤波器组特征虽然透明但难以捕捉高层语义细节，性能不如自监督特征；而自监督特征缺乏可解释性且可能忽略细粒度频谱异常。需要一种结合两者优点的特征提取方法。

Method: 提出WST-X系列特征提取器，基于小波散射变换，将小波与类似深度卷积网络的非线性特性相结合。研究1D和2D WST分别提取声学细节和高阶结构异常。通过小平均尺度(J)结合高频率和方向分辨率(Q, L)来捕捉细微伪影。

Result: 在Deepfake-Eval-2024数据集上的实验结果表明，WST-X显著优于现有前端方法。分析发现小平均尺度结合高频率和方向分辨率对于捕捉细微伪影至关重要。

Conclusion: 小波散射变换提供了平移不变性和形变稳定性特征，为语音深度伪造检测提供了鲁棒且可解释的解决方案，结合了手工特征的可解释性和自监督特征的高性能优势。

Abstract: Designing front-ends for speech deepfake detectors primarily focuses on two categories. Hand-crafted filterbank features are transparent but are limited in capturing high-level semantic details, often resulting in performance gaps compared to self-supervised (SSL) features. SSL features, in turn, lack interpretability and may overlook fine-grained spectral anomalies. We propose the WST-X series, a novel family of feature extractors that combines the best of both worlds via the wavelet scattering transform (WST), integrating wavelets with nonlinearities analogous to deep convolutional networks. We investigate 1D and 2D WSTs to extract acoustic details and higher-order structural anomalies, respectively. Experimental results on the recent and challenging Deepfake-Eval-2024 dataset indicate that WST-X outperforms existing front-ends by a wide margin. Our analysis reveals that a small averaging scale ($J$), combined with high-frequency and directional resolutions ($Q, L$), is critical for capturing subtle artifacts. This underscores the value of translation-invariant and deformation-stable features for robust and interpretable speech deepfake detection.

</details>


### [16] [Mići Princ -- A Little Boy Teaching Speech Technologies the Chakavian Dialect](https://arxiv.org/abs/2602.03245)
*Nikola Ljubešić,Peter Rupnik,Tea Perinčić*

Main category: eess.AS

TL;DR: 将《小王子》查卡维亚方言译本制作成计算机可读、AI就绪的数据集，包含文本和音频对齐，用于方言保存、AI研究和数字出版


<details>
  <summary>Details</summary>
Motivation: 1) 保存珍贵的查卡维亚方言内容，超越有限的印刷和音频版本；2) 为AI相关应用提供数据，如方言语音识别模型适配；3) 希望数据集能转化为数字在线版本，让更多人欣赏方言文学之美

Method: 发布《小王子》查卡维亚方言译本的印刷和音频书籍作为计算机可读数据集，文本和音频在单词级别对齐，并存储在CLARIN.SI知识库中。使用该数据集适配Whisper-large-v3语音识别模型，使其适用于查卡维亚方言

Result: 成功将Whisper-large-v3模型适配到查卡维亚方言，在选定测试数据上将词错误率降低了一半，字符级错误减少了三分之二。数据集已公开发布，可供研究和应用使用

Conclusion: 该数据集不仅保存了珍贵的方言文化遗产，还为AI研究和方言研究提供了宝贵资源，未来可进一步开发为数字在线版本，扩大受众范围，促进方言保护和AI技术发展

Abstract: This paper documents our efforts in releasing the printed and audio book of the translation of the famous novel The Little Prince into the Chakavian dialect, as a computer-readable, AI-ready dataset, with the textual and the audio components of the two releases now aligned on the level of each written and spoken word. Our motivation for working on this release is multiple. The first one is our wish to preserve the highly valuable and specific content beyond the small editions of the printed and the audio book. With the dataset published in the CLARIN.SI repository, this content is from now on at the fingertips of any interested individual. The second motivation is to make the data available for various artificial-intelligence-related usage scenarios, such as the one we follow upon inside this paper already -- adapting the Whisper-large-v3 open automatic speech recognition model, with decent performance on standard Croatian, to Chakavian dialectal speech. We can happily report that with adapting the model, the word error rate on the selected test data has being reduced to a half, while we managed to remove up to two thirds of the error on character level. We envision many more usages of this dataset beyond the set of experiments we have already performed, both on tasks of artificial intelligence research and application, as well as dialectal research. The third motivation for this release is our hope that this, now highly structured dataset, will be transformed into a digital online edition of this work, allowing individuals beyond the research and technology communities to enjoy the beauty of the message of the little boy in the desert, told through the spectacular prism of the Chakavian dialect.

</details>


### [17] [A Unified SVD-Modal Solution for Sparse Sound Field Reconstruction with Hybrid Spherical-Linear Microphone Arrays](https://arxiv.org/abs/2602.03398)
*Shunxi Xu,Thushara Abhayapala,Craig T. Jin*

Main category: eess.AS

TL;DR: 提出基于奇异值分解的数据驱动稀疏恢复框架，用于混合球面线性麦克风阵列，通过模态分析提升空间选择性，在混响环境下优于纯球面阵列和直接拼接方法。


<details>
  <summary>Details</summary>
Motivation: 传统球面麦克风阵列(SMA)在稀疏声场重建中存在局限性，需要更有效的混合阵列处理方法来提升空间选择性和重建鲁棒性。

Method: 采用奇异值分解(SVD)处理传输算子，获得正交的麦克风和场模态。在纯球面阵列情况下退化为球谐函数，而引入线性麦克风阵列(LMA)则产生超越球谐函数的补充模态。

Result: 模态分析显示频率范围内与球谐函数存在一致偏差，证实了改进的空间选择性。混响环境实验显示能量图失配和角度误差在频率、距离和声源数量方面均减小，优于纯球面阵列和直接拼接方法。

Conclusion: SVD模态处理为混合阵列提供了原理性和统一的处理方法，实现了鲁棒的稀疏声场重建，证明了该方法在混合阵列处理中的优越性。

Abstract: We propose a data-driven sparse recovery framework for hybrid spherical linear microphone arrays using singular value decomposition (SVD) of the transfer operator. The SVD yields orthogonal microphone and field modes, reducing to spherical harmonics (SH) in the SMA-only case, while incorporating LMAs introduces complementary modes beyond SH. Modal analysis reveals consistent divergence from SH across frequency, confirming the improved spatial selectivity. Experiments in reverberant conditions show reduced energy-map mismatch and angular error across frequency, distance, and source count, outperforming SMA-only and direct concatenation. The results demonstrate that SVD-modal processing provides a principled and unified treatment of hybrid arrays for robust sparse sound-field reconstruction.

</details>


### [18] [Conditional Flow Matching for Visually-Guided Acoustic Highlighting](https://arxiv.org/abs/2602.03762)
*Hugo Malard,Gael Le Lan,Daniel Wong,David Lou Alon,Yi-Chiao Wu,Sanjeel Parekh*

Main category: eess.AS

TL;DR: 提出基于条件流匹配的生成式框架，用于视觉引导的音频高亮处理，通过滚动损失和跨模态融合模块解决音频重混音中的模糊性问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于判别式模型的方法在处理音频重混音时存在局限性，因为从平衡不良到良好平衡的音频之间不存在自然的一对一映射关系，导致视觉和听觉焦点错位。

Method: 将任务重构为生成问题，采用条件流匹配框架，引入滚动损失惩罚最终步骤的漂移以稳定长程流积分，并提出融合音频和视觉线索的条件模块进行跨模态源选择。

Result: 广泛的定量和定性评估表明，该方法在视觉引导音频重混音任务上持续超越先前最先进的判别式方法。

Conclusion: 视觉引导的音频重混音问题最好通过生成式建模来解决，条件流匹配框架结合滚动损失和跨模态融合模块能够有效处理音频重混音中的固有模糊性。

Abstract: Visually-guided acoustic highlighting seeks to rebalance audio in alignment with the accompanying video, creating a coherent audio-visual experience. While visual saliency and enhancement have been widely studied, acoustic highlighting remains underexplored, often leading to misalignment between visual and auditory focus. Existing approaches use discriminative models, which struggle with the inherent ambiguity in audio remixing, where no natural one-to-one mapping exists between poorly-balanced and well-balanced audio mixes. To address this limitation, we reframe this task as a generative problem and introduce a Conditional Flow Matching (CFM) framework. A key challenge in iterative flow-based generation is that early prediction errors -- in selecting the correct source to enhance -- compound over steps and push trajectories off-manifold. To address this, we introduce a rollout loss that penalizes drift at the final step, encouraging self-correcting trajectories and stabilizing long-range flow integration. We further propose a conditioning module that fuses audio and visual cues before vector field regression, enabling explicit cross-modal source selection. Extensive quantitative and qualitative evaluations show that our method consistently surpasses the previous state-of-the-art discriminative approach, establishing that visually-guided audio remixing is best addressed through generative modeling.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [19] [VividVoice: A Unified Framework for Scene-Aware Visually-Driven Speech Synthesis](https://arxiv.org/abs/2602.02591)
*Chengyuan Ma,Jiawei Jin,Ruijie Xiong,Chunxiang Jin,Canxiang Yan,Wenming Yang*

Main category: cs.SD

TL;DR: VividVoice是一个场景感知的视觉驱动语音合成框架，通过构建大规模混合多模态数据集Vivid-210K和设计解耦记忆库架构D-MSVA模块，解决了数据稀缺和模态解耦两大挑战，显著提升了音频保真度、内容清晰度和多模态一致性。


<details>
  <summary>Details</summary>
Motivation: 现有语音生成模型在创建与现实物理世界一致的沉浸式听觉体验方面存在局限，需要解决数据稀缺和模态解耦两大核心挑战。

Method: 1. 构建大规模高质量混合多模态数据集Vivid-210K，通过创新程序化流程首次建立视觉场景、说话人身份和音频之间的强相关性；2. 设计核心对齐模块D-MSVA，采用解耦记忆库架构和跨模态混合监督策略，实现从视觉场景到音色和环境声学特征的细粒度对齐。

Result: 主客观实验结果表明，VividVoice在音频保真度、内容清晰度和多模态一致性方面显著优于现有基线模型。

Conclusion: VividVoice成功解决了场景感知视觉驱动语音合成的关键问题，通过创新的数据集构建和模型设计，为创建与现实世界一致的沉浸式听觉体验提供了有效解决方案。

Abstract: We introduce and define a novel task-Scene-Aware Visually-Driven Speech Synthesis, aimed at addressing the limitations of existing speech generation models in creating immersive auditory experiences that align with the real physical world. To tackle the two core challenges of data scarcity and modality decoupling, we propose VividVoice, a unified generative framework. First, we constructed a large-scale, high-quality hybrid multimodal dataset, Vivid-210K, which, through an innovative programmatic pipeline, establishes a strong correlation between visual scenes, speaker identity, and audio for the first time. Second, we designed a core alignment module, D-MSVA, which leverages a decoupled memory bank architecture and a cross-modal hybrid supervision strategy to achieve fine-grained alignment from visual scenes to timbre and environmental acoustic features. Both subjective and objective experimental results provide strong evidence that VividVoice significantly outperforms existing baseline models in terms of audio fidelity, content clarity, and multimodal consistency. Our demo is available at https://chengyuann.github.io/VividVoice/.

</details>


### [20] [When Noise Lowers The Loss: Rethinking Likelihood-Based Evaluation in Music Large Language Models](https://arxiv.org/abs/2602.02738)
*Xiaosha Li,Chun Liu,Ziyu Wang*

Main category: cs.SD

TL;DR: 论文发现音乐大语言模型的交叉熵损失在遇到系统损坏的音乐时反而降低，这削弱了其作为质量指标的可靠性。通过噪声注入实验，作者发现损失曲线形状（而非绝对值）能反映模型对音乐完整性的辨别能力。


<details>
  <summary>Details</summary>
Motivation: 随着音乐大语言模型的兴起，需要可靠的方法来评估输出质量，特别是区分高质量作品和"垃圾音乐"。作者观察到标准交叉熵损失在模型遇到系统损坏的音乐时反而降低，这质疑了其作为独立质量指标的有效性。

Method: 引入噪声注入实验，在音乐上下文中注入不同长度的受控噪声信号。假设模型损失对这些扰动的反应（特别是短注入时的急剧增加"峰值"区域）可以作为模型辨别音乐完整性的代理指标。在音频波形域中使用MusicGen模型进行实验。

Result: 实验证实音乐大语言模型对局部纹理级干扰的反应比对全局语义损坏更强烈。损失曲线的形状（而非绝对值）编码了关于生成内容质量的关键信息。Music LLMs对局部、纹理级破坏的反应比对全局语义损坏更敏感。

Conclusion: 基于损失曲线形状的评估方法可以作为一种无标签、模型内在的框架来评估音乐质量，为更原则性的训练目标和更精确的基准测试开辟了新途径。

Abstract: The rise of music large language models (LLMs) demands robust methods of evaluating output quality, especially in distinguishing high-quality compositions from "garbage music". Curiously, we observe that the standard cross-entropy loss -- a core training metric -- often decrease when models encounter systematically corrupted music, undermining its validity as a standalone quality indicator. To investigate this paradox, we introduce noise injection experiment, where controlled noise signal of varying lengths are injected into musical contexts. We hypothesize that a model's loss reacting positively to these perturbations, specifically a sharp increase ("Peak" area) for short injection, can serve as a proxy for its ability to discern musical integrity. Experiments with MusicGen models in the audio waveform domain confirm that Music LLMs respond more strongly to local, texture-level disruptions than to global semantic corruption. Beyond exposing this bias, our results highlight a new principle: the shape of the loss curve -- rather than its absolute value -- encodes critical information about the quality of the generated content (i.e., model behavior). We envision this profile-based evaluation as a label-free, model-intrinsic framework for assessing musical quality -- opening the door to more principled training objectives and sharper benchmarks.

</details>


### [21] [Synthetic Data Augmentation for Medical Audio Classification: A Preliminary Evaluation](https://arxiv.org/abs/2602.02955)
*David McShannon,Anthony Mella,Nicholas Dietrich*

Main category: cs.SD

TL;DR: 呼吸音分类中，三种生成式数据增强方法（VAE、GAN、扩散模型）对标准CNN分类器性能提升有限，仅模型集成带来轻微改善。


<details>
  <summary>Details</summary>
Motivation: 医学音频分类面临信噪比低、特征细微、类内变异大、类别不平衡和训练数据有限等挑战。合成数据增强被提出作为缓解这些限制的潜在策略，但先前研究的方法不一致且结果混杂。

Method: 使用基线深度卷积神经网络在中等不平衡数据集（73%:27%）上训练，评估三种生成式增强策略：变分自编码器、生成对抗网络和扩散模型，在受控实验条件下进行测试。

Result: 无增强的基线模型F1分数为0.645。单个增强策略未观察到性能提升，多个配置显示中性或性能下降。仅增强模型集成带来F1分数的适度改善（0.664）。

Conclusion: 对于医学音频分类，合成增强应用于标准CNN分类器时可能无法持续提升性能。未来工作应关注任务特定数据特征、模型-增强兼容性以及合成增强在医学音频应用中有效的评估框架。

Abstract: Medical audio classification remains challenging due to low signal-to-noise ratios, subtle discriminative features, and substantial intra-class variability, often compounded by class imbalance and limited training data. Synthetic data augmentation has been proposed as a potential strategy to mitigate these constraints; however, prior studies report inconsistent methodological approaches and mixed empirical results. In this preliminary study, we explore the impact of synthetic augmentation on respiratory sound classification using a baseline deep convolutional neural network trained on a moderately imbalanced dataset (73%:27%). Three generative augmentation strategies (variational autoencoders, generative adversarial networks, and diffusion models) were assessed under controlled experimental conditions. The baseline model without augmentation achieved an F1-score of 0.645. Across individual augmentation strategies, performance gains were not observed, with several configurations demonstrating neutral or degraded classification performance. Only an ensemble of augmented models yielded a modest improvement in F1-score (0.664). These findings suggest that, for medical audio classification, synthetic augmentation may not consistently enhance performance when applied to a standard CNN classifier. Future work should focus on delineating task-specific data characteristics, model-augmentation compatibility, and evaluation frameworks necessary for synthetic augmentation to be effective in medical audio applications.

</details>


### [22] [Rethinking Music Captioning with Music Metadata LLMs](https://arxiv.org/abs/2602.03023)
*Irmak Bukey,Zhepei Wang,Chris Donahue,Nicholas J. Bryan*

Main category: cs.SD

TL;DR: 提出基于元数据的音乐描述方法：先训练元数据预测模型从音频推断详细元数据，再通过预训练大语言模型生成描述，相比端到端方法更灵活高效


<details>
  <summary>Details</summary>
Motivation: 音乐描述任务需要高质量描述数据，但这类数据稀缺。传统方法使用大语言模型从元数据合成描述作为训练数据，但这会固定风格样式并将事实信息与语言风格纠缠在一起

Method: 提出两阶段方法：1) 训练元数据预测模型从音频推断详细元数据；2) 在推理时通过预训练大语言模型将元数据转换为富有表现力的描述

Result: 相比在LLM生成的描述上训练的端到端基线：1) 在更少训练时间内达到相当性能；2) 提供灵活性可在训练后轻松改变风格；3) 支持音频和部分元数据提示，实现强大的元数据补全

Conclusion: 基于元数据的音乐描述方法比端到端方法更高效灵活，支持风格定制和元数据补全任务，为音乐理解和可控音乐生成提供更好解决方案

Abstract: Music captioning, or the task of generating a natural language description of music, is useful for both music understanding and controllable music generation. Training captioning models, however, typically requires high-quality music caption data which is scarce compared to metadata (e.g., genre, mood, etc.). As a result, it is common to use large language models (LLMs) to synthesize captions from metadata to generate training data for captioning models, though this process imposes a fixed stylization and entangles factual information with natural language style. As a more direct approach, we propose metadata-based captioning. We train a metadata prediction model to infer detailed music metadata from audio and then convert it into expressive captions via pre-trained LLMs at inference time. Compared to a strong end-to-end baseline trained on LLM-generated captions derived from metadata, our method: (1) achieves comparable performance in less training time over end-to-end captioners, (2) offers flexibility to easily change stylization post-training, enabling output captions to be tailored to specific stylistic and quality requirements, and (3) can be prompted with audio and partial metadata to enable powerful metadata imputation or in-filling--a common task for organizing music data.

</details>


### [23] [GRAM: Spatial general-purpose audio representations for real-world environments](https://arxiv.org/abs/2602.03307)
*Goksenin Yuksel,Marcel van Gerven,Kiki van der Heijden*

Main category: cs.SD

TL;DR: GRAM是一个通用真实世界音频模型，使用多通道掩码自编码器学习空间音频表示，在模拟和真实环境中的表现优于现有音频基础模型。


<details>
  <summary>Details</summary>
Motivation: 当前音频基础模型主要针对单通道、干净音频，在真实混响和噪声环境中的性能有限，且大多忽略空间维度，无法处理声音定位任务。

Method: 提出GRAM模型，采用多通道掩码自编码器高效学习空间音频表示，并在模拟自然空间声学环境和真实环境记录上进行评估。

Result: GRAM在NatHEAR和HEAR基准测试中优于所有最先进的自监督音频基础模型，且仅使用少量训练数据；在模拟环境中实现最先进的定位性能，并能有效泛化到真实环境记录。

Conclusion: GRAM代表了向鲁棒的真实世界空间音频基础模型的重要进展，解决了现有模型在真实声学环境中的局限性。

Abstract: Audio foundation models learn general-purpose audio representations that facilitate a wide range of downstream tasks. While the performance of these models has greatly increased for conventional single-channel, dry audio clips, their success in real-world acoustic environments with reverberation and noise is limited. Furthermore, most audio foundation models ignore the spatial dimension of real-world acoustic environments, ruling out tasks involving sound localization. To address these limitations, we propose GRAM: a general-purpose real-world audio model that employs a multi-channel masked autoencoder to efficiently learn spatial audio representations. We evaluated GRAM and other audio foundation models in a standardized manner on high-quality simulations of naturalistic, spatial acoustic environments as well as recordings of real-world environments and release these two complementary benchmark task suites: NatHEAR and RealSELD. Our results demonstrate that GRAM outperforms all state-of-the-art self-supervised audio foundation models on NatHEAR and the clean, single-channel version HEAR, while using only a fraction of the training data. GRAM also shows state-of-the-art localization performance in simulated environments and generalizes efficiently to real-world recordings in RealSELD. Taken together, GRAM presents a significant advance toward robust spatial audio foundation models for real-world environments.

</details>


### [24] [PACE: Pretrained Audio Continual Learning](https://arxiv.org/abs/2602.03355)
*Chang Li,Kanglei Zhou,Liyuan Wang*

Main category: cs.SD

TL;DR: 该论文提出了首个音频持续学习的系统基准，并开发了PACE方法来解决预训练音频模型在持续学习中的挑战，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 音频是分析语音、音乐和环境声音的基本模态，但预训练音频模型在现实世界数据分布随时间变化时表现脆弱。目前缺乏针对音频持续学习的系统基准，且视觉领域的参数高效微调策略直接迁移到音频效果不佳。

Method: 提出PACE方法：1）通过正则化解析分类器增强首次会话适应；2）使用自适应子空间正交PEFT实现多会话适应以改进语义对齐；3）引入基于频谱图的边界感知扰动来减少表示重叠并提高稳定性。

Result: 在六个不同的音频持续学习基准测试中，PACE方法显著优于最先进的基线方法，标志着向稳健和可扩展的音频持续学习迈出了重要一步。

Conclusion: 该研究揭示了音频骨干网络与视觉网络的本质差异，提出了针对音频持续学习的有效解决方案，为音频模型的持续学习能力提供了系统基准和方法论基础。

Abstract: Audio is a fundamental modality for analyzing speech, music, and environmental sounds. Although pretrained audio models have significantly advanced audio understanding, they remain fragile in real-world settings where data distributions shift over time. In this work, we present the first systematic benchmark for audio continual learning (CL) with pretrained models (PTMs), together with a comprehensive analysis of its unique challenges. Unlike in vision, where parameter-efficient fine-tuning (PEFT) has proven effective for CL, directly transferring such strategies to audio leads to poor performance. This stems from a fundamental property of audio backbones: they focus on low-level spectral details rather than structured semantics, causing severe upstream-downstream misalignment. Through extensive empirical study, we identify analytic classifiers with first-session adaptation (FSA) as a promising direction, but also reveal two major limitations: representation saturation in coarse-grained scenarios and representation drift in fine-grained scenarios. To address these challenges, we propose PACE, a novel method that enhances FSA via a regularized analytic classifier and enables multi-session adaptation through adaptive subspace-orthogonal PEFT for improved semantic alignment. In addition, we introduce spectrogram-based boundary-aware perturbations to mitigate representation overlap and improve stability. Experiments on six diverse audio CL benchmarks demonstrate that PACE substantially outperforms state-of-the-art baselines, marking an important step toward robust and scalable audio continual learning with PTMs.

</details>


### [25] [CoCoEmo: Composable and Controllable Human-Like Emotional TTS via Activation Steering](https://arxiv.org/abs/2602.03420)
*Siyi Wang,Shihong Tan,Siyi Liu,Hong Jia,Gongping Huang,James Bailey,Ting Dang*

Main category: cs.SD

TL;DR: 本文首次系统分析了混合TTS模型中情感控制的激活引导技术，提出了可量化的引导框架和多评分者评估协议，实现了可组合的混合情感合成和可靠的文本-情感不匹配合成。


<details>
  <summary>Details</summary>
Motivation: 人类语音中的情感表达是细微且组合性的，通常涉及多个有时相互冲突的情感线索，可能与语言内容不一致。而大多数表达性文本转语音系统强制使用单一话语级情感，压缩了情感多样性，抑制了混合或文本-情感不对齐的表达。

Method: 引入了一个可量化的、可控的激活引导框架，通过潜在方向向量进行情感控制，并设计了多评分者评估协议。该方法特别关注在混合TTS架构中应在何处应用引导，以及如何评估复杂的情感行为。

Result: 首次证明情感韵律和表达变异性主要由TTS语言模块而非流匹配模块合成。提供了一种轻量级的引导方法，能够生成自然、类人的情感语音，支持可组合的混合情感合成和可靠的文本-情感不匹配合成。

Conclusion: 激活引导为混合TTS模型中的情感控制提供了有效的解决方案，揭示了情感表达在TTS架构中的分布特性，为实现更自然、复杂的情感语音合成开辟了新途径。

Abstract: Emotional expression in human speech is nuanced and compositional, often involving multiple, sometimes conflicting, affective cues that may diverge from linguistic content. In contrast, most expressive text-to-speech systems enforce a single utterance-level emotion, collapsing affective diversity and suppressing mixed or text-emotion-misaligned expression. While activation steering via latent direction vectors offers a promising solution, it remains unclear whether emotion representations are linearly steerable in TTS, where steering should be applied within hybrid TTS architectures, and how such complex emotion behaviors should be evaluated. This paper presents the first systematic analysis of activation steering for emotional control in hybrid TTS models, introducing a quantitative, controllable steering framework, and multi-rater evaluation protocols that enable composable mixed-emotion synthesis and reliable text-emotion mismatch synthesis. Our results demonstrate, for the first time, that emotional prosody and expressive variability are primarily synthesized by the TTS language module instead of the flow-matching module, and also provide a lightweight steering approach for generating natural, human-like emotional speech.

</details>


### [26] [D3PIA: A Discrete Denoising Diffusion Model for Piano Accompaniment Generation From Lead sheet](https://arxiv.org/abs/2602.03523)
*Eunjin Choi,Hounsu Kim,Hayeon Bang,Taegyun Kwon,Juhan Nam*

Main category: cs.SD

TL;DR: D3PIA：基于离散扩散的钢琴伴奏生成模型，利用钢琴卷表示中的局部对齐，通过邻域注意力机制增强局部上下文建模，在POP909数据集上优于连续扩散和Transformer基线。


<details>
  <summary>Details</summary>
Motivation: 在符号音乐领域生成钢琴伴奏是一个具有挑战性的任务，需要根据给定的旋律和和弦约束（如主旋律谱）生成完整的钢琴音乐。现有方法在保持和弦条件方面存在不足。

Method: 提出离散扩散钢琴伴奏生成模型D3PIA，利用钢琴卷表示中的局部对齐。采用邻域注意力机制编码主旋律谱并作为条件预测伴奏音符状态，增强局部上下文建模能力。

Result: 在POP909数据集上的客观评估显示，D3PIA比连续扩散和Transformer基线更忠实地保持和弦条件。主观听力测试表明D3PIA生成的伴奏具有更好的音乐连贯性。

Conclusion: D3PIA通过离散扩散和邻域注意力机制，在钢琴伴奏生成任务中取得了优于现有方法的性能，能够生成更忠实于和弦约束且音乐连贯性更好的伴奏。

Abstract: Generating piano accompaniments in the symbolic music domain is a challenging task that requires producing a complete piece of piano music from given melody and chord constraints, such as those provided by a lead sheet. In this paper, we propose a discrete diffusion-based piano accompaniment generation model, D3PIA, leveraging local alignment between lead sheet and accompaniment in piano-roll representation. D3PIA incorporates Neighborhood Attention (NA) to both encode the lead sheet and condition it for predicting note states in the piano accompaniment. This design enhances local contextual modeling by efficiently attending to nearby melody and chord conditions. We evaluate our model using the POP909 dataset, a widely used benchmark for piano accompaniment generation. Objective evaluation results demonstrate that D3PIA preserves chord conditions more faithfully compared to continuous diffusion-based and Transformer-based baselines. Furthermore, a subjective listening test indicates that D3PIA generates more musically coherent accompaniments than the comparison models.

</details>


### [27] [EarResp-ANS : Audio-Based On-Device Respiration Rate Estimation on Earphones with Adaptive Noise Suppression](https://arxiv.org/abs/2602.03549)
*Michael Küttner,Valeria Zitz,Supraja Ramesh,Michael Beigl,Tobias Röddiger*

Main category: cs.SD

TL;DR: EarResp-ANS：首个在商用耳机上实现完全设备端实时呼吸率估计的系统，使用自适应噪声抑制技术，无需神经网络或音频流传输，在真实噪声环境下表现鲁棒。


<details>
  <summary>Details</summary>
Motivation: 呼吸率是临床评估和心理健康的关键生命体征，但由于缺乏非侵入式传感技术，日常生活中很少监测。耳内音频传感因高社会接受度和闭塞效应放大生理声音而具有潜力，但现有方法在真实噪声下常失效或依赖计算昂贵的模型。

Method: 采用基于LMS的自适应噪声抑制技术，在衰减环境噪声的同时保留呼吸相关声学成分，无需神经网络或音频流传输，直接在耳机上运行，满足可穿戴设备的能耗和隐私限制。

Result: 在18名参与者的研究中，在音乐、食堂噪声和白噪声（高达80 dB SPL）等真实声学条件下，系统全局MAE为0.84 CPM，通过自动异常值拒绝可降至0.47 CPM，处理器负载低于2%。

Conclusion: EarResp-ANS首次实现了在商用耳机上进行完全设备端、实时的呼吸率估计，解决了现有方法在真实噪声环境下的局限性，同时满足可穿戴设备的能耗和隐私要求，为日常呼吸监测提供了实用解决方案。

Abstract: Respiratory rate (RR) is a key vital sign for clinical assessment and mental well-being, yet it is rarely monitored in everyday life due to the lack of unobtrusive sensing technologies. In-ear audio sensing is promising due to its high social acceptance and the amplification of physiological sounds caused by the occlusion effect; however, existing approaches often fail under real-world noise or rely on computationally expensive models. We present EarResp-ANS, the first system enabling fully on-device, real-time RR estimation on commercial earphones. The system employs LMS-based adaptive noise suppression (ANS) to attenuate ambient noise while preserving respiration-related acoustic components, without requiring neural networks or audio streaming, thereby explicitly addressing the energy and privacy constraints of wearable devices. We evaluate EarResp-ANS in a study with 18 participants under realistic acoustic conditions, including music, cafeteria noise, and white noise up to 80 dB SPL. EarResp-ANS achieves robust performance with a global MAE of 0.84 CPM , reduced to 0.47 CPM via automatic outlier rejection, while operating with less than 2% processor load directly on the earphone.

</details>


### [28] [Adaptive Evidence Weighting for Audio-Spatiotemporal Fusion](https://arxiv.org/abs/2602.03817)
*Oscar Ovanger,Levi Harris,Timothy H. Keitt*

Main category: cs.SD

TL;DR: FINCH是一个自适应对数线性证据融合框架，将音频分类器与时空上下文预测器结合，通过学习样本级门控函数来估计上下文信息的可靠性，实现风险可控的融合。


<details>
  <summary>Details</summary>
Motivation: 机器学习系统通常有多个证据源用于同一预测目标，但这些源在不同输入中的可靠性和信息量不同。在生物声学分类中，物种识别可以从声学信号和时空上下文（位置、季节）推断，但实践中通常只有判别式预测器而非校准的生成模型。

Method: 提出FINCH框架，将预训练的音频分类器与结构化时空预测器集成。学习一个样本级门控函数，从不确定性和信息量统计中估计上下文信息的可靠性。该融合族包含音频分类器作为特例，并明确限制上下文证据的影响。

Result: 在多个基准测试中，FINCH始终优于固定权重融合和仅音频基线，即使上下文信息单独较弱时也能提高鲁棒性和错误权衡。在CBI上达到最先进性能，在BirdSet的多个子集上获得竞争性或改进性能。

Conclusion: FINCH提供了一个轻量级、可解释、基于证据的方法，通过自适应融合多源证据实现风险可控的预测，在生物声学分类中表现出优越性能。

Abstract: Many machine learning systems have access to multiple sources of evidence for the same prediction target, yet these sources often differ in reliability and informativeness across inputs. In bioacoustic classification, species identity may be inferred both from the acoustic signal and from spatiotemporal context such as location and season; while Bayesian inference motivates multiplicative evidence combination, in practice we typically only have access to discriminative predictors rather than calibrated generative models. We introduce \textbf{F}usion under \textbf{IN}dependent \textbf{C}onditional \textbf{H}ypotheses (\textbf{FINCH}), an adaptive log-linear evidence fusion framework that integrates a pre-trained audio classifier with a structured spatiotemporal predictor. FINCH learns a per-sample gating function that estimates the reliability of contextual information from uncertainty and informativeness statistics. The resulting fusion family \emph{contains} the audio-only classifier as a special case and explicitly bounds the influence of contextual evidence, yielding a risk-contained hypothesis class with an interpretable audio-only fallback. Across benchmarks, FINCH consistently outperforms fixed-weight fusion and audio-only baselines, improving robustness and error trade-offs even when contextual information is weak in isolation. We achieve state-of-the-art performance on CBI and competitive or improved performance on several subsets of BirdSet using a lightweight, interpretable, evidence-based approach. Code is available: \texttt{\href{https://anonymous.4open.science/r/birdnoise-85CD/README.md}{anonymous-repository}}

</details>

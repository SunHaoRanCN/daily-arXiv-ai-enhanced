{"id": "2602.03868", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2602.03868", "abs": "https://arxiv.org/abs/2602.03868", "authors": ["Chandrashekar M S", "Vineet Singh", "Lakshmi Pedapudi"], "title": "Benchmarking Automatic Speech Recognition for Indian Languages in Agricultural Contexts", "comment": "9 pages, 6 figures", "summary": "The digitization of agricultural advisory services in India requires robust Automatic Speech Recognition (ASR) systems capable of accurately transcribing domain-specific terminology in multiple Indian languages. This paper presents a benchmarking framework for evaluating ASR performance in agricultural contexts across Hindi, Telugu, and Odia languages. We introduce evaluation metrics including Agriculture Weighted Word Error Rate (AWWER) and domain-specific utility scoring to complement traditional metrics. Our evaluation of 10,934 audio recordings, each transcribed by up to 10 ASR models, reveals performance variations across languages and models, with Hindi achieving the best overall performance (WER: 16.2%) while Odia presents the greatest challenges (best WER: 35.1%, achieved only with speaker diarization). We characterize audio quality challenges inherent to real-world agricultural field recordings and demonstrate that speaker diarization with best-speaker selection can substantially reduce WER for multi-speaker recordings (upto 66% depending on the proportion of multi-speaker audio). We identify recurring error patterns in agricultural terminology and provide practical recommendations for improving ASR systems in low-resource agricultural domains. The study establishes baseline benchmarks for future agricultural ASR development.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u8bc4\u4f30\u5370\u5ea6\u519c\u4e1a\u9886\u57df\u591a\u8bed\u8a00\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\uff08ASR\uff09\u6027\u80fd\u7684\u57fa\u51c6\u6846\u67b6\uff0c\u6db5\u76d6\u5370\u5730\u8bed\u3001\u6cf0\u5362\u56fa\u8bed\u548c\u5965\u91cc\u4e9a\u8bed\uff0c\u5f15\u5165\u4e86\u519c\u4e1a\u52a0\u6743\u8bcd\u9519\u8bef\u7387\u7b49\u65b0\u6307\u6807\uff0c\u5206\u6790\u4e8610,934\u6761\u519c\u4e1a\u5f55\u97f3\uff0c\u53d1\u73b0\u5370\u5730\u8bed\u8868\u73b0\u6700\u4f73\uff0c\u5965\u91cc\u4e9a\u8bed\u6311\u6218\u6700\u5927\uff0c\u5e76\u8bc1\u660e\u8bf4\u8bdd\u4eba\u5206\u79bb\u6280\u672f\u80fd\u663e\u8457\u63d0\u5347\u591a\u8bf4\u8bdd\u4eba\u5f55\u97f3\u7684\u8bc6\u522b\u51c6\u786e\u7387\u3002", "motivation": "\u5370\u5ea6\u519c\u4e1a\u54a8\u8be2\u670d\u52a1\u7684\u6570\u5b57\u5316\u9700\u8981\u80fd\u591f\u51c6\u786e\u8f6c\u5f55\u591a\u79cd\u5370\u5ea6\u8bed\u8a00\u4e2d\u519c\u4e1a\u9886\u57df\u4e13\u4e1a\u672f\u8bed\u7684ASR\u7cfb\u7edf\u3002\u76ee\u524d\u7f3a\u4e4f\u9488\u5bf9\u519c\u4e1a\u9886\u57df\u3001\u7279\u522b\u662f\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u6709\u6548\u8bc4\u4f30\u6846\u67b6\u548c\u57fa\u51c6\uff0c\u8fd9\u963b\u788d\u4e86\u519c\u4e1aASR\u7cfb\u7edf\u7684\u5f00\u53d1\u548c\u5e94\u7528\u3002", "method": "1. \u5efa\u7acb\u5305\u542b\u5370\u5730\u8bed\u3001\u6cf0\u5362\u56fa\u8bed\u548c\u5965\u91cc\u4e9a\u8bed\u4e09\u79cd\u8bed\u8a00\u7684\u519c\u4e1aASR\u8bc4\u4f30\u6846\u67b6\uff1b2. \u5f15\u5165\u519c\u4e1a\u52a0\u6743\u8bcd\u9519\u8bef\u7387\uff08AWWER\uff09\u548c\u9886\u57df\u7279\u5b9a\u6548\u7528\u8bc4\u5206\u7b49\u65b0\u6307\u6807\uff1b3. \u6536\u96c6\u5e76\u5206\u679010,934\u6761\u519c\u4e1a\u97f3\u9891\u5f55\u97f3\uff1b4. \u4f7f\u7528\u6700\u591a10\u4e2aASR\u6a21\u578b\u8fdb\u884c\u8f6c\u5f55\u8bc4\u4f30\uff1b5. \u5206\u6790\u97f3\u9891\u8d28\u91cf\u6311\u6218\u548c\u9519\u8bef\u6a21\u5f0f\uff1b6. \u8bc4\u4f30\u8bf4\u8bdd\u4eba\u5206\u79bb\u6280\u672f\u5bf9\u591a\u8bf4\u8bdd\u4eba\u5f55\u97f3\u7684\u6548\u679c\u3002", "result": "1. \u5370\u5730\u8bed\u8868\u73b0\u6700\u4f73\uff08WER\uff1a16.2%\uff09\uff0c\u5965\u91cc\u4e9a\u8bed\u6311\u6218\u6700\u5927\uff08\u6700\u4f73WER\uff1a35.1%\uff0c\u4ec5\u5728\u4f7f\u7528\u8bf4\u8bdd\u4eba\u5206\u79bb\u65f6\u8fbe\u5230\uff09\uff1b2. \u8bf4\u8bdd\u4eba\u5206\u79bb\u7ed3\u5408\u6700\u4f73\u8bf4\u8bdd\u4eba\u9009\u62e9\u80fd\u663e\u8457\u964d\u4f4e\u591a\u8bf4\u8bdd\u4eba\u5f55\u97f3\u7684WER\uff08\u6700\u9ad8\u53ef\u8fbe66%\uff09\uff1b3. \u8bc6\u522b\u51fa\u519c\u4e1a\u672f\u8bed\u4e2d\u7684\u5e38\u89c1\u9519\u8bef\u6a21\u5f0f\uff1b4. \u5efa\u7acb\u4e86\u519c\u4e1aASR\u7684\u57fa\u51c6\u6027\u80fd\u6570\u636e\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u519c\u4e1a\u9886\u57df\u7684ASR\u7cfb\u7edf\u5f00\u53d1\u5efa\u7acb\u4e86\u91cd\u8981\u7684\u57fa\u51c6\u6846\u67b6\uff0c\u63ed\u793a\u4e86\u4e0d\u540c\u8bed\u8a00\u548c\u6a21\u578b\u5728\u519c\u4e1a\u8bed\u5883\u4e0b\u7684\u6027\u80fd\u5dee\u5f02\uff0c\u8bc1\u660e\u4e86\u8bf4\u8bdd\u4eba\u5206\u79bb\u6280\u672f\u5bf9\u63d0\u5347\u591a\u8bf4\u8bdd\u4eba\u5f55\u97f3\u8bc6\u522b\u51c6\u786e\u6027\u7684\u6709\u6548\u6027\uff0c\u5e76\u4e3a\u6539\u8fdb\u4f4e\u8d44\u6e90\u519c\u4e1a\u9886\u57df\u7684ASR\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u5efa\u8bae\u3002"}}
{"id": "2602.03891", "categories": ["eess.AS", "cs.AI", "cs.CV", "cs.MM", "cs.SD"], "pdf": "https://arxiv.org/pdf/2602.03891", "abs": "https://arxiv.org/abs/2602.03891", "authors": ["Seohyun Joo", "Yoori Oh"], "title": "Sounding Highlights: Dual-Pathway Audio Encoders for Audio-Visual Video Highlight Detection", "comment": "5 pages, 2 figures, to appear in ICASSP 2026", "summary": "Audio-visual video highlight detection aims to automatically identify the most salient moments in videos by leveraging both visual and auditory cues. However, existing models often underutilize the audio modality, focusing on high-level semantic features while failing to fully leverage the rich, dynamic characteristics of sound. To address this limitation, we propose a novel framework, Dual-Pathway Audio Encoders for Video Highlight Detection (DAViHD). The dual-pathway audio encoder is composed of a semantic pathway for content understanding and a dynamic pathway that captures spectro-temporal dynamics. The semantic pathway extracts high-level information by identifying the content within the audio, such as speech, music, or specific sound events. The dynamic pathway employs a frequency-adaptive mechanism as time evolves to jointly model these dynamics, enabling it to identify transient acoustic events via salient spectral bands and rapid energy changes. We integrate the novel audio encoder into a full audio-visual framework and achieve new state-of-the-art performance on the large-scale Mr.HiSum benchmark. Our results demonstrate that a sophisticated, dual-faceted audio representation is key to advancing the field of highlight detection.", "AI": {"tldr": "\u63d0\u51faDAViHD\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u8def\u5f84\u97f3\u9891\u7f16\u7801\u5668\uff08\u8bed\u4e49\u8def\u5f84\u548c\u52a8\u6001\u8def\u5f84\uff09\u589e\u5f3a\u97f3\u89c6\u9891\u9ad8\u5149\u68c0\u6d4b\uff0c\u5728Mr.HiSum\u57fa\u51c6\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u97f3\u89c6\u9891\u9ad8\u5149\u68c0\u6d4b\u6a21\u578b\u5bf9\u97f3\u9891\u6a21\u6001\u5229\u7528\u4e0d\u8db3\uff0c\u4e3b\u8981\u5173\u6ce8\u9ad8\u5c42\u8bed\u4e49\u7279\u5f81\uff0c\u672a\u80fd\u5145\u5206\u5229\u7528\u58f0\u97f3\u7684\u4e30\u5bcc\u52a8\u6001\u7279\u6027\u3002", "method": "\u63d0\u51fa\u53cc\u8def\u5f84\u97f3\u9891\u7f16\u7801\u5668\uff1a\u8bed\u4e49\u8def\u5f84\u63d0\u53d6\u97f3\u9891\u5185\u5bb9\uff08\u5982\u8bed\u97f3\u3001\u97f3\u4e50\u3001\u7279\u5b9a\u58f0\u97f3\u4e8b\u4ef6\uff09\u7684\u9ad8\u5c42\u4fe1\u606f\uff1b\u52a8\u6001\u8def\u5f84\u91c7\u7528\u9891\u7387\u81ea\u9002\u5e94\u673a\u5236\u5efa\u6a21\u65f6\u9891\u52a8\u6001\u7279\u6027\uff0c\u901a\u8fc7\u663e\u8457\u9891\u8c31\u5e26\u548c\u5feb\u901f\u80fd\u91cf\u53d8\u5316\u8bc6\u522b\u77ac\u6001\u58f0\u5b66\u4e8b\u4ef6\u3002", "result": "\u5728\u5927\u89c4\u6a21Mr.HiSum\u57fa\u51c6\u4e0a\u53d6\u5f97\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "\u590d\u6742\u3001\u53cc\u65b9\u9762\u7684\u97f3\u9891\u8868\u793a\u662f\u63a8\u8fdb\u9ad8\u5149\u68c0\u6d4b\u9886\u57df\u53d1\u5c55\u7684\u5173\u952e\u3002"}}
{"id": "2602.04307", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2602.04307", "abs": "https://arxiv.org/abs/2602.04307", "authors": ["Chien-Chun Wang", "Hung-Shin Lee", "Hsin-Min Wang", "Berlin Chen"], "title": "Universal Robust Speech Adaptation for Cross-Domain Speech Recognition and Enhancement", "comment": "Accepted to IEEE Transactions on Audio, Speech and Language Processing (IEEE TASLP)", "summary": "Pre-trained models for automatic speech recognition (ASR) and speech enhancement (SE) have exhibited remarkable capabilities under matched noise and channel conditions. However, these models often suffer from severe performance degradation when confronted with domain shifts, particularly in the presence of unseen noise and channel distortions. In view of this, we in this paper present URSA-GAN, a unified and domain-aware generative framework specifically designed to mitigate mismatches in both noise and channel conditions. URSA-GAN leverages a dual-embedding architecture that consists of a noise encoder and a channel encoder, each pre-trained with limited in-domain data to capture domain-relevant representations. These embeddings condition a GAN-based speech generator, facilitating the synthesis of speech that is acoustically aligned with the target domain while preserving phonetic content. To enhance generalization further, we propose dynamic stochastic perturbation, a novel regularization technique that introduces controlled variability into the embeddings during generation, promoting robustness to unseen domains. Empirical results demonstrate that URSA-GAN effectively reduces character error rates in ASR and improves perceptual metrics in SE across diverse noisy and mismatched channel scenarios. Notably, evaluations on compound test conditions with both channel and noise degradations confirm the generalization ability of URSA-GAN, yielding relative improvements of 16.16% in ASR performance and 15.58% in SE metrics.", "AI": {"tldr": "URSA-GAN\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u9886\u57df\u611f\u77e5\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u5d4c\u5165\u67b6\u6784\uff08\u566a\u58f0\u7f16\u7801\u5668\u548c\u901a\u9053\u7f16\u7801\u5668\uff09\u548c\u52a8\u6001\u968f\u673a\u6270\u52a8\u6280\u672f\uff0c\u89e3\u51b3ASR\u548c\u8bed\u97f3\u589e\u5f3a\u5728\u566a\u58f0\u548c\u901a\u9053\u5931\u914d\u6761\u4ef6\u4e0b\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\u3002", "motivation": "\u9884\u8bad\u7ec3\u7684ASR\u548c\u8bed\u97f3\u589e\u5f3a\u6a21\u578b\u5728\u5339\u914d\u7684\u566a\u58f0\u548c\u901a\u9053\u6761\u4ef6\u4e0b\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u9762\u5bf9\u9886\u57df\u8f6c\u79fb\uff08\u7279\u522b\u662f\u672a\u89c1\u8fc7\u7684\u566a\u58f0\u548c\u901a\u9053\u5931\u771f\uff09\u65f6\u6027\u80fd\u4e25\u91cd\u4e0b\u964d\uff0c\u9700\u8981\u89e3\u51b3\u8fd9\u79cd\u5931\u914d\u95ee\u9898\u3002", "method": "\u63d0\u51faURSA-GAN\u6846\u67b6\uff1a1\uff09\u4f7f\u7528\u566a\u58f0\u7f16\u7801\u5668\u548c\u901a\u9053\u7f16\u7801\u5668\u7ec4\u6210\u7684\u53cc\u5d4c\u5165\u67b6\u6784\uff0c\u6bcf\u4e2a\u7f16\u7801\u5668\u7528\u6709\u9650\u7684\u9886\u57df\u5185\u6570\u636e\u9884\u8bad\u7ec3\u4ee5\u6355\u83b7\u9886\u57df\u76f8\u5173\u8868\u793a\uff1b2\uff09\u8fd9\u4e9b\u5d4c\u5165\u6761\u4ef6\u5316\u57fa\u4e8eGAN\u7684\u8bed\u97f3\u751f\u6210\u5668\uff0c\u5408\u6210\u4e0e\u76ee\u6807\u9886\u57df\u58f0\u5b66\u5bf9\u9f50\u7684\u8bed\u97f3\uff1b3\uff09\u63d0\u51fa\u52a8\u6001\u968f\u673a\u6270\u52a8\u6280\u672f\uff0c\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u5411\u5d4c\u5165\u5f15\u5165\u53d7\u63a7\u53d8\u5f02\u6027\uff0c\u589e\u5f3a\u5bf9\u672a\u89c1\u9886\u57df\u7684\u9c81\u68d2\u6027\u3002", "result": "URSA-GAN\u5728\u591a\u79cd\u566a\u58f0\u548c\u5931\u914d\u901a\u9053\u573a\u666f\u4e2d\u6709\u6548\u964d\u4f4e\u4e86ASR\u7684\u5b57\u7b26\u9519\u8bef\u7387\uff0c\u6539\u5584\u4e86\u8bed\u97f3\u589e\u5f3a\u7684\u611f\u77e5\u6307\u6807\u3002\u5728\u540c\u65f6\u5305\u542b\u901a\u9053\u548c\u566a\u58f0\u9000\u5316\u7684\u590d\u5408\u6d4b\u8bd5\u6761\u4ef6\u4e0b\uff0cASR\u6027\u80fd\u76f8\u5bf9\u63d0\u534716.16%\uff0c\u8bed\u97f3\u589e\u5f3a\u6307\u6807\u76f8\u5bf9\u63d0\u534715.58%\u3002", "conclusion": "URSA-GAN\u901a\u8fc7\u7edf\u4e00\u7684\u9886\u57df\u611f\u77e5\u751f\u6210\u6846\u67b6\uff0c\u6210\u529f\u7f13\u89e3\u4e86\u566a\u58f0\u548c\u901a\u9053\u6761\u4ef6\u5931\u914d\u95ee\u9898\uff0c\u5728ASR\u548c\u8bed\u97f3\u589e\u5f3a\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u672a\u89c1\u9886\u57df\u6761\u4ef6\u4e0b\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2602.04796", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2602.04796", "abs": "https://arxiv.org/abs/2602.04796", "authors": ["Amir Ivry", "Shinji Watanabe"], "title": "LALM-as-a-Judge: Benchmarking Large Audio-Language Models for Safety Evaluation in Multi-Turn Spoken Dialogues", "comment": null, "summary": "Spoken dialogues with and between voice agents are becoming increasingly common, yet assessing them for their socially harmful content such as violence, harassment, and hate remains text-centric and fails to account for audio-specific cues and transcription errors. We present LALM-as-a-Judge, the first controlled benchmark and systematic study of large audio-language models (LALMs) as safety judges for multi-turn spoken dialogues. We generate 24,000 unsafe and synthetic spoken dialogues in English that consist of 3-10 turns, by having a single dialogue turn including content with one of 8 harmful categories (e.g., violence) and on one of 5 grades, from very mild to severe. On 160 dialogues, 5 human raters confirmed reliable unsafe detection and a meaningful severity scale. We benchmark three open-source LALMs: Qwen2-Audio, Audio Flamingo 3, and MERaLiON as zero-shot judges that output a scalar safety score in [0,1] across audio-only, transcription-only, or multimodal inputs, along with a transcription-only LLaMA baseline. We measure the judges' sensitivity to detecting unsafe content, the specificity in ordering severity levels, and the stability of the score in dialogue turns. Results reveal architecture- and modality-dependent trade-offs: the most sensitive judge is also the least stable across turns, while stable configurations sacrifice detection of mild harmful content. Transcription quality is a key bottleneck: Whisper-Large may significantly reduce sensitivity for transcription-only modes, while largely preserving severity ordering. Audio becomes crucial when paralinguistic cues or transcription fidelity are category-critical. We summarize all findings and provide actionable guidance for practitioners.", "AI": {"tldr": "\u9996\u4e2a\u7cfb\u7edf\u7814\u7a76\u5927\u578b\u97f3\u9891\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u591a\u8f6e\u53e3\u8bed\u5bf9\u8bdd\u5b89\u5168\u8bc4\u4f30\u5668\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u63ed\u793a\u4e86\u67b6\u6784\u548c\u6a21\u6001\u95f4\u7684\u6743\u8861\uff1a\u6700\u654f\u611f\u7684\u6a21\u578b\u7a33\u5b9a\u6027\u6700\u5dee\uff0c\u800c\u7a33\u5b9a\u914d\u7f6e\u4f1a\u727a\u7272\u5bf9\u8f7b\u5ea6\u6709\u5bb3\u5185\u5bb9\u7684\u68c0\u6d4b\u3002", "motivation": "\u5f53\u524d\u8bed\u97f3\u5bf9\u8bdd\u5b89\u5168\u8bc4\u4f30\u4e3b\u8981\u57fa\u4e8e\u6587\u672c\uff0c\u5ffd\u7565\u4e86\u97f3\u9891\u7279\u5b9a\u7ebf\u7d22\u548c\u8f6c\u5f55\u9519\u8bef\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u5904\u7406\u591a\u8f6e\u53e3\u8bed\u5bf9\u8bdd\u7684\u97f3\u9891\u8bed\u8a00\u6a21\u578b\u5b89\u5168\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u521b\u5efa\u5305\u542b24,000\u4e2a\u82f1\u8bed\u5408\u6210\u8bed\u97f3\u5bf9\u8bdd\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u6db5\u76d68\u79cd\u6709\u5bb3\u7c7b\u522b\u548c5\u4e2a\u4e25\u91cd\u7a0b\u5ea6\u7b49\u7ea7\u3002\u8bc4\u4f30\u4e09\u79cd\u5f00\u6e90LALM\u4f5c\u4e3a\u96f6\u6837\u672c\u5b89\u5168\u8bc4\u4f30\u5668\uff0c\u6bd4\u8f83\u97f3\u9891\u3001\u8f6c\u5f55\u548c\u591a\u6a21\u6001\u8f93\u5165\u6a21\u5f0f\uff0c\u5e76\u4e0e\u7eaf\u6587\u672cLLaMA\u57fa\u7ebf\u5bf9\u6bd4\u3002", "result": "\u53d1\u73b0\u67b6\u6784\u548c\u6a21\u6001\u4f9d\u8d56\u7684\u6743\u8861\uff1a\u6700\u654f\u611f\u7684\u8bc4\u4f30\u5668\u5728\u5bf9\u8bdd\u8f6e\u6b21\u95f4\u6700\u4e0d\u7a33\u5b9a\uff0c\u800c\u7a33\u5b9a\u914d\u7f6e\u4f1a\u727a\u7272\u5bf9\u8f7b\u5ea6\u6709\u5bb3\u5185\u5bb9\u7684\u68c0\u6d4b\u3002\u8f6c\u5f55\u8d28\u91cf\u662f\u5173\u952e\u74f6\u9888\uff0c\u97f3\u9891\u6a21\u6001\u5728\u526f\u8bed\u8a00\u7ebf\u7d22\u6216\u8f6c\u5f55\u4fdd\u771f\u5ea6\u5173\u952e\u65f6\u81f3\u5173\u91cd\u8981\u3002", "conclusion": "\u4e3a\u4ece\u4e1a\u8005\u63d0\u4f9b\u5b9e\u7528\u6307\u5bfc\uff0c\u5f3a\u8c03\u9700\u8981\u5e73\u8861\u654f\u611f\u6027\u3001\u7a33\u5b9a\u6027\u548c\u6a21\u6001\u9009\u62e9\uff0c\u97f3\u9891\u6a21\u6001\u5728\u7279\u5b9a\u6709\u5bb3\u7c7b\u522b\u8bc4\u4f30\u4e2d\u5177\u6709\u4e0d\u53ef\u66ff\u4ee3\u7684\u4ef7\u503c\u3002"}}
{"id": "2602.03855", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.03855", "abs": "https://arxiv.org/abs/2602.03855", "authors": ["Le Minh Triet Tran", "Sarah Reynaud", "Ronan Fablet", "Adrien Merlini", "Fran\u00e7ois Rousseau", "Mai Quyen Pham"], "title": "Majorization-Minimization Networks for Inverse Problems: An Application to EEG Imaging", "comment": null, "summary": "Inverse problems are often ill-posed and require optimization schemes with strong stability and convergence guarantees. While learning-based approaches such as deep unrolling and meta-learning achieve strong empirical performance, they typically lack explicit control over descent and curvature, limiting robustness. We propose a learned Majorization-Minimization (MM) framework for inverse problems within a bilevel optimization setting. Instead of learning a full optimizer, we learn a structured curvature majorant that governs each MM step while preserving classical MM descent guarantees. The majorant is parameterized by a lightweight recurrent neural network and explicitly constrained to satisfy valid MM conditions. For cosine-similarity losses, we derive explicit curvature bounds yielding diagonal majorants. When analytic bounds are unavailable, we rely on efficient Hessian-vector product-based spectral estimation to automatically upper-bound local curvature without forming the Hessian explicitly. Experiments on EEG source imaging demonstrate improved accuracy, stability, and cross-dataset generalization over deep-unrolled and meta-learning baselines.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u53cc\u5c42\u4f18\u5316\u7684\u5b66\u4e60\u578bMajorization-Minimization\u6846\u67b6\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7RNN\u5b66\u4e60\u7ed3\u6784\u5316\u66f2\u7387\u4e0a\u754c\uff0c\u5728\u4fdd\u6301\u4f20\u7edfMM\u4e0b\u964d\u4fdd\u8bc1\u7684\u540c\u65f6\u63d0\u5347\u9006\u95ee\u9898\u7684\u6c42\u89e3\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5b66\u4e60\u7684\u65b9\u6cd5\uff08\u5982\u6df1\u5ea6\u5c55\u5f00\u548c\u5143\u5b66\u4e60\uff09\u867d\u7136\u7ecf\u9a8c\u6027\u80fd\u5f3a\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u4e0b\u964d\u548c\u66f2\u7387\u7684\u663e\u5f0f\u63a7\u5236\uff0c\u9650\u5236\u4e86\u9c81\u68d2\u6027\u3002\u9700\u8981\u4e00\u79cd\u65e2\u4fdd\u6301\u7406\u8bba\u4fdd\u8bc1\u53c8\u80fd\u63d0\u5347\u6027\u80fd\u7684\u4f18\u5316\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u5b66\u4e60\u578bMM\u6846\u67b6\uff1a1\uff09\u4e0d\u5b66\u4e60\u5b8c\u6574\u4f18\u5316\u5668\uff0c\u800c\u662f\u5b66\u4e60\u7ed3\u6784\u5316\u66f2\u7387\u4e0a\u754c\uff1b2\uff09\u4f7f\u7528\u8f7b\u91cf\u7ea7RNN\u53c2\u6570\u5316\u4e0a\u754c\uff0c\u5e76\u663e\u5f0f\u7ea6\u675f\u6ee1\u8db3\u6709\u6548MM\u6761\u4ef6\uff1b3\uff09\u5bf9\u4f59\u5f26\u76f8\u4f3c\u5ea6\u635f\u5931\u63a8\u5bfc\u663e\u5f0f\u66f2\u7387\u8fb9\u754c\u5f97\u5230\u5bf9\u89d2\u4e0a\u754c\uff1b4\uff09\u5bf9\u65e0\u6cd5\u89e3\u6790\u63a8\u5bfc\u7684\u60c5\u51b5\uff0c\u4f7f\u7528\u57fa\u4e8eHessian-\u5411\u91cf\u79ef\u7684\u8c31\u4f30\u8ba1\u81ea\u52a8\u4e0a\u754c\u5c40\u90e8\u66f2\u7387\u3002", "result": "\u5728EEG\u6e90\u6210\u50cf\u5b9e\u9a8c\u4e2d\uff0c\u76f8\u6bd4\u6df1\u5ea6\u5c55\u5f00\u548c\u5143\u5b66\u4e60\u57fa\u7ebf\uff0c\u8be5\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u3001\u7a33\u5b9a\u6027\u548c\u8de8\u6570\u636e\u96c6\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u5747\u6709\u63d0\u5347\u3002", "conclusion": "\u5b66\u4e60\u578bMM\u6846\u67b6\u6210\u529f\u7ed3\u5408\u4e86\u5b66\u4e60\u65b9\u6cd5\u7684\u7ecf\u9a8c\u4f18\u52bf\u548c\u4f20\u7edf\u4f18\u5316\u65b9\u6cd5\u7684\u7406\u8bba\u4fdd\u8bc1\uff0c\u4e3a\u9006\u95ee\u9898\u63d0\u4f9b\u4e86\u66f4\u7a33\u5065\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.03873", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2602.03873", "abs": "https://arxiv.org/abs/2602.03873", "authors": ["Hong Jia", "Weibin Li", "Jingyao Wu", "Xiaofeng Yu", "Yan Gao", "Jintao Cheng", "Xiaoyu Tang", "Feng Xia", "Ting Dang"], "title": "Decoding Ambiguous Emotions with Test-Time Scaling in Audio-Language Models", "comment": null, "summary": "Emotion recognition from human speech is a critical enabler for socially aware conversational AI. However, while most prior work frames emotion recognition as a categorical classification problem, real-world affective states are often ambiguous, overlapping, and context-dependent, posing significant challenges for both annotation and automatic modeling. Recent large-scale audio language models (ALMs) offer new opportunities for nuanced affective reasoning without explicit emotion supervision, but their capacity to handle ambiguous emotions remains underexplored. At the same time, advances in inference-time techniques such as test-time scaling (TTS) have shown promise for improving generalization and adaptability in hard NLP tasks, but their relevance to affective computing is still largely unknown. In this work, we introduce the first benchmark for ambiguous emotion recognition in speech with ALMs under test-time scaling. Our evaluation systematically compares eight state-of-the-art ALMs and five TTS strategies across three prominent speech emotion datasets. We further provide an in-depth analysis of the interaction between model capacity, TTS, and affective ambiguity, offering new insights into the computational and representational challenges of ambiguous emotion understanding. Our benchmark establishes a foundation for developing more robust, context-aware, and emotionally intelligent speech-based AI systems, and highlights key future directions for bridging the gap between model assumptions and the complexity of real-world human emotion.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u5728\u6d4b\u8bd5\u65f6\u7f29\u653e\u6761\u4ef6\u4e0b\u4f7f\u7528\u97f3\u9891\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u6a21\u7cca\u8bed\u97f3\u60c5\u611f\u8bc6\u522b\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7cfb\u7edf\u8bc4\u4f30\u4e868\u4e2a\u5148\u8fdbALM\u548c5\u79cdTTS\u7b56\u7565\uff0c\u5206\u6790\u4e86\u6a21\u578b\u5bb9\u91cf\u3001TTS\u4e0e\u60c5\u611f\u6a21\u7cca\u6027\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u60c5\u611f\u72b6\u6001\u5f80\u5f80\u662f\u6a21\u7cca\u3001\u91cd\u53e0\u4e14\u4f9d\u8d56\u8bed\u5883\u7684\uff0c\u800c\u4f20\u7edf\u7684\u60c5\u611f\u8bc6\u522b\u65b9\u6cd5\u901a\u5e38\u5c06\u5176\u89c6\u4e3a\u5206\u7c7b\u95ee\u9898\uff0c\u8fd9\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u9762\u4e34\u6311\u6218\u3002\u540c\u65f6\uff0c\u867d\u7136\u5927\u578b\u97f3\u9891\u8bed\u8a00\u6a21\u578b\u4e3a\u65e0\u9700\u663e\u5f0f\u60c5\u611f\u76d1\u7763\u7684\u7ec6\u81f4\u60c5\u611f\u63a8\u7406\u63d0\u4f9b\u4e86\u65b0\u673a\u4f1a\uff0c\u4f46\u5176\u5904\u7406\u6a21\u7cca\u60c5\u611f\u7684\u80fd\u529b\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002\u6d4b\u8bd5\u65f6\u7f29\u653e\u6280\u672f\u5728NLP\u4efb\u52a1\u4e2d\u663e\u793a\u51fa\u6f5c\u529b\uff0c\u4f46\u5728\u60c5\u611f\u8ba1\u7b97\u4e2d\u7684\u5e94\u7528\u4ecd\u672a\u77e5\u3002", "method": "\u521b\u5efa\u4e86\u9996\u4e2a\u5728\u6d4b\u8bd5\u65f6\u7f29\u653e\u6761\u4ef6\u4e0b\u4f7f\u7528\u97f3\u9891\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u6a21\u7cca\u8bed\u97f3\u60c5\u611f\u8bc6\u522b\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002\u7cfb\u7edf\u6bd4\u8f83\u4e868\u4e2a\u6700\u5148\u8fdb\u7684\u97f3\u9891\u8bed\u8a00\u6a21\u578b\u548c5\u79cd\u6d4b\u8bd5\u65f6\u7f29\u653e\u7b56\u7565\uff0c\u5728\u4e09\u4e2a\u8457\u540d\u7684\u8bed\u97f3\u60c5\u611f\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\u3002\u6df1\u5165\u5206\u6790\u4e86\u6a21\u578b\u5bb9\u91cf\u3001\u6d4b\u8bd5\u65f6\u7f29\u653e\u548c\u60c5\u611f\u6a21\u7cca\u6027\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\u3002", "result": "\u5efa\u7acb\u4e86\u6a21\u7cca\u8bed\u97f3\u60c5\u611f\u8bc6\u522b\u7684\u57fa\u51c6\u6846\u67b6\uff0c\u63d0\u4f9b\u4e86\u5173\u4e8e\u6a21\u578b\u5bb9\u91cf\u3001\u6d4b\u8bd5\u65f6\u7f29\u653e\u548c\u60c5\u611f\u6a21\u7cca\u6027\u76f8\u4e92\u4f5c\u7528\u7684\u6df1\u5165\u5206\u6790\uff0c\u63ed\u793a\u4e86\u6a21\u7cca\u60c5\u611f\u7406\u89e3\u7684\u8ba1\u7b97\u548c\u8868\u5f81\u6311\u6218\u3002", "conclusion": "\u8be5\u57fa\u51c6\u4e3a\u5f00\u53d1\u66f4\u9c81\u68d2\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u548c\u60c5\u611f\u667a\u80fd\u7684\u8bed\u97f3AI\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u7a81\u51fa\u4e86\u5f25\u5408\u6a21\u578b\u5047\u8bbe\u4e0e\u73b0\u5b9e\u4e16\u754c\u4eba\u7c7b\u60c5\u611f\u590d\u6742\u6027\u4e4b\u95f4\u5dee\u8ddd\u7684\u5173\u952e\u672a\u6765\u65b9\u5411\u3002"}}
{"id": "2602.03856", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.03856", "abs": "https://arxiv.org/abs/2602.03856", "authors": ["Edward Gunn", "Adam Hosford", "Robert Jones", "Leo Zeitler", "Ian Groves", "Victoria Nockles"], "title": "The Turing Synthetic Radar Dataset: A dataset for pulse deinterleaving", "comment": "7 pages 6 figures, submitted to International Radar Symposium 2026", "summary": "We present the Turing Synthetic Radar Dataset, a comprehensive dataset to serve both as a benchmark for radar pulse deinterleaving research and as an enabler of new research methods. The dataset addresses the critical problem of separating interleaved radar pulses from multiple unknown emitters for electronic warfare applications and signal intelligence. Our dataset contains a total of 6000 pulse trains over two receiver configurations, totalling to almost 3 billion pulses, featuring realistic scenarios with up to 110 emitters and significant parameter space overlap. To encourage dataset adoption and establish standardised evaluation procedures, we have launched an accompanying Turing Deinterleaving Challenge, for which models need to associate pulses in interleaved pulse trains to the correct emitter by clustering and maximising metrics such as the V-measure. The Turing Synthetic Radar Dataset is one of the first publicly available, comprehensively simulated pulse train datasets aimed to facilitate sophisticated model development in the electronic warfare community", "AI": {"tldr": "\u63d0\u51fa\u4e86Turing\u5408\u6210\u96f7\u8fbe\u6570\u636e\u96c6\uff0c\u5305\u542b6000\u4e2a\u8109\u51b2\u5e8f\u5217\uff0c\u603b\u8ba1\u8fd130\u4ebf\u4e2a\u8109\u51b2\uff0c\u7528\u4e8e\u96f7\u8fbe\u8109\u51b2\u89e3\u4ea4\u7ec7\u7814\u7a76\u548c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e76\u914d\u5957\u4e86Turing\u89e3\u4ea4\u7ec7\u6311\u6218\u8d5b", "motivation": "\u89e3\u51b3\u7535\u5b50\u6218\u548c\u4fe1\u53f7\u60c5\u62a5\u4e2d\u591a\u4e2a\u672a\u77e5\u53d1\u5c04\u5668\u4ea4\u7ec7\u96f7\u8fbe\u8109\u51b2\u5206\u79bb\u7684\u5173\u952e\u95ee\u9898\uff0c\u4e3a\u96f7\u8fbe\u8109\u51b2\u89e3\u4ea4\u7ec7\u7814\u7a76\u63d0\u4f9b\u57fa\u51c6\u6570\u636e\u96c6\u5e76\u4fc3\u8fdb\u65b0\u7814\u7a76\u65b9\u6cd5\u7684\u53d1\u5c55", "method": "\u521b\u5efa\u4e86\u5305\u542b6000\u4e2a\u8109\u51b2\u5e8f\u5217\u7684\u7efc\u5408\u5408\u6210\u96f7\u8fbe\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u4e24\u79cd\u63a5\u6536\u5668\u914d\u7f6e\uff0c\u6a21\u62df\u4e86\u6700\u591a110\u4e2a\u53d1\u5c04\u5668\u7684\u771f\u5b9e\u573a\u666f\uff0c\u5177\u6709\u663e\u8457\u7684\u53c2\u6570\u7a7a\u95f4\u91cd\u53e0", "result": "\u5efa\u7acb\u4e86\u5305\u542b\u8fd130\u4ebf\u4e2a\u8109\u51b2\u7684\u5168\u9762\u6570\u636e\u96c6\uff0c\u652f\u6301\u6700\u591a110\u4e2a\u53d1\u5c04\u5668\u7684\u590d\u6742\u573a\u666f\uff0c\u5e76\u63a8\u51fa\u4e86\u914d\u5957\u7684Turing\u89e3\u4ea4\u7ec7\u6311\u6218\u8d5b\u4ee5\u4fc3\u8fdb\u6570\u636e\u96c6\u91c7\u7528\u548c\u6807\u51c6\u5316\u8bc4\u4f30", "conclusion": "Turing\u5408\u6210\u96f7\u8fbe\u6570\u636e\u96c6\u662f\u9996\u6279\u516c\u5f00\u53ef\u7528\u7684\u7efc\u5408\u6a21\u62df\u8109\u51b2\u5e8f\u5217\u6570\u636e\u96c6\u4e4b\u4e00\uff0c\u65e8\u5728\u4fc3\u8fdb\u7535\u5b50\u6218\u793e\u533a\u4e2d\u590d\u6742\u6a21\u578b\u7684\u53d1\u5c55\uff0c\u4e3a\u96f7\u8fbe\u8109\u51b2\u89e3\u4ea4\u7ec7\u7814\u7a76\u63d0\u4f9b\u91cd\u8981\u8d44\u6e90"}}
{"id": "2602.04217", "categories": ["cs.SD", "cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2602.04217", "abs": "https://arxiv.org/abs/2602.04217", "authors": ["Takanori Ashihara", "Shota Horiguchi", "Kohei Matsuura", "Tsubasa Ochiai", "Marc Delcroix"], "title": "Frontend Token Enhancement for Token-Based Speech Recognition", "comment": "Accepted at ICASSP 2026", "summary": "Discretized representations of speech signals are efficient alternatives to continuous features for various speech applications, including automatic speech recognition (ASR) and speech language models. However, these representations, such as semantic or phonetic tokens derived from clustering outputs of self-supervised learning (SSL) speech models, are susceptible to environmental noise, which can degrade backend task performance. In this work, we introduce a frontend system that estimates clean speech tokens from noisy speech and evaluate it on an ASR backend using semantic tokens. We consider four types of enhancement models based on their input/output domains: wave-to-wave, token-to-token, continuous SSL features-to-token, and wave-to-token. These models are trained independently of ASR backends. Experiments on the CHiME-4 dataset demonstrate that wave-to-token enhancement achieves the best performance among the frontends. Moreover, it mostly outperforms the ASR system based on continuous SSL features.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u524d\u7aef\u7cfb\u7edf\uff0c\u7528\u4e8e\u4ece\u5e26\u566a\u8bed\u97f3\u4e2d\u4f30\u8ba1\u5e72\u51c0\u8bed\u97f3token\uff0c\u5e76\u5728\u4f7f\u7528\u8bed\u4e49token\u7684ASR\u540e\u7aef\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0c\u5176\u4e2dwave-to-token\u589e\u5f3a\u65b9\u6cd5\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u8bed\u97f3\u4fe1\u53f7\u7684\u79bb\u6563\u5316\u8868\u793a\uff08\u5982\u8bed\u4e49\u6216\u97f3\u7d20token\uff09\u662f\u8bed\u97f3\u5e94\u7528\u7684\u9ad8\u6548\u66ff\u4ee3\u65b9\u6848\uff0c\u4f46\u8fd9\u4e9b\u8868\u793a\u5bb9\u6613\u53d7\u5230\u73af\u5883\u566a\u58f0\u5f71\u54cd\uff0c\u4ece\u800c\u964d\u4f4e\u540e\u7aef\u4efb\u52a1\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u524d\u7aef\u589e\u5f3a\u7cfb\u7edf\uff0c\u8003\u8651\u56db\u79cd\u8f93\u5165/\u8f93\u51fa\u57df\u6a21\u578b\uff1awave-to-wave\u3001token-to-token\u3001continuous SSL features-to-token\u548cwave-to-token\uff0c\u8fd9\u4e9b\u6a21\u578b\u72ec\u7acb\u4e8eASR\u540e\u7aef\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u5728CHiME-4\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cwave-to-token\u589e\u5f3a\u5728\u524d\u7aef\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u5e76\u4e14\u5927\u591a\u4f18\u4e8e\u57fa\u4e8e\u8fde\u7eedSSL\u7279\u5f81\u7684ASR\u7cfb\u7edf\u3002", "conclusion": "wave-to-token\u589e\u5f3a\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u5347\u5e26\u566a\u73af\u5883\u4e0b\u57fa\u4e8e\u79bb\u6563token\u7684ASR\u7cfb\u7edf\u6027\u80fd\uff0c\u5c55\u793a\u4e86\u524d\u7aef\u589e\u5f3a\u5bf9\u79bb\u6563\u8bed\u97f3\u8868\u793a\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2602.03858", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.03858", "abs": "https://arxiv.org/abs/2602.03858", "authors": ["Shuntaro Suzuki", "Shuitsu Koyama", "Shinnosuke Hirano", "Shunya Nagashima"], "title": "PENGUIN: General Vital Sign Reconstruction from PPG with Flow Matching State Space Model", "comment": "Accepted for presentation at ICASSP2026", "summary": "Photoplethysmography (PPG) plays a crucial role in continuous cardiovascular health monitoring as a non-invasive and cost-effective modality. However, PPG signals are susceptible to motion artifacts and noise, making accurate estimation of vital signs such as arterial blood pressure (ABP) challenging. Existing estimation methods are often restricted to a single-task or environment, limiting their generalizability across diverse PPG decoding scenarios. Moreover, recent general-purpose approaches typically rely on predictions over multi-second intervals, discarding the morphological characteristics of vital signs. To address these challenges, we propose PENGUIN, a generative flow-matching framework that extends deep state space models, enabling fine-grained conditioning on PPG for reconstructing multiple vital signs as continuous waveforms. We evaluate PENGUIN using six real-world PPG datasets across three distinct vital sign reconstruction tasks (electrocardiogram reconstruction, respiratory monitoring, and ABP monitoring). Our method consistently outperformed both task-specific and general-purpose baselines, demonstrating PENGUIN as a general framework for robust vital sign reconstruction from PPG.", "AI": {"tldr": "PENGUIN\u662f\u4e00\u4e2a\u57fa\u4e8e\u751f\u6210\u6d41\u5339\u914d\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u6269\u5c55\u6df1\u5ea6\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff0c\u4ecePPG\u4fe1\u53f7\u4e2d\u91cd\u5efa\u591a\u79cd\u751f\u547d\u4f53\u5f81\u7684\u8fde\u7eed\u6ce2\u5f62\uff0c\u5728\u591a\u4e2a\u4efb\u52a1\u548c\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "PPG\u4fe1\u53f7\u6613\u53d7\u8fd0\u52a8\u4f2a\u5f71\u548c\u566a\u58f0\u5f71\u54cd\uff0c\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u5c40\u9650\u4e8e\u5355\u4e00\u4efb\u52a1\u6216\u73af\u5883\uff0c\u8981\u4e48\u4f9d\u8d56\u591a\u79d2\u95f4\u9694\u9884\u6d4b\u800c\u4e22\u5f03\u5f62\u6001\u7279\u5f81\uff0c\u7f3a\u4e4f\u8de8\u4e0d\u540cPPG\u89e3\u7801\u573a\u666f\u7684\u901a\u7528\u6027\u3002", "method": "\u63d0\u51faPENGUIN\u6846\u67b6\uff0c\u7ed3\u5408\u751f\u6210\u6d41\u5339\u914d\u548c\u6df1\u5ea6\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff0c\u5b9e\u73b0\u5bf9PPG\u4fe1\u53f7\u7684\u7ec6\u7c92\u5ea6\u6761\u4ef6\u5316\uff0c\u91cd\u5efa\u591a\u79cd\u751f\u547d\u4f53\u5f81\u7684\u8fde\u7eed\u6ce2\u5f62\u3002", "result": "\u57286\u4e2a\u771f\u5b9e\u4e16\u754cPPG\u6570\u636e\u96c6\u548c3\u4e2a\u751f\u547d\u4f53\u5f81\u91cd\u5efa\u4efb\u52a1\uff08\u5fc3\u7535\u56fe\u91cd\u5efa\u3001\u547c\u5438\u76d1\u6d4b\u3001\u52a8\u8109\u8840\u538b\u76d1\u6d4b\uff09\u4e0a\uff0cPENGUIN\u59cb\u7ec8\u4f18\u4e8e\u4efb\u52a1\u4e13\u7528\u548c\u901a\u7528\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "PENGUIN\u4f5c\u4e3a\u4e00\u4e2a\u901a\u7528\u6846\u67b6\uff0c\u80fd\u591f\u4ecePPG\u4fe1\u53f7\u4e2d\u7a33\u5065\u5730\u91cd\u5efa\u751f\u547d\u4f53\u5f81\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2602.04776", "categories": ["cs.SD", "cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2602.04776", "abs": "https://arxiv.org/abs/2602.04776", "authors": ["M\u00e1t\u00e9 Gedeon", "P\u00e9ter Mihajlik"], "title": "Speaker-Aware Simulation Improves Conversational Speech Recognition", "comment": null, "summary": "Automatic speech recognition (ASR) for conversational speech remains challenging due to the limited availability of large-scale, well-annotated multi-speaker dialogue data and the complex temporal dynamics of natural interactions. Speaker-aware simulated conversations (SASC) offer an effective data augmentation strategy by transforming single-speaker recordings into realistic multi-speaker dialogues. However, prior work has primarily focused on English data, leaving questions about the applicability to lower-resource languages. In this paper, we adapt and implement the SASC framework for Hungarian conversational ASR. We further propose C-SASC, an extended variant that incorporates pause modeling conditioned on utterance duration, enabling a more faithful representation of local temporal dependencies observed in human conversation while retaining the simplicity and efficiency of the original approach. We generate synthetic Hungarian dialogues from the BEA-Large corpus and combine them with real conversational data for ASR training. Both SASC and C-SASC are evaluated extensively under a wide range of simulation configurations, using conversational statistics derived from CallHome, BEA-Dialogue, and GRASS corpora. Experimental results show that speaker-aware conversational simulation consistently improves recognition performance over naive concatenation-based augmentation. While the additional duration conditioning in C-SASC yields modest but systematic gains--most notably in character-level error rates--its effectiveness depends on the match between source conversational statistics and the target domain. Overall, our findings confirm the robustness of speaker-aware conversational simulation for Hungarian ASR and highlight the benefits and limitations of increasingly detailed temporal modeling in synthetic dialogue generation.", "AI": {"tldr": "\u5c06\u8bf4\u8bdd\u4eba\u611f\u77e5\u6a21\u62df\u5bf9\u8bdd\uff08SASC\uff09\u6846\u67b6\u5e94\u7528\u4e8e\u5308\u7259\u5229\u8bed\u5bf9\u8bddASR\uff0c\u5e76\u63d0\u51fa\u5e26\u6682\u505c\u5efa\u6a21\u7684C-SASC\u53d8\u4f53\uff0c\u901a\u8fc7\u5408\u6210\u5bf9\u8bdd\u6570\u636e\u589e\u5f3a\u63d0\u5347\u8bc6\u522b\u6027\u80fd\u3002", "motivation": "\u5bf9\u8bdd\u8bed\u97f3\u8bc6\u522b\u9762\u4e34\u591a\u8bf4\u8bdd\u4eba\u5bf9\u8bdd\u6570\u636e\u7a00\u7f3a\u7684\u6311\u6218\uff0c\u73b0\u6709SASC\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u82f1\u8bed\uff0c\u9700\u8981\u9a8c\u8bc1\u5176\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\uff08\u5982\u5308\u7259\u5229\u8bed\uff09\u4e2d\u7684\u9002\u7528\u6027\u3002", "method": "\u5c06SASC\u6846\u67b6\u9002\u914d\u5308\u7259\u5229\u8bed\uff0c\u63d0\u51faC-SASC\u53d8\u4f53\uff08\u57fa\u4e8e\u8bdd\u8bed\u65f6\u957f\u7684\u6682\u505c\u5efa\u6a21\uff09\uff0c\u4eceBEA-Large\u8bed\u6599\u751f\u6210\u5408\u6210\u5bf9\u8bdd\uff0c\u7ed3\u5408\u771f\u5b9e\u5bf9\u8bdd\u6570\u636e\u8fdb\u884cASR\u8bad\u7ec3\u3002", "result": "\u8bf4\u8bdd\u4eba\u611f\u77e5\u5bf9\u8bdd\u6a21\u62df\u76f8\u6bd4\u7b80\u5355\u62fc\u63a5\u589e\u5f3a\u663e\u8457\u63d0\u5347\u8bc6\u522b\u6027\u80fd\uff0cC-SASC\u5728\u5b57\u7b26\u7ea7\u9519\u8bef\u7387\u4e0a\u5e26\u6765\u9002\u5ea6\u4f46\u7cfb\u7edf\u7684\u6539\u8fdb\uff0c\u6548\u679c\u53d6\u51b3\u4e8e\u6e90\u5bf9\u8bdd\u7edf\u8ba1\u4e0e\u76ee\u6807\u57df\u7684\u5339\u914d\u5ea6\u3002", "conclusion": "SASC\u6846\u67b6\u5bf9\u5308\u7259\u5229\u8bedASR\u5177\u6709\u9c81\u68d2\u6027\uff0c\u66f4\u7cbe\u7ec6\u7684\u65f6\u95f4\u5efa\u6a21\u5728\u5408\u6210\u5bf9\u8bdd\u751f\u6210\u4e2d\u65e2\u6709\u76ca\u5904\u4e5f\u6709\u5c40\u9650\u6027\uff0c\u9700\u8003\u8651\u7edf\u8ba1\u5339\u914d\u95ee\u9898\u3002"}}
{"id": "2602.03860", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2602.03860", "abs": "https://arxiv.org/abs/2602.03860", "authors": ["Yanchao Jiang", "Pierluigi Poggiolini"], "title": "Polynomial Closed-Form Model for Evaluating Nonlinear Interference in Any Island", "comment": "This paper is a follow-up to P. Poggiolini, Y. Jiang, Y. Gao, F. Forghieri, \"Polynomial Closed Form Model for Ultra-Wideband Transmission Systems,\" arXiv:2508.21563", "summary": "Polynomial closed-form GN model is proposed by expressing the spatial power profile of each channel along a span as a polynomial. In this paper, we present the generic closed-form expression for all contributions of self-, cross-, and multi-channel interference. The full derivation is provided.", "AI": {"tldr": "\u63d0\u51fa\u591a\u9879\u5f0f\u95ed\u5f0fGN\u6a21\u578b\uff0c\u901a\u8fc7\u5c06\u6bcf\u4e2a\u4fe1\u9053\u5728\u5149\u7ea4\u8de8\u6bb5\u5185\u7684\u7a7a\u95f4\u529f\u7387\u5206\u5e03\u8868\u793a\u4e3a\u591a\u9879\u5f0f\uff0c\u63a8\u5bfc\u51fa\u6240\u6709\u81ea\u5e72\u6270\u3001\u4ea4\u53c9\u5e72\u6270\u548c\u591a\u4fe1\u9053\u5e72\u6270\u7684\u901a\u7528\u95ed\u5f0f\u8868\u8fbe\u5f0f\u3002", "motivation": "\u4f20\u7edfGN\u6a21\u578b\u8ba1\u7b97\u590d\u6742\uff0c\u9700\u8981\u6570\u503c\u79ef\u5206\uff0c\u96be\u4ee5\u5b9e\u65f6\u5e94\u7528\u3002\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u3001\u89e3\u6790\u5f62\u5f0f\u7684\u6a21\u578b\u6765\u51c6\u786e\u8bc4\u4f30\u5149\u7ea4\u901a\u4fe1\u7cfb\u7edf\u4e2d\u7684\u975e\u7ebf\u6027\u5e72\u6270\u3002", "method": "\u5c06\u6bcf\u4e2a\u4fe1\u9053\u6cbf\u5149\u7ea4\u8de8\u6bb5\u7684\u7a7a\u95f4\u529f\u7387\u5206\u5e03\u5efa\u6a21\u4e3a\u591a\u9879\u5f0f\u51fd\u6570\uff0c\u57fa\u4e8e\u6b64\u63a8\u5bfc\u975e\u7ebf\u6027\u5e72\u6270\u7684\u901a\u7528\u95ed\u5f0f\u8868\u8fbe\u5f0f\uff0c\u5305\u62ec\u81ea\u5e72\u6270\u3001\u4ea4\u53c9\u5e72\u6270\u548c\u591a\u4fe1\u9053\u5e72\u6270\u7684\u5b8c\u6574\u6570\u5b66\u63a8\u5bfc\u3002", "result": "\u83b7\u5f97\u4e86\u6240\u6709\u7c7b\u578b\u975e\u7ebf\u6027\u5e72\u6270\u7684\u901a\u7528\u95ed\u5f0f\u8868\u8fbe\u5f0f\uff0c\u907f\u514d\u4e86\u6570\u503c\u79ef\u5206\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u4e3a\u5149\u7ea4\u901a\u4fe1\u7cfb\u7edf\u6027\u80fd\u8bc4\u4f30\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u6790\u5de5\u5177\u3002", "conclusion": "\u591a\u9879\u5f0f\u95ed\u5f0fGN\u6a21\u578b\u4e3a\u5149\u7ea4\u975e\u7ebf\u6027\u5e72\u6270\u5206\u6790\u63d0\u4f9b\u4e86\u7cbe\u786e\u4e14\u8ba1\u7b97\u9ad8\u6548\u7684\u89e3\u6790\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u5b9e\u65f6\u7cfb\u7edf\u8bbe\u8ba1\u548c\u6027\u80fd\u4f18\u5316\u3002"}}
{"id": "2602.04085", "categories": ["cs.SD", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.04085", "abs": "https://arxiv.org/abs/2602.04085", "authors": ["Min Jang", "Orevaoghene Ahia", "Nazif Tamer", "Sachin Kumar", "Yulia Tsvetkov", "Noah A. Smith"], "title": "BASS: Benchmarking Audio LMs for Musical Structure and Semantic Reasoning", "comment": null, "summary": "Music understanding is a complex task that often requires reasoning over both structural and semantic elements of audio. We introduce BASS, designed to evaluate music understanding and reasoning in audio language models across four broad categories: structural segmentation, lyric transcription, musicological analysis, and artist collaboration. BASS comprises 2658 questions spanning 12 tasks, 1993 unique songs and covering over 138 hours of music from a wide range of genres and tracks, crafted to assess musicological knowledge and reasoning in real-world scenarios. We evaluate 14 open-source and frontier multimodal LMs, finding that even state-of-the-art models struggle on higher-level reasoning tasks such as structural segmentation and artist collaboration, while performing best on lyric transcription. Our analysis reveals that current models leverage linguistic priors effectively but remain limited in reasoning over musical structure, vocal, and musicological attributes. BASS provides an evaluation framework with widespread applications in music recommendation and search and has the potential to guide the development of audio LMs.", "AI": {"tldr": "BASS\u662f\u4e00\u4e2a\u8bc4\u4f30\u97f3\u9891\u8bed\u8a00\u6a21\u578b\u97f3\u4e50\u7406\u89e3\u4e0e\u63a8\u7406\u80fd\u529b\u7684\u57fa\u51c6\uff0c\u5305\u542b2658\u4e2a\u95ee\u9898\u300112\u4e2a\u4efb\u52a1\uff0c\u6db5\u76d61993\u9996\u6b4c\u66f2\uff0c\u53d1\u73b0\u5f53\u524d\u6a21\u578b\u5728\u6b4c\u8bcd\u8f6c\u5f55\u8868\u73b0\u6700\u597d\uff0c\u4f46\u5728\u9ad8\u5c42\u6b21\u63a8\u7406\u4efb\u52a1\u5982\u7ed3\u6784\u5206\u5272\u548c\u827a\u672f\u5bb6\u5408\u4f5c\u65b9\u9762\u4ecd\u6709\u56f0\u96be\u3002", "motivation": "\u97f3\u4e50\u7406\u89e3\u9700\u8981\u540c\u65f6\u5904\u7406\u97f3\u9891\u7684\u7ed3\u6784\u548c\u8bed\u4e49\u5143\u7d20\uff0c\u4f46\u5f53\u524d\u7f3a\u4e4f\u7cfb\u7edf\u8bc4\u4f30\u97f3\u9891\u8bed\u8a00\u6a21\u578b\u97f3\u4e50\u7406\u89e3\u4e0e\u63a8\u7406\u80fd\u529b\u7684\u57fa\u51c6\u3002", "method": "\u5f00\u53d1BASS\u57fa\u51c6\uff0c\u5305\u542b\u56db\u4e2a\u5927\u7c7b\uff1a\u7ed3\u6784\u5206\u5272\u3001\u6b4c\u8bcd\u8f6c\u5f55\u3001\u97f3\u4e50\u5b66\u5206\u6790\u3001\u827a\u672f\u5bb6\u5408\u4f5c\uff0c\u517112\u4e2a\u4efb\u52a1\u30012658\u4e2a\u95ee\u9898\uff0c\u8986\u76d61993\u9996\u72ec\u7279\u6b4c\u66f2\u3001138\u5c0f\u65f6\u97f3\u4e50\uff0c\u6db5\u76d6\u591a\u79cd\u6d41\u6d3e\u3002", "result": "\u8bc4\u4f30\u4e8614\u4e2a\u5f00\u6e90\u548c\u524d\u6cbf\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\uff0c\u53d1\u73b0\u5373\u4f7f\u6700\u5148\u8fdb\u7684\u6a21\u578b\u5728\u7ed3\u6784\u5206\u5272\u548c\u827a\u672f\u5bb6\u5408\u4f5c\u7b49\u9ad8\u5c42\u6b21\u63a8\u7406\u4efb\u52a1\u4e0a\u4ecd\u56f0\u96be\uff0c\u5728\u6b4c\u8bcd\u8f6c\u5f55\u4e0a\u8868\u73b0\u6700\u597d\u3002\u6a21\u578b\u80fd\u6709\u6548\u5229\u7528\u8bed\u8a00\u5148\u9a8c\uff0c\u4f46\u5728\u97f3\u4e50\u7ed3\u6784\u3001\u4eba\u58f0\u548c\u97f3\u4e50\u5b66\u5c5e\u6027\u63a8\u7406\u65b9\u9762\u6709\u9650\u3002", "conclusion": "BASS\u4e3a\u97f3\u4e50\u7406\u89e3\u548c\u63a8\u7406\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u5728\u97f3\u4e50\u63a8\u8350\u548c\u641c\u7d22\u4e2d\u6709\u5e7f\u6cdb\u5e94\u7528\u6f5c\u529b\uff0c\u5e76\u80fd\u6307\u5bfc\u97f3\u9891\u8bed\u8a00\u6a21\u578b\u7684\u672a\u6765\u53d1\u5c55\u3002"}}
{"id": "2602.04016", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.04016", "abs": "https://arxiv.org/abs/2602.04016", "authors": ["Vahid Yazdnian", "Yasaman Ghasempour"], "title": "A Multi-Modal Foundational Model for Wireless Communication and Sensing", "comment": null, "summary": "Artificial intelligence is a key enabler for next-generation wireless communication and sensing. Yet, today's learning-based wireless techniques do not generalize well: most models are task-specific, environment-dependent, and limited to narrow sensing modalities, requiring costly retraining when deployed in new scenarios. This work introduces a task-agnostic, multi-modal foundational model for physical-layer wireless systems that learns transferable, physics-aware representations across heterogeneous modalities, enabling robust generalization across tasks and environments. Our framework employs a physics-guided self-supervised pretraining strategy incorporating a dedicated physical token to capture cross-modal physical correspondences governed by electromagnetic propagation. The learned representations enable efficient adaptation to diverse downstream tasks, including massive multi-antenna optimization, wireless channel estimation, and device localization, using limited labeled data. Our extensive evaluations demonstrate superior generalization, robustness to deployment shifts, and reduced data requirements compared to task-specific baselines.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u4efb\u52a1\u65e0\u5173\u3001\u591a\u6a21\u6001\u7684\u65e0\u7ebf\u7269\u7406\u5c42\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u7269\u7406\u5f15\u5bfc\u7684\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u5b66\u4e60\u53ef\u8fc1\u79fb\u7684\u7269\u7406\u611f\u77e5\u8868\u793a\uff0c\u5b9e\u73b0\u8de8\u4efb\u52a1\u548c\u73af\u5883\u7684\u9c81\u68d2\u6cdb\u5316\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u5b66\u4e60\u7684\u65e0\u7ebf\u6280\u672f\u6cdb\u5316\u80fd\u529b\u5dee\uff1a\u5927\u591a\u6570\u6a21\u578b\u662f\u4efb\u52a1\u7279\u5b9a\u3001\u73af\u5883\u4f9d\u8d56\u4e14\u5c40\u9650\u4e8e\u72ed\u7a84\u611f\u77e5\u6a21\u6001\uff0c\u5728\u65b0\u573a\u666f\u90e8\u7f72\u65f6\u9700\u8981\u6602\u8d35\u7684\u91cd\u65b0\u8bad\u7ec3\u3002", "method": "\u91c7\u7528\u7269\u7406\u5f15\u5bfc\u7684\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u7b56\u7565\uff0c\u5f15\u5165\u4e13\u7528\u7269\u7406\u4ee4\u724c\u6765\u6355\u83b7\u7535\u78c1\u4f20\u64ad\u652f\u914d\u7684\u8de8\u6a21\u6001\u7269\u7406\u5bf9\u5e94\u5173\u7cfb\uff0c\u5b66\u4e60\u53ef\u8fc1\u79fb\u7684\u7269\u7406\u611f\u77e5\u8868\u793a\u3002", "result": "\u8be5\u6a21\u578b\u5728\u591a\u79cd\u4e0b\u6e38\u4efb\u52a1\uff08\u5927\u89c4\u6a21\u591a\u5929\u7ebf\u4f18\u5316\u3001\u65e0\u7ebf\u4fe1\u9053\u4f30\u8ba1\u3001\u8bbe\u5907\u5b9a\u4f4d\uff09\u4e0a\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u6cdb\u5316\u80fd\u529b\u3001\u5bf9\u90e8\u7f72\u53d8\u5316\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u51cf\u5c11\u6570\u636e\u9700\u6c42\u3002", "conclusion": "\u63d0\u51fa\u7684\u4efb\u52a1\u65e0\u5173\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u80fd\u591f\u5b66\u4e60\u8de8\u5f02\u6784\u6a21\u6001\u7684\u53ef\u8fc1\u79fb\u7269\u7406\u8868\u793a\uff0c\u4e3a\u4e0b\u4e00\u4ee3\u65e0\u7ebf\u901a\u4fe1\u548c\u611f\u77e5\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2602.04160", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2602.04160", "abs": "https://arxiv.org/abs/2602.04160", "authors": ["Vikentii Pankov", "Artem Gribul", "Oktai Tatanov", "Vladislav Proskurov", "Yuliya Korotkova", "Darima Mylzenova", "Dmitrii Vypirailenko"], "title": "PFluxTTS: Hybrid Flow-Matching TTS with Robust Cross-Lingual Voice Cloning and Inference-Time Model Fusion", "comment": "Accepted at ICASSP 2026", "summary": "We present PFluxTTS, a hybrid text-to-speech system addressing three gaps in flow-matching TTS: the stability-naturalness trade-off, weak cross-lingual voice cloning, and limited audio quality from low-rate mel features. Our contributions are: (1) a dual-decoder design combining duration-guided and alignment-free models through inference-time vector-field fusion; (2) robust cloning using a sequence of speech-prompt embeddings in a FLUX-based decoder, preserving speaker traits across languages without prompt transcripts; and (3) a modified PeriodWave vocoder with super-resolution to 48 kHz. On cross-lingual in-the-wild data, PFluxTTS clearly outperforms F5-TTS, FishSpeech, and SparkTTS, matches ChatterBox in naturalness (MOS 4.11) while achieving 23% lower WER (6.9% vs. 9.0%), and surpasses ElevenLabs in speaker similarity (+0.32 SMOS). The system remains robust in challenging scenarios where most open-source models fail, while requiring only short reference audio and no extra training. Audio demos are available at https://braskai.github.io/pfluxtts/", "AI": {"tldr": "PFluxTTS\u662f\u4e00\u4e2a\u6df7\u5408\u6587\u672c\u8f6c\u8bed\u97f3\u7cfb\u7edf\uff0c\u901a\u8fc7\u53cc\u89e3\u7801\u5668\u8bbe\u8ba1\u3001\u9c81\u68d2\u7684\u8de8\u8bed\u8a00\u8bed\u97f3\u514b\u9686\u548c\u9ad8\u8d28\u91cf\u58f0\u7801\u5668\uff0c\u89e3\u51b3\u4e86\u6d41\u5339\u914dTTS\u5728\u7a33\u5b9a\u6027-\u81ea\u7136\u5ea6\u6743\u8861\u3001\u8de8\u8bed\u8a00\u8bed\u97f3\u514b\u9686\u548c\u97f3\u9891\u8d28\u91cf\u65b9\u9762\u7684\u4e09\u4e2a\u5173\u952e\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u6d41\u5339\u914dTTS\u4e2d\u7684\u4e09\u4e2a\u5173\u952e\u95ee\u9898\uff1a1) \u7a33\u5b9a\u6027\u4e0e\u81ea\u7136\u5ea6\u7684\u6743\u8861\u95ee\u9898\uff1b2) \u8de8\u8bed\u8a00\u8bed\u97f3\u514b\u9686\u80fd\u529b\u5f31\uff1b3) \u4f4e\u901f\u7387\u6885\u5c14\u7279\u5f81\u5bfc\u81f4\u7684\u97f3\u9891\u8d28\u91cf\u53d7\u9650\u3002", "method": "1) \u53cc\u89e3\u7801\u5668\u8bbe\u8ba1\uff1a\u7ed3\u5408\u65f6\u957f\u5f15\u5bfc\u6a21\u578b\u548c\u5bf9\u9f50\u81ea\u7531\u6a21\u578b\uff0c\u901a\u8fc7\u63a8\u7406\u65f6\u5411\u91cf\u573a\u878d\u5408\uff1b2) \u9c81\u68d2\u514b\u9686\uff1a\u5728FLUX-based\u89e3\u7801\u5668\u4e2d\u4f7f\u7528\u8bed\u97f3\u63d0\u793a\u5d4c\u5165\u5e8f\u5217\uff0c\u65e0\u9700\u63d0\u793a\u6587\u672c\u5373\u53ef\u8de8\u8bed\u8a00\u4fdd\u6301\u8bf4\u8bdd\u4eba\u7279\u5f81\uff1b3) \u6539\u8fdb\u7684PeriodWave\u58f0\u7801\u5668\uff1a\u652f\u630148kHz\u8d85\u5206\u8fa8\u7387\u3002", "result": "\u5728\u8de8\u8bed\u8a00\u91ce\u5916\u6570\u636e\u4e0a\uff0cPFluxTTS\u660e\u663e\u4f18\u4e8eF5-TTS\u3001FishSpeech\u548cSparkTTS\uff0c\u5728\u81ea\u7136\u5ea6\u4e0a\u4e0eChatterBox\u76f8\u5f53\uff08MOS 4.11\uff09\uff0c\u540c\u65f6WER\u964d\u4f4e23%\uff086.9% vs. 9.0%\uff09\uff0c\u5728\u8bf4\u8bdd\u4eba\u76f8\u4f3c\u5ea6\u4e0a\u8d85\u8fc7ElevenLabs\uff08+0.32 SMOS\uff09\u3002\u7cfb\u7edf\u5728\u5927\u591a\u6570\u5f00\u6e90\u6a21\u578b\u5931\u8d25\u7684\u6311\u6218\u6027\u573a\u666f\u4e2d\u4fdd\u6301\u9c81\u68d2\u6027\u3002", "conclusion": "PFluxTTS\u901a\u8fc7\u521b\u65b0\u7684\u6df7\u5408\u67b6\u6784\u89e3\u51b3\u4e86\u6d41\u5339\u914dTTS\u7684\u5173\u952e\u9650\u5236\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u8de8\u8bed\u8a00\u8bed\u97f3\u5408\u6210\uff0c\u4ec5\u9700\u77ed\u53c2\u8003\u97f3\u9891\u4e14\u65e0\u9700\u989d\u5916\u8bad\u7ec3\uff0c\u5728\u81ea\u7136\u5ea6\u3001\u51c6\u786e\u6027\u548c\u8bf4\u8bdd\u4eba\u76f8\u4f3c\u5ea6\u65b9\u9762\u5747\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2602.04018", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2602.04018", "abs": "https://arxiv.org/abs/2602.04018", "authors": ["Sima Ghafoori", "Anna Cetera", "Ali Rabiee", "MH Farhadi", "Rahul Singh", "Mariusz Furmanek", "Yalda Shahriari", "Reza Abiri"], "title": "Cross-Frequency Bispectral EEG Analysis of Reach-to-Grasp Planning and Execution", "comment": "manuscript is 34 pages, 6 figures, 2 tables, journal -- supplementary material is 9 pages, 3 figures, 1 table", "summary": "Neural control of grasping arises from nonlinear interactions across multiple brain rhythms, yet EEG-based motor decoding has largely relied on linear, second-order spectral features. Here, we examine whether higher-order cross-frequency dynamics distinguish motor planning from execution during natural reach-to-grasp behavior. EEG was recorded in a cue-based paradigm during executed precision and power grips, enabling stage-resolved analysis of preparatory and execution-related neural activity.\n  Cross-frequency bispectral analysis was used to compute bicoherence matrices across canonical frequency band pairs, from which magnitude- and phase-based features were extracted. Classification, permutation-based feature selection, and within-subject statistical testing showed that execution is characterized by substantially stronger and more discriminative nonlinear coupling than planning, with dominant contributions from beta- and gamma-driven interactions. In contrast, decoding of precision versus power grips achieved comparable performance during planning and execution, indicating that grasp-type representations emerge during planning and persist into execution.\n  Spatial and spectral analyses further revealed that informative bispectral features reflect coordinated activity across prefrontal, central, and occipital regions. Despite substantial feature redundancy, effective dimensionality reduction preserved decoding performance. Together, these findings demonstrate that nonlinear cross-frequency coupling provides an interpretable and robust marker of motor planning and execution, extending bispectral EEG analysis to ecologically valid grasping and supporting its relevance for brain-computer interfaces and neuroprosthetic control.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u9ad8\u9636\u4ea4\u53c9\u9891\u7387\u53cc\u8c31\u5206\u6790\u53d1\u73b0\uff0c\u8fd0\u52a8\u6267\u884c\u9636\u6bb5\u6bd4\u8ba1\u5212\u9636\u6bb5\u5177\u6709\u66f4\u5f3a\u7684\u975e\u7ebf\u6027\u8111\u7535\u8026\u5408\u7279\u5f81\uff0c\u800c\u6293\u63e1\u7c7b\u578b\u8868\u5f81\u5728\u8ba1\u5212\u9636\u6bb5\u5df2\u5f62\u6210\u5e76\u6301\u7eed\u5230\u6267\u884c\u9636\u6bb5\u3002", "motivation": "\u4f20\u7edf\u8111\u7535\u8fd0\u52a8\u89e3\u7801\u4e3b\u8981\u4f9d\u8d56\u7ebf\u6027\u4e8c\u9636\u9891\u8c31\u7279\u5f81\uff0c\u4f46\u795e\u7ecf\u6293\u63e1\u63a7\u5236\u6e90\u4e8e\u591a\u4e2a\u8111\u8282\u5f8b\u7684\u975e\u7ebf\u6027\u76f8\u4e92\u4f5c\u7528\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u9ad8\u9636\u4ea4\u53c9\u9891\u7387\u52a8\u6001\u662f\u5426\u80fd\u533a\u5206\u81ea\u7136\u6293\u63e1\u884c\u4e3a\u4e2d\u7684\u8fd0\u52a8\u8ba1\u5212\u4e0e\u6267\u884c\u9636\u6bb5\u3002", "method": "\u91c7\u7528\u7ebf\u7d22\u63d0\u793a\u8303\u5f0f\u8bb0\u5f55\u6267\u884c\u7cbe\u786e\u6293\u63e1\u548c\u529b\u91cf\u6293\u63e1\u65f6\u7684\u8111\u7535\u4fe1\u53f7\uff0c\u4f7f\u7528\u4ea4\u53c9\u9891\u7387\u53cc\u8c31\u5206\u6790\u8ba1\u7b97\u6807\u51c6\u9891\u5e26\u5bf9\u7684\u53cc\u76f8\u5e72\u77e9\u9635\uff0c\u63d0\u53d6\u5e45\u5ea6\u548c\u76f8\u4f4d\u7279\u5f81\uff0c\u901a\u8fc7\u5206\u7c7b\u3001\u57fa\u4e8e\u6392\u5217\u7684\u7279\u5f81\u9009\u62e9\u548c\u53d7\u8bd5\u8005\u5185\u7edf\u8ba1\u6d4b\u8bd5\u8fdb\u884c\u5206\u6790\u3002", "result": "\u8fd0\u52a8\u6267\u884c\u9636\u6bb5\u8868\u73b0\u51fa\u6bd4\u8ba1\u5212\u9636\u6bb5\u663e\u8457\u66f4\u5f3a\u3001\u66f4\u5177\u533a\u5206\u6027\u7684\u975e\u7ebf\u6027\u8026\u5408\uff0c\u4e3b\u8981\u7531\u03b2\u548c\u03b3\u9891\u5e26\u9a71\u52a8\u3002\u7cbe\u786e\u6293\u63e1\u4e0e\u529b\u91cf\u6293\u63e1\u7684\u89e3\u7801\u5728\u8ba1\u5212\u548c\u6267\u884c\u9636\u6bb5\u8868\u73b0\u76f8\u5f53\uff0c\u8868\u660e\u6293\u63e1\u7c7b\u578b\u8868\u5f81\u5728\u8ba1\u5212\u9636\u6bb5\u5df2\u5f62\u6210\u5e76\u6301\u7eed\u5230\u6267\u884c\u3002\u4fe1\u606f\u6027\u53cc\u8c31\u7279\u5f81\u53cd\u6620\u4e86\u524d\u989d\u53f6\u3001\u4e2d\u592e\u533a\u548c\u6795\u53f6\u533a\u57df\u7684\u534f\u8c03\u6d3b\u52a8\u3002", "conclusion": "\u975e\u7ebf\u6027\u4ea4\u53c9\u9891\u7387\u8026\u5408\u4e3a\u8fd0\u52a8\u8ba1\u5212\u548c\u6267\u884c\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u4e14\u7a33\u5065\u7684\u795e\u7ecf\u6807\u8bb0\uff0c\u5c06\u53cc\u8c31\u8111\u7535\u5206\u6790\u6269\u5c55\u5230\u751f\u6001\u6709\u6548\u7684\u6293\u63e1\u884c\u4e3a\uff0c\u652f\u6301\u5176\u5728\u8111\u673a\u63a5\u53e3\u548c\u795e\u7ecf\u5047\u80a2\u63a7\u5236\u4e2d\u7684\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2602.04062", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2602.04062", "abs": "https://arxiv.org/abs/2602.04062", "authors": ["Helena Serpi", "Christina", "Politi"], "title": "Ultra-Fast Device-Free Visible Light Sensing and Localization via Reflection-Based \u0394RSS and Deep Learning", "comment": "6 pages, 3 figures", "summary": "We propose an Ultra-Fast, Device-Free Visible Light Sensing and Positioning system that captures spatiotemporal variations in single-LED VLC channel responses, using ceiling-mounted photodetectors, to accurately and non-intrusively infer human presence and position through optical signal reflection modeling. The system is highly adaptive and ready to serve different real-world sensing and positioning scenarios using one or more ML based models from the library of multi-architecture deep neural network ensembles we have developed.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8d85\u5feb\u901f\u3001\u65e0\u8bbe\u5907\u7684\u53ef\u89c1\u5149\u4f20\u611f\u4e0e\u5b9a\u4f4d\u7cfb\u7edf\uff0c\u5229\u7528\u5929\u82b1\u677f\u5b89\u88c5\u7684\u5149\u7535\u63a2\u6d4b\u5668\u6355\u83b7\u5355LED VLC\u4fe1\u9053\u54cd\u5e94\u7684\u65f6\u7a7a\u53d8\u5316\uff0c\u901a\u8fc7\u5149\u5b66\u4fe1\u53f7\u53cd\u5c04\u5efa\u6a21\u975e\u4fb5\u5165\u5f0f\u63a8\u65ad\u4eba\u5458\u5b58\u5728\u4e0e\u4f4d\u7f6e\u3002", "motivation": "\u4f20\u7edf\u4eba\u5458\u611f\u77e5\u4e0e\u5b9a\u4f4d\u7cfb\u7edf\u901a\u5e38\u9700\u8981\u7528\u6237\u643a\u5e26\u8bbe\u5907\u6216\u5b89\u88c5\u590d\u6742\u4f20\u611f\u5668\uff0c\u5b58\u5728\u4fb5\u5165\u6027\u548c\u90e8\u7f72\u6210\u672c\u95ee\u9898\u3002\u9700\u8981\u4e00\u79cd\u65e0\u9700\u7528\u6237\u8bbe\u5907\u3001\u975e\u4fb5\u5165\u5f0f\u3001\u9ad8\u9002\u5e94\u6027\u4e14\u80fd\u5feb\u901f\u54cd\u5e94\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u5929\u82b1\u677f\u5b89\u88c5\u7684\u5149\u7535\u63a2\u6d4b\u5668\u6355\u83b7\u5355LED\u53ef\u89c1\u5149\u901a\u4fe1\u4fe1\u9053\u7684\u65f6\u7a7a\u53d8\u5316\uff0c\u901a\u8fc7\u5149\u5b66\u4fe1\u53f7\u53cd\u5c04\u5efa\u6a21\u5206\u6790\u4fe1\u53f7\u7279\u5f81\u3002\u7cfb\u7edf\u91c7\u7528\u591a\u67b6\u6784\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u96c6\u6210\u5e93\uff0c\u5305\u542b\u591a\u79cd\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u53ef\u6839\u636e\u4e0d\u540c\u573a\u666f\u9009\u62e9\u5408\u9002\u6a21\u578b\u3002", "result": "\u7cfb\u7edf\u80fd\u591f\u51c6\u786e\u3001\u975e\u4fb5\u5165\u5f0f\u5730\u63a8\u65ad\u4eba\u5458\u5b58\u5728\u548c\u4f4d\u7f6e\uff0c\u5177\u6709\u9ad8\u5ea6\u9002\u5e94\u6027\uff0c\u53ef\u670d\u52a1\u4e8e\u4e0d\u540c\u7684\u73b0\u5b9e\u4e16\u754c\u4f20\u611f\u548c\u5b9a\u4f4d\u573a\u666f\uff0c\u4e14\u54cd\u5e94\u901f\u5ea6\u6781\u5feb\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u4e3a\u4eba\u5458\u611f\u77e5\u4e0e\u5b9a\u4f4d\u63d0\u4f9b\u4e86\u4e00\u79cd\u521b\u65b0\u7684\u65e0\u8bbe\u5907\u3001\u975e\u4fb5\u5165\u5f0f\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u53ef\u89c1\u5149\u4f20\u611f\u548c\u5148\u8fdb\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u548c\u5feb\u901f\u54cd\u5e94\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5b9e\u9645\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2602.04535", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2602.04535", "abs": "https://arxiv.org/abs/2602.04535", "authors": ["Xuenan Xu", "Yiming Ren", "Liwei Liu", "Wen Wu", "Baoxiang Li", "Chaochao Lu", "Shuai Wang", "Chao Zhang"], "title": "HoliAntiSpoof: Audio LLM for Holistic Speech Anti-Spoofing", "comment": null, "summary": "Recent advances in speech synthesis and editing have made speech spoofing increasingly challenging. However, most existing methods treat spoofing as binary classification, overlooking that diverse spoofing techniques manipulate multiple, coupled speech attributes and their semantic effects. In this paper, we introduce HoliAntiSpoof, the first audio large language model (ALLM) framework for holistic speech anti-spoofing analysis. HoliAntiSpoof reformulates spoofing analysis as a unified text generation task, enabling joint reasoning over spoofing methods, affected speech attributes, and their semantic impacts. To support semantic-level analysis, we introduce DailyTalkEdit, a new anti-spoofing benchmark that simulates realistic conversational manipulations and provides annotations of semantic influence. Extensive experiments demonstrate that HoliAntiSpoof outperforms conventional baselines across multiple settings, while preliminary results show that in-context learning further improves out-of-domain generalization. These findings indicate that ALLMs not only enhance speech spoofing detection performance but also enable interpretable analysis of spoofing behaviors and their semantic effects, pointing towards more trustworthy and explainable speech security. Data and code are publicly available.", "AI": {"tldr": "HoliAntiSpoof\uff1a\u9996\u4e2a\u7528\u4e8e\u6574\u4f53\u8bed\u97f3\u53cd\u6b3a\u9a97\u5206\u6790\u7684\u97f3\u9891\u5927\u8bed\u8a00\u6a21\u578b\u6846\u67b6\uff0c\u5c06\u6b3a\u9a97\u5206\u6790\u91cd\u65b0\u5b9a\u4e49\u4e3a\u7edf\u4e00\u7684\u6587\u672c\u751f\u6210\u4efb\u52a1\uff0c\u80fd\u591f\u8054\u5408\u63a8\u7406\u6b3a\u9a97\u65b9\u6cd5\u3001\u53d7\u5f71\u54cd\u7684\u8bed\u97f3\u5c5e\u6027\u53ca\u5176\u8bed\u4e49\u5f71\u54cd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5927\u591a\u5c06\u8bed\u97f3\u6b3a\u9a97\u89c6\u4e3a\u4e8c\u5143\u5206\u7c7b\u95ee\u9898\uff0c\u5ffd\u7565\u4e86\u591a\u6837\u5316\u7684\u6b3a\u9a97\u6280\u672f\u4f1a\u64cd\u7eb5\u591a\u4e2a\u8026\u5408\u7684\u8bed\u97f3\u5c5e\u6027\u53ca\u5176\u8bed\u4e49\u6548\u679c\u3002\u9700\u8981\u66f4\u5168\u9762\u7684\u5206\u6790\u65b9\u6cd5\u6765\u7406\u89e3\u6b3a\u9a97\u884c\u4e3a\u7684\u8bed\u4e49\u5f71\u54cd\u3002", "method": "\u63d0\u51faHoliAntiSpoof\u6846\u67b6\uff0c\u5c06\u6b3a\u9a97\u5206\u6790\u91cd\u65b0\u5b9a\u4e49\u4e3a\u7edf\u4e00\u7684\u6587\u672c\u751f\u6210\u4efb\u52a1\u3002\u5f15\u5165DailyTalkEdit\u65b0\u57fa\u51c6\uff0c\u6a21\u62df\u771f\u5b9e\u5bf9\u8bdd\u64cd\u7eb5\u5e76\u63d0\u4f9b\u8bed\u4e49\u5f71\u54cd\u6807\u6ce8\u3002\u652f\u6301\u8054\u5408\u63a8\u7406\u6b3a\u9a97\u65b9\u6cd5\u3001\u53d7\u5f71\u54cd\u7684\u8bed\u97f3\u5c5e\u6027\u548c\u8bed\u4e49\u5f71\u54cd\u3002", "result": "HoliAntiSpoof\u5728\u591a\u79cd\u8bbe\u7f6e\u4e0b\u4f18\u4e8e\u4f20\u7edf\u57fa\u7ebf\u65b9\u6cd5\u3002\u521d\u6b65\u7ed3\u679c\u663e\u793a\u4e0a\u4e0b\u6587\u5b66\u4e60\u8fdb\u4e00\u6b65\u63d0\u9ad8\u4e86\u57df\u5916\u6cdb\u5316\u80fd\u529b\u3002\u6a21\u578b\u4e0d\u4ec5\u63d0\u5347\u4e86\u6b3a\u9a97\u68c0\u6d4b\u6027\u80fd\uff0c\u8fd8\u80fd\u5bf9\u6b3a\u9a97\u884c\u4e3a\u53ca\u5176\u8bed\u4e49\u6548\u679c\u8fdb\u884c\u53ef\u89e3\u91ca\u5206\u6790\u3002", "conclusion": "\u97f3\u9891\u5927\u8bed\u8a00\u6a21\u578b\u4e0d\u4ec5\u80fd\u63d0\u5347\u8bed\u97f3\u6b3a\u9a97\u68c0\u6d4b\u6027\u80fd\uff0c\u8fd8\u80fd\u5b9e\u73b0\u5bf9\u6b3a\u9a97\u884c\u4e3a\u53ca\u5176\u8bed\u4e49\u6548\u679c\u7684\u53ef\u89e3\u91ca\u5206\u6790\uff0c\u4e3a\u66f4\u53ef\u4fe1\u3001\u53ef\u89e3\u91ca\u7684\u8bed\u97f3\u5b89\u5168\u6307\u660e\u4e86\u65b9\u5411\u3002\u6570\u636e\u548c\u4ee3\u7801\u5df2\u516c\u5f00\u3002"}}
{"id": "2602.04083", "categories": ["eess.SP", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.04083", "abs": "https://arxiv.org/abs/2602.04083", "authors": ["Alexandre Barbosa de Lima"], "title": "Structure-Informed Estimation for Pilot-Limited MIMO Channels via Tensor Decomposition", "comment": null, "summary": "Channel estimation in wideband multiple-input multiple-output (MIMO) systems faces fundamental pilot overhead limitations in high-dimensional beyond-5G and sixth-generation (6G) scenarios. This paper presents a hybrid tensor-neural architecture that formulates pilot-limited channel estimation as low-rank tensor completion from sparse observations -- a fundamentally different setting from prior tensor methods that assume fully observed received signal tensors. A canonical polyadic (CP) baseline implemented via a projection-based scheme (Tucker completion under partial observations) and Tucker decompositions are compared under varying signal-to-noise ratio (SNR) and scattering conditions: CP performs well for specular channels matching the multipath model, while Tucker provides greater robustness under model mismatch. A lightweight three-dimensional (3D) U-Net learns residual components beyond the low-rank structure, bridging algebraic models and realistic propagation effects. Empirical recovery threshold analysis shows that sample complexity scales approximately with intrinsic model dimensionality $L(N_r + N_t + N_f)$ rather than ambient tensor size $N_r N_t N_f$, where $L$ denotes the number of dominant propagation paths. Experiments on synthetic channels demonstrate 10-20\\,dB normalized mean-square error (NMSE) improvement over least-squares (LS) and orthogonal matching pursuit (OMP) baselines at 5-10\\% pilot density, while evaluations on DeepMIMO ray-tracing channels show 24-44\\% additional NMSE reduction over pure tensor-based methods.", "AI": {"tldr": "\u63d0\u51fa\u6df7\u5408\u5f20\u91cf-\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u901a\u8fc7\u4f4e\u79e9\u5f20\u91cf\u8865\u5168\u89e3\u51b3\u5bbd\u5e26MIMO\u7cfb\u7edf\u5bfc\u9891\u53d7\u9650\u7684\u4fe1\u9053\u4f30\u8ba1\u95ee\u9898\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u663e\u8457\u964d\u4f4eNMSE", "motivation": "\u5bbd\u5e26MIMO\u7cfb\u7edf\u57285G/6G\u9ad8\u7ef4\u573a\u666f\u4e0b\u9762\u4e34\u5bfc\u9891\u5f00\u9500\u9650\u5236\uff0c\u4f20\u7edf\u4fe1\u9053\u4f30\u8ba1\u65b9\u6cd5\u6027\u80fd\u53d7\u9650\uff0c\u9700\u8981\u65b0\u65b9\u6cd5\u5728\u7a00\u758f\u89c2\u6d4b\u4e0b\u5b9e\u73b0\u9ad8\u6548\u4fe1\u9053\u4f30\u8ba1", "method": "\u6df7\u5408\u5f20\u91cf-\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff1a1) \u57fa\u4e8eCP\u548cTucker\u5206\u89e3\u7684\u4f4e\u79e9\u5f20\u91cf\u8865\u5168\u4f5c\u4e3a\u57fa\u7840\uff1b2) \u8f7b\u91cf\u7ea73D U-Net\u5b66\u4e60\u4f4e\u79e9\u7ed3\u6784\u4e4b\u5916\u7684\u6b8b\u5dee\u5206\u91cf\uff1b3) \u5c06\u4fe1\u9053\u4f30\u8ba1\u5efa\u6a21\u4e3a\u7a00\u758f\u89c2\u6d4b\u4e0b\u7684\u5f20\u91cf\u8865\u5168\u95ee\u9898", "result": "1) \u6837\u672c\u590d\u6742\u5ea6\u4e0e\u5185\u5728\u6a21\u578b\u7ef4\u5ea6L(Nr+Nt+Nf)\u6210\u6b63\u6bd4\u800c\u975e\u73af\u5883\u5f20\u91cf\u5927\u5c0fNrNtNf\uff1b2) \u5408\u6210\u4fe1\u9053\uff1a5-10%\u5bfc\u9891\u5bc6\u5ea6\u4e0bNMSE\u6bd4LS\u548cOMP\u63d0\u534710-20dB\uff1b3) DeepMIMO\u5c04\u7ebf\u8ffd\u8e2a\u4fe1\u9053\uff1a\u6bd4\u7eaf\u5f20\u91cf\u65b9\u6cd5\u989d\u5916\u964d\u4f4e24-44% NMSE", "conclusion": "\u6df7\u5408\u5f20\u91cf-\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u80fd\u6709\u6548\u89e3\u51b3\u5bfc\u9891\u53d7\u9650\u7684\u5bbd\u5e26MIMO\u4fe1\u9053\u4f30\u8ba1\u95ee\u9898\uff0c\u7ed3\u5408\u4ee3\u6570\u6a21\u578b\u548c\u5b9e\u9645\u4f20\u64ad\u6548\u5e94\uff0c\u5728\u7a00\u758f\u89c2\u6d4b\u4e0b\u5b9e\u73b0\u9ad8\u6027\u80fd\u4fe1\u9053\u6062\u590d"}}
{"id": "2602.04680", "categories": ["cs.SD", "cs.AI", "cs.CL", "cs.MM"], "pdf": "https://arxiv.org/pdf/2602.04680", "abs": "https://arxiv.org/abs/2602.04680", "authors": ["Haina Zhu", "Yao Xiao", "Xiquan Li", "Ziyang Ma", "Jianwei Yu", "Bowen Zhang", "Mingqi Yang", "Xie Chen"], "title": "Audio ControlNet for Fine-Grained Audio Generation and Editing", "comment": null, "summary": "We study the fine-grained text-to-audio (T2A) generation task. While recent models can synthesize high-quality audio from text descriptions, they often lack precise control over attributes such as loudness, pitch, and sound events. Unlike prior approaches that retrain models for specific control types, we propose to train ControlNet models on top of pre-trained T2A backbones to achieve controllable generation over loudness, pitch, and event roll. We introduce two designs, T2A-ControlNet and T2A-Adapter, and show that the T2A-Adapter model offers a more efficient structure with strong control ability. With only 38M additional parameters, T2A-Adapter achieves state-of-the-art performance on the AudioSet-Strong in both event-level and segment-level F1 scores. We further extend this framework to audio editing, proposing T2A-Editor for removing and inserting audio events at time locations specified by instructions. Models, code, dataset pipelines, and benchmarks will be released to support future research on controllable audio generation and editing.", "AI": {"tldr": "\u63d0\u51faT2A-Adapter\u65b9\u6cd5\uff0c\u901a\u8fc7\u4ec5\u589e\u52a03800\u4e07\u53c2\u6570\u5b9e\u73b0\u6587\u672c\u5230\u97f3\u9891\u751f\u6210\u4e2d\u7684\u7cbe\u786e\u63a7\u5236\uff08\u54cd\u5ea6\u3001\u97f3\u9ad8\u3001\u4e8b\u4ef6\uff09\uff0c\u5e76\u5728AudioSet-Strong\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\uff0c\u540c\u65f6\u6269\u5c55\u81f3\u97f3\u9891\u7f16\u8f91\u4efb\u52a1\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u5230\u97f3\u9891\u751f\u6210\u6a21\u578b\u867d\u7136\u80fd\u5408\u6210\u9ad8\u8d28\u91cf\u97f3\u9891\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u54cd\u5ea6\u3001\u97f3\u9ad8\u548c\u58f0\u97f3\u4e8b\u4ef6\u7b49\u5c5e\u6027\u7684\u7cbe\u786e\u63a7\u5236\u3002\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u4e3a\u6bcf\u79cd\u63a7\u5236\u7c7b\u578b\u91cd\u65b0\u8bad\u7ec3\u6a21\u578b\uff0c\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u5728\u9884\u8bad\u7ec3\u7684T2A\u9aa8\u5e72\u6a21\u578b\u4e0a\u8bad\u7ec3ControlNet\u6a21\u578b\uff0c\u63d0\u51faT2A-ControlNet\u548cT2A-Adapter\u4e24\u79cd\u8bbe\u8ba1\u3002T2A-Adapter\u91c7\u7528\u66f4\u9ad8\u6548\u7684\u7ed3\u6784\uff0c\u4ec5\u589e\u52a03800\u4e07\u53c2\u6570\u5373\u53ef\u5b9e\u73b0\u5f3a\u63a7\u5236\u80fd\u529b\u3002", "result": "T2A-Adapter\u5728AudioSet-Strong\u6570\u636e\u96c6\u4e0a\uff0c\u5728\u4e8b\u4ef6\u7ea7\u522b\u548c\u7247\u6bb5\u7ea7\u522b\u7684F1\u5206\u6570\u5747\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\u3002\u8fdb\u4e00\u6b65\u6269\u5c55\u6846\u67b6\u63d0\u51faT2A-Editor\uff0c\u5b9e\u73b0\u57fa\u4e8e\u6307\u4ee4\u7684\u97f3\u9891\u4e8b\u4ef6\u79fb\u9664\u548c\u63d2\u5165\u7f16\u8f91\u3002", "conclusion": "\u63d0\u51fa\u7684T2A-Adapter\u65b9\u6cd5\u901a\u8fc7\u8f7b\u91cf\u7ea7\u9002\u914d\u5668\u5b9e\u73b0\u4e86\u6587\u672c\u5230\u97f3\u9891\u751f\u6210\u7684\u7cbe\u786e\u63a7\u5236\uff0c\u4e3a\u53ef\u63a7\u97f3\u9891\u751f\u6210\u548c\u7f16\u8f91\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u76f8\u5173\u8d44\u6e90\u5c06\u5f00\u6e90\u4ee5\u652f\u6301\u672a\u6765\u7814\u7a76\u3002"}}
{"id": "2602.04084", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2602.04084", "abs": "https://arxiv.org/abs/2602.04084", "authors": ["Yanan Zhao", "Xingchao Jian", "Feng Ji", "Wee Peng Tay", "Antonio Ortega"], "title": "Uncertainty Principle for Vertex-Time Graph Signal Processing", "comment": "Submitted to IEEE Transactions on Signal Processing", "summary": "We present an uncertainty principle for graph signals in the vertex-time domain, unifying the classical time-frequency and graph uncertainty principles within a single framework. By defining vertex-time and spectral-frequency spreads, we quantify signal localization across these domains. Our framework identifies a class of signals that achieve maximum concentration in both the spatial and temporal domains. These signals serve as fundamental atoms for a new vertex-time dictionary, enhancing signal reconstruction under practical constraints, such as intermittent data commonly encountered in sensor and social networks. Furthermore, we introduce a novel graph topology inference method leveraging the uncertainty principle. Numerical experiments on synthetic and real datasets validate the effectiveness of our approach, demonstrating improved reconstruction accuracy, greater robustness to noise, and enhanced graph learning performance compared to existing methods.", "AI": {"tldr": "\u63d0\u51fa\u56fe\u4fe1\u53f7\u5728\u9876\u70b9-\u65f6\u57df\u7684\u4e0d\u786e\u5b9a\u6027\u539f\u7406\uff0c\u7edf\u4e00\u7ecf\u5178\u65f6\u9891\u548c\u56fe\u4e0d\u786e\u5b9a\u6027\u539f\u7406\uff0c\u5b9a\u4e49\u9876\u70b9-\u65f6\u57df\u548c\u8c31-\u9891\u57df\u6269\u5c55\uff0c\u8bc6\u522b\u6700\u5927\u6d53\u5ea6\u4fe1\u53f7\u4f5c\u4e3a\u65b0\u5b57\u5178\u539f\u5b50\uff0c\u5e76\u57fa\u4e8e\u6b64\u63d0\u51fa\u56fe\u62d3\u6251\u63a8\u65ad\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u4e0d\u786e\u5b9a\u6027\u539f\u7406\u5206\u522b\u5904\u7406\u65f6\u9891\u57df\u548c\u56fe\u57df\uff0c\u7f3a\u4e4f\u7edf\u4e00\u6846\u67b6\u3002\u4f20\u611f\u5668\u7f51\u7edc\u548c\u793e\u4ea4\u7f51\u7edc\u4e2d\u5e38\u9047\u5230\u95f4\u6b47\u6027\u6570\u636e\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u4fe1\u53f7\u8868\u793a\u548c\u91cd\u6784\u65b9\u6cd5\uff0c\u540c\u65f6\u56fe\u62d3\u6251\u63a8\u65ad\u4e5f\u662f\u91cd\u8981\u95ee\u9898\u3002", "method": "\u5b9a\u4e49\u9876\u70b9-\u65f6\u57df\u548c\u8c31-\u9891\u57df\u7684\u4fe1\u53f7\u6269\u5c55\u5ea6\u91cf\uff0c\u63a8\u5bfc\u7edf\u4e00\u7684\u4e0d\u786e\u5b9a\u6027\u539f\u7406\u3002\u8bc6\u522b\u5728\u7a7a\u95f4\u548c\u65f6\u95f4\u57df\u540c\u65f6\u8fbe\u5230\u6700\u5927\u6d53\u5ea6\u7684\u4fe1\u53f7\u7c7b\uff0c\u6784\u5efa\u9876\u70b9-\u65f6\u57df\u5b57\u5178\u3002\u57fa\u4e8e\u4e0d\u786e\u5b9a\u6027\u539f\u7406\u63d0\u51fa\u65b0\u7684\u56fe\u62d3\u6251\u63a8\u65ad\u65b9\u6cd5\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff1a\u63d0\u9ad8\u4e86\u4fe1\u53f7\u91cd\u6784\u7cbe\u5ea6\uff0c\u589e\u5f3a\u4e86\u5bf9\u566a\u58f0\u7684\u9c81\u68d2\u6027\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u83b7\u5f97\u4e86\u66f4\u597d\u7684\u56fe\u5b66\u4e60\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684\u9876\u70b9-\u65f6\u57df\u4e0d\u786e\u5b9a\u6027\u539f\u7406\u6210\u529f\u7edf\u4e00\u4e86\u7ecf\u5178\u65f6\u9891\u548c\u56fe\u4e0d\u786e\u5b9a\u6027\u539f\u7406\uff0c\u4e3a\u56fe\u4fe1\u53f7\u5904\u7406\u63d0\u4f9b\u4e86\u65b0\u6846\u67b6\uff0c\u5728\u4fe1\u53f7\u91cd\u6784\u548c\u56fe\u62d3\u6251\u63a8\u65ad\u65b9\u9762\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2602.04683", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2602.04683", "abs": "https://arxiv.org/abs/2602.04683", "authors": ["Dongchao Yang", "Yuanyuan Wang", "Dading Chong", "Songxiang Liu", "Xixin Wu", "Helen Meng"], "title": "UniAudio 2.0: A Unified Audio Language Model with Text-Aligned Factorized Audio Tokenization", "comment": null, "summary": "We study two foundational problems in audio language models: (1) how to design an audio tokenizer that can serve as an intermediate representation for both understanding and generation; and (2) how to build an audio foundation model that generalizes in few-shot and zero-shot settings, analogous to large language models. To this end, we make the following two contributions. First, we propose ReasoningCodec, a discrete audio codec that factorizes audio into (i) reasoning tokens, which encode text-aligned, high-level analysis and planning representations for audio understanding and hierarchical generation, and (ii) reconstruction tokens, which encode semantic-rich acoustic cues for high-fidelity waveform reconstruction. This design achieves understanding performance comparable to strong continuous representations while improving generation quality and reconstruction fidelity over prior discrete tokenizers. Second, we introduce a unified autoregressive architecture for text and audio, together with multi-stage training and multi-task data construction. Using this framework, we train UniAudio 2.0 on 100B text tokens and 60B audio tokens. Across a wide range of speech, sound, and music tasks, UniAudio 2.0 performs competitively on in-domain evaluations and demonstrates strong few-shot and zero-shot generalization to unseen tasks. Demo, code, and checkpoints will be available at \\href{https://dongchaoyang.top/UniAudio2Demo/}{https://dongchaoyang.top/UniAudio2Demo/}.", "AI": {"tldr": "\u63d0\u51faReasoningCodec\u97f3\u9891\u5206\u8bcd\u5668\u548cUniAudio 2.0\u7edf\u4e00\u97f3\u9891\u8bed\u8a00\u6a21\u578b\uff0c\u89e3\u51b3\u97f3\u9891\u7406\u89e3\u4e0e\u751f\u6210\u7684\u7edf\u4e00\u8868\u793a\u95ee\u9898\uff0c\u5728\u591a\u79cd\u97f3\u9891\u4efb\u52a1\u4e0a\u5b9e\u73b0\u5f3a\u5927\u7684\u5c11\u6837\u672c\u548c\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u97f3\u9891\u8bed\u8a00\u6a21\u578b\u7684\u4e24\u4e2a\u57fa\u7840\u95ee\u9898\uff1a1) \u8bbe\u8ba1\u65e2\u80fd\u7528\u4e8e\u7406\u89e3\u53c8\u80fd\u7528\u4e8e\u751f\u6210\u7684\u97f3\u9891\u5206\u8bcd\u5668\uff1b2) \u6784\u5efa\u7c7b\u4f3c\u5927\u8bed\u8a00\u6a21\u578b\u7684\u97f3\u9891\u57fa\u7840\u6a21\u578b\uff0c\u5b9e\u73b0\u5c11\u6837\u672c\u548c\u96f6\u6837\u672c\u6cdb\u5316\u3002", "method": "\u63d0\u51faReasoningCodec\u79bb\u6563\u97f3\u9891\u7f16\u89e3\u7801\u5668\uff0c\u5c06\u97f3\u9891\u5206\u89e3\u4e3a\u63a8\u7406token\uff08\u6587\u672c\u5bf9\u9f50\u7684\u9ad8\u5c42\u5206\u6790\u89c4\u5212\uff09\u548c\u91cd\u5efatoken\uff08\u8bed\u4e49\u4e30\u5bcc\u7684\u58f0\u5b66\u7ebf\u7d22\uff09\u3002\u91c7\u7528\u7edf\u4e00\u81ea\u56de\u5f52\u67b6\u6784\uff0c\u591a\u9636\u6bb5\u8bad\u7ec3\u548c\u591a\u4efb\u52a1\u6570\u636e\u6784\u5efa\uff0c\u5728100B\u6587\u672ctoken\u548c60B\u97f3\u9891token\u4e0a\u8bad\u7ec3UniAudio 2.0\u3002", "result": "ReasoningCodec\u5728\u7406\u89e3\u6027\u80fd\u4e0a\u4e0e\u5f3a\u8fde\u7eed\u8868\u793a\u76f8\u5f53\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86\u751f\u6210\u8d28\u91cf\u548c\u91cd\u5efa\u4fdd\u771f\u5ea6\u3002UniAudio 2.0\u5728\u8bed\u97f3\u3001\u58f0\u97f3\u548c\u97f3\u4e50\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5728\u9886\u57df\u5185\u8bc4\u4f30\u4e2d\u5177\u6709\u7ade\u4e89\u529b\uff0c\u5e76\u5728\u672a\u89c1\u4efb\u52a1\u4e0a\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u5c11\u6837\u672c\u548c\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u901a\u8fc7ReasoningCodec\u548cUniAudio 2.0\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u97f3\u9891\u7406\u89e3\u4e0e\u751f\u6210\u7684\u7edf\u4e00\u8868\u793a\u95ee\u9898\uff0c\u4e3a\u5b9e\u73b0\u7c7b\u4f3c\u5927\u8bed\u8a00\u6a21\u578b\u7684\u97f3\u9891\u57fa\u7840\u6a21\u578b\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84\uff0c\u5728\u591a\u79cd\u97f3\u9891\u4efb\u52a1\u4e0a\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2602.04126", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2602.04126", "abs": "https://arxiv.org/abs/2602.04126", "authors": ["Sojeong Park", "Hyun Jong Yang"], "title": "Semantic Pilot Design for Data-Aided Channel Estimation Using a Large Language Model", "comment": null, "summary": "This paper proposes a semantic pilot design for data-aided channel estimation in text-inclusive data transmission, using a large language model (LLM). In this scenario, channel impairments often appear as typographical errors in the decoded text, which can be corrected using an LLM. The proposed method compares the initially decoded text with the LLM-corrected version to identify reliable decoded symbols. A set of selected symbols, referred to as a semantic pilot, is used as an additional pilot for data-aided channel estimation. To the best of our knowledge, this work is the first to leverage semantic information for reliable symbol selection. Simulation results demonstrate that the proposed scheme outperforms conventional pilot-only estimation, achieving lower normalized mean squared error and phase error of the estimated channel, as well as reduced bit error rate.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u8bed\u4e49\u5bfc\u9891\u8bbe\u8ba1\uff0c\u7528\u4e8e\u6587\u672c\u6570\u636e\u4f20\u8f93\u4e2d\u7684\u4fe1\u9053\u4f30\u8ba1\uff0c\u901a\u8fc7LLM\u7ea0\u6b63\u89e3\u7801\u6587\u672c\u4e2d\u7684\u9519\u8bef\u6765\u8bc6\u522b\u53ef\u9760\u7b26\u53f7\u4f5c\u4e3a\u989d\u5916\u5bfc\u9891\u3002", "motivation": "\u5728\u5305\u542b\u6587\u672c\u7684\u6570\u636e\u4f20\u8f93\u4e2d\uff0c\u4fe1\u9053\u635f\u4f24\u901a\u5e38\u8868\u73b0\u4e3a\u89e3\u7801\u6587\u672c\u4e2d\u7684\u62fc\u5199\u9519\u8bef\u3002\u4f20\u7edf\u5bfc\u9891\u4f30\u8ba1\u65b9\u6cd5\u53ef\u80fd\u4e0d\u591f\u9ad8\u6548\uff0c\u9700\u8981\u5229\u7528\u8bed\u4e49\u4fe1\u606f\u6765\u63d0\u9ad8\u4fe1\u9053\u4f30\u8ba1\u7684\u53ef\u9760\u6027\u3002", "method": "\u4f7f\u7528LLM\u7ea0\u6b63\u521d\u59cb\u89e3\u7801\u6587\u672c\u4e2d\u7684\u9519\u8bef\uff0c\u5c06\u7ea0\u6b63\u524d\u540e\u7684\u6587\u672c\u8fdb\u884c\u6bd4\u8f83\u6765\u8bc6\u522b\u53ef\u9760\u89e3\u7801\u7b26\u53f7\uff0c\u9009\u62e9\u8fd9\u4e9b\u53ef\u9760\u7b26\u53f7\u4f5c\u4e3a\"\u8bed\u4e49\u5bfc\u9891\"\u7528\u4e8e\u6570\u636e\u8f85\u52a9\u7684\u4fe1\u9053\u4f30\u8ba1\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6848\u4f18\u4e8e\u4f20\u7edf\u7684\u4ec5\u4f7f\u7528\u5bfc\u9891\u7684\u4f30\u8ba1\u65b9\u6cd5\uff0c\u5728\u4f30\u8ba1\u4fe1\u9053\u7684\u5f52\u4e00\u5316\u5747\u65b9\u8bef\u5dee\u3001\u76f8\u4f4d\u8bef\u5dee\u4ee5\u53ca\u6bd4\u7279\u8bef\u7801\u7387\u65b9\u9762\u90fd\u6709\u663e\u8457\u964d\u4f4e\u3002", "conclusion": "\u8fd9\u662f\u9996\u6b21\u5229\u7528\u8bed\u4e49\u4fe1\u606f\u8fdb\u884c\u53ef\u9760\u7b26\u53f7\u9009\u62e9\u7684\u5de5\u4f5c\uff0c\u8bc1\u660e\u4e86\u8bed\u4e49\u5bfc\u9891\u8bbe\u8ba1\u80fd\u6709\u6548\u63d0\u9ad8\u6570\u636e\u8f85\u52a9\u4fe1\u9053\u4f30\u8ba1\u7684\u6027\u80fd\uff0c\u4e3a\u901a\u4fe1\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2602.04702", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2602.04702", "abs": "https://arxiv.org/abs/2602.04702", "authors": ["Tuan Dat Phuong", "Duc-Tuan Truong", "Long-Vu Hoang", "Trang Nguyen Thi Thu"], "title": "Fine-Grained Frame Modeling in Multi-head Self-Attention for Speech Deepfake Detection", "comment": "Accepted by ICASSP 2026", "summary": "Transformer-based models have shown strong performance in speech deepfake detection, largely due to the effectiveness of the multi-head self-attention (MHSA) mechanism. MHSA provides frame-level attention scores, which are particularly valuable because deepfake artifacts often occur in small, localized regions along the temporal dimension of speech. This makes fine-grained frame modeling essential for accurately detecting subtle spoofing cues. In this work, we propose fine-grained frame modeling (FGFM) for MHSA-based speech deepfake detection, where the most informative frames are first selected through a multi-head voting (MHV) module. These selected frames are then refined via a cross-layer refinement (CLR) module to enhance the model's ability to learn subtle spoofing cues. Experimental results demonstrate that our method outperforms the baseline model and achieves Equal Error Rate (EER) of 0.90%, 1.88%, and 6.64% on the LA21, DF21, and ITW datasets, respectively. These consistent improvements across multiple benchmarks highlight the effectiveness of our fine-grained modeling for robust speech deepfake detection.", "AI": {"tldr": "\u63d0\u51faFGFM\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u5934\u6295\u7968\u9009\u62e9\u4fe1\u606f\u91cf\u6700\u5927\u7684\u5e27\uff0c\u518d\u901a\u8fc7\u8de8\u5c42\u7cbe\u70bc\u589e\u5f3a\u6a21\u578b\u5b66\u4e60\u7ec6\u5fae\u4f2a\u9020\u7ebf\u7d22\u7684\u80fd\u529b\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "\u57fa\u4e8eTransformer\u7684\u6a21\u578b\u5728\u8bed\u97f3\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3b\u8981\u5f97\u76ca\u4e8e\u591a\u5934\u81ea\u6ce8\u610f\u529b\u673a\u5236\u3002MHSA\u63d0\u4f9b\u5e27\u7ea7\u6ce8\u610f\u529b\u5206\u6570\uff0c\u8fd9\u5bf9\u4e8e\u68c0\u6d4b\u8bed\u97f3\u65f6\u95f4\u7ef4\u5ea6\u4e0a\u5c0f\u8303\u56f4\u5c40\u90e8\u533a\u57df\u7684\u4f2a\u9020\u75d5\u8ff9\u7279\u522b\u6709\u4ef7\u503c\uff0c\u56e0\u6b64\u7ec6\u7c92\u5ea6\u5e27\u5efa\u6a21\u5bf9\u4e8e\u51c6\u786e\u68c0\u6d4b\u7ec6\u5fae\u4f2a\u9020\u7ebf\u7d22\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51fa\u7ec6\u7c92\u5ea6\u5e27\u5efa\u6a21\u65b9\u6cd5\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u6a21\u5757\uff1a1\uff09\u591a\u5934\u6295\u7968\u6a21\u5757\uff1a\u9009\u62e9\u4fe1\u606f\u91cf\u6700\u5927\u7684\u5e27\uff1b2\uff09\u8de8\u5c42\u7cbe\u70bc\u6a21\u5757\uff1a\u7cbe\u70bc\u6240\u9009\u5e27\u4ee5\u589e\u5f3a\u6a21\u578b\u5b66\u4e60\u7ec6\u5fae\u4f2a\u9020\u7ebf\u7d22\u7684\u80fd\u529b\u3002", "result": "\u5728LA21\u3001DF21\u548cITW\u6570\u636e\u96c6\u4e0a\u5206\u522b\u8fbe\u52300.90%\u30011.88%\u548c6.64%\u7684\u7b49\u9519\u8bef\u7387\uff0c\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4e00\u81f4\u6539\u8fdb\u3002", "conclusion": "\u63d0\u51fa\u7684\u7ec6\u7c92\u5ea6\u5efa\u6a21\u65b9\u6cd5\u5728\u8bed\u97f3\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u4e2d\u8868\u73b0\u51fa\u6709\u6548\u6027\uff0c\u901a\u8fc7\u9009\u62e9\u4fe1\u606f\u91cf\u6700\u5927\u7684\u5e27\u5e76\u8fdb\u884c\u7cbe\u70bc\uff0c\u80fd\u591f\u66f4\u597d\u5730\u5b66\u4e60\u7ec6\u5fae\u4f2a\u9020\u7ebf\u7d22\uff0c\u5b9e\u73b0\u9c81\u68d2\u7684\u68c0\u6d4b\u6027\u80fd\u3002"}}
{"id": "2602.04169", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2602.04169", "abs": "https://arxiv.org/abs/2602.04169", "authors": ["Longxin Bai", "Jingchao Zhang", "Liyan Qiao"], "title": "Spatial Angular Pseudo-Derivative Searching: A Single Snapshot Super-resolution Sparse DOA Scheme with Potential for Practical Application", "comment": null, "summary": "Accurate, high-resolution, and real-time DOA estimation is a cornerstone of environmental perception in automotive radar systems. While sparse signal recovery techniques offer super-resolution and high-precision estimation, their prohibitive computational complexity remains a primary bottleneck for practical deployment. This paper proposes a sparse DOA estimation scheme specifically tailored for the stringent requirements of automotive radar such as limited computational resources, restricted array apertures, and a single snapshot. By introducing the concept of the spatial angular pseudo-derivative and incorporating this property as a constraint into a standard L0-norm minimization problem, we formulate an objective function that more faithfully characterizes the physical properties of the DOA problem. The associated solver, designated as the SAPD search algorithm, naturally transforms the high-dimensional optimization task into an efficient grid-search scheme. The SAPD algorithm circumvents high-order matrix inversions and computationally intensive iterations. We provide an analysis of the computational complexity and convergence properties of the proposed algorithm. Extensive numerical simulations demonstrate that the SAPD method achieves a superior balance of real-time efficiency, high precision, and super-resolution, making it highly suitable for next-generation automotive radar applications.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u7a7a\u95f4\u89d2\u5ea6\u4f2a\u5bfc\u6570\u7ea6\u675f\u7684\u7a00\u758fDOA\u4f30\u8ba1\u7b97\u6cd5SAPD\uff0c\u9488\u5bf9\u6c7d\u8f66\u96f7\u8fbe\u7684\u8ba1\u7b97\u8d44\u6e90\u6709\u9650\u3001\u9635\u5217\u5b54\u5f84\u53d7\u9650\u3001\u5355\u5feb\u62cd\u7b49\u9650\u5236\uff0c\u5b9e\u73b0\u5b9e\u65f6\u9ad8\u6548\u7684\u9ad8\u7cbe\u5ea6\u8d85\u5206\u8fa8DOA\u4f30\u8ba1\u3002", "motivation": "\u6c7d\u8f66\u96f7\u8fbe\u7cfb\u7edf\u9700\u8981\u51c6\u786e\u3001\u9ad8\u5206\u8fa8\u7387\u3001\u5b9e\u65f6\u7684DOA\u4f30\u8ba1\uff0c\u4f46\u73b0\u6709\u7a00\u758f\u4fe1\u53f7\u6062\u590d\u65b9\u6cd5\u867d\u7136\u80fd\u63d0\u4f9b\u8d85\u5206\u8fa8\u548c\u9ad8\u7cbe\u5ea6\u4f30\u8ba1\uff0c\u5176\u8ba1\u7b97\u590d\u6742\u5ea6\u8fc7\u9ad8\uff0c\u96be\u4ee5\u5728\u5b9e\u9645\u6c7d\u8f66\u96f7\u8fbe\u7cfb\u7edf\u4e2d\u90e8\u7f72\u3002", "method": "\u63d0\u51fa\u7a7a\u95f4\u89d2\u5ea6\u4f2a\u5bfc\u6570\u6982\u5ff5\uff0c\u5c06\u5176\u4f5c\u4e3a\u7ea6\u675f\u6761\u4ef6\u878d\u5165\u6807\u51c6\u7684L0\u8303\u6570\u6700\u5c0f\u5316\u95ee\u9898\uff0c\u6784\u5efa\u66f4\u7b26\u5408DOA\u95ee\u9898\u7269\u7406\u7279\u6027\u7684\u76ee\u6807\u51fd\u6570\u3002\u76f8\u5e94\u7684\u6c42\u89e3\u5668SAPD\u641c\u7d22\u7b97\u6cd5\u5c06\u9ad8\u7ef4\u4f18\u5316\u4efb\u52a1\u8f6c\u5316\u4e3a\u9ad8\u6548\u7684\u7f51\u683c\u641c\u7d22\u65b9\u6848\uff0c\u907f\u514d\u9ad8\u9636\u77e9\u9635\u6c42\u9006\u548c\u8ba1\u7b97\u5bc6\u96c6\u7684\u8fed\u4ee3\u8fc7\u7a0b\u3002", "result": "SAPD\u7b97\u6cd5\u5728\u8ba1\u7b97\u590d\u6742\u5ea6\u548c\u6536\u655b\u6027\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u6570\u503c\u4eff\u771f\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u5b9e\u65f6\u6548\u7387\u3001\u9ad8\u7cbe\u5ea6\u548c\u8d85\u5206\u8fa8\u7387\u4e4b\u95f4\u5b9e\u73b0\u4e86\u4f18\u8d8a\u7684\u5e73\u8861\uff0c\u975e\u5e38\u9002\u5408\u4e0b\u4e00\u4ee3\u6c7d\u8f66\u96f7\u8fbe\u5e94\u7528\u3002", "conclusion": "SAPD\u65b9\u6cd5\u901a\u8fc7\u5f15\u5165\u7a7a\u95f4\u89d2\u5ea6\u4f2a\u5bfc\u6570\u7ea6\u675f\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u7a00\u758fDOA\u4f30\u8ba1\u5728\u6c7d\u8f66\u96f7\u8fbe\u7cfb\u7edf\u4e2d\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u95ee\u9898\uff0c\u4e3a\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.04187", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2602.04187", "abs": "https://arxiv.org/abs/2602.04187", "authors": ["Yuzhu Lei", "Guanding Yu"], "title": "GPINND: A deep-learning-based state of health estimation for Lithium-ion battery", "comment": null, "summary": "Electrochemical models offer superior interpretability and reliability for battery degradation diagnosis. However, the high computational cost of iterative parameter identification severely hinders the practical implementation of electrochemically informed state of health (SOH) estimation in real-time systems. To address this challenge, this paper proposes an SOH estimation method that integrates deep learning with electrochemical mechanisms and adopts a sequential training strategy. First, we construct a hybrid-driven surrogate model to learn internal electrochemical dynamics by fusing high-fidelity simulation data with physical constraints. This model subsequently serves as an accurate and differentiable physical kernel for voltage reconstruction. Then, we develop a self-supervised framework to train a parameter identification network by minimizing the voltage reconstruction error. The resulting model enables the non-iterative identification of aging parameters from external measurements. Finally, utilizing the identified parameters as physicochemical health indicators, we establish a high-precision SOH estimation network that leverages data-driven residual correction to compensate for identification deviations. Crucially, a sequential training strategy is applied across these modules to effectively mitigate convergence issues and improve the accuracy of each module. Experimental results demonstrate that the proposed method achieves an average voltage reconstruction root mean square error (RMSE) of 0.0198 V and an SOH estimation RMSE of 0.0014.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u4e0e\u7535\u5316\u5b66\u673a\u5236\u7684\u7535\u6c60\u5065\u5eb7\u72b6\u6001\u4f30\u8ba1\u65b9\u6cd5\uff0c\u901a\u8fc7\u6784\u5efa\u6df7\u5408\u9a71\u52a8\u4ee3\u7406\u6a21\u578b\u3001\u81ea\u76d1\u7763\u53c2\u6570\u8bc6\u522b\u7f51\u7edc\u548c\u6b8b\u5dee\u4fee\u6b63\u7f51\u7edc\uff0c\u5b9e\u73b0\u975e\u8fed\u4ee3\u5f0f\u8001\u5316\u53c2\u6570\u8bc6\u522b\u548c\u9ad8\u7cbe\u5ea6SOH\u4f30\u8ba1\u3002", "motivation": "\u7535\u5316\u5b66\u6a21\u578b\u5728\u7535\u6c60\u9000\u5316\u8bca\u65ad\u4e2d\u5177\u6709\u4f18\u8d8a\u7684\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u9760\u6027\uff0c\u4f46\u8fed\u4ee3\u53c2\u6570\u8bc6\u522b\u7684\u9ad8\u8ba1\u7b97\u6210\u672c\u4e25\u91cd\u963b\u788d\u4e86\u7535\u5316\u5b66\u4fe1\u606f\u5065\u5eb7\u72b6\u6001\u4f30\u8ba1\u5728\u5b9e\u9645\u5b9e\u65f6\u7cfb\u7edf\u4e2d\u7684\u5b9e\u65bd\u3002", "method": "1. \u6784\u5efa\u6df7\u5408\u9a71\u52a8\u4ee3\u7406\u6a21\u578b\uff0c\u878d\u5408\u9ad8\u4fdd\u771f\u4eff\u771f\u6570\u636e\u548c\u7269\u7406\u7ea6\u675f\u5b66\u4e60\u5185\u90e8\u7535\u5316\u5b66\u52a8\u529b\u5b66\uff1b2. \u5f00\u53d1\u81ea\u76d1\u7763\u6846\u67b6\u8bad\u7ec3\u53c2\u6570\u8bc6\u522b\u7f51\u7edc\uff0c\u6700\u5c0f\u5316\u7535\u538b\u91cd\u6784\u8bef\u5dee\uff1b3. \u5229\u7528\u8bc6\u522b\u53c2\u6570\u4f5c\u4e3a\u7269\u7406\u5316\u5b66\u5065\u5eb7\u6307\u6807\uff0c\u5efa\u7acb\u9ad8\u7cbe\u5ea6SOH\u4f30\u8ba1\u7f51\u7edc\uff0c\u91c7\u7528\u6570\u636e\u9a71\u52a8\u6b8b\u5dee\u4fee\u6b63\u8865\u507f\u8bc6\u522b\u504f\u5dee\uff1b4. \u91c7\u7528\u987a\u5e8f\u8bad\u7ec3\u7b56\u7565\u89e3\u51b3\u6536\u655b\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5e73\u5747\u7535\u538b\u91cd\u6784\u5747\u65b9\u6839\u8bef\u5dee\u4e3a0.0198V\uff0cSOH\u4f30\u8ba1\u5747\u65b9\u6839\u8bef\u5dee\u4e3a0.0014\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u4f30\u8ba1\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e86\u7535\u5316\u5b66\u6a21\u578b\u5b9e\u65f6\u5e94\u7528\u7684\u6311\u6218\uff0c\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u4e0e\u7269\u7406\u673a\u5236\u7684\u7ed3\u5408\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u51c6\u786e\u7684\u7535\u6c60\u5065\u5eb7\u72b6\u6001\u4f30\u8ba1\uff0c\u4e3a\u5b9e\u65f6\u7535\u6c60\u7ba1\u7406\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2602.04209", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2602.04209", "abs": "https://arxiv.org/abs/2602.04209", "authors": ["Libiao Lou", "Yuan Liu", "Fotis Foukalas", "Hongjiang Lei", "Gaofeng Pan", "Theodoros A. Tsiftsis", "Hongwu Liu"], "title": "Maneuverable-Jamming-Aided Secure Communication and Sensing in A2G-ISAC Systems", "comment": "14 pages, 13 figures. arXiv admin note: substantial text overlap with arXiv:2505.08523", "summary": "In this paper, we propose a maneuverablejamming-aided secure communication and sensing (SCS) scheme for an air-to-ground integrated sensing and communication (A2G-ISAC) system, where a dual-functional source UAV and a maneuverable jamming UAV operate collaboratively in a hybrid monostatic-bistatic radar configuration. The maneuverable jamming UAV emits artificial noise to assist the source UAV in detecting multiple ground targets while interfering with an eavesdropper. The effects of residual interference caused by imperfect successive interference cancellation on the received signal-to-interference-plus-noise ratio are considered, which degrades the system performance. To maximize the average secrecy rate (ASR) under transmit power budget, UAV maneuvering constraints, and sensing requirements, the dual-UAV trajectory and beamforming are jointly optimized. Given that secure communication and sensing fundamentally conflict in terms of resource allocation, making it difficult to achieve optimal performance for both simultaneously, we adopt a two-phase design to address this challenge. By dividing the mission into the secure communication (SC) phase and the SCS phase, the A2G-ISAC system can focus on optimizing distinct objectives separately. In the SC phase, a block coordinate descent algorithm employing the trust-region successive convex approximation and semidefinite relaxation iteratively optimizes dual-UAV trajectory and beamforming. For the SCS phase, a weighted distance minimization problem determines the suitable dual-UAV sensing positions by a greedy algorithm, followed by the joint optimization of source beamforming and jamming beamforming. Simulation results demonstrate that the proposed scheme achieves the highest ASR among benchmarks while maintaining robust sensing performance, and confirm the impact of the SIC residual interference on both secure communication and sensing.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7528\u4e8e\u7a7a\u5bf9\u5730\u96c6\u6210\u611f\u77e5\u4e0e\u901a\u4fe1\u7cfb\u7edf\u7684\u53ef\u673a\u52a8\u5e72\u6270\u8f85\u52a9\u5b89\u5168\u901a\u4fe1\u4e0e\u611f\u77e5\u65b9\u6848\uff0c\u901a\u8fc7\u53cc\u65e0\u4eba\u673a\u534f\u540c\u5de5\u4f5c\u5728\u6df7\u5408\u5355\u57fa\u5730-\u53cc\u57fa\u5730\u96f7\u8fbe\u914d\u7f6e\u4e2d\uff0c\u4f18\u5316\u8f68\u8ff9\u548c\u6ce2\u675f\u6210\u5f62\u4ee5\u6700\u5927\u5316\u5e73\u5747\u4fdd\u5bc6\u7387\u3002", "motivation": "\u5728\u7a7a\u5bf9\u5730\u96c6\u6210\u611f\u77e5\u4e0e\u901a\u4fe1\u7cfb\u7edf\u4e2d\uff0c\u5b89\u5168\u901a\u4fe1\u548c\u611f\u77e5\u5728\u8d44\u6e90\u5206\u914d\u4e0a\u5b58\u5728\u6839\u672c\u51b2\u7a81\uff0c\u96be\u4ee5\u540c\u65f6\u5b9e\u73b0\u6700\u4f18\u6027\u80fd\u3002\u540c\u65f6\uff0c\u4e0d\u5b8c\u7f8e\u7684\u8fde\u7eed\u5e72\u6270\u6d88\u9664\u4ea7\u751f\u7684\u6b8b\u4f59\u5e72\u6270\u4f1a\u964d\u4f4e\u7cfb\u7edf\u6027\u80fd\uff0c\u9700\u8981\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u8bbe\u8ba1\uff1a\u5b89\u5168\u901a\u4fe1\u9636\u6bb5\u4f7f\u7528\u5757\u5750\u6807\u4e0b\u964d\u7b97\u6cd5\u7ed3\u5408\u4fe1\u4efb\u57df\u9010\u6b21\u51f8\u903c\u8fd1\u548c\u534a\u5b9a\u677e\u5f1b\u4f18\u5316\u53cc\u65e0\u4eba\u673a\u8f68\u8ff9\u548c\u6ce2\u675f\u6210\u5f62\uff1b\u5b89\u5168\u901a\u4fe1\u4e0e\u611f\u77e5\u9636\u6bb5\u901a\u8fc7\u52a0\u6743\u8ddd\u79bb\u6700\u5c0f\u5316\u95ee\u9898\u786e\u5b9a\u5408\u9002\u7684\u53cc\u65e0\u4eba\u673a\u611f\u77e5\u4f4d\u7f6e\uff0c\u7136\u540e\u8054\u5408\u4f18\u5316\u6e90\u6ce2\u675f\u6210\u5f62\u548c\u5e72\u6270\u6ce2\u675f\u6210\u5f62\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6848\u5728\u57fa\u51c6\u65b9\u6cd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u9ad8\u7684\u5e73\u5747\u4fdd\u5bc6\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9c81\u68d2\u7684\u611f\u77e5\u6027\u80fd\uff0c\u5e76\u8bc1\u5b9e\u4e86\u8fde\u7eed\u5e72\u6270\u6d88\u9664\u6b8b\u4f59\u5e72\u6270\u5bf9\u5b89\u5168\u901a\u4fe1\u548c\u611f\u77e5\u7684\u5f71\u54cd\u3002", "conclusion": "\u63d0\u51fa\u7684\u53ef\u673a\u52a8\u5e72\u6270\u8f85\u52a9\u5b89\u5168\u901a\u4fe1\u4e0e\u611f\u77e5\u65b9\u6848\u901a\u8fc7\u4e24\u9636\u6bb5\u8bbe\u8ba1\u548c\u8054\u5408\u4f18\u5316\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u7a7a\u5bf9\u5730\u96c6\u6210\u611f\u77e5\u4e0e\u901a\u4fe1\u7cfb\u7edf\u4e2d\u5b89\u5168\u901a\u4fe1\u4e0e\u611f\u77e5\u7684\u8d44\u6e90\u51b2\u7a81\u95ee\u9898\uff0c\u5728\u4fdd\u8bc1\u611f\u77e5\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u901a\u4fe1\u5b89\u5168\u6027\u3002"}}
{"id": "2602.04266", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.04266", "abs": "https://arxiv.org/abs/2602.04266", "authors": ["Jiaze Wang", "Qinghao Zhao", "Zizheng Chen", "Zhejun Sun", "Deyun Zhang", "Yuxi Zhou", "Shenda Hong"], "title": "Aortic Valve Disease Detection from PPG via Physiology-Informed Self-Supervised Learning", "comment": "28 pages, 7 figures. Under review", "summary": "Traditional diagnosis of aortic valve disease relies on echocardiography, but its cost and required expertise limit its use in large-scale early screening. Photoplethysmography (PPG) has emerged as a promising screening modality due to its widespread availability in wearable devices and its ability to reflect underlying hemodynamic dynamics. However, the extreme scarcity of gold-standard labeled PPG data severely constrains the effectiveness of data-driven approaches. To address this challenge, we propose and validate a new paradigm, Physiology-Guided Self-Supervised Learning (PG-SSL), aimed at unlocking the value of large-scale unlabeled PPG data for efficient screening of Aortic Stenosis (AS) and Aortic Regurgitation (AR). Using over 170,000 unlabeled PPG samples from the UK Biobank, we formalize clinical knowledge into a set of PPG morphological phenotypes and construct a pulse pattern recognition proxy task for self-supervised pre-training. A dual-branch, gated-fusion architecture is then employed for efficient fine-tuning on a small labeled subset. The proposed PG-SSL framework achieves AUCs of 0.765 and 0.776 for AS and AR screening, respectively, significantly outperforming supervised baselines trained on limited labeled data. Multivariable analysis further validates the model output as an independent digital biomarker with sustained prognostic value after adjustment for standard clinical risk factors. This study demonstrates that PG-SSL provides an effective, domain knowledge-driven solution to label scarcity in medical artificial intelligence and shows strong potential for enabling low-cost, large-scale early screening of aortic valve disease.", "AI": {"tldr": "\u63d0\u51fa\u751f\u7406\u5b66\u5f15\u5bfc\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u6846\u67b6\uff0c\u5229\u7528\u5927\u89c4\u6a21\u65e0\u6807\u7b7ePPG\u6570\u636e\u89e3\u51b3\u4e3b\u52a8\u8109\u74e3\u819c\u75be\u75c5\u7b5b\u67e5\u4e2d\u7684\u6807\u7b7e\u7a00\u7f3a\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u7b5b\u67e5\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u8d85\u58f0\u5fc3\u52a8\u56fe\u8bca\u65ad\u4e3b\u52a8\u8109\u74e3\u819c\u75be\u75c5\u6210\u672c\u9ad8\u4e14\u9700\u8981\u4e13\u4e1a\u77e5\u8bc6\uff0c\u96be\u4ee5\u7528\u4e8e\u5927\u89c4\u6a21\u65e9\u671f\u7b5b\u67e5\u3002PPG\u4f5c\u4e3a\u53ef\u7a7f\u6234\u8bbe\u5907\u5e7f\u6cdb\u4f7f\u7528\u7684\u6280\u672f\u5177\u6709\u7b5b\u67e5\u6f5c\u529b\uff0c\u4f46\u91d1\u6807\u51c6\u6807\u7b7e\u6570\u636e\u6781\u5ea6\u7a00\u7f3a\u9650\u5236\u4e86\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "method": "\u63d0\u51fa\u751f\u7406\u5b66\u5f15\u5bfc\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u6846\u67b6\uff1a1) \u5c06\u4e34\u5e8a\u77e5\u8bc6\u5f62\u5f0f\u5316\u4e3aPPG\u5f62\u6001\u8868\u578b\uff1b2) \u6784\u5efa\u8109\u640f\u6a21\u5f0f\u8bc6\u522b\u4ee3\u7406\u4efb\u52a1\u8fdb\u884c\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\uff1b3) \u4f7f\u7528\u53cc\u5206\u652f\u95e8\u63a7\u878d\u5408\u67b6\u6784\u5728\u5c0f\u89c4\u6a21\u6807\u7b7e\u6570\u636e\u4e0a\u8fdb\u884c\u9ad8\u6548\u5fae\u8c03\u3002", "result": "\u5728\u8d85\u8fc717\u4e07\u4e2a\u65e0\u6807\u7b7ePPG\u6837\u672c\u4e0a\u8bad\u7ec3\uff0c\u4e3b\u52a8\u8109\u72ed\u7a84\u548c\u4e3b\u52a8\u8109\u53cd\u6d41\u7b5b\u67e5\u7684AUC\u5206\u522b\u8fbe\u52300.765\u548c0.776\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u4e8e\u6709\u9650\u6807\u7b7e\u6570\u636e\u7684\u76d1\u7763\u57fa\u7ebf\u3002\u591a\u53d8\u91cf\u5206\u6790\u9a8c\u8bc1\u6a21\u578b\u8f93\u51fa\u4f5c\u4e3a\u72ec\u7acb\u6570\u5b57\u751f\u7269\u6807\u5fd7\u7269\u5177\u6709\u6301\u7eed\u9884\u540e\u4ef7\u503c\u3002", "conclusion": "PG-SSL\u4e3a\u533b\u5b66\u4eba\u5de5\u667a\u80fd\u4e2d\u7684\u6807\u7b7e\u7a00\u7f3a\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u9886\u57df\u77e5\u8bc6\u9a71\u52a8\u89e3\u51b3\u65b9\u6848\uff0c\u5c55\u793a\u4e86\u5b9e\u73b0\u4f4e\u6210\u672c\u3001\u5927\u89c4\u6a21\u4e3b\u52a8\u8109\u74e3\u819c\u75be\u75c5\u65e9\u671f\u7b5b\u67e5\u7684\u5f3a\u5927\u6f5c\u529b\u3002"}}
{"id": "2602.04316", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2602.04316", "abs": "https://arxiv.org/abs/2602.04316", "authors": ["Zhenyu Chen", "Ke Xiao", "Xiaomei Tang", "Jing Lei", "Muzi Yuan", "Guangfu Sun"], "title": "Joint Fractional Delay and Doppler Frequency Estimator Under Spectrum Wrapping Phenomenon for LEO-ICAN AFDM Signals", "comment": "5 pages,7 figures. This paper is currently under review at an IEEE journal", "summary": "With the rapid development of low earth orbit (LEO) satellites, the design of integrated communication and navigation (ICAN) signals has attracted increasing attention, especially in the field of vehicle-to-everything (V2X). As a new-generation waveform, Affine Frequency Division Multiplexing (AFDM) features high robustness against Doppler effects, a simple modulation structure, and low pilot overhead, making it a promising candidate for high-dynamic LEO satellite scenarios. However, LEO-ICAN AFDM signals face challenges in fractional delay and Doppler frequency estimation. Existing studies that ignore its inherent spectrum wrapping phenomenon may lead to deviations of varying degrees in model construction. This paper conducts an in-depth derivation of AFDM's input-output relationship under fractional cases, reveals the envelope characteristics of its equivalent channel, and proposes a joint estimation algorithm based on peak-to-sidelobe power ratio (PSPR) detection and early-late gate (ELG) to estimate fractional Doppler frequency and delay. Simulations show that the algorithm has low complexity, low guard interval overhead, and high precision compared with traditional methods.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9LEO\u536b\u661fICAN\u4fe1\u53f7\u4e2d\u7684AFDM\u6ce2\u5f62\uff0c\u89e3\u51b3\u4e86\u5206\u6570\u5ef6\u8fdf\u548c\u591a\u666e\u52d2\u9891\u7387\u4f30\u8ba1\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8ePSPR\u68c0\u6d4b\u548cELG\u7684\u8054\u5408\u4f30\u8ba1\u7b97\u6cd5\uff0c\u5728\u590d\u6742\u5ea6\u548c\u7cbe\u5ea6\u4e0a\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u968f\u7740\u4f4e\u5730\u7403\u8f68\u9053\u536b\u661f\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u96c6\u6210\u901a\u4fe1\u5bfc\u822a\u4fe1\u53f7\u8bbe\u8ba1\u5728\u8f66\u8054\u7f51\u9886\u57df\u53d7\u5230\u5173\u6ce8\u3002AFDM\u4f5c\u4e3a\u65b0\u4e00\u4ee3\u6ce2\u5f62\u5177\u6709\u6297\u591a\u666e\u52d2\u6548\u5e94\u5f3a\u3001\u8c03\u5236\u7ed3\u6784\u7b80\u5355\u3001\u5bfc\u9891\u5f00\u9500\u4f4e\u7b49\u4f18\u70b9\uff0c\u9002\u5408\u9ad8\u52a8\u6001LEO\u536b\u661f\u573a\u666f\u3002\u7136\u800c\uff0cLEO-ICAN AFDM\u4fe1\u53f7\u9762\u4e34\u5206\u6570\u5ef6\u8fdf\u548c\u591a\u666e\u52d2\u9891\u7387\u4f30\u8ba1\u7684\u6311\u6218\uff0c\u73b0\u6709\u7814\u7a76\u5ffd\u7565\u5176\u56fa\u6709\u7684\u9891\u8c31\u6298\u53e0\u73b0\u8c61\u53ef\u80fd\u5bfc\u81f4\u6a21\u578b\u6784\u5efa\u504f\u5dee\u3002", "method": "\u672c\u6587\u6df1\u5165\u63a8\u5bfc\u4e86AFDM\u5728\u5206\u6570\u60c5\u51b5\u4e0b\u7684\u8f93\u5165\u8f93\u51fa\u5173\u7cfb\uff0c\u63ed\u793a\u4e86\u5176\u7b49\u6548\u4fe1\u9053\u7684\u5305\u7edc\u7279\u6027\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\u5cf0\u503c\u65c1\u74e3\u529f\u7387\u6bd4\u68c0\u6d4b\u548c\u65e9\u665a\u95e8\u9650\u7684\u8054\u5408\u4f30\u8ba1\u7b97\u6cd5\u6765\u4f30\u8ba1\u5206\u6570\u591a\u666e\u52d2\u9891\u7387\u548c\u5ef6\u8fdf\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u4f20\u7edf\u65b9\u6cd5\u76f8\u6bd4\uff0c\u8be5\u7b97\u6cd5\u5177\u6709\u4f4e\u590d\u6742\u5ea6\u3001\u4f4e\u4fdd\u62a4\u95f4\u9694\u5f00\u9500\u548c\u9ad8\u7cbe\u5ea6\u7684\u4f18\u52bf\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u8054\u5408\u4f30\u8ba1\u7b97\u6cd5\u6709\u6548\u89e3\u51b3\u4e86AFDM\u5728LEO-ICAN\u573a\u666f\u4e2d\u7684\u5206\u6570\u5ef6\u8fdf\u548c\u591a\u666e\u52d2\u9891\u7387\u4f30\u8ba1\u95ee\u9898\uff0c\u4e3a\u9ad8\u52a8\u6001\u536b\u661f\u901a\u4fe1\u5bfc\u822a\u4e00\u4f53\u5316\u4fe1\u53f7\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.04331", "categories": ["eess.SP", "cs.IT"], "pdf": "https://arxiv.org/pdf/2602.04331", "abs": "https://arxiv.org/abs/2602.04331", "authors": ["Luca Antonelli", "Antonio Alberto D'Amico", "Luca Sanguinetti"], "title": "An Enhanced Polar-Domain Dictionary Design for Elevated BSs in Near-Field U-MIMO", "comment": "6 pages, 8 figures, presented at Asilomar Conference on Signals, Systems, and Computers 2025", "summary": "Near-field U-MIMO communications require carefully optimized sampling grids in both angular and distance domains. However, most existing grid design methods neglect the influence of base station height, assuming instead that the base station is positioned at ground level - a simplification that rarely reflects real-world deployments. To overcome this limitation, we propose a generalized grid design framework that accommodates arbitrary base station locations. Unlike conventional correlation-based approaches, our method optimizes the grid based on the minimization of the optimal normalized mean squared error, leading to more accurate channel representation. We evaluate the performance of a hybrid U-MIMO system operating at sub-THz frequencies, considering the P-SOMP algorithm for channel estimation. Analytical and numerical results show that the proposed design enhances both channel estimation accuracy and spectral efficiency compared to existing alternatives.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u9002\u7528\u4e8e\u4efb\u610f\u57fa\u7ad9\u4f4d\u7f6e\u7684\u4e09\u7ef4\u8fd1\u573aU-MIMO\u91c7\u6837\u7f51\u683c\u8bbe\u8ba1\u6846\u67b6\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4fe1\u9053\u4f30\u8ba1\u7cbe\u5ea6\u548c\u9891\u8c31\u6548\u7387", "motivation": "\u73b0\u6709\u8fd1\u573aU-MIMO\u7f51\u683c\u8bbe\u8ba1\u65b9\u6cd5\u5927\u591a\u5047\u8bbe\u57fa\u7ad9\u4f4d\u4e8e\u5730\u9762\uff0c\u5ffd\u7565\u4e86\u57fa\u7ad9\u9ad8\u5ea6\u7684\u5f71\u54cd\uff0c\u8fd9\u4e0e\u5b9e\u9645\u90e8\u7f72\u60c5\u51b5\u4e0d\u7b26\uff0c\u9650\u5236\u4e86\u7cfb\u7edf\u6027\u80fd", "method": "\u63d0\u51fa\u5e7f\u4e49\u7f51\u683c\u8bbe\u8ba1\u6846\u67b6\uff0c\u57fa\u4e8e\u6700\u4f18\u5f52\u4e00\u5316\u5747\u65b9\u8bef\u5dee\u6700\u5c0f\u5316\u539f\u5219\u4f18\u5316\u7f51\u683c\uff0c\u800c\u975e\u4f20\u7edf\u76f8\u5173\u6027\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u4efb\u610f\u57fa\u7ad9\u4f4d\u7f6e", "result": "\u5728sub-THz\u9891\u6bb5\u7684\u6df7\u5408U-MIMO\u7cfb\u7edf\u4e2d\uff0c\u4f7f\u7528P-SOMP\u7b97\u6cd5\u8fdb\u884c\u4fe1\u9053\u4f30\u8ba1\uff0c\u5206\u6790\u8868\u660e\u6240\u63d0\u8bbe\u8ba1\u76f8\u6bd4\u73b0\u6709\u65b9\u6848\u663e\u8457\u63d0\u5347\u4fe1\u9053\u4f30\u8ba1\u7cbe\u5ea6\u548c\u9891\u8c31\u6548\u7387", "conclusion": "\u8003\u8651\u57fa\u7ad9\u9ad8\u5ea6\u7684\u4e09\u7ef4\u7f51\u683c\u8bbe\u8ba1\u5bf9\u8fd1\u573aU-MIMO\u901a\u4fe1\u81f3\u5173\u91cd\u8981\uff0c\u6240\u63d0\u6846\u67b6\u80fd\u66f4\u51c6\u786e\u5730\u8868\u793a\u4fe1\u9053\uff0c\u4e3a\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u66f4\u4f18\u6027\u80fd"}}
{"id": "2602.04410", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2602.04410", "abs": "https://arxiv.org/abs/2602.04410", "authors": ["Niclas F\u00fchrling", "Hyeon Seok Rou", "Giuseppe Abreu", "David Gonz\u00e1lez G.", "Osvaldo Gonsa"], "title": "Rigid Body Localization via Gaussian Belief Propagation with Quadratic Angle Approximation", "comment": null, "summary": "Gaussian belief propagation (GaBP) is a technique that relies on linearized error and input-output models to yield low-complexity solutions to complex estimation problems, which has been recently shown to be effective in the design of range-based GaBP schemes for stationary and moving rigid body localization (RBL) in three-dimensional (3D) space, as long as an accurate prior on the orientation of the target rigid body is available. In this article we present a novel range-based RBL scheme via GaBP that removes the latter limitation. To this end, the proposed method incorporates a quadratic angle approximation to linearize the relative orientation between the prior and the target rigid body, enabling high precision estimates of corresponding rotation angles even for large deviations. Leveraging the resulting linearized model, we derive the corresponding message-passing (MP) rules to obtain estimates of the translation vector and rotation matrix of the target rigid body, relative to a prior reference frame. Numerical results corroborate the good performance of the proposed angle approximation itself, as well as the consequent RBL performance in terms of root mean square errors (RMSEs) in comparison to the state-of-the-art (SotA), while maintaining a low computational complexity", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u9ad8\u65af\u7f6e\u4fe1\u4f20\u64ad\u7684\u521a\u6027\u4f53\u5b9a\u4f4d\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e8c\u6b21\u89d2\u5ea6\u8fd1\u4f3c\u6d88\u9664\u5bf9\u76ee\u6807\u65b9\u5411\u5148\u9a8c\u7684\u4f9d\u8d56\uff0c\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u65cb\u8f6c\u89d2\u5ea6\u4f30\u8ba1", "motivation": "\u73b0\u6709\u57fa\u4e8e\u9ad8\u65af\u7f6e\u4fe1\u4f20\u64ad\u7684\u521a\u6027\u4f53\u5b9a\u4f4d\u65b9\u6cd5\u9700\u8981\u51c6\u786e\u7684\u76ee\u6807\u65b9\u5411\u5148\u9a8c\u4fe1\u606f\uff0c\u8fd9\u9650\u5236\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u3002\u672c\u6587\u65e8\u5728\u6d88\u9664\u8fd9\u4e00\u9650\u5236\uff0c\u5f00\u53d1\u4e00\u79cd\u65e0\u9700\u7cbe\u786e\u65b9\u5411\u5148\u9a8c\u7684\u5b9a\u4f4d\u65b9\u6848\u3002", "method": "\u91c7\u7528\u4e8c\u6b21\u89d2\u5ea6\u8fd1\u4f3c\u6765\u7ebf\u6027\u5316\u5148\u9a8c\u4e0e\u76ee\u6807\u521a\u6027\u4f53\u4e4b\u95f4\u7684\u76f8\u5bf9\u65b9\u5411\uff0c\u5373\u4f7f\u5728\u5927\u504f\u5dee\u60c5\u51b5\u4e0b\u4e5f\u80fd\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u65cb\u8f6c\u89d2\u5ea6\u4f30\u8ba1\u3002\u57fa\u4e8e\u7ebf\u6027\u5316\u6a21\u578b\u63a8\u5bfc\u51fa\u76f8\u5e94\u7684\u6d88\u606f\u4f20\u9012\u89c4\u5219\uff0c\u7528\u4e8e\u4f30\u8ba1\u76ee\u6807\u521a\u6027\u4f53\u76f8\u5bf9\u4e8e\u5148\u9a8c\u53c2\u8003\u7cfb\u7684\u5e73\u79fb\u5411\u91cf\u548c\u65cb\u8f6c\u77e9\u9635\u3002", "result": "\u6570\u503c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u89d2\u5ea6\u8fd1\u4f3c\u65b9\u6cd5\u672c\u8eab\u6027\u80fd\u826f\u597d\uff0c\u521a\u6027\u4f53\u5b9a\u4f4d\u5728\u5747\u65b9\u6839\u8bef\u5dee\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8f83\u4f4e\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "conclusion": "\u6210\u529f\u5f00\u53d1\u4e86\u4e00\u79cd\u65e0\u9700\u7cbe\u786e\u65b9\u5411\u5148\u9a8c\u7684\u57fa\u4e8e\u9ad8\u65af\u7f6e\u4fe1\u4f20\u64ad\u7684\u521a\u6027\u4f53\u5b9a\u4f4d\u65b9\u6848\uff0c\u901a\u8fc7\u4e8c\u6b21\u89d2\u5ea6\u8fd1\u4f3c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u4f30\u8ba1\uff0c\u5728\u6027\u80fd\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2602.04465", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2602.04465", "abs": "https://arxiv.org/abs/2602.04465", "authors": ["Pia Addabbo", "Diego Reale", "Antonio Pauciullo", "Gianfranco Fornaro", "Danilo Orlando"], "title": "An Information-Theoretic Detector for Multiple Scatterers in SAR Tomography", "comment": null, "summary": "Persistent scatterer interferometry and Synthetic Aperture Radar (SAR) Tomography are powerful tools for the detection and time monitoring of persistent scatterers. They have been proven to be effective in urban scenarios, especially for buildings and infrastructures 3-D reconstruction and monitoring of deformation. In urban areas, occurrence of layover leads to the presence of multiple contributions within the same image pixel from scatterers located at different heights. In the context of SAR Tomography, this problem can be addressed by considering a multiple hypothesis test to detect the presence of feasible multiple scatterers [1][2]. In the present paper, we consider this problem in the framework of the information theory and exploit the theoretical tool, developed in [3], to design a one-stage adaptive architecture for multiple hypothesis testing problems in the context of SAR Tomography. Moreover, we resort to the compressive sensing approach for the estimation of the unknown parameters under each hypothesis. This architecture has been verified on both simulated as well as real data also in comparison with suitable counterparts.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u4fe1\u606f\u7406\u8bba\u548c\u538b\u7f29\u611f\u77e5\u7684SAR\u5c42\u6790\u6210\u50cf\u591a\u5047\u8bbe\u68c0\u9a8c\u81ea\u9002\u5e94\u67b6\u6784\uff0c\u7528\u4e8e\u89e3\u51b3\u57ce\u5e02\u533a\u57df\u53e0\u63a9\u95ee\u9898\u4e2d\u7684\u591a\u6563\u5c04\u4f53\u68c0\u6d4b\u4e0e\u53c2\u6570\u4f30\u8ba1\u3002", "motivation": "\u5728\u57ce\u5e02SAR\u5c42\u6790\u6210\u50cf\u4e2d\uff0c\u53e0\u63a9\u73b0\u8c61\u5bfc\u81f4\u540c\u4e00\u50cf\u7d20\u5305\u542b\u6765\u81ea\u4e0d\u540c\u9ad8\u5ea6\u7684\u591a\u4e2a\u6563\u5c04\u4f53\u8d21\u732e\uff0c\u9700\u8981\u6709\u6548\u65b9\u6cd5\u68c0\u6d4b\u548c\u5206\u79bb\u8fd9\u4e9b\u591a\u6563\u5c04\u4f53\u3002", "method": "\u91c7\u7528\u4fe1\u606f\u7406\u8bba\u6846\u67b6\uff0c\u8bbe\u8ba1\u5355\u9636\u6bb5\u81ea\u9002\u5e94\u591a\u5047\u8bbe\u68c0\u9a8c\u67b6\u6784\uff0c\u7ed3\u5408\u538b\u7f29\u611f\u77e5\u65b9\u6cd5\u4f30\u8ba1\u5404\u5047\u8bbe\u4e0b\u7684\u672a\u77e5\u53c2\u6570\u3002", "result": "\u8be5\u67b6\u6784\u5728\u6a21\u62df\u548c\u771f\u5b9e\u6570\u636e\u4e0a\u8fdb\u884c\u4e86\u9a8c\u8bc1\uff0c\u5e76\u4e0e\u5408\u9002\u7684\u5bf9\u6bd4\u65b9\u6cd5\u8fdb\u884c\u4e86\u6bd4\u8f83\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u57fa\u4e8e\u4fe1\u606f\u7406\u8bba\u548c\u538b\u7f29\u611f\u77e5\u7684\u81ea\u9002\u5e94\u591a\u5047\u8bbe\u68c0\u9a8c\u67b6\u6784\u80fd\u591f\u6709\u6548\u89e3\u51b3SAR\u5c42\u6790\u6210\u50cf\u4e2d\u7684\u591a\u6563\u5c04\u4f53\u68c0\u6d4b\u95ee\u9898\uff0c\u9002\u7528\u4e8e\u57ce\u5e02\u533a\u57df\u53e0\u63a9\u573a\u666f\u3002"}}
{"id": "2602.04623", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2602.04623", "abs": "https://arxiv.org/abs/2602.04623", "authors": ["Yanbin He", "Geethu Joseph"], "title": "Total Variation Sparse Bayesian Learning for Block Sparsity via Majorization-Minimization", "comment": "Submitted to EUSIPCO", "summary": "Block sparsity is a widely exploited structure in sparse recovery, offering significant gains when signal blocks are known. Yet, practical signals often exhibit unknown block boundaries and isolated non-zero entries, which challenge traditional approaches. A promising method to handle such complex sparsity patterns is the difference-of-logs total variation (DoL-TV) regularized sparse Bayesian learning (SBL). However, due to the complex form of DoL-TV term, the resulting optimization problem is hard to solve. This paper develops a new optimization framework for the DoL-TV SBL cost function. By introducing an exponential reparameterization of the SBL hyperparameters, we reveal a novel structure that admits a majorization-minimization formulation and naturally extends to unknown noise variance estimation. Sparse recovery results on both synthetic data and extended source direction-of-arrival estimation demonstrate improved accuracy and runtime performance compared to benchmark methods.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u4f18\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u5177\u6709\u5dee\u5f02\u5bf9\u6570\u603b\u53d8\u5206\u6b63\u5219\u5316\u7684\u7a00\u758f\u8d1d\u53f6\u65af\u5b66\u4e60\u95ee\u9898\uff0c\u901a\u8fc7\u6307\u6570\u91cd\u53c2\u6570\u5316\u5b9e\u73b0\u9ad8\u6548\u6c42\u89e3\uff0c\u5728\u5408\u6210\u6570\u636e\u548cDOA\u4f30\u8ba1\u4e2d\u8868\u73b0\u4f18\u4e8e\u57fa\u51c6\u65b9\u6cd5\u3002", "motivation": "\u5757\u7a00\u758f\u6027\u5728\u7a00\u758f\u6062\u590d\u4e2d\u88ab\u5e7f\u6cdb\u5229\u7528\uff0c\u4f46\u5f53\u4fe1\u53f7\u5757\u8fb9\u754c\u672a\u77e5\u4e14\u5b58\u5728\u5b64\u7acb\u975e\u96f6\u503c\u65f6\uff0c\u4f20\u7edf\u65b9\u6cd5\u9762\u4e34\u6311\u6218\u3002\u5dee\u5f02\u5bf9\u6570\u603b\u53d8\u5206\u6b63\u5219\u5316\u7684\u7a00\u758f\u8d1d\u53f6\u65af\u5b66\u4e60\u80fd\u5904\u7406\u590d\u6742\u7a00\u758f\u6a21\u5f0f\uff0c\u4f46\u5176\u4f18\u5316\u95ee\u9898\u56e0\u590d\u6742\u5f62\u5f0f\u800c\u96be\u4ee5\u6c42\u89e3\u3002", "method": "\u901a\u8fc7\u5f15\u5165SBL\u8d85\u53c2\u6570\u7684\u6307\u6570\u91cd\u53c2\u6570\u5316\uff0c\u63ed\u793a\u65b0\u7684\u7ed3\u6784\uff0c\u91c7\u7528\u4e3b\u4f18\u5316-\u6700\u5c0f\u5316\u6846\u67b6\uff0c\u5e76\u81ea\u7136\u6269\u5c55\u5230\u672a\u77e5\u566a\u58f0\u65b9\u5dee\u4f30\u8ba1\u3002", "result": "\u5728\u5408\u6210\u6570\u636e\u548c\u6269\u5c55\u6e90\u65b9\u5411\u5230\u8fbe\u4f30\u8ba1\u4e2d\u7684\u7a00\u758f\u6062\u590d\u7ed3\u679c\u8868\u660e\uff0c\u76f8\u6bd4\u57fa\u51c6\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u548c\u8fd0\u884c\u65f6\u95f4\u6027\u80fd\u4e0a\u90fd\u6709\u6539\u8fdb\u3002", "conclusion": "\u63d0\u51fa\u7684\u4f18\u5316\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86DoL-TV SBL\u6210\u672c\u51fd\u6570\u7684\u6c42\u89e3\u96be\u9898\uff0c\u901a\u8fc7\u6307\u6570\u91cd\u53c2\u6570\u5316\u548c\u4e3b\u4f18\u5316-\u6700\u5c0f\u5316\u65b9\u6cd5\u5b9e\u73b0\u4e86\u9ad8\u6548\u7a00\u758f\u6062\u590d\uff0c\u9002\u7528\u4e8e\u5177\u6709\u672a\u77e5\u5757\u8fb9\u754c\u548c\u5b64\u7acb\u975e\u96f6\u503c\u7684\u590d\u6742\u7a00\u758f\u6a21\u5f0f\u3002"}}
{"id": "2602.04650", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.04650", "abs": "https://arxiv.org/abs/2602.04650", "authors": ["Ariel Rodrigez", "Alejandro Lancho", "Amir Weiss"], "title": "Learning to Separate RF Signals Under Uncertainty: Detect-Then-Separate vs. Unified Joint Models", "comment": "6 pages, 6 figures, 1 table, accepted at the 2026 IEEE International Conference on Communications", "summary": "The increasingly crowded radio frequency (RF) spectrum forces communication signals to coexist, creating heterogeneous interferers whose structure often departs from Gaussian models. Recovering the interference-contaminated signal of interest in such settings is a central challenge, especially in single-channel RF processing. Existing data-driven methods often assume that the interference type is known, yielding ensembles of specialized models that scale poorly with the number of interferers. We show that detect-then-separate (DTS) strategies admit an analytical justification: within a Gaussian mixture framework, a plug-in maximum a posteriori detector followed by type-conditioned optimal estimation achieves asymptotic minimum mean-square error optimality under a mild temporal-diversity condition. This makes DTS a principled benchmark, but its reliance on multiple type-specific models limits scalability. Motivated by this, we propose a unified joint model (UJM), in which a single deep neural architecture learns to jointly detect and separate when applied directly to the received signal. Using tailored UNet architectures for baseband (complex-valued) RF signals, we compare DTS and UJM on synthetic and recorded interference types, showing that a capacity-matched UJM can match oracle-aided DTS performance across diverse signal-to-interference-and-noise ratios, interference types, and constellation orders, including mismatched training and testing type-uncertainty proportions. These findings highlight UJM as a scalable and practical alternative to DTS, while opening new directions for unified separation under broader regimes.", "AI": {"tldr": "\u63d0\u51fa\u7edf\u4e00\u8054\u5408\u6a21\u578b(UJM)\u7528\u4e8e\u5355\u901a\u9053\u5c04\u9891\u4fe1\u53f7\u5e72\u6270\u68c0\u6d4b\u4e0e\u5206\u79bb\uff0c\u76f8\u6bd4\u4f20\u7edf\u68c0\u6d4b-\u5206\u79bb\u7b56\u7565(DTS)\u5177\u6709\u66f4\u597d\u7684\u53ef\u6269\u5c55\u6027\uff0c\u5728\u591a\u79cd\u5e72\u6270\u7c7b\u578b\u548c\u4fe1\u566a\u6bd4\u6761\u4ef6\u4e0b\u5339\u914d\u6700\u4f18\u6027\u80fd\u3002", "motivation": "\u5c04\u9891\u9891\u8c31\u65e5\u76ca\u62e5\u6324\u5bfc\u81f4\u5f02\u6784\u5e72\u6270\u589e\u591a\uff0c\u73b0\u6709\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u901a\u5e38\u5047\u8bbe\u5e72\u6270\u7c7b\u578b\u5df2\u77e5\uff0c\u5bfc\u81f4\u4e13\u7528\u6a21\u578b\u96c6\u5408\u96be\u4ee5\u6269\u5c55\u3002\u68c0\u6d4b-\u5206\u79bb\u7b56\u7565(DTS)\u867d\u6709\u7406\u8bba\u4f9d\u636e\u4f46\u4f9d\u8d56\u591a\u4e2a\u7c7b\u578b\u7279\u5b9a\u6a21\u578b\uff0c\u9650\u5236\u4e86\u53ef\u6269\u5c55\u6027\u3002", "method": "\u63d0\u51fa\u7edf\u4e00\u8054\u5408\u6a21\u578b(UJM)\uff0c\u4f7f\u7528\u5355\u4e00\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u76f4\u63a5\u5bf9\u63a5\u6536\u4fe1\u53f7\u8fdb\u884c\u8054\u5408\u68c0\u6d4b\u548c\u5206\u79bb\u3002\u91c7\u7528\u9488\u5bf9\u57fa\u5e26(\u590d\u503c)\u5c04\u9891\u4fe1\u53f7\u5b9a\u5236\u7684UNet\u67b6\u6784\uff0c\u5e76\u4e0eDTS\u7b56\u7565\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u5728\u5408\u6210\u548c\u8bb0\u5f55\u7684\u5e72\u6270\u7c7b\u578b\u4e0a\uff0c\u5bb9\u91cf\u5339\u914d\u7684UJM\u80fd\u591f\u5728\u4e0d\u540c\u4fe1\u5e72\u566a\u6bd4\u3001\u5e72\u6270\u7c7b\u578b\u548c\u661f\u5ea7\u9636\u6570\u4e0b\u5339\u914doracle\u8f85\u52a9DTS\u7684\u6027\u80fd\uff0c\u5305\u62ec\u8bad\u7ec3\u548c\u6d4b\u8bd5\u4e2d\u7c7b\u578b\u4e0d\u786e\u5b9a\u6027\u6bd4\u4f8b\u4e0d\u5339\u914d\u7684\u60c5\u51b5\u3002", "conclusion": "UJM\u4f5c\u4e3aDTS\u7684\u53ef\u6269\u5c55\u5b9e\u7528\u66ff\u4ee3\u65b9\u6848\uff0c\u4e3a\u66f4\u5e7f\u6cdb\u673a\u5236\u4e0b\u7684\u7edf\u4e00\u5206\u79bb\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\uff0c\u5c55\u793a\u4e86\u5355\u4e00\u6a21\u578b\u5904\u7406\u591a\u79cd\u5e72\u6270\u7c7b\u578b\u7684\u6f5c\u529b\u3002"}}
{"id": "2602.04681", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2602.04681", "abs": "https://arxiv.org/abs/2602.04681", "authors": ["Yinghao Wang", "Lintao Xu", "Shujian Yu", "Enzo Tartaglione", "Van-Tam Nguyen"], "title": "HFMCA: Orthonormal Feature Learning for EEG-based Brain Decoding", "comment": null, "summary": "Electroencephalography (EEG) analysis is critical for brain-computer interfaces and neuroscience, but the intrinsic noise and high dimensionality of EEG signals hinder effective feature learning. We propose a self-supervised framework based on the Hierarchical Functional Maximal Correlation Algorithm (HFMCA), which learns orthonormal EEG representations by enforcing feature decorrelation and reducing redundancy. This design enables robust capture of essential brain dynamics for various EEG recognition tasks. We validate HFMCA on two benchmark datasets, SEED and BCIC-2A, where pretraining with HFMCA consistently outperforms competitive self-supervised baselines, achieving notable gains in classification accuracy. Across diverse EEG tasks, our method demonstrates superior cross-subject generalization under leave-one-subject-out validation, advancing state-of-the-art by 2.71\\% on SEED emotion recognition and 2.57\\% on BCIC-2A motor imagery classification.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5206\u5c42\u529f\u80fd\u6700\u5927\u76f8\u5173\u7b97\u6cd5\uff08HFMCA\uff09\u7684\u81ea\u76d1\u7763\u6846\u67b6\uff0c\u5b66\u4e60\u6b63\u4ea4\u5f52\u4e00\u5316\u7684EEG\u8868\u5f81\uff0c\u901a\u8fc7\u7279\u5f81\u53bb\u76f8\u5173\u548c\u5197\u4f59\u51cf\u5c11\u6765\u6355\u6349\u8111\u52a8\u529b\u5b66\uff0c\u5728SEED\u548cBCIC-2A\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "EEG\u4fe1\u53f7\u5b58\u5728\u56fa\u6709\u566a\u58f0\u548c\u9ad8\u7ef4\u5ea6\u7279\u6027\uff0c\u963b\u788d\u4e86\u6709\u6548\u7279\u5f81\u5b66\u4e60\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u63d0\u53d6\u7a33\u5065\u8868\u5f81\u7684\u65b9\u6cd5\u6765\u6539\u8fdb\u8111\u673a\u63a5\u53e3\u548c\u795e\u7ecf\u79d1\u5b66\u7814\u7a76\u3002", "method": "\u63d0\u51fa\u5206\u5c42\u529f\u80fd\u6700\u5927\u76f8\u5173\u7b97\u6cd5\uff08HFMCA\uff09\u7684\u81ea\u76d1\u7763\u6846\u67b6\uff0c\u901a\u8fc7\u5f3a\u5236\u7279\u5f81\u53bb\u76f8\u5173\u548c\u51cf\u5c11\u5197\u4f59\u6765\u5b66\u4e60\u6b63\u4ea4\u5f52\u4e00\u5316\u7684EEG\u8868\u5f81\uff0c\u4ece\u800c\u6355\u6349\u672c\u8d28\u7684\u8111\u52a8\u529b\u5b66\u3002", "result": "\u5728SEED\u548cBCIC-2A\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cHFMCA\u9884\u8bad\u7ec3\u59cb\u7ec8\u4f18\u4e8e\u7ade\u4e89\u7684\u81ea\u76d1\u7763\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728SEED\u60c5\u7eea\u8bc6\u522b\u4e0a\u63d0\u53472.71%\uff0c\u5728BCIC-2A\u8fd0\u52a8\u60f3\u8c61\u5206\u7c7b\u4e0a\u63d0\u53472.57%\uff0c\u5728\u8de8\u88ab\u8bd5\u6cdb\u5316\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "HFMCA\u6846\u67b6\u80fd\u591f\u6709\u6548\u5b66\u4e60\u7a33\u5065\u7684EEG\u8868\u5f81\uff0c\u5728\u591a\u79cdEEG\u8bc6\u522b\u4efb\u52a1\u4e2d\u5b9e\u73b0\u5353\u8d8a\u7684\u8de8\u88ab\u8bd5\u6cdb\u5316\u80fd\u529b\uff0c\u63a8\u52a8\u4e86\u8111\u673a\u63a5\u53e3\u548c\u795e\u7ecf\u79d1\u5b66\u7814\u7a76\u7684\u8fdb\u5c55\u3002"}}
{"id": "2602.04703", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.04703", "abs": "https://arxiv.org/abs/2602.04703", "authors": ["Sina Tavakolian", "Nhan Thanh Nguyen", "Ahmed Alkhateeb", "Markku Juntti"], "title": "Knowledge Distillation for mmWave Beam Prediction Using Sub-6 GHz Channels", "comment": "5 pages, 4 figures. Accepted for publication at IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) 2026", "summary": "Beamforming in millimeter-wave (mmWave) high-mobility environments typically incurs substantial training overhead. While prior studies suggest that sub-6 GHz channels can be exploited to predict optimal mmWave beams, existing methods depend on large deep learning (DL) models with prohibitive computational and memory requirements. In this paper, we propose a computationally efficient framework for sub-6 GHz channel-mmWave beam mapping based on the knowledge distillation (KD) technique. We develop two compact student DL architectures based on individual and relational distillation strategies, which retain only a few hidden layers yet closely mimic the performance of large teacher DL models. Extensive simulations demonstrate that the proposed student models achieve the teacher's beam prediction accuracy and spectral efficiency while reducing trainable parameters and computational complexity by 99%.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u77e5\u8bc6\u84b8\u998f\u7684\u9ad8\u6548\u6beb\u7c73\u6ce2\u6ce2\u675f\u6210\u5f62\u6846\u67b6\uff0c\u4f7f\u7528\u7d27\u51d1\u5b66\u751f\u6a21\u578b\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6", "motivation": "\u6beb\u7c73\u6ce2\u9ad8\u79fb\u52a8\u6027\u73af\u5883\u4e2d\u7684\u6ce2\u675f\u6210\u5f62\u901a\u5e38\u9700\u8981\u5927\u91cf\u8bad\u7ec3\u5f00\u9500\uff0c\u73b0\u6709\u57fa\u4e8esub-6 GHz\u4fe1\u9053\u9884\u6d4b\u6beb\u7c73\u6ce2\u6ce2\u675f\u7684\u65b9\u6cd5\u4f9d\u8d56\u5927\u578b\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u8ba1\u7b97\u548c\u5185\u5b58\u9700\u6c42\u8fc7\u9ad8", "method": "\u57fa\u4e8e\u77e5\u8bc6\u84b8\u998f\u6280\u672f\u5f00\u53d1\u9ad8\u6548\u6846\u67b6\uff0c\u8bbe\u8ba1\u4e24\u79cd\u7d27\u51d1\u5b66\u751fDL\u67b6\u6784\uff08\u4e2a\u4f53\u84b8\u998f\u548c\u5173\u7cfb\u84b8\u998f\u7b56\u7565\uff09\uff0c\u4ec5\u4fdd\u7559\u5c11\u91cf\u9690\u85cf\u5c42\u4f46\u80fd\u6a21\u62df\u5927\u578b\u6559\u5e08\u6a21\u578b\u7684\u6027\u80fd", "result": "\u5b66\u751f\u6a21\u578b\u8fbe\u5230\u6559\u5e08\u6a21\u578b\u7684\u6ce2\u675f\u9884\u6d4b\u7cbe\u5ea6\u548c\u9891\u8c31\u6548\u7387\uff0c\u540c\u65f6\u5c06\u53ef\u8bad\u7ec3\u53c2\u6570\u548c\u8ba1\u7b97\u590d\u6742\u5ea6\u964d\u4f4e99%", "conclusion": "\u63d0\u51fa\u7684\u77e5\u8bc6\u84b8\u998f\u6846\u67b6\u80fd\u6709\u6548\u89e3\u51b3\u6beb\u7c73\u6ce2\u6ce2\u675f\u6210\u5f62\u4e2d\u7684\u8ba1\u7b97\u6548\u7387\u95ee\u9898\uff0c\u4e3a\u9ad8\u79fb\u52a8\u6027\u73af\u5883\u63d0\u4f9b\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2602.04704", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2602.04704", "abs": "https://arxiv.org/abs/2602.04704", "authors": ["Jonas Pirkl", "Jonathan Ott", "Maximilian Stahlke", "George Yammine", "Tobias Feigl", "Christopher Mutschler"], "title": "Resilient Channel Charting Under Varying Radio Link Availability", "comment": null, "summary": "Channel charting (CC) has become a key technology for RF-based localization, enabling unsupervised radio fingerprinting, even in non line of sight scenarios, with a minimum of reference position labels. However, most CC models assume fixed-size inputs, such as a constant number of antennas or channel measurements. In practical systems, antennas may fail, signals may be blocked, or antenna sets may change during handovers, making fixed-input architectures fragile. Existing radio-fingerprinting approaches address this by training separate models for each antenna configuration, but the resulting training effort scales prohibitively with the array size. We propose Adaptive Positioning (AdaPos), a CC architecture that natively handles variable numbers of channel measurements. AdaPos combines convolutional feature extraction with a transformer-based encoder using learnable antenna identifiers and self-attention to fuse arbitrary subsets of CSI inputs. Experiments on two public real-world datasets (SISO and MIMO) show that AdaPos maintains state-of-the-art accuracy under missing-antenna conditions and replaces roughly 57 configuration-specific models with a single unified model. With AdaPos and our novel training strategies, we provide resilience to both individual antenna failures and full-array outages.", "AI": {"tldr": "AdaPos\u662f\u4e00\u79cd\u81ea\u9002\u5e94\u5b9a\u4f4d\u67b6\u6784\uff0c\u901a\u8fc7\u7ed3\u5408\u5377\u79ef\u7279\u5f81\u63d0\u53d6\u548c\u57fa\u4e8etransformer\u7684\u7f16\u7801\u5668\uff0c\u80fd\u591f\u539f\u751f\u5904\u7406\u53ef\u53d8\u6570\u91cf\u7684\u4fe1\u9053\u6d4b\u91cf\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u4fe1\u9053\u56fe\u8868\u6280\u672f\u4e2d\u56fa\u5b9a\u8f93\u5165\u5c3a\u5bf8\u7684\u9650\u5236\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u4fe1\u9053\u56fe\u8868\u6280\u672f\u5047\u8bbe\u56fa\u5b9a\u5c3a\u5bf8\u8f93\u5165\uff08\u5982\u6052\u5b9a\u5929\u7ebf\u6570\u91cf\uff09\uff0c\u4f46\u5728\u5b9e\u9645\u7cfb\u7edf\u4e2d\u5929\u7ebf\u53ef\u80fd\u6545\u969c\u3001\u4fe1\u53f7\u53ef\u80fd\u88ab\u963b\u6321\u6216\u5929\u7ebf\u914d\u7f6e\u53ef\u80fd\u5728\u5207\u6362\u65f6\u6539\u53d8\uff0c\u5bfc\u81f4\u56fa\u5b9a\u8f93\u5165\u67b6\u6784\u8106\u5f31\u3002\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u4e3a\u6bcf\u79cd\u5929\u7ebf\u914d\u7f6e\u8bad\u7ec3\u5355\u72ec\u6a21\u578b\uff0c\u8bad\u7ec3\u5de5\u4f5c\u91cf\u968f\u9635\u5217\u5c3a\u5bf8\u5448\u6307\u6570\u589e\u957f\u3002", "method": "AdaPos\u7ed3\u5408\u5377\u79ef\u7279\u5f81\u63d0\u53d6\u548c\u57fa\u4e8etransformer\u7684\u7f16\u7801\u5668\uff0c\u4f7f\u7528\u53ef\u5b66\u4e60\u7684\u5929\u7ebf\u6807\u8bc6\u7b26\u548c\u81ea\u6ce8\u610f\u529b\u673a\u5236\u6765\u878d\u5408\u4efb\u610f\u5b50\u96c6\u7684CSI\u8f93\u5165\u3002\u901a\u8fc7\u65b0\u9896\u7684\u8bad\u7ec3\u7b56\u7565\uff0c\u63d0\u4f9b\u5bf9\u5355\u4e2a\u5929\u7ebf\u6545\u969c\u548c\u5168\u9635\u5217\u4e2d\u65ad\u7684\u5f39\u6027\u3002", "result": "\u5728\u4e24\u4e2a\u516c\u5f00\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\uff08SISO\u548cMIMO\uff09\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cAdaPos\u5728\u7f3a\u5931\u5929\u7ebf\u6761\u4ef6\u4e0b\u4fdd\u6301\u6700\u5148\u8fdb\u7684\u7cbe\u5ea6\uff0c\u7528\u5355\u4e2a\u7edf\u4e00\u6a21\u578b\u66ff\u4ee3\u4e86\u7ea657\u4e2a\u914d\u7f6e\u7279\u5b9a\u6a21\u578b\u3002", "conclusion": "AdaPos\u63d0\u4f9b\u4e86\u4e00\u79cd\u80fd\u591f\u539f\u751f\u5904\u7406\u53ef\u53d8\u6570\u91cf\u4fe1\u9053\u6d4b\u91cf\u7684\u81ea\u9002\u5e94\u5b9a\u4f4d\u67b6\u6784\uff0c\u89e3\u51b3\u4e86\u5b9e\u9645\u7cfb\u7edf\u4e2d\u5929\u7ebf\u914d\u7f6e\u53d8\u5316\u7684\u6311\u6218\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u6a21\u578b\u8bad\u7ec3\u5de5\u4f5c\u91cf\u5e76\u63d0\u9ad8\u4e86\u7cfb\u7edf\u5f39\u6027\u3002"}}
{"id": "2602.04728", "categories": ["eess.SP", "cs.IT", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.04728", "abs": "https://arxiv.org/abs/2602.04728", "authors": ["Xavier Tardy", "Gr\u00e9goire Lefebvre", "Apostolos Kountouris", "Ha\u00effa Fares", "Amor Nafkha"], "title": "Cross-Attention Transformer for Joint Multi-Receiver Uplink Neural Decoding", "comment": "6 pages, 3 figures, 3 tables, conference submission", "summary": "We propose a cross-attention Transformer for joint decoding of uplink OFDM signals received by multiple coordinated access points. A shared per-receiver encoder learns time-frequency structure within each received grid, and a token-wise cross-attention module fuses the receivers to produce soft log-likelihood ratios for a standard channel decoder, without requiring explicit per-receiver channel estimates. Trained with a bit-metric objective, the model adapts its fusion to per-receiver reliability, tolerates missing or degraded links, and remains robust when pilots are sparse. Across realistic Wi-Fi channels, it consistently outperforms classical pipelines and strong convolutional baselines, frequently matching (and in some cases surpassing) a powerful baseline that assumes perfect channel knowledge per access point. Despite its expressiveness, the architecture is compact, has low computational cost (low GFLOPs), and achieves low latency on GPUs, making it a practical building block for next-generation Wi-Fi receivers.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u8de8\u6ce8\u610f\u529bTransformer\uff0c\u7528\u4e8e\u8054\u5408\u89e3\u7801\u591a\u4e2a\u534f\u8c03\u63a5\u5165\u70b9\u63a5\u6536\u7684\u4e0a\u884cOFDM\u4fe1\u53f7\uff0c\u65e0\u9700\u663e\u5f0f\u4fe1\u9053\u4f30\u8ba1\uff0c\u901a\u8fc7\u6bd4\u7279\u5ea6\u91cf\u76ee\u6807\u8bad\u7ec3\uff0c\u5728Wi-Fi\u4fe1\u9053\u4e2d\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edfWi-Fi\u63a5\u6536\u5668\u9700\u8981\u663e\u5f0f\u7684\u6bcf\u63a5\u6536\u5668\u4fe1\u9053\u4f30\u8ba1\uff0c\u8fd9\u5728\u5bfc\u9891\u7a00\u758f\u6216\u4fe1\u9053\u6761\u4ef6\u590d\u6742\u65f6\u6027\u80fd\u53d7\u9650\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u81ea\u9002\u5e94\u878d\u5408\u591a\u4e2a\u63a5\u6536\u5668\u4fe1\u53f7\u3001\u5bb9\u5fcd\u94fe\u8def\u9000\u5316\u3001\u4e14\u8ba1\u7b97\u9ad8\u6548\u7684\u8054\u5408\u89e3\u7801\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u8de8\u6ce8\u610f\u529bTransformer\u67b6\u6784\uff1a1\uff09\u5171\u4eab\u7684\u6bcf\u63a5\u6536\u5668\u7f16\u7801\u5668\u5b66\u4e60\u6bcf\u4e2a\u63a5\u6536\u7f51\u683c\u7684\u65f6\u9891\u7ed3\u6784\uff1b2\uff09\u57fa\u4e8etoken\u7684\u8de8\u6ce8\u610f\u529b\u6a21\u5757\u878d\u5408\u591a\u4e2a\u63a5\u6536\u5668\u4fe1\u606f\uff1b3\uff09\u8f93\u51fa\u8f6f\u5bf9\u6570\u4f3c\u7136\u6bd4\u4f9b\u6807\u51c6\u4fe1\u9053\u89e3\u7801\u5668\u4f7f\u7528\uff1b4\uff09\u4f7f\u7528\u6bd4\u7279\u5ea6\u91cf\u76ee\u6807\u8fdb\u884c\u7aef\u5230\u7aef\u8bad\u7ec3\u3002", "result": "\u5728\u771f\u5b9eWi-Fi\u4fe1\u9053\u4e2d\uff0c\u8be5\u65b9\u6cd5\u6301\u7eed\u4f18\u4e8e\u4f20\u7edf\u6d41\u6c34\u7ebf\u548c\u5f3a\u5377\u79ef\u57fa\u7ebf\uff0c\u7ecf\u5e38\u5339\u914d\uff08\u67d0\u4e9b\u60c5\u51b5\u4e0b\u8d85\u8d8a\uff09\u5047\u8bbe\u5b8c\u7f8e\u4fe1\u9053\u77e5\u8bc6\u7684\u5f3a\u5927\u57fa\u7ebf\u3002\u6a21\u578b\u7d27\u51d1\u3001\u8ba1\u7b97\u6210\u672c\u4f4e\uff08\u4f4eGFLOPs\uff09\u3001\u5728GPU\u4e0a\u5ef6\u8fdf\u4f4e\u3002", "conclusion": "\u63d0\u51fa\u7684\u8de8\u6ce8\u610f\u529bTransformer\u662f\u4e00\u79cd\u5b9e\u7528\u7684\u4e0b\u4e00\u4ee3Wi-Fi\u63a5\u6536\u5668\u6784\u5efa\u5757\uff0c\u80fd\u591f\u81ea\u9002\u5e94\u878d\u5408\u591a\u63a5\u6536\u5668\u4fe1\u53f7\u3001\u5bb9\u5fcd\u94fe\u8def\u9000\u5316\u3001\u5728\u5bfc\u9891\u7a00\u758f\u65f6\u4fdd\u6301\u9c81\u68d2\u6027\uff0c\u4e14\u8ba1\u7b97\u6548\u7387\u9ad8\u3002"}}
{"id": "2602.04803", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2602.04803", "abs": "https://arxiv.org/abs/2602.04803", "authors": ["Riccardo Tedeschi", "Luigi Ghionda", "Alessandro Nadalini", "Yvan Tortorella", "Arpan Suravi Prasad", "Luca Benini", "Davide Rossi", "Francesco Conti"], "title": "Safe-NEureka: a Hybrid Modular Redundant DNN Accelerator for On-board Satellite AI Processing", "comment": "22 pages, 13 figures, ACM journal format", "summary": "Low Earth Orbit (LEO) constellations are revolutionizing the space sector, with on-board Artificial Intelligence (AI) becoming pivotal for next-generation satellites. AI acceleration is essential for safety-critical functions such as autonomous Guidance, Navigation, and Control (GNC), where errors cannot be tolerated, and performance-critical processing of high-bandwidth sensor data, where occasional errors are tolerable. Consequently, AI accelerators for satellites must combine robust protection against radiation-induced faults with high throughput. This paper presents Safe-NEureka, a Hybrid Modular Redundant Deep Neural Network (DNN) accelerator for heterogeneous RISC-V systems. It operates in two modes: a redundancy mode utilizing Dual Modular Redundancy (DMR) with hardware-based recovery, and a performance mode repurposing redundant datapaths to maximize parallel throughput. Furthermore, its memory interface is protected by Error Correction Codes (ECCs), and the controller by Triple Modular Redundancy (TMR). Implementation in GlobalFoundries 12nm technology shows a 96 reduction in faulty executions in redundancy mode, with a manageable 15 area overhead. In performance mode, the architecture achieves near-baseline speeds on 3x3 dense convolutions with a 5 throughput and 11 efficiency reduction, compared to 48 and 53 in redundancy mode. This flexibility ensures high overheads are limited to critical tasks, establishing Safe-NEureka as a versatile solution for space applications.", "AI": {"tldr": "Safe-NEureka\u662f\u4e00\u4e2a\u7528\u4e8e\u536b\u661f\u7684\u6df7\u5408\u6a21\u5757\u5197\u4f59DNN\u52a0\u901f\u5668\uff0c\u652f\u6301\u5197\u4f59\u6a21\u5f0f\uff08DMR\u786c\u4ef6\u6062\u590d\uff09\u548c\u6027\u80fd\u6a21\u5f0f\uff08\u6700\u5927\u5316\u5e76\u884c\u541e\u5410\uff09\uff0c\u5728\u8f90\u5c04\u9632\u62a4\u548c\u6027\u80fd\u95f4\u53d6\u5f97\u5e73\u8861\u3002", "motivation": "LEO\u661f\u5ea7\u536b\u661f\u9700\u8981AI\u52a0\u901f\u5668\u6765\u5904\u7406\u5b89\u5168\u5173\u952e\uff08\u5982\u81ea\u4e3bGNC\uff09\u548c\u6027\u80fd\u5173\u952e\uff08\u5982\u4f20\u611f\u5668\u6570\u636e\u5904\u7406\uff09\u4efb\u52a1\u3002\u8fd9\u4e9b\u52a0\u901f\u5668\u5fc5\u987b\u540c\u65f6\u5177\u5907\u8f90\u5c04\u6545\u969c\u9632\u62a4\u548c\u9ad8\u541e\u5410\u80fd\u529b\u3002", "method": "\u63d0\u51faSafe-NEureka\u6df7\u5408\u6a21\u5757\u5197\u4f59DNN\u52a0\u901f\u5668\uff0c\u91c7\u7528\u5f02\u6784RISC-V\u7cfb\u7edf\u3002\u5305\u542b\u4e24\u79cd\u6a21\u5f0f\uff1a\u5197\u4f59\u6a21\u5f0f\u4f7f\u7528DMR\u52a0\u786c\u4ef6\u6062\u590d\uff1b\u6027\u80fd\u6a21\u5f0f\u91cd\u7528\u5197\u4f59\u6570\u636e\u8def\u5f84\u6700\u5927\u5316\u5e76\u884c\u541e\u5410\u3002\u5185\u5b58\u63a5\u53e3\u7528ECC\u4fdd\u62a4\uff0c\u63a7\u5236\u5668\u7528TMR\u4fdd\u62a4\u3002", "result": "\u5728GlobalFoundries 12nm\u6280\u672f\u4e2d\u5b9e\u73b0\uff1a\u5197\u4f59\u6a21\u5f0f\u4e0b\u6545\u969c\u6267\u884c\u51cf\u5c1196\u500d\uff0c\u9762\u79ef\u5f00\u950015%\uff1b\u6027\u80fd\u6a21\u5f0f\u4e0b3x3\u5bc6\u96c6\u5377\u79ef\u63a5\u8fd1\u57fa\u7ebf\u901f\u5ea6\uff0c\u541e\u5410\u91cf\u964d\u4f4e5%\uff0c\u6548\u7387\u964d\u4f4e11%\uff08\u5197\u4f59\u6a21\u5f0f\u4e0b\u5206\u522b\u4e3a48%\u548c53%\uff09\u3002", "conclusion": "Safe-NEureka\u901a\u8fc7\u7075\u6d3b\u7684\u6df7\u5408\u5197\u4f59\u8bbe\u8ba1\uff0c\u5728\u5173\u952e\u4efb\u52a1\u4e2d\u63d0\u4f9b\u9ad8\u53ef\u9760\u6027\uff0c\u5728\u975e\u5173\u952e\u4efb\u52a1\u4e2d\u4fdd\u6301\u9ad8\u6027\u80fd\uff0c\u6210\u4e3a\u592a\u7a7a\u5e94\u7528\u7684\u901a\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}

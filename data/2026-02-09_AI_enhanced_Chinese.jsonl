{"id": "2602.06156", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2602.06156", "abs": "https://arxiv.org/abs/2602.06156", "authors": ["Bianca S. de C. da Silva", "Pedro H. C. de Souza", "Luciano L. Mendes"], "title": "PAPR Reduction in OFDM Systems Using Neural Networks: A Case Study on the Importance of Dataset Generalization", "comment": null, "summary": "In [1], we introduced a NN designed to reduce the PAPR in OFDM systems. However, the original study did not include explicit generalization tests to assess how well the NN would perform on previously unseen data, which prevented a comprehensive evaluation of the model's robustness and applicability in diverse scenarios. To address this gap, we conducted additional generalization assessments, the results of which are presented in this case study. These results serve both to complement and to refine the original analysis reported in [1]. Most importantly, the overall conclusions of the initial study remain valid: the NN is still able to reduce the PAPR level to a desired reference value, also with a lower computational cost, confirming the effectiveness and practical applicability of the proposed method across a more generalized setting.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5bf9\u5148\u524d\u63d0\u51fa\u7684\u964d\u4f4eOFDM\u7cfb\u7edfPAPR\u7684\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u4e86\u6cdb\u5316\u80fd\u529b\u6d4b\u8bd5\uff0c\u9a8c\u8bc1\u4e86\u6a21\u578b\u5728\u672a\u89c1\u6570\u636e\u4e0a\u7684\u6709\u6548\u6027\uff0c\u786e\u8ba4\u4e86\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u7a33\u5065\u6027\u3002", "motivation": "\u539f\u59cb\u7814\u7a76\u867d\u7136\u63d0\u51fa\u4e86\u964d\u4f4eOFDM\u7cfb\u7edfPAPR\u7684\u795e\u7ecf\u7f51\u7edc\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u7684\u7cfb\u7edf\u8bc4\u4f30\uff0c\u65e0\u6cd5\u5168\u9762\u8bc4\u4f30\u6a21\u578b\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u7684\u9c81\u68d2\u6027\u548c\u9002\u7528\u6027\u3002", "method": "\u5bf9\u5148\u524d\u63d0\u51fa\u7684\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u989d\u5916\u7684\u6cdb\u5316\u8bc4\u4f30\u6d4b\u8bd5\uff0c\u4f7f\u7528\u672a\u89c1\u6570\u636e\u9a8c\u8bc1\u6a21\u578b\u6027\u80fd\uff0c\u8865\u5145\u548c\u5b8c\u5584\u539f\u59cb\u5206\u6790\u3002", "result": "\u6cdb\u5316\u6d4b\u8bd5\u7ed3\u679c\u8868\u660e\uff0c\u795e\u7ecf\u7f51\u7edc\u4ecd\u80fd\u5c06PAPR\u964d\u4f4e\u5230\u671f\u671b\u7684\u53c2\u8003\u503c\uff0c\u4e14\u8ba1\u7b97\u6210\u672c\u66f4\u4f4e\uff0c\u9a8c\u8bc1\u4e86\u6a21\u578b\u5728\u66f4\u5e7f\u6cdb\u573a\u666f\u4e0b\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u539f\u59cb\u7814\u7a76\u7684\u4e3b\u8981\u7ed3\u8bba\u4ecd\u7136\u6210\u7acb\uff1a\u63d0\u51fa\u7684\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\u5728\u6cdb\u5316\u8bbe\u7f6e\u4e0b\u4f9d\u7136\u6709\u6548\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\uff0c\u4e14\u8ba1\u7b97\u6210\u672c\u66f4\u4f4e\u3002"}}
{"id": "2602.06313", "categories": ["eess.SP", "cs.IT"], "pdf": "https://arxiv.org/pdf/2602.06313", "abs": "https://arxiv.org/abs/2602.06313", "authors": ["Xiaokun Tuo", "Ming-Min Zhao", "Xiang Wang", "Changsheng You", "Min-Jian Zhao"], "title": "Hybrid-Field Joint Channel and Visible Region Estimation for RIS-Assisted Communications", "comment": "13 pages, 8 figures", "summary": "In reconfigurable intelligent surface (RIS)-assisted millimeter-wave (mmWave) communication systems, the large-scale RIS introduces pronounced geometric effects that lead to the coexistence of far-field and near-field propagation. Furthermore, random blockages induce spatial non-stationarity across the RIS array, causing signals from different scatterers to illuminate only partial regions, referred to as visible regions (VRs). This renders conventional far-field and fully visible array-based channel models inadequate and makes channel estimation particularly challenging. In this paper, we investigate the non-stationary cascaded channel estimation problem in a hybrid-field propagation environment, where the RIS-base station (BS) link operates in the far-field, while the user-RIS link exhibits near-field characteristics with partial visibility. To address the resulting high-dimensional and coupled estimation problem, a reduced-dimensional sparse bilinear representation is developed by exploiting the structural characteristics of the cascaded channel. In particular, a dictionary compression technique is proposed to represent the high-dimensional coupled dictionary using a low-dimensional polar-domain dictionary weighted by a visibility matrix, thereby significantly reducing the problem scale. Based on this representation, a turbo-structured joint Bayesian estimation (TS-JBE) approach is proposed to simultaneously estimate the channel gains, VRs, and off-grid parameters, thereby avoiding error propagation inherent in existing sequential methods. Simulation results demonstrate that the proposed method significantly improves the estimation accuracy compared with existing approaches.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8eRIS\u8f85\u52a9\u6beb\u7c73\u6ce2\u901a\u4fe1\u7cfb\u7edf\u4e2d\u975e\u5e73\u7a33\u7ea7\u8054\u4fe1\u9053\u4f30\u8ba1\u7684Turbo\u7ed3\u6784\u8054\u5408\u8d1d\u53f6\u65af\u4f30\u8ba1\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u6df7\u5408\u573a\u4f20\u64ad\u548c\u90e8\u5206\u53ef\u89c1\u6027\u5e26\u6765\u7684\u6311\u6218\u3002", "motivation": "\u5728RIS\u8f85\u52a9\u7684\u6beb\u7c73\u6ce2\u901a\u4fe1\u7cfb\u7edf\u4e2d\uff0c\u5927\u89c4\u6a21RIS\u5f15\u5165\u663e\u8457\u7684\u51e0\u4f55\u6548\u5e94\uff0c\u5bfc\u81f4\u8fdc\u573a\u548c\u8fd1\u573a\u4f20\u64ad\u5171\u5b58\u3002\u968f\u673a\u906e\u6321\u5f15\u8d77RIS\u9635\u5217\u7684\u7a7a\u95f4\u975e\u5e73\u7a33\u6027\uff0c\u4f7f\u5f97\u6765\u81ea\u4e0d\u540c\u6563\u5c04\u4f53\u7684\u4fe1\u53f7\u4ec5\u7167\u5c04\u90e8\u5206\u533a\u57df\uff08\u53ef\u89c1\u533a\u57df\uff09\uff0c\u4f20\u7edf\u4fe1\u9053\u6a21\u578b\u4e0d\u518d\u9002\u7528\uff0c\u4fe1\u9053\u4f30\u8ba1\u53d8\u5f97\u7279\u522b\u56f0\u96be\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u964d\u7ef4\u7a00\u758f\u53cc\u7ebf\u6027\u8868\u793a\u65b9\u6cd5\uff0c\u5229\u7528\u7ea7\u8054\u4fe1\u9053\u7684\u7ed3\u6784\u7279\u6027\u3002\u5177\u4f53\u5305\u62ec\uff1a1\uff09\u5b57\u5178\u538b\u7f29\u6280\u672f\uff0c\u7528\u4f4e\u7ef4\u6781\u57df\u5b57\u5178\u52a0\u6743\u53ef\u89c1\u6027\u77e9\u9635\u8868\u793a\u9ad8\u7ef4\u8026\u5408\u5b57\u5178\uff1b2\uff09Turbo\u7ed3\u6784\u8054\u5408\u8d1d\u53f6\u65af\u4f30\u8ba1\u65b9\u6cd5\uff0c\u540c\u65f6\u4f30\u8ba1\u4fe1\u9053\u589e\u76ca\u3001\u53ef\u89c1\u533a\u57df\u548c\u79bb\u7f51\u53c2\u6570\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u4f30\u8ba1\u7cbe\u5ea6\u3002", "conclusion": "\u8be5\u7814\u7a76\u89e3\u51b3\u4e86RIS\u8f85\u52a9\u6beb\u7c73\u6ce2\u7cfb\u7edf\u4e2d\u6df7\u5408\u573a\u4f20\u64ad\u548c\u90e8\u5206\u53ef\u89c1\u6027\u5e26\u6765\u7684\u4fe1\u9053\u4f30\u8ba1\u6311\u6218\uff0c\u63d0\u51fa\u7684\u8054\u5408\u8d1d\u53f6\u65af\u4f30\u8ba1\u65b9\u6cd5\u907f\u514d\u4e86\u73b0\u6709\u987a\u5e8f\u65b9\u6cd5\u7684\u8bef\u5dee\u4f20\u64ad\u95ee\u9898\uff0c\u4e3a\u5b9e\u9645\u7cfb\u7edf\u90e8\u7f72\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.06376", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2602.06376", "abs": "https://arxiv.org/abs/2602.06376", "authors": ["Thyagaraja Marathe", "Tyler G. R. Reid", "Srinivas Tantry", "Michael O'Meara"], "title": "Xona Pulsar Single-Satellite Positioning: System Perspective and Experimental Validation", "comment": null, "summary": "Xona is deploying Pulsar, a low Earth orbit (LEO) commercial navigation system designed to deliver resilient positioning, navigation, and timing (PNT) where traditional solutions fall short. Pulsar satellites broadcast dedicated signals optimized for commercial users. This brings rapid geometry change, strong Doppler observability, and robust timing, enabling new approaches to positioning even when only one satellite is visible. Internet of Things (IoT) applications often prioritize availability over sub-meter accuracy in urban canyons, semi-indoor spaces, and other constrained environments. Many platforms are battery-powered, have strict size, weight, and power (SWaP) limits, and cannot support complex multi-sensor architectures. Leveraging LEO dynamics and signal strength, Pulsar can maintain navigation capability under these conditions without specialized user hardware.\n  Here we present a single-satellite positioning (SSP) concept that uses available Pulsar measurements to estimate user position and receiver clock states without external aiding. Early in Pulsar deployment, only one or two satellites may be in view, yet this still benefits stationary or near-stationary users, including in semi-indoor and indoor settings. We discuss algorithmic details and system implications: SSP enables positioning with minimal satellite visibility, reduces reliance on dense constellations, and supports integration into resource-constrained platforms. We present simulation and live sky results. High-fidelity constellation simulations configured for Pulsar provide controlled performance assessment. We also present early findings from a Pulsar-enabled receiver using observations from the Pulsar-0 satellite on orbit. Preliminary tests demonstrate meter-level accuracy outdoors and indoors, highlighting potential under varied reception conditions.", "AI": {"tldr": "Xona\u7684Pulsar LEO\u5bfc\u822a\u7cfb\u7edf\u5229\u7528\u5355\u9897\u536b\u661f\u5b9e\u73b0\u5b9a\u4f4d\uff0c\u7279\u522b\u9002\u7528\u4e8eIoT\u8bbe\u5907\u5728\u53d7\u9650\u73af\u5883\u4e2d\u7684\u5bfc\u822a\u9700\u6c42\uff0c\u65e0\u9700\u5916\u90e8\u8f85\u52a9\u5373\u53ef\u8fbe\u5230\u7c73\u7ea7\u7cbe\u5ea6\u3002", "motivation": "\u4f20\u7edf\u5bfc\u822a\u7cfb\u7edf\u5728\u5ba4\u5185\u3001\u57ce\u5e02\u5ce1\u8c37\u7b49\u53d7\u9650\u73af\u5883\u4e2d\u6027\u80fd\u4e0b\u964d\uff0c\u800cIoT\u8bbe\u5907\u901a\u5e38\u53d7\u9650\u4e8e\u5c3a\u5bf8\u3001\u91cd\u91cf\u548c\u529f\u8017\uff0c\u65e0\u6cd5\u652f\u6301\u590d\u6742\u7684\u591a\u4f20\u611f\u5668\u67b6\u6784\u3002\u9700\u8981\u4e00\u79cd\u80fd\u5728\u536b\u661f\u53ef\u89c1\u6027\u6781\u4f4e\u60c5\u51b5\u4e0b\u4ecd\u80fd\u5de5\u4f5c\u7684\u5bfc\u822a\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u5355\u536b\u661f\u5b9a\u4f4d\uff08SSP\uff09\u6982\u5ff5\uff0c\u5229\u7528Pulsar LEO\u536b\u661f\u7684\u52a8\u6001\u7279\u6027\u548c\u4fe1\u53f7\u5f3a\u5ea6\uff0c\u4ec5\u4f7f\u7528\u4e00\u9897\u536b\u661f\u7684\u6d4b\u91cf\u6570\u636e\u6765\u4f30\u8ba1\u7528\u6237\u4f4d\u7f6e\u548c\u63a5\u6536\u673a\u65f6\u949f\u72b6\u6001\uff0c\u65e0\u9700\u5916\u90e8\u8f85\u52a9\u3002\u901a\u8fc7\u9ad8\u4fdd\u771f\u661f\u5ea7\u6a21\u62df\u548c\u5b9e\u9645\u5728\u8f68\u536b\u661f\uff08Pulsar-0\uff09\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u4eff\u771f\u548c\u5b9e\u9645\u6d4b\u8bd5\u663e\u793a\uff0c\u5373\u4f7f\u5728\u5ba4\u5185\u548c\u5ba4\u5916\u53d7\u9650\u73af\u5883\u4e2d\uff0c\u5355\u536b\u661f\u5b9a\u4f4d\u4e5f\u80fd\u8fbe\u5230\u7c73\u7ea7\u7cbe\u5ea6\u3002\u521d\u6b65\u6d4b\u8bd5\u8bc1\u660e\u4e86\u5728\u5404\u79cd\u63a5\u6536\u6761\u4ef6\u4e0b\u7684\u6f5c\u5728\u6027\u80fd\u3002", "conclusion": "Pulsar\u7cfb\u7edf\u7684\u5355\u536b\u661f\u5b9a\u4f4d\u6280\u672f\u80fd\u591f\u5728\u536b\u661f\u53ef\u89c1\u6027\u6781\u4f4e\u7684\u60c5\u51b5\u4e0b\u63d0\u4f9b\u53ef\u9760\u7684\u5bfc\u822a\u80fd\u529b\uff0c\u7279\u522b\u9002\u5408\u8d44\u6e90\u53d7\u9650\u7684IoT\u5e73\u53f0\uff0c\u4e3a\u4f20\u7edf\u5bfc\u822a\u7cfb\u7edf\u65e0\u6cd5\u8986\u76d6\u7684\u73af\u5883\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.06399", "categories": ["eess.SP", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.06399", "abs": "https://arxiv.org/abs/2602.06399", "authors": ["Xin Jin", "Tiejun Lv", "Yashuai Cao", "Jie Zeng", "Mugen Peng"], "title": "ARIS-RSMA Enhanced ISAC System: Joint Rate Splitting and Beamforming Design", "comment": "5 pages, 5 figures, accepted by IEEE Wireless Communications Letters", "summary": "This letter proposes an active reconfigurable intelligent surface (ARIS) assisted rate-splitting multiple access (RSMA) integrated sensing and communication (ISAC) system to overcome the fairness bottleneck in multi-target sensing under obstructed line-of-sight environments. Beamforming at the transceiver and ARIS, along with rate splitting, are optimized to maximize the minimum multi-target echo signal-to-interference-plus-noise ratio under multi-user rate and power constraints. The intricate non-convex problem is decoupled into three subproblems and solved iteratively by majorization-minimization (MM) and sequential rank-one constraint relaxation (SROCR) algorithms. Simulations show our scheme outperforms nonorthogonal multiple access, space-division multiple access, and passive RIS baselines, approaching sensing-only upper bounds.", "AI": {"tldr": "\u63d0\u51fa\u4e3b\u52a8\u53ef\u91cd\u6784\u667a\u80fd\u8868\u9762\u8f85\u52a9\u7684\u901f\u7387\u5206\u5272\u591a\u5740\u63a5\u5165\u96c6\u6210\u611f\u77e5\u901a\u4fe1\u7cfb\u7edf\uff0c\u89e3\u51b3\u906e\u6321\u73af\u5883\u4e0b\u591a\u76ee\u6807\u611f\u77e5\u7684\u516c\u5e73\u6027\u95ee\u9898", "motivation": "\u5728\u89c6\u7ebf\u88ab\u906e\u6321\u7684\u73af\u5883\u4e2d\uff0c\u591a\u76ee\u6807\u611f\u77e5\u5b58\u5728\u516c\u5e73\u6027\u74f6\u9888\uff0c\u9700\u8981\u65b0\u7684\u7cfb\u7edf\u8bbe\u8ba1\u6765\u63d0\u5347\u611f\u77e5\u6027\u80fd", "method": "\u4f18\u5316\u6536\u53d1\u5668\u548cARIS\u7684\u6ce2\u675f\u6210\u5f62\u4ee5\u53ca\u901f\u7387\u5206\u5272\uff0c\u4f7f\u7528MM\u548cSROCR\u7b97\u6cd5\u5c06\u975e\u51f8\u95ee\u9898\u5206\u89e3\u4e3a\u4e09\u4e2a\u5b50\u95ee\u9898\u8fed\u4ee3\u6c42\u89e3", "result": "\u4eff\u771f\u663e\u793a\u8be5\u65b9\u6848\u4f18\u4e8e\u975e\u6b63\u4ea4\u591a\u5740\u3001\u7a7a\u5206\u591a\u5740\u548c\u88ab\u52a8RIS\u57fa\u7ebf\uff0c\u63a5\u8fd1\u7eaf\u611f\u77e5\u7684\u4e0a\u754c", "conclusion": "ARIS\u8f85\u52a9\u7684RSMA ISAC\u7cfb\u7edf\u80fd\u6709\u6548\u63d0\u5347\u906e\u6321\u73af\u5883\u4e0b\u591a\u76ee\u6807\u611f\u77e5\u7684\u516c\u5e73\u6027\u548c\u6027\u80fd"}}
{"id": "2602.06180", "categories": ["eess.AS", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.06180", "abs": "https://arxiv.org/abs/2602.06180", "authors": ["Kaiyuan Zhang", "Mohan Shi", "Eray Eren", "Natarajan Balaji Shankar", "Zilai Wang", "Abeer Alwan"], "title": "STACodec: Semantic Token Assignment for Balancing Acoustic Fidelity and Semantic Information in Audio Codecs", "comment": "ICASSP 2026", "summary": "Neural audio codecs are widely used for audio compression and can be integrated into token-based language models. Traditional codecs preserve acoustic details well but lack semantic information. Recent hybrid codecs attempt to incorporate semantic information through distillation, but this often degrades reconstruction performance, making it difficult to achieve both. To address this limitation, we introduce STACodec, a unified codec that integrates semantic information from self-supervised learning (SSL) models into the first layer of residual vector quantization (RVQ-1) via semantic token assignment (STA). To further eliminate reliance on SSL-based semantic tokenizers and improve efficiency during inference, we propose a semantic pre-distillation (SPD) module, which predicts semantic tokens directly for assignment to the first RVQ layer during inference. Experimental results show that STACodec outperforms existing hybrid codecs in both audio reconstruction and downstream semantic tasks, demonstrating a better balance between acoustic fidelity and semantic capability.", "AI": {"tldr": "STACodec\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u97f3\u9891\u7f16\u89e3\u7801\u5668\uff0c\u901a\u8fc7\u8bed\u4e49\u4ee4\u724c\u5206\u914d\u5c06\u81ea\u76d1\u7763\u5b66\u4e60\u7684\u8bed\u4e49\u4fe1\u606f\u96c6\u6210\u5230\u6b8b\u5dee\u5411\u91cf\u91cf\u5316\u7684\u7b2c\u4e00\u5c42\uff0c\u5b9e\u73b0\u4e86\u58f0\u5b66\u4fdd\u771f\u5ea6\u548c\u8bed\u4e49\u80fd\u529b\u7684\u66f4\u597d\u5e73\u8861\u3002", "motivation": "\u4f20\u7edf\u795e\u7ecf\u97f3\u9891\u7f16\u89e3\u7801\u5668\u80fd\u5f88\u597d\u5730\u4fdd\u7559\u58f0\u5b66\u7ec6\u8282\u4f46\u7f3a\u4e4f\u8bed\u4e49\u4fe1\u606f\uff0c\u800c\u73b0\u6709\u7684\u6df7\u5408\u7f16\u89e3\u7801\u5668\u901a\u8fc7\u84b8\u998f\u6574\u5408\u8bed\u4e49\u4fe1\u606f\u65f6\u5f80\u5f80\u4f1a\u964d\u4f4e\u91cd\u5efa\u6027\u80fd\uff0c\u96be\u4ee5\u540c\u65f6\u5b9e\u73b0\u58f0\u5b66\u4fdd\u771f\u548c\u8bed\u4e49\u80fd\u529b\u3002", "method": "1. \u901a\u8fc7\u8bed\u4e49\u4ee4\u724c\u5206\u914d\u5c06\u81ea\u76d1\u7763\u5b66\u4e60\u6a21\u578b\u7684\u8bed\u4e49\u4fe1\u606f\u96c6\u6210\u5230RVQ-1\u5c42\uff1b2. \u63d0\u51fa\u8bed\u4e49\u9884\u84b8\u998f\u6a21\u5757\uff0c\u5728\u63a8\u7406\u65f6\u76f4\u63a5\u9884\u6d4b\u8bed\u4e49\u4ee4\u724c\u5206\u914d\u7ed9\u7b2c\u4e00RVQ\u5c42\uff0c\u6d88\u9664\u5bf9SSL\u8bed\u4e49\u5206\u8bcd\u5668\u7684\u4f9d\u8d56\u5e76\u63d0\u9ad8\u6548\u7387\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cSTACodec\u5728\u97f3\u9891\u91cd\u5efa\u548c\u4e0b\u6e38\u8bed\u4e49\u4efb\u52a1\u4e0a\u90fd\u4f18\u4e8e\u73b0\u6709\u7684\u6df7\u5408\u7f16\u89e3\u7801\u5668\uff0c\u5728\u58f0\u5b66\u4fdd\u771f\u5ea6\u548c\u8bed\u4e49\u80fd\u529b\u4e4b\u95f4\u53d6\u5f97\u4e86\u66f4\u597d\u7684\u5e73\u8861\u3002", "conclusion": "STACodec\u6210\u529f\u89e3\u51b3\u4e86\u4f20\u7edf\u7f16\u89e3\u7801\u5668\u7f3a\u4e4f\u8bed\u4e49\u4fe1\u606f\u548c\u6df7\u5408\u7f16\u89e3\u7801\u5668\u91cd\u5efa\u6027\u80fd\u4e0b\u964d\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u58f0\u5b66\u4fdd\u771f\u4e0e\u8bed\u4e49\u80fd\u529b\u7684\u7edf\u4e00\uff0c\u4e3a\u97f3\u9891\u538b\u7f29\u548c\u57fa\u4e8e\u4ee4\u724c\u7684\u8bed\u8a00\u6a21\u578b\u96c6\u6210\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.06271", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2602.06271", "abs": "https://arxiv.org/abs/2602.06271", "authors": ["Kurumi Sashida", "Gouhei Tanaka"], "title": "Misophonia Trigger Sound Detection on Synthetic Soundscapes Using a Hybrid Model with a Frozen Pre-Trained CNN and a Time-Series Module", "comment": "13 pages, 3 figures. Submitted to IJCNN 2026", "summary": "Misophonia is a disorder characterized by a decreased tolerance to specific everyday sounds (trigger sounds) that can evoke intense negative emotional responses such as anger, panic, or anxiety. These reactions can substantially impair daily functioning and quality of life. Assistive technologies that selectively detect trigger sounds could help reduce distress and improve well-being. In this study, we investigate sound event detection (SED) to localize intervals of trigger sounds in continuous environmental audio as a foundational step toward such assistive support. Motivated by the scarcity of real-world misophonia data, we generate synthetic soundscapes tailored to misophonia trigger sound detection using audio synthesis techniques. Then, we perform trigger sound detection tasks using hybrid CNN-based models. The models combine feature extraction using a frozen pre-trained CNN backbone with a trainable time-series module such as gated recurrent units (GRUs), long short-term memories (LSTMs), echo state networks (ESNs), and their bidirectional variants. The detection performance is evaluated using common SED metrics, including Polyphonic Sound Detection Score 1 (PSDS1). On the multi-class trigger SED task, bidirectional temporal modeling consistently improves detection performance, with Bidirectional GRU (BiGRU) achieving the best overall accuracy. Notably, the Bidirectional ESN (BiESN) attains competitive performance while requiring orders of magnitude fewer trainable parameters by optimizing only the readout. We further simulate user personalization via a few-shot \"eating sound\" detection task with at most five support clips, in which BiGRU and BiESN are compared. In this strict adaptation setting, BiESN shows robust and stable performance, suggesting that lightweight temporal modules are promising for personalized misophonia trigger SED.", "AI": {"tldr": "\u7814\u7a76\u5f00\u53d1\u4e86\u9488\u5bf9\u6050\u97f3\u75c7\u89e6\u53d1\u58f0\u97f3\u7684\u5408\u6210\u97f3\u666f\u751f\u6210\u548c\u68c0\u6d4b\u7cfb\u7edf\uff0c\u4f7f\u7528\u6df7\u5408CNN\u6a21\u578b\u7ed3\u5408\u4e0d\u540c\u65f6\u5e8f\u6a21\u5757\uff0c\u53cc\u5411GRU\u8868\u73b0\u6700\u4f73\uff0c\u53cc\u5411ESN\u53c2\u6570\u66f4\u5c11\u4f46\u6027\u80fd\u63a5\u8fd1\uff0c\u5728\u4e2a\u6027\u5316\u68c0\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u7a33\u5b9a\u3002", "motivation": "\u6050\u97f3\u75c7\u60a3\u8005\u5bf9\u7279\u5b9a\u65e5\u5e38\u58f0\u97f3\uff08\u89e6\u53d1\u58f0\u97f3\uff09\u4ea7\u751f\u5f3a\u70c8\u8d1f\u9762\u60c5\u7eea\u53cd\u5e94\uff0c\u4e25\u91cd\u5f71\u54cd\u751f\u6d3b\u8d28\u91cf\u3002\u7531\u4e8e\u771f\u5b9e\u4e16\u754c\u6050\u97f3\u75c7\u6570\u636e\u7a00\u7f3a\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u9009\u62e9\u6027\u68c0\u6d4b\u89e6\u53d1\u58f0\u97f3\u7684\u8f85\u52a9\u6280\u672f\u6765\u5e2e\u52a9\u60a3\u8005\u51cf\u8f7b\u75db\u82e6\u3002", "method": "1. \u4f7f\u7528\u97f3\u9891\u5408\u6210\u6280\u672f\u751f\u6210\u9488\u5bf9\u6050\u97f3\u75c7\u89e6\u53d1\u58f0\u97f3\u68c0\u6d4b\u7684\u5408\u6210\u97f3\u666f\uff1b2. \u91c7\u7528\u6df7\u5408CNN\u6a21\u578b\uff0c\u7ed3\u5408\u51bb\u7ed3\u7684\u9884\u8bad\u7ec3CNN\u9aa8\u5e72\u7f51\u7edc\u8fdb\u884c\u7279\u5f81\u63d0\u53d6\uff0c\u4ee5\u53ca\u53ef\u8bad\u7ec3\u7684\u65f6\u95f4\u5e8f\u5217\u6a21\u5757\uff08GRU\u3001LSTM\u3001ESN\u53ca\u5176\u53cc\u5411\u53d8\u4f53\uff09\uff1b3. \u5728\u591a\u7c7b\u89e6\u53d1\u58f0\u97f3\u68c0\u6d4b\u4efb\u52a1\u4e2d\u8bc4\u4f30\u6027\u80fd\uff1b4. \u901a\u8fc7\u5c11\u91cf\u6837\u672c\u7684\"\u8fdb\u98df\u58f0\u97f3\"\u68c0\u6d4b\u4efb\u52a1\u6a21\u62df\u7528\u6237\u4e2a\u6027\u5316\u9700\u6c42\u3002", "result": "\u5728\u591a\u7c7b\u89e6\u53d1\u58f0\u97f3\u68c0\u6d4b\u4efb\u52a1\u4e2d\uff0c\u53cc\u5411\u65f6\u5e8f\u5efa\u6a21\u6301\u7eed\u63d0\u5347\u68c0\u6d4b\u6027\u80fd\uff0c\u53cc\u5411GRU\uff08BiGRU\uff09\u83b7\u5f97\u6700\u4f73\u6574\u4f53\u51c6\u786e\u7387\u3002\u53cc\u5411ESN\uff08BiESN\uff09\u4ec5\u9700\u6570\u91cf\u7ea7\u66f4\u5c11\u7684\u53ef\u8bad\u7ec3\u53c2\u6570\uff08\u4ec5\u4f18\u5316\u8bfb\u51fa\u5c42\uff09\u5373\u8fbe\u5230\u7ade\u4e89\u6027\u6027\u80fd\u3002\u5728\u4e2a\u6027\u5316\u68c0\u6d4b\u4efb\u52a1\u4e2d\uff0cBiESN\u8868\u73b0\u51fa\u7a33\u5065\u7a33\u5b9a\u7684\u6027\u80fd\u3002", "conclusion": "\u8f7b\u91cf\u7ea7\u65f6\u5e8f\u6a21\u5757\uff08\u7279\u522b\u662f\u53cc\u5411ESN\uff09\u5bf9\u4e8e\u4e2a\u6027\u5316\u7684\u6050\u97f3\u75c7\u89e6\u53d1\u58f0\u97f3\u68c0\u6d4b\u5177\u6709\u524d\u666f\uff0c\u80fd\u591f\u5728\u53c2\u6570\u6548\u7387\u9ad8\u7684\u540c\u65f6\u4fdd\u6301\u826f\u597d\u6027\u80fd\uff0c\u9002\u5408\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u4e2a\u6027\u5316\u9700\u6c42\u3002"}}
{"id": "2602.06682", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2602.06682", "abs": "https://arxiv.org/abs/2602.06682", "authors": ["Francesco Zanirato", "Alessio Curzio", "Francesco Ardizzon", "Elisa Sbalchiero", "Luca Canzian", "Stefano Tomasin", "Nicola Laurenti", "Jaron Samson"], "title": "Lightweight Pilot Estimation on LEO Satellite Signals for Enhanced SOP Navigation", "comment": "Submitted to NAVITEC 2026", "summary": "The computation of positioning, navigation and timing (PNT) via signal of opportunity (SOP), where signals originally transmitted for communication, such as 5G, Wi-Fi, or DVB-S, are exploited due to their ubiquity and spectral characteristics, is an emerging research field. However, relying on these signals presents challenges, including limited knowledge of the signal modulation and the need to identify recurring sequences for correlation. We offer a guide to implement a receiver capable of capturing broadband downlink Ku-band signals from low Earth orbit (LEO) satellites (e.g., Starlink and OneWeb) and estimating the recurring symbols for SOP measurements. The methodology integrates recent approaches in the literature, highlighting the most effective aspects while guiding the replication of experiments even under limitations on the front-end gain and bandwidth. Using the proposed model, we can identify recurring symbols transmitted by Starlink satellites, which are then used to collect Doppler shift measurements over a 600 s interval. A position, velocity, and time (PVT) solution is also computed via least squares (LS), which achieves a positioning error of approximately 268 m after a post-fit refinement.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u4f4e\u5730\u7403\u8f68\u9053\u536b\u661f\uff08\u5982Starlink\u548cOneWeb\uff09\u7684\u5bbd\u5e26\u4e0b\u884cKu\u6ce2\u6bb5\u4fe1\u53f7\u4f5c\u4e3a\u673a\u4f1a\u4fe1\u53f7\u8fdb\u884c\u5b9a\u4f4d\u3001\u5bfc\u822a\u548c\u5b9a\u65f6\u6d4b\u91cf\u7684\u63a5\u6536\u673a\u5b9e\u73b0\u6307\u5357\uff0c\u80fd\u591f\u8bc6\u522b\u91cd\u590d\u7b26\u53f7\u5e76\u8ba1\u7b97\u4f4d\u7f6e\u89e3\u3002", "motivation": "\u673a\u4f1a\u4fe1\u53f7\uff08\u59825G\u3001Wi-Fi\u3001DVB-S\uff09\u56e0\u5176\u666e\u904d\u6027\u548c\u9891\u8c31\u7279\u6027\u88ab\u7528\u4e8ePNT\u8ba1\u7b97\uff0c\u4f46\u9762\u4e34\u4fe1\u53f7\u8c03\u5236\u77e5\u8bc6\u6709\u9650\u548c\u9700\u8981\u8bc6\u522b\u91cd\u590d\u5e8f\u5217\u8fdb\u884c\u76f8\u5173\u5904\u7406\u7684\u6311\u6218\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u5229\u7528LEO\u536b\u661f\u5bbd\u5e26\u4e0b\u884c\u4fe1\u53f7\u8fdb\u884cSOP\u6d4b\u91cf\u7684\u5b9e\u73b0\u95ee\u9898\u3002", "method": "\u6574\u5408\u6587\u732e\u4e2d\u7684\u6700\u65b0\u65b9\u6cd5\uff0c\u63d0\u51fa\u63a5\u6536\u673a\u5b9e\u73b0\u6307\u5357\uff0c\u80fd\u591f\u5728\u524d\u7aef\u589e\u76ca\u548c\u5e26\u5bbd\u53d7\u9650\u6761\u4ef6\u4e0b\u6355\u83b7LEO\u536b\u661f\u7684\u5bbd\u5e26\u4e0b\u884cKu\u6ce2\u6bb5\u4fe1\u53f7\uff0c\u8bc6\u522b\u4f20\u8f93\u4e2d\u7684\u91cd\u590d\u7b26\u53f7\uff0c\u5e76\u5229\u7528\u8fd9\u4e9b\u7b26\u53f7\u6536\u96c6\u591a\u666e\u52d2\u9891\u79fb\u6d4b\u91cf\u503c\u3002", "result": "\u4f7f\u7528\u6240\u63d0\u6a21\u578b\u6210\u529f\u8bc6\u522b\u4e86Starlink\u536b\u661f\u4f20\u8f93\u7684\u91cd\u590d\u7b26\u53f7\uff0c\u5e76\u5728600\u79d2\u65f6\u95f4\u95f4\u9694\u5185\u6536\u96c6\u4e86\u591a\u666e\u52d2\u9891\u79fb\u6d4b\u91cf\u503c\u3002\u901a\u8fc7\u6700\u5c0f\u4e8c\u4e58\u6cd5\u8ba1\u7b97\u7684\u4f4d\u7f6e\u3001\u901f\u5ea6\u548c\u65f6\u95f4\u89e3\uff0c\u7ecf\u8fc7\u540e\u62df\u5408\u7ec6\u5316\u540e\u5b9e\u73b0\u4e86\u7ea6268\u7c73\u7684\u5b9a\u4f4d\u8bef\u5dee\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5229\u7528LEO\u536b\u661f\u673a\u4f1a\u4fe1\u53f7\u8fdb\u884cPNT\u6d4b\u91cf\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u5b9e\u73b0\u65b9\u6848\uff0c\u5373\u4f7f\u5728\u786c\u4ef6\u9650\u5236\u6761\u4ef6\u4e0b\u4e5f\u80fd\u6709\u6548\u5de5\u4f5c\uff0c\u5c55\u793a\u4e86\u5229\u7528\u5546\u4e1a\u901a\u4fe1\u4fe1\u53f7\u8fdb\u884c\u5bfc\u822a\u5b9a\u4f4d\u7684\u6f5c\u529b\u3002"}}
{"id": "2602.06213", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2602.06213", "abs": "https://arxiv.org/abs/2602.06213", "authors": ["Jayeon Yi", "Minje Kim"], "title": "From Hallucination to Articulation: Language Model-Driven Losses for Ultra Low-Bitrate Neural Speech Coding", "comment": "To appear in ICASSP 2026. Demo wavs, code, and checkpoints (currently) availble at https://github.com/stet-stet/lmloss-icassp2026", "summary": "``Phoneme Hallucinations (PH)'' commonly occur in low-bitrate DNN-based codecs. It is the generative decoder's attempt to synthesize plausible outputs from excessively compressed tokens missing some semantic information. In this work, we propose language model-driven losses (LM loss) and show they may alleviate PHs better than a semantic distillation (SD) objective in very-low-bitrate settings. The proposed LM losses build upon language models pretrained to associate speech with text. When ground-truth transcripts are unavailable, we propose to modify a popular automatic speech recognition (ASR) model, Whisper, to compare the decoded utterance against the ASR-inferred transcriptions of the input speech. Else, we propose to use the timed-text regularizer (TTR) to compare WavLM representations of the decoded utterance against BERT representations of the ground-truth transcriptions. We test and compare LM losses against an SD objective, using a reference codec whose three-stage training regimen was designed after several popular codecs. Subjective and objective evaluations conclude that LM losses may provide stronger guidance to extract semantic information from self-supervised speech representations, boosting human-perceived semantic adherence while preserving overall output quality. Demo samples, code, and checkpoints are available online.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4f7f\u7528\u8bed\u8a00\u6a21\u578b\u9a71\u52a8\u7684\u635f\u5931\u51fd\u6570\u6765\u7f13\u89e3\u4f4e\u6bd4\u7279\u7387\u8bed\u97f3\u7f16\u89e3\u7801\u5668\u4e2d\u7684\u97f3\u7d20\u5e7b\u89c9\u95ee\u9898\uff0c\u76f8\u6bd4\u8bed\u4e49\u84b8\u998f\u76ee\u6807\u5728\u6781\u4f4e\u6bd4\u7279\u7387\u4e0b\u6548\u679c\u66f4\u597d\u3002", "motivation": "\u4f4e\u6bd4\u7279\u7387DNN\u7f16\u89e3\u7801\u5668\u4e2d\u5e38\u51fa\u73b0\"\u97f3\u7d20\u5e7b\u89c9\"\u73b0\u8c61\uff0c\u8fd9\u662f\u751f\u6210\u5f0f\u89e3\u7801\u5668\u5728\u8bed\u4e49\u4fe1\u606f\u8fc7\u5ea6\u538b\u7f29\u4e22\u5931\u65f6\u5c1d\u8bd5\u5408\u6210\u5408\u7406\u8f93\u51fa\u7684\u7ed3\u679c\u3002\u9700\u8981\u66f4\u597d\u7684\u65b9\u6cd5\u6765\u7f13\u89e3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u8bed\u8a00\u6a21\u578b\u9a71\u52a8\u7684\u635f\u5931\u51fd\u6570\uff1a1\uff09\u5f53\u65e0\u771f\u5b9e\u6587\u672c\u65f6\uff0c\u4fee\u6539Whisper ASR\u6a21\u578b\u6bd4\u8f83\u89e3\u7801\u8bed\u97f3\u4e0eASR\u63a8\u65ad\u7684\u6587\u672c\uff1b2\uff09\u4f7f\u7528\u5b9a\u65f6\u6587\u672c\u6b63\u5219\u5316\u5668\u6bd4\u8f83\u89e3\u7801\u8bed\u97f3\u7684WavLM\u8868\u793a\u4e0e\u771f\u5b9e\u6587\u672c\u7684BERT\u8868\u793a\u3002\u8fd9\u4e9b\u65b9\u6cd5\u57fa\u4e8e\u9884\u8bad\u7ec3\u7684\u8bed\u8a00\u6a21\u578b\u6765\u5173\u8054\u8bed\u97f3\u4e0e\u6587\u672c\u3002", "result": "\u4e3b\u89c2\u548c\u5ba2\u89c2\u8bc4\u4f30\u8868\u660e\uff0c\u8bed\u8a00\u6a21\u578b\u635f\u5931\u6bd4\u8bed\u4e49\u84b8\u998f\u76ee\u6807\u80fd\u63d0\u4f9b\u66f4\u5f3a\u7684\u6307\u5bfc\uff0c\u4ece\u81ea\u76d1\u7763\u8bed\u97f3\u8868\u793a\u4e2d\u63d0\u53d6\u8bed\u4e49\u4fe1\u606f\uff0c\u63d0\u5347\u4eba\u7c7b\u611f\u77e5\u7684\u8bed\u4e49\u4e00\u81f4\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u6574\u4f53\u8f93\u51fa\u8d28\u91cf\u3002", "conclusion": "\u8bed\u8a00\u6a21\u578b\u9a71\u52a8\u7684\u635f\u5931\u51fd\u6570\u5728\u6781\u4f4e\u6bd4\u7279\u7387\u8bbe\u7f6e\u4e0b\u80fd\u6709\u6548\u7f13\u89e3\u97f3\u7d20\u5e7b\u89c9\u95ee\u9898\uff0c\u6bd4\u8bed\u4e49\u84b8\u998f\u65b9\u6cd5\u66f4\u6709\u6548\uff0c\u589e\u5f3a\u4e86\u8bed\u4e49\u4fe1\u606f\u7684\u63d0\u53d6\u80fd\u529b\u3002"}}
{"id": "2602.06460", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2602.06460", "abs": "https://arxiv.org/abs/2602.06460", "authors": ["Injune Hwang", "Jaejun Lee", "Kyogu Lee"], "title": "EMG-to-Speech with Fewer Channels", "comment": null, "summary": "Surface electromyography (EMG) is a promising modality for silent speech interfaces, but its effectiveness depends heavily on sensor placement and channel availability. In this work, we investigate the contribution of individual and combined EMG channels to speech reconstruction performance. Our findings reveal that while certain EMG channels are individually more informative, the highest performance arises from subsets that leverage complementary relationships among channels. We also analyzed phoneme classification accuracy under channel ablations and observed interpretable patterns reflecting the anatomical roles of the underlying muscles. To address performance degradation from channel reduction, we pretrained models on full 8-channel data using random channel dropout and fine-tuned them on reduced-channel subsets. Fine-tuning consistently outperformed training from scratch for 4 - 6 channel settings, with the best dropout strategy depending on the number of channels. These results suggest that performance degradation from sensor reduction can be mitigated through pretraining and channel-aware design, supporting the development of lightweight and practical EMG-based silent speech systems.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u8868\u9762\u808c\u7535\u56fe\uff08EMG\uff09\u901a\u9053\u5bf9\u65e0\u58f0\u8bed\u97f3\u91cd\u5efa\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u4e92\u8865\u901a\u9053\u7ec4\u5408\u6027\u80fd\u6700\u4f73\uff0c\u5e76\u63d0\u51fa\u901a\u8fc7\u9884\u8bad\u7ec3\u548c\u901a\u9053\u611f\u77e5\u8bbe\u8ba1\u7f13\u89e3\u4f20\u611f\u5668\u51cf\u5c11\u5e26\u6765\u7684\u6027\u80fd\u4e0b\u964d\u3002", "motivation": "\u8868\u9762\u808c\u7535\u56fe\uff08EMG\uff09\u662f\u65e0\u58f0\u8bed\u97f3\u63a5\u53e3\u7684\u6709\u524d\u666f\u7684\u6a21\u6001\uff0c\u4f46\u5176\u6548\u679c\u4e25\u91cd\u4f9d\u8d56\u4e8e\u4f20\u611f\u5668\u653e\u7f6e\u548c\u901a\u9053\u53ef\u7528\u6027\u3002\u7814\u7a76\u65e8\u5728\u4e86\u89e3\u5355\u4e2a\u548c\u7ec4\u5408EMG\u901a\u9053\u5bf9\u8bed\u97f3\u91cd\u5efa\u6027\u80fd\u7684\u8d21\u732e\uff0c\u4ee5\u652f\u6301\u8f7b\u91cf\u7ea7\u5b9e\u7528EMG\u65e0\u58f0\u8bed\u97f3\u7cfb\u7edf\u7684\u5f00\u53d1\u3002", "method": "\u7814\u7a76\u5206\u6790\u4e86\u5355\u4e2a\u548c\u7ec4\u5408EMG\u901a\u9053\u5bf9\u8bed\u97f3\u91cd\u5efa\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u5305\u62ec\u97f3\u7d20\u5206\u7c7b\u51c6\u786e\u6027\u5728\u901a\u9053\u6d88\u878d\u4e0b\u7684\u8868\u73b0\u3002\u4e3a\u5e94\u5bf9\u901a\u9053\u51cf\u5c11\u5bfc\u81f4\u7684\u6027\u80fd\u4e0b\u964d\uff0c\u91c7\u7528\u5728\u5b8c\u65748\u901a\u9053\u6570\u636e\u4e0a\u9884\u8bad\u7ec3\u6a21\u578b\uff08\u4f7f\u7528\u968f\u673a\u901a\u9053\u4e22\u5f03\uff09\u5e76\u5728\u51cf\u5c11\u901a\u9053\u5b50\u96c6\u4e0a\u5fae\u8c03\u7684\u65b9\u6cd5\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u67d0\u4e9bEMG\u901a\u9053\u5355\u72ec\u66f4\u5177\u4fe1\u606f\u6027\uff0c\u4f46\u6700\u9ad8\u6027\u80fd\u6765\u81ea\u5229\u7528\u901a\u9053\u95f4\u4e92\u8865\u5173\u7cfb\u7684\u5b50\u96c6\u3002\u97f3\u7d20\u5206\u7c7b\u51c6\u786e\u6027\u5728\u901a\u9053\u6d88\u878d\u4e0b\u663e\u793a\u51fa\u53cd\u6620\u5e95\u5c42\u808c\u8089\u89e3\u5256\u4f5c\u7528\u7684\u53ef\u89e3\u91ca\u6a21\u5f0f\u3002\u5fae\u8c03\u57284-6\u901a\u9053\u8bbe\u7f6e\u4e2d\u59cb\u7ec8\u4f18\u4e8e\u4ece\u5934\u8bad\u7ec3\uff0c\u6700\u4f73\u4e22\u5f03\u7b56\u7565\u53d6\u51b3\u4e8e\u901a\u9053\u6570\u91cf\u3002", "conclusion": "\u901a\u8fc7\u9884\u8bad\u7ec3\u548c\u901a\u9053\u611f\u77e5\u8bbe\u8ba1\u53ef\u4ee5\u7f13\u89e3\u4f20\u611f\u5668\u51cf\u5c11\u5e26\u6765\u7684\u6027\u80fd\u4e0b\u964d\uff0c\u8fd9\u652f\u6301\u4e86\u8f7b\u91cf\u7ea7\u5b9e\u7528EMG\u65e0\u58f0\u8bed\u97f3\u7cfb\u7edf\u7684\u5f00\u53d1\u3002\u7814\u7a76\u7ed3\u679c\u4e3a\u4f18\u5316EMG\u4f20\u611f\u5668\u914d\u7f6e\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002"}}
{"id": "2602.06755", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2602.06755", "abs": "https://arxiv.org/abs/2602.06755", "authors": ["Anton Tishchenko", "Demos Serghiou", "Hamidreza Taghvaee", "Arman Shojaeifard", "Ahmed Elzanaty", "Gabriele Gradoni", "Mohsen Khalily", "Rahim Tafazolli"], "title": "Multi-Functional RIS-enabled Radar and Communication Coexistence: Channel Modeling and a Sub-6 GHz Indoor Measurement Campaign", "comment": null, "summary": "In this work, we analyze a multi-functional reconfigurable intelligent surface (MF-RIS)-enabled radar and communication coexistence (RCC) system, detailing the key aspects of its phase synthesis codebook generation and the implemented localization algorithm for real-time user tracking based on density-based spatial clustering of applications with noise (DBSCAN), which features a Kalman filter for the prediction of user mobility. We derived a 3GPP-compatible radar cross-section (RCS) and re-radiation pattern-based channel model for the described MF-RIS system, supplementing it with channel measurements. We obtained large and small-scale characteristics, including path loss, shadow fading, Rician K-factor, cluster powers, and RMS delay spread. The study finds that Sub-6 GHz indoor propagation is largely free of blind spots, even with a blocked line-of-sight (LoS) path. Therefore, the proposed channel model includes non-line-of-sight (NLoS) paths, including the ones created by the MF-RIS. We also performed an experimental evaluation of the channel throughput in a fifth generation (5G) new radio (NR) single user multiple-input-multiple-output (SU-MIMO) system, reporting a 74\\% reduction in throughput variance and a 12.5\\% sum-rate improvement within the MF-RIS near-field compared to the no-RIS setup. This result shows that the MF-RIS can minimize delay spread and increase the coherence bandwidth by creating virtual-LoS (vLoS) path for the moving user, thereby effectively hardening wireless MIMO channels.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u529f\u80fd\u53ef\u91cd\u6784\u667a\u80fd\u8868\u9762\uff08MF-RIS\uff09\u652f\u6301\u7684\u96f7\u8fbe\u901a\u4fe1\u5171\u5b58\u7cfb\u7edf\uff0c\u901a\u8fc7DBSCAN\u805a\u7c7b\u548c\u5361\u5c14\u66fc\u6ee4\u6ce2\u5b9e\u73b0\u5b9e\u65f6\u7528\u6237\u8ddf\u8e2a\uff0c\u5efa\u7acb\u4e863GPP\u517c\u5bb9\u7684\u4fe1\u9053\u6a21\u578b\uff0c\u5b9e\u9a8c\u663e\u793a\u57285G NR SU-MIMO\u7cfb\u7edf\u4e2d\u541e\u5410\u91cf\u65b9\u5dee\u964d\u4f4e74%\uff0c\u548c\u901f\u7387\u63d0\u534712.5%\u3002", "motivation": "\u7814\u7a76\u591a\u529f\u80fd\u53ef\u91cd\u6784\u667a\u80fd\u8868\u9762\u5728\u96f7\u8fbe\u901a\u4fe1\u5171\u5b58\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\uff0c\u89e3\u51b3\u5ba4\u5185Sub-6 GHz\u4f20\u64ad\u4e2d\u7684\u76f2\u70b9\u95ee\u9898\uff0c\u901a\u8fc7\u521b\u5efa\u865a\u62df\u89c6\u8ddd\u8def\u5f84\u6765\u786c\u5316\u65e0\u7ebfMIMO\u4fe1\u9053\uff0c\u63d0\u9ad8\u7cfb\u7edf\u6027\u80fd\u3002", "method": "1) \u8bbe\u8ba1MF-RIS\u652f\u6301\u7684RCC\u7cfb\u7edf\u67b6\u6784\uff1b2) \u5f00\u53d1\u76f8\u4f4d\u5408\u6210\u7801\u672c\u751f\u6210\u65b9\u6cd5\uff1b3) \u57fa\u4e8eDBSCAN\u805a\u7c7b\u548c\u5361\u5c14\u66fc\u6ee4\u6ce2\u5b9e\u73b0\u5b9e\u65f6\u7528\u6237\u8ddf\u8e2a\u5b9a\u4f4d\u7b97\u6cd5\uff1b4) \u63a8\u5bfc3GPP\u517c\u5bb9\u7684RCS\u548c\u518d\u8f90\u5c04\u6a21\u5f0f\u4fe1\u9053\u6a21\u578b\uff1b5) \u8fdb\u884c\u4fe1\u9053\u6d4b\u91cf\u83b7\u53d6\u5927\u5c3a\u5ea6\u548c\u5c0f\u5c3a\u5ea6\u7279\u6027\u53c2\u6570\u3002", "result": "1) Sub-6 GHz\u5ba4\u5185\u4f20\u64ad\u57fa\u672c\u65e0\u76f2\u70b9\uff0c\u5373\u4f7f\u89c6\u8ddd\u8def\u5f84\u88ab\u906e\u6321\uff1b2) \u57285G NR SU-MIMO\u7cfb\u7edf\u4e2d\uff0cMF-RIS\u8fd1\u573a\u533a\u57df\u76f8\u6bd4\u65e0RIS\u8bbe\u7f6e\uff0c\u541e\u5410\u91cf\u65b9\u5dee\u964d\u4f4e74%\uff0c\u548c\u901f\u7387\u63d0\u534712.5%\uff1b3) MF-RIS\u80fd\u51cf\u5c11\u5ef6\u8fdf\u6269\u5c55\uff0c\u589e\u52a0\u76f8\u5e72\u5e26\u5bbd\uff0c\u901a\u8fc7\u4e3a\u79fb\u52a8\u7528\u6237\u521b\u5efa\u865a\u62df\u89c6\u8ddd\u8def\u5f84\u6709\u6548\u786c\u5316\u65e0\u7ebfMIMO\u4fe1\u9053\u3002", "conclusion": "MF-RIS\u5728\u96f7\u8fbe\u901a\u4fe1\u5171\u5b58\u7cfb\u7edf\u4e2d\u80fd\u6709\u6548\u6539\u5584\u4fe1\u9053\u7279\u6027\uff0c\u51cf\u5c11\u76f2\u533a\uff0c\u63d0\u9ad8\u7cfb\u7edf\u541e\u5410\u91cf\u548c\u7a33\u5b9a\u6027\uff0c\u4e3a\u672a\u6765\u65e0\u7ebf\u901a\u4fe1\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u6280\u672f\u65b9\u6848\u3002"}}
{"id": "2602.06290", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2602.06290", "abs": "https://arxiv.org/abs/2602.06290", "authors": ["Yingying Gao", "Shilei Zhang", "Runyan Yang", "Zihao Cui", "Junlan Feng"], "title": "B-GRPO: Unsupervised Speech Emotion Recognition based on Batched-Group Relative Policy Optimization", "comment": "Accepted by ICASSP2026", "summary": "Unsupervised speech emotion recognition (SER) focuses on addressing the problem of data sparsity and annotation bias of emotional speech. Reinforcement learning (RL) is a promising method which enhances the performance through rule-based or model-based verification functions rather than human annotations. We treat the sample selection during the learning process as a long-term procedure and whether to select a sample as the action to make policy, thus achieving the application of RL to measure sample quality in SER. We propose a modified Group Relative Policy Optimization (GRPO) to adapt it to classification problems, which takes the samples in a batch as a group and uses the average reward of these samples as the baseline to calculate the advantage. And rather than using a verifiable reward function as in GRPO, we put forward self-reward functions and teacher-reward functions to encourage the model to produce high-confidence outputs. Experiments indicate that the proposed method improves the performance of baseline without RL by 19.8%.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u65e0\u76d1\u7763\u8bed\u97f3\u60c5\u611f\u8bc6\u522b\u65b9\u6cd5\uff0c\u901a\u8fc7\u6539\u8fdb\u7684GRPO\u7b97\u6cd5\u548c\u81ea\u5956\u52b1/\u6559\u5e08\u5956\u52b1\u51fd\u6570\u6765\u8bc4\u4f30\u6837\u672c\u8d28\u91cf\uff0c\u63d0\u5347\u6a21\u578b\u6027\u80fd19.8%", "motivation": "\u89e3\u51b3\u8bed\u97f3\u60c5\u611f\u8bc6\u522b\u4e2d\u6570\u636e\u7a00\u758f\u548c\u6807\u6ce8\u504f\u5dee\u7684\u95ee\u9898\uff0c\u4f20\u7edf\u76d1\u7763\u65b9\u6cd5\u4f9d\u8d56\u4eba\u5de5\u6807\u6ce8\uff0c\u6210\u672c\u9ad8\u4e14\u5b58\u5728\u504f\u5dee\uff0c\u9700\u8981\u65e0\u76d1\u7763\u6216\u5f31\u76d1\u7763\u7684\u89e3\u51b3\u65b9\u6848", "method": "\u5c06\u6837\u672c\u9009\u62e9\u8fc7\u7a0b\u5efa\u6a21\u4e3a\u957f\u671f\u51b3\u7b56\u95ee\u9898\uff0c\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u8bc4\u4f30\u6837\u672c\u8d28\u91cf\uff1b\u63d0\u51fa\u6539\u8fdb\u7684GRPO\u7b97\u6cd5\u9002\u5e94\u5206\u7c7b\u4efb\u52a1\uff0c\u91c7\u7528\u6279\u6b21\u6837\u672c\u4f5c\u4e3a\u7ec4\u8ba1\u7b97\u4f18\u52bf\uff1b\u8bbe\u8ba1\u81ea\u5956\u52b1\u51fd\u6570\u548c\u6559\u5e08\u5956\u52b1\u51fd\u6570\u9f13\u52b1\u6a21\u578b\u4ea7\u751f\u9ad8\u7f6e\u4fe1\u5ea6\u8f93\u51fa", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u63d0\u51fa\u7684\u65b9\u6cd5\u76f8\u6bd4\u65e0\u5f3a\u5316\u5b66\u4e60\u7684\u57fa\u7ebf\u6a21\u578b\uff0c\u6027\u80fd\u63d0\u5347\u4e8619.8%", "conclusion": "\u5f3a\u5316\u5b66\u4e60\u53ef\u4ee5\u6709\u6548\u5e94\u7528\u4e8e\u65e0\u76d1\u7763\u8bed\u97f3\u60c5\u611f\u8bc6\u522b\uff0c\u901a\u8fc7\u5408\u7406\u7684\u5956\u52b1\u51fd\u6570\u8bbe\u8ba1\u548cGRPO\u6539\u8fdb\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u4e3a\u89e3\u51b3\u6570\u636e\u6807\u6ce8\u95ee\u9898\u63d0\u4f9b\u65b0\u601d\u8def"}}
{"id": "2602.06602", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2602.06602", "abs": "https://arxiv.org/abs/2602.06602", "authors": ["Yuancheng Wang", "Zhenyu Tang", "Yun Wang", "Arthur Hinsvark", "Yingru Liu", "Yinghao Li", "Kainan Peng", "Junyi Ao", "Mingbo Ma", "Mike Seltzer", "Qing He", "Xubo Liu"], "title": "Scaling Speech Tokenizers with Diffusion Autoencoders", "comment": "ICLR 2026", "summary": "Speech tokenizers are foundational to speech language models, yet existing approaches face two major challenges: (1) balancing trade-offs between encoding semantics for understanding and acoustics for reconstruction, and (2) achieving low bit rates and low token rates. We propose Speech Diffusion Tokenizer (SiTok), a diffusion autoencoder that jointly learns semantic-rich representations through supervised learning and enables high-fidelity audio reconstruction with diffusion. We scale SiTok to 1.6B parameters and train it on 2 million hours of speech. Experiments show that SiTok outperforms strong baselines on understanding, reconstruction and generation tasks, at an extremely low token rate of $12.5$ Hz and a bit-rate of 200 bits-per-second.", "AI": {"tldr": "Speech Diffusion Tokenizer (SiTok) \u662f\u4e00\u79cd\u6269\u6563\u81ea\u7f16\u7801\u5668\uff0c\u901a\u8fc7\u76d1\u7763\u5b66\u4e60\u8054\u5408\u5b66\u4e60\u8bed\u4e49\u4e30\u5bcc\u7684\u8868\u793a\uff0c\u540c\u65f6\u5229\u7528\u6269\u6563\u5b9e\u73b0\u9ad8\u4fdd\u771f\u97f3\u9891\u91cd\u5efa\uff0c\u5728\u6781\u4f4e\u768412.5Hz\u4ee4\u724c\u7387\u548c200bps\u6bd4\u7279\u7387\u4e0b\uff0c\u5728\u7406\u89e3\u3001\u91cd\u5efa\u548c\u751f\u6210\u4efb\u52a1\u4e0a\u8d85\u8d8a\u73b0\u6709\u57fa\u7ebf\u3002", "motivation": "\u73b0\u6709\u8bed\u97f3\u5206\u8bcd\u5668\u9762\u4e34\u4e24\u5927\u6311\u6218\uff1a1\uff09\u5728\u7f16\u7801\u8bed\u4e49\uff08\u7528\u4e8e\u7406\u89e3\uff09\u548c\u58f0\u5b66\uff08\u7528\u4e8e\u91cd\u5efa\uff09\u4e4b\u95f4\u7684\u6743\u8861\uff1b2\uff09\u5b9e\u73b0\u4f4e\u6bd4\u7279\u7387\u548c\u4f4e\u4ee4\u724c\u7387\u3002\u9700\u8981\u4e00\u79cd\u80fd\u540c\u65f6\u89e3\u51b3\u8fd9\u4e24\u4e2a\u95ee\u9898\u7684\u8bed\u97f3\u5206\u8bcd\u65b9\u6cd5\u3002", "method": "\u63d0\u51faSpeech Diffusion Tokenizer (SiTok)\uff0c\u4e00\u79cd\u6269\u6563\u81ea\u7f16\u7801\u5668\uff0c\u901a\u8fc7\u76d1\u7763\u5b66\u4e60\u8054\u5408\u5b66\u4e60\u8bed\u4e49\u4e30\u5bcc\u7684\u8868\u793a\uff0c\u5e76\u5229\u7528\u6269\u6563\u5b9e\u73b0\u9ad8\u4fdd\u771f\u97f3\u9891\u91cd\u5efa\u3002\u5c06\u6a21\u578b\u6269\u5c55\u523016\u4ebf\u53c2\u6570\uff0c\u5728200\u4e07\u5c0f\u65f6\u7684\u8bed\u97f3\u6570\u636e\u4e0a\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "SiTok\u5728\u7406\u89e3\u3001\u91cd\u5efa\u548c\u751f\u6210\u4efb\u52a1\u4e0a\u5747\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u6a21\u578b\uff0c\u540c\u65f6\u5b9e\u73b0\u4e86\u6781\u4f4e\u768412.5Hz\u4ee4\u724c\u7387\u548c200bps\u6bd4\u7279\u7387\uff0c\u5728\u8bed\u4e49\u7f16\u7801\u548c\u58f0\u5b66\u91cd\u5efa\u4e4b\u95f4\u53d6\u5f97\u4e86\u826f\u597d\u5e73\u8861\u3002", "conclusion": "SiTok\u901a\u8fc7\u6269\u6563\u81ea\u7f16\u7801\u5668\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u8bed\u97f3\u5206\u8bcd\u5668\u5728\u8bed\u4e49-\u58f0\u5b66\u6743\u8861\u548c\u4f4e\u901f\u7387\u65b9\u9762\u7684\u6311\u6218\uff0c\u4e3a\u8bed\u97f3\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u7684\u8bed\u97f3\u8868\u793a\u57fa\u7840\u3002"}}
{"id": "2602.06816", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2602.06816", "abs": "https://arxiv.org/abs/2602.06816", "authors": ["Corentin Fonteneau"], "title": "On the Design of an Optimal Multi-Tone Jammer Against the Wiener Interpolation Filter", "comment": null, "summary": "In the context of civilian and military communications, anti-jamming techniques are essential to ensure information integrity in the presence of malicious interference. A conventional time-domain approach relies on computing the Wiener interpolation filter to estimate and suppress the jamming waveform from the received samples. It is widely acknowledged that this method is effective for protecting wideband systems against narrowband interference. In this work, this paradigm is questioned through the design of a $K$-tone jamming waveform that is intrinsically difficult to estimate assuming a $L$-tap Wiener interpolation filter. This design relies on an optimization procedure that maximizes the analytical Bayesian mean squared error associated with the jamming waveform estimate. Additionally, an analytical proof is provided showing that a multi-tone jamming waveform composed of $L/2+1$ tones is sufficient to render the Wiener-filter-based anti-jamming module completely ineffective. The analytical results are validated through Monte Carlo simulations assuming both perfect knowledge and practical estimates of the correlation functions of the received signal.", "AI": {"tldr": "\u8bba\u6587\u8d28\u7591\u4f20\u7edfWiener\u63d2\u503c\u6ee4\u6ce2\u5668\u6297\u5e72\u6270\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u8bbe\u8ba1\u4e86\u4e00\u79cdK\u97f3\u5e72\u6270\u6ce2\u5f62\uff0c\u901a\u8fc7\u4f18\u5316\u4f7f\u5e72\u6270\u96be\u4ee5\u88ab\u4f30\u8ba1\uff0c\u8bc1\u660eL/2+1\u4e2a\u97f3\u8c03\u5373\u53ef\u4f7fWiener\u6ee4\u6ce2\u5668\u6297\u5e72\u6270\u6a21\u5757\u5b8c\u5168\u5931\u6548\u3002", "motivation": "\u5728\u519b\u6c11\u901a\u4fe1\u4e2d\uff0c\u6297\u5e72\u6270\u6280\u672f\u5bf9\u4fdd\u969c\u4fe1\u606f\u5b8c\u6574\u6027\u81f3\u5173\u91cd\u8981\u3002\u4f20\u7edf\u65f6\u57df\u65b9\u6cd5\u4f9d\u8d56Wiener\u63d2\u503c\u6ee4\u6ce2\u5668\u4f30\u8ba1\u548c\u6291\u5236\u5e72\u6270\u6ce2\u5f62\uff0c\u5e7f\u6cdb\u8ba4\u4e3a\u5bf9\u5bbd\u5e26\u7cfb\u7edf\u6297\u7a84\u5e26\u5e72\u6270\u6709\u6548\u3002\u672c\u6587\u8d28\u7591\u8fd9\u4e00\u8303\u5f0f\uff0c\u65e8\u5728\u8bbe\u8ba1\u4e00\u79cd\u96be\u4ee5\u88abWiener\u6ee4\u6ce2\u5668\u4f30\u8ba1\u7684\u5e72\u6270\u6ce2\u5f62\u3002", "method": "\u8bbe\u8ba1K\u97f3\u5e72\u6270\u6ce2\u5f62\uff0c\u901a\u8fc7\u4f18\u5316\u7a0b\u5e8f\u6700\u5927\u5316\u5e72\u6270\u6ce2\u5f62\u4f30\u8ba1\u7684\u8d1d\u53f6\u65af\u5747\u65b9\u8bef\u5dee\u3002\u63d0\u4f9b\u7406\u8bba\u8bc1\u660e\uff1a\u7531L/2+1\u4e2a\u97f3\u8c03\u7ec4\u6210\u7684\u591a\u97f3\u5e72\u6270\u6ce2\u5f62\u8db3\u4ee5\u4f7f\u57fa\u4e8eWiener\u6ee4\u6ce2\u5668\u7684\u6297\u5e72\u6270\u6a21\u5757\u5b8c\u5168\u5931\u6548\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660e\uff0c\u7cbe\u5fc3\u8bbe\u8ba1\u7684K\u97f3\u5e72\u6270\u6ce2\u5f62\u80fd\u591f\u6709\u6548\u5bf9\u6297\u4f20\u7edfWiener\u6ee4\u6ce2\u5668\u6297\u5e72\u6270\u65b9\u6cd5\u3002\u901a\u8fc7\u8499\u7279\u5361\u6d1b\u4eff\u771f\u9a8c\u8bc1\u4e86\u7406\u8bba\u7ed3\u679c\uff0c\u5305\u62ec\u5b8c\u7f8e\u5df2\u77e5\u548c\u5b9e\u9645\u4f30\u8ba1\u63a5\u6536\u4fe1\u53f7\u76f8\u5173\u51fd\u6570\u4e24\u79cd\u60c5\u51b5\u3002", "conclusion": "\u4f20\u7edfWiener\u63d2\u503c\u6ee4\u6ce2\u5668\u6297\u5e72\u6270\u65b9\u6cd5\u5b58\u5728\u56fa\u6709\u5f31\u70b9\uff0c\u7279\u5b9a\u8bbe\u8ba1\u7684K\u97f3\u5e72\u6270\u6ce2\u5f62\u80fd\u591f\u4f7f\u5176\u5931\u6548\u3002\u8fd9\u6311\u6218\u4e86\u4f20\u7edf\u6297\u5e72\u6270\u8303\u5f0f\u7684\u6709\u6548\u6027\uff0c\u5bf9\u901a\u4fe1\u5b89\u5168\u8bbe\u8ba1\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2602.06917", "categories": ["eess.AS", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.06917", "abs": "https://arxiv.org/abs/2602.06917", "authors": ["Sumit Kumar", "Suraj Jaiswal", "Parampreet Singh", "Vipul Arora"], "title": "Automatic Detection and Analysis of Singing Mistakes for Music Pedagogy", "comment": "Under Review at Transactions of Audio Speech and Language Processing", "summary": "The advancement of machine learning in audio analysis has opened new possibilities for technology-enhanced music education. This paper introduces a framework for automatic singing mistake detection in the context of music pedagogy, supported by a newly curated dataset. The dataset comprises synchronized teacher learner vocal recordings, with annotations marking different types of mistakes made by learners. Using this dataset, we develop different deep learning models for mistake detection and benchmark them. To compare the efficacy of mistake detection systems, a new evaluation methodology is proposed. Experiments indicate that the proposed learning-based methods are superior to rule-based methods. A systematic study of errors and a cross-teacher study reveal insights into music pedagogy that can be utilised for various music applications. This work sets out new directions of research in music pedagogy. The codes and dataset are publicly available.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u81ea\u52a8\u6b4c\u5531\u9519\u8bef\u68c0\u6d4b\u6846\u67b6\uff0c\u5305\u542b\u65b0\u6807\u6ce8\u7684\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5728\u97f3\u4e50\u6559\u80b2\u4e2d\u4f18\u4e8e\u57fa\u4e8e\u89c4\u5219\u7684\u65b9\u6cd5\u3002", "motivation": "\u673a\u5668\u5b66\u4e60\u5728\u97f3\u9891\u5206\u6790\u4e2d\u7684\u53d1\u5c55\u4e3a\u6280\u672f\u589e\u5f3a\u7684\u97f3\u4e50\u6559\u80b2\u63d0\u4f9b\u4e86\u65b0\u53ef\u80fd\u3002\u5f53\u524d\u9700\u8981\u81ea\u52a8\u5316\u7684\u6b4c\u5531\u9519\u8bef\u68c0\u6d4b\u7cfb\u7edf\u6765\u8f85\u52a9\u97f3\u4e50\u6559\u5b66\u3002", "method": "1) \u6784\u5efa\u5305\u542b\u6559\u5e08-\u5b66\u4e60\u8005\u540c\u6b65\u5f55\u97f3\u7684\u65b0\u6570\u636e\u96c6\uff0c\u6807\u6ce8\u5b66\u4e60\u8005\u9519\u8bef\u7c7b\u578b\uff1b2) \u5f00\u53d1\u591a\u79cd\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u8fdb\u884c\u9519\u8bef\u68c0\u6d4b\uff1b3) \u63d0\u51fa\u65b0\u7684\u8bc4\u4f30\u65b9\u6cd5\u6bd4\u8f83\u9519\u8bef\u68c0\u6d4b\u7cfb\u7edf\u6548\u679c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u63d0\u51fa\u7684\u57fa\u4e8e\u5b66\u4e60\u7684\u65b9\u6cd5\u4f18\u4e8e\u57fa\u4e8e\u89c4\u5219\u7684\u65b9\u6cd5\u3002\u901a\u8fc7\u7cfb\u7edf\u9519\u8bef\u5206\u6790\u548c\u8de8\u6559\u5e08\u7814\u7a76\uff0c\u63ed\u793a\u4e86\u97f3\u4e50\u6559\u5b66\u6cd5\u7684\u89c1\u89e3\uff0c\u53ef\u7528\u4e8e\u5404\u79cd\u97f3\u4e50\u5e94\u7528\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u97f3\u4e50\u6559\u5b66\u6cd5\u7814\u7a76\u8bbe\u5b9a\u4e86\u65b0\u65b9\u5411\uff0c\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5df2\u516c\u5f00\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u6280\u672f\u589e\u5f3a\u7684\u97f3\u4e50\u6559\u80b2\u53d1\u5c55\u3002"}}
{"id": "2602.06765", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2602.06765", "abs": "https://arxiv.org/abs/2602.06765", "authors": ["Peng Zhang", "Qingyu Luo", "Philip J. B. Jackson", "Wenwu Wang"], "title": "Hierarchical Activity Recognition and Captioning from Long-Form Audio", "comment": "Accepted by ICASSP 2026", "summary": "Complex activities in real-world audio unfold over extended durations and exhibit hierarchical structure, yet most prior work focuses on short clips and isolated events. To bridge this gap, we introduce MultiAct, a new dataset and benchmark for multi-level structured understanding of human activities from long-form audio. MultiAct comprises long-duration kitchen recordings annotated at three semantic levels (activities, sub-activities and events) and paired with fine-grained captions and high-level summaries. We further propose a unified hierarchical model that jointly performs classification, detection, sequence prediction and multi-resolution captioning. Experiments on MultiAct establish strong baselines and reveal key challenges in modelling hierarchical and compositional structure of long-form audio. A promising direction for future work is the exploration of methods better suited to capturing the complex, long-range relationships in long-form audio.", "AI": {"tldr": "MultiAct\uff1a\u7528\u4e8e\u957f\u97f3\u9891\u591a\u7ea7\u7ed3\u6784\u5316\u7406\u89e3\u7684\u65b0\u6570\u636e\u96c6\u548c\u57fa\u51c6\uff0c\u5305\u542b\u53a8\u623f\u5f55\u97f3\u7684\u4e09\u7ea7\u8bed\u4e49\u6807\u6ce8\uff08\u6d3b\u52a8\u3001\u5b50\u6d3b\u52a8\u3001\u4e8b\u4ef6\uff09\uff0c\u5e76\u63d0\u51fa\u7edf\u4e00\u5c42\u6b21\u6a21\u578b\u8fdb\u884c\u591a\u4efb\u52a1\u5b66\u4e60\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u590d\u6742\u97f3\u9891\u6d3b\u52a8\u6301\u7eed\u65f6\u95f4\u957f\u4e14\u5177\u6709\u5c42\u6b21\u7ed3\u6784\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u5927\u591a\u5173\u6ce8\u77ed\u7247\u6bb5\u548c\u5b64\u7acb\u4e8b\u4ef6\u3002\u9700\u8981\u586b\u8865\u8fd9\u4e00\u7814\u7a76\u7a7a\u767d\uff0c\u5b9e\u73b0\u5bf9\u957f\u97f3\u9891\u7684\u591a\u5c42\u6b21\u7ed3\u6784\u5316\u7406\u89e3\u3002", "method": "\u5f15\u5165MultiAct\u6570\u636e\u96c6\uff0c\u5305\u542b\u957f\u65f6\u95f4\u53a8\u623f\u5f55\u97f3\uff0c\u6807\u6ce8\u4e86\u4e09\u4e2a\u8bed\u4e49\u5c42\u6b21\uff08\u6d3b\u52a8\u3001\u5b50\u6d3b\u52a8\u3001\u4e8b\u4ef6\uff09\uff0c\u5e76\u914d\u6709\u7ec6\u7c92\u5ea6\u63cf\u8ff0\u548c\u9ad8\u5c42\u6458\u8981\u3002\u63d0\u51fa\u7edf\u4e00\u7684\u5c42\u6b21\u6a21\u578b\uff0c\u8054\u5408\u6267\u884c\u5206\u7c7b\u3001\u68c0\u6d4b\u3001\u5e8f\u5217\u9884\u6d4b\u548c\u591a\u5206\u8fa8\u7387\u63cf\u8ff0\u751f\u6210\u3002", "result": "\u5728MultiAct\u4e0a\u5efa\u7acb\u4e86\u5f3a\u57fa\u7ebf\uff0c\u63ed\u793a\u4e86\u5efa\u6a21\u957f\u97f3\u9891\u5c42\u6b21\u548c\u7ec4\u5408\u7ed3\u6784\u7684\u5173\u952e\u6311\u6218\u3002\u5b9e\u9a8c\u8868\u660e\u9700\u8981\u66f4\u597d\u7684\u65b9\u6cd5\u6765\u6355\u6349\u957f\u97f3\u9891\u4e2d\u7684\u590d\u6742\u957f\u7a0b\u5173\u7cfb\u3002", "conclusion": "MultiAct\u4e3a\u957f\u97f3\u9891\u591a\u5c42\u6b21\u7406\u89e3\u63d0\u4f9b\u4e86\u65b0\u7684\u6570\u636e\u96c6\u548c\u57fa\u51c6\uff0c\u672a\u6765\u7814\u7a76\u65b9\u5411\u662f\u63a2\u7d22\u66f4\u9002\u5408\u6355\u6349\u957f\u97f3\u9891\u4e2d\u590d\u6742\u957f\u7a0b\u5173\u7cfb\u7684\u65b9\u6cd5\u3002"}}
{"id": "2602.06819", "categories": ["eess.SP", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.06819", "abs": "https://arxiv.org/abs/2602.06819", "authors": ["Ahsan Mehmood", "Naveed Ul Hassan", "Ghassan M. Kraidy"], "title": "Bridging 6G IoT and AI: LLM-Based Efficient Approach for Physical Layer's Optimization Tasks", "comment": "This paper is submitted to IEEE IoT Journal and is currently under review", "summary": "This paper investigates the role of large language models (LLMs) in sixth-generation (6G) Internet of Things (IoT) networks and proposes a prompt-engineering-based real-time feedback and verification (PE-RTFV) framework that perform physical-layer's optimization tasks through an iteratively process. By leveraging the naturally available closed-loop feedback inherent in wireless communication systems, PE-RTFV enables real-time physical-layer optimization without requiring model retraining. The proposed framework employs an optimization LLM (O-LLM) to generate task-specific structured prompts, which are provided to an agent LLM (A-LLM) to produce task-specific solutions. Utilizing real-time system feedback, the O-LLM iteratively refines the prompts to guide the A-LLM toward improved solutions in a gradient-descent-like optimization process. We test PE-RTFV approach on wireless-powered IoT testbed case study on user-goal-driven constellation design through semantically solving rate-energy (RE)-region optimization problem which demonstrates that PE-RTFV achieves near-genetic-algorithm performance within only a few iterations, validating its effectiveness for complex physical-layer optimization tasks in resource-constrained IoT networks.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u63d0\u793a\u5de5\u7a0b\u7684\u5b9e\u65f6\u53cd\u9988\u9a8c\u8bc1\u6846\u67b6PE-RTFV\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u57286G\u7269\u8054\u7f51\u4e2d\u5b9e\u73b0\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u7684\u7269\u7406\u5c42\u4f18\u5316", "motivation": "\u63a2\u7d22\u5927\u8bed\u8a00\u6a21\u578b\u57286G\u7269\u8054\u7f51\u7f51\u7edc\u4e2d\u7684\u4f5c\u7528\uff0c\u89e3\u51b3\u7269\u7406\u5c42\u4f18\u5316\u4efb\u52a1\u9700\u8981\u6a21\u578b\u91cd\u65b0\u8bad\u7ec3\u7684\u95ee\u9898\uff0c\u5229\u7528\u65e0\u7ebf\u901a\u4fe1\u7cfb\u7edf\u56fa\u6709\u7684\u95ed\u73af\u53cd\u9988\u5b9e\u73b0\u5b9e\u65f6\u4f18\u5316", "method": "\u63d0\u51faPE-RTFV\u6846\u67b6\uff1a\u4f7f\u7528\u4f18\u5316LLM\u751f\u6210\u4efb\u52a1\u7279\u5b9a\u7684\u7ed3\u6784\u5316\u63d0\u793a\uff0c\u63d0\u4f9b\u7ed9\u4ee3\u7406LLM\u4ea7\u751f\u89e3\u51b3\u65b9\u6848\uff1b\u5229\u7528\u5b9e\u65f6\u7cfb\u7edf\u53cd\u9988\uff0c\u4f18\u5316LLM\u8fed\u4ee3\u6539\u8fdb\u63d0\u793a\uff0c\u4ee5\u68af\u5ea6\u4e0b\u964d\u5f0f\u8fc7\u7a0b\u5f15\u5bfc\u4ee3\u7406LLM\u83b7\u5f97\u66f4\u597d\u89e3", "result": "\u5728\u65e0\u7ebf\u4f9b\u7535\u7269\u8054\u7f51\u6d4b\u8bd5\u5e73\u53f0\u4e0a\u8fdb\u884c\u7528\u6237\u76ee\u6807\u9a71\u52a8\u7684\u661f\u5ea7\u8bbe\u8ba1\u6848\u4f8b\u7814\u7a76\uff0c\u901a\u8fc7\u8bed\u4e49\u89e3\u51b3\u901f\u7387-\u80fd\u91cf\u533a\u57df\u4f18\u5316\u95ee\u9898\uff0cPE-RTFV\u5728\u4ec5\u51e0\u6b21\u8fed\u4ee3\u5185\u8fbe\u5230\u63a5\u8fd1\u9057\u4f20\u7b97\u6cd5\u7684\u6027\u80fd", "conclusion": "PE-RTFV\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u8d44\u6e90\u53d7\u9650\u7269\u8054\u7f51\u7f51\u7edc\u4e2d\u590d\u6742\u7269\u7406\u5c42\u4f18\u5316\u4efb\u52a1\uff0c\u65e0\u9700\u6a21\u578b\u91cd\u65b0\u8bad\u7ec3\uff0c\u5b9e\u73b0\u4e86\u5b9e\u65f6\u4f18\u5316"}}
{"id": "2602.06921", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2602.06921", "abs": "https://arxiv.org/abs/2602.06921", "authors": ["Klaus Linhard", "Philipp Bulling"], "title": "The Combination of Several Decorrelation Methods to Improve Acoustic Feedback Cancellation", "comment": null, "summary": "This paper extends an acoustic feedback cancellation system by incorporating multiple decorrelation methods. The baseline system is based on a frequency-domain Kalman filter implemented in a multi-delay structure. The proposed extensions include a variable time delay line, prediction, distortion compensation, and a simplified reverberation model. Each extension is analyzed, and a practical parameter range is defined.\n  While existing literature often focuses on a single extension, such as prediction, to describe an optimal system, this work demonstrates that each individual extension contributes to performance improvements. Furthermore, the combination of all proposed extensions results in a superior system. The evaluation is conducted using publicly available datasets, with performance assessed through system distance metrics and the objective speech quality measure PSEQ.", "AI": {"tldr": "\u672c\u6587\u6269\u5c55\u4e86\u57fa\u4e8e\u9891\u57df\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\u7684\u58f0\u5b66\u53cd\u9988\u6d88\u9664\u7cfb\u7edf\uff0c\u901a\u8fc7\u6574\u5408\u591a\u79cd\u53bb\u76f8\u5173\u65b9\u6cd5\uff08\u53ef\u53d8\u65f6\u5ef6\u7ebf\u3001\u9884\u6d4b\u3001\u5931\u771f\u8865\u507f\u548c\u7b80\u5316\u6df7\u54cd\u6a21\u578b\uff09\u6765\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6587\u732e\u901a\u5e38\u53ea\u5173\u6ce8\u5355\u4e00\u6269\u5c55\u65b9\u6cd5\uff08\u5982\u9884\u6d4b\uff09\u6765\u6784\u5efa\u6700\u4f18\u7cfb\u7edf\uff0c\u4f46\u672c\u6587\u8ba4\u4e3a\u6bcf\u4e2a\u6269\u5c55\u90fd\u80fd\u72ec\u7acb\u8d21\u732e\u6027\u80fd\u63d0\u5347\uff0c\u4e14\u7ec4\u5408\u6240\u6709\u6269\u5c55\u80fd\u83b7\u5f97\u66f4\u4f18\u7684\u7cfb\u7edf\u6027\u80fd\u3002", "method": "\u5728\u57fa\u4e8e\u591a\u5ef6\u8fdf\u7ed3\u6784\u7684\u9891\u57df\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\u57fa\u7840\u4e0a\uff0c\u5f15\u5165\u4e86\u56db\u79cd\u6269\u5c55\uff1a\u53ef\u53d8\u65f6\u5ef6\u7ebf\u3001\u9884\u6d4b\u3001\u5931\u771f\u8865\u507f\u548c\u7b80\u5316\u6df7\u54cd\u6a21\u578b\uff0c\u5e76\u5bf9\u6bcf\u4e2a\u6269\u5c55\u8fdb\u884c\u4e86\u5206\u6790\u5e76\u5b9a\u4e49\u4e86\u5b9e\u7528\u53c2\u6570\u8303\u56f4\u3002", "result": "\u8bc4\u4f30\u4f7f\u7528\u516c\u5f00\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u7cfb\u7edf\u8ddd\u79bb\u5ea6\u91cf\u548c\u5ba2\u89c2\u8bed\u97f3\u8d28\u91cf\u6307\u6807PSEQ\u8fdb\u884c\u6027\u80fd\u8bc4\u4f30\u3002\u7ed3\u679c\u8868\u660e\u6bcf\u4e2a\u6269\u5c55\u90fd\u80fd\u72ec\u7acb\u63d0\u5347\u6027\u80fd\uff0c\u4e14\u6240\u6709\u6269\u5c55\u7ec4\u5408\u540e\u80fd\u83b7\u5f97\u6700\u4f18\u7684\u7cfb\u7edf\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u6574\u5408\u591a\u79cd\u53bb\u76f8\u5173\u65b9\u6cd5\u6269\u5c55\u58f0\u5b66\u53cd\u9988\u6d88\u9664\u7cfb\u7edf\uff0c\u6bcf\u4e2a\u6269\u5c55\u90fd\u6709\u72ec\u7acb\u8d21\u732e\uff0c\u7ec4\u5408\u6240\u6709\u6269\u5c55\u80fd\u83b7\u5f97\u6700\u4f73\u6027\u80fd\uff0c\u4e3a\u5b9e\u9645\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u5b9e\u7528\u53c2\u6570\u6307\u5bfc\u3002"}}
{"id": "2602.06823", "categories": ["cs.SD", "cs.AI", "eess.AS", "eess.SP"], "pdf": "https://arxiv.org/pdf/2602.06823", "abs": "https://arxiv.org/abs/2602.06823", "authors": ["David Lopez-Ayala", "Asier Cabello", "Pablo Zinemanas", "Emilio Molina", "Martin Rocamora"], "title": "AI-Generated Music Detection in Broadcast Monitoring", "comment": null, "summary": "AI music generators have advanced to the point where their outputs are often indistinguishable from human compositions. While detection methods have emerged, they are typically designed and validated in music streaming contexts with clean, full-length tracks. Broadcast audio, however, poses a different challenge: music appears as short excerpts, often masked by dominant speech, conditions under which existing detectors fail. In this work, we introduce AI-OpenBMAT, the first dataset tailored to broadcast-style AI-music detection. It contains 3,294 one-minute audio excerpts (54.9 hours) that follow the duration patterns and loudness relations of real television audio, combining human-made production music with stylistically matched continuations generated with Suno v3.5. We benchmark a CNN baseline and state-of-the-art SpectTTTra models to assess SNR and duration robustness, and evaluate on a full broadcast scenario. Across all settings, models that excel in streaming scenarios suffer substantial degradation, with F1-scores dropping below 60% when music is in the background or has a short duration. These results highlight speech masking and short music length as critical open challenges for AI music detection, and position AI-OpenBMAT as a benchmark for developing detectors capable of meeting industrial broadcast requirements.", "AI": {"tldr": "AI-OpenBMAT\u662f\u9996\u4e2a\u9488\u5bf9\u5e7f\u64ad\u97f3\u9891\u573a\u666f\u7684AI\u97f3\u4e50\u68c0\u6d4b\u6570\u636e\u96c6\uff0c\u5305\u542b3294\u4e2a\u4e00\u5206\u949f\u97f3\u9891\u7247\u6bb5\uff0c\u6a21\u62df\u771f\u5b9e\u7535\u89c6\u97f3\u9891\u7684\u65f6\u957f\u6a21\u5f0f\u548c\u54cd\u5ea6\u5173\u7cfb\uff0c\u7528\u4e8e\u8bc4\u4f30AI\u97f3\u4e50\u68c0\u6d4b\u5668\u5728\u5e7f\u64ad\u73af\u5883\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709AI\u97f3\u4e50\u68c0\u6d4b\u65b9\u6cd5\u4e3b\u8981\u5728\u97f3\u4e50\u6d41\u5a92\u4f53\u573a\u666f\u4e2d\u8bbe\u8ba1\u548c\u9a8c\u8bc1\uff0c\u4f7f\u7528\u5e72\u51c0\u3001\u5b8c\u6574\u7684\u97f3\u8f68\u3002\u4f46\u5e7f\u64ad\u97f3\u9891\u4e2d\u97f3\u4e50\u901a\u5e38\u4ee5\u77ed\u7247\u6bb5\u51fa\u73b0\uff0c\u4e14\u5e38\u88ab\u4e3b\u5bfc\u8bed\u97f3\u63a9\u76d6\uff0c\u73b0\u6709\u68c0\u6d4b\u5668\u5728\u8fd9\u4e9b\u6761\u4ef6\u4e0b\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u521b\u5efaAI-OpenBMAT\u6570\u636e\u96c6\uff0c\u5305\u542b3294\u4e2a\u4e00\u5206\u949f\u97f3\u9891\u7247\u6bb5\uff0854.9\u5c0f\u65f6\uff09\uff0c\u6a21\u62df\u771f\u5b9e\u7535\u89c6\u97f3\u9891\u7684\u65f6\u957f\u6a21\u5f0f\u548c\u54cd\u5ea6\u5173\u7cfb\u3002\u6570\u636e\u96c6\u7ed3\u5408\u4eba\u5de5\u5236\u4f5c\u7684\u5236\u4f5c\u97f3\u4e50\u548c\u4f7f\u7528Suno v3.5\u751f\u6210\u7684\u98ce\u683c\u5339\u914d\u7684\u5ef6\u7eed\u97f3\u4e50\u3002\u4f7f\u7528CNN\u57fa\u7ebf\u548c\u6700\u5148\u8fdb\u7684SpectTTTra\u6a21\u578b\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc4\u4f30\u4fe1\u566a\u6bd4\u548c\u65f6\u957f\u9c81\u68d2\u6027\u3002", "result": "\u5728\u6d41\u5a92\u4f53\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u5f02\u7684\u6a21\u578b\u5728\u5e7f\u64ad\u573a\u666f\u4e2d\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u5f53\u97f3\u4e50\u5904\u4e8e\u80cc\u666f\u6216\u65f6\u957f\u8f83\u77ed\u65f6\uff0cF1\u5206\u6570\u964d\u81f360%\u4ee5\u4e0b\u3002\u8bed\u97f3\u63a9\u76d6\u548c\u77ed\u97f3\u4e50\u957f\u5ea6\u6210\u4e3aAI\u97f3\u4e50\u68c0\u6d4b\u7684\u5173\u952e\u6311\u6218\u3002", "conclusion": "AI-OpenBMAT\u4e3a\u5f00\u53d1\u6ee1\u8db3\u5de5\u4e1a\u5e7f\u64ad\u8981\u6c42\u7684\u68c0\u6d4b\u5668\u63d0\u4f9b\u4e86\u57fa\u51c6\uff0c\u7a81\u51fa\u4e86\u5e7f\u64ad\u73af\u5883\u4e2dAI\u97f3\u4e50\u68c0\u6d4b\u7684\u7279\u6b8a\u6311\u6218\uff0c\u7279\u522b\u662f\u8bed\u97f3\u63a9\u76d6\u548c\u77ed\u97f3\u4e50\u7247\u6bb5\u95ee\u9898\u3002"}}
{"id": "2602.06846", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2602.06846", "abs": "https://arxiv.org/abs/2602.06846", "authors": ["Ziyu Luo", "Lin Chen", "Qiang Qu", "Xiaoming Chen", "Yiran Shen"], "title": "DynFOA: Generating First-Order Ambisonics with Conditional Diffusion for Dynamic and Acoustically Complex 360-Degree Videos", "comment": null, "summary": "Spatial audio is crucial for creating compelling immersive 360-degree video experiences. However, generating realistic spatial audio, such as first-order ambisonics (FOA), from 360-degree videos in complex acoustic scenes remains challenging. Existing methods often overlook the dynamic nature and acoustic complexity of 360-degree scenes, fail to fully account for dynamic sound sources, and neglect complex environmental effects such as occlusion, reflections, and reverberation, which are influenced by scene geometries and materials. We propose DynFOA, a framework based on dynamic acoustic perception and conditional diffusion, for generating high-fidelity FOA from 360-degree videos. DynFOA first performs visual processing via a video encoder, which detects and localizes multiple dynamic sound sources, estimates their depth and semantics, and reconstructs the scene geometry and materials using a 3D Gaussian Splatting. This reconstruction technique accurately models occlusion, reflections, and reverberation based on the geometries and materials of the reconstructed 3D scene and the listener's viewpoint. The audio encoder then captures the spatial motion and temporal 4D sound source trajectories to fine-tune the diffusion-based FOA generator. The fine-tuned FOA generator adjusts spatial cues in real time, ensuring consistent directional fidelity during listener head rotation and complex environmental changes. Extensive evaluations demonstrate that DynFOA consistently outperforms existing methods across metrics such as spatial accuracy, acoustic fidelity, and distribution matching, while also improving the user experience. Therefore, DynFOA provides a robust and scalable approach to rendering realistic dynamic spatial audio for VR and immersive media applications.", "AI": {"tldr": "DynFOA\uff1a\u57fa\u4e8e\u52a8\u6001\u58f0\u5b66\u611f\u77e5\u548c\u6761\u4ef6\u6269\u6563\u7684\u6846\u67b6\uff0c\u4ece360\u5ea6\u89c6\u9891\u751f\u6210\u9ad8\u4fdd\u771f\u4e00\u9636Ambisonics\u7a7a\u95f4\u97f3\u9891\uff0c\u89e3\u51b3\u4e86\u590d\u6742\u58f0\u5b66\u573a\u666f\u4e2d\u7684\u52a8\u6001\u58f0\u6e90\u548c\u73af\u5883\u6548\u5e94\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u751f\u6210360\u5ea6\u89c6\u9891\u7684\u7a7a\u95f4\u97f3\u9891\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff1a\u5ffd\u89c6\u573a\u666f\u7684\u52a8\u6001\u6027\u548c\u58f0\u5b66\u590d\u6742\u6027\uff0c\u672a\u80fd\u5145\u5206\u8003\u8651\u52a8\u6001\u58f0\u6e90\uff0c\u5ffd\u7565\u7531\u573a\u666f\u51e0\u4f55\u548c\u6750\u6599\u5f71\u54cd\u7684\u906e\u6321\u3001\u53cd\u5c04\u3001\u6df7\u54cd\u7b49\u73af\u5883\u6548\u5e94\u3002", "method": "DynFOA\u9996\u5148\u901a\u8fc7\u89c6\u9891\u7f16\u7801\u5668\u8fdb\u884c\u89c6\u89c9\u5904\u7406\uff0c\u68c0\u6d4b\u548c\u5b9a\u4f4d\u591a\u4e2a\u52a8\u6001\u58f0\u6e90\uff0c\u4f30\u8ba1\u5176\u6df1\u5ea6\u548c\u8bed\u4e49\uff0c\u4f7f\u75283D\u9ad8\u65af\u6e85\u5c04\u91cd\u5efa\u573a\u666f\u51e0\u4f55\u548c\u6750\u6599\u3002\u97f3\u9891\u7f16\u7801\u5668\u6355\u6349\u7a7a\u95f4\u8fd0\u52a8\u548c\u65f6\u95f44D\u58f0\u6e90\u8f68\u8ff9\uff0c\u5fae\u8c03\u57fa\u4e8e\u6269\u6563\u7684FOA\u751f\u6210\u5668\uff0c\u5b9e\u65f6\u8c03\u6574\u7a7a\u95f4\u7ebf\u7d22\u3002", "result": "\u5e7f\u6cdb\u8bc4\u4f30\u8868\u660e\uff0cDynFOA\u5728\u7a7a\u95f4\u7cbe\u5ea6\u3001\u58f0\u5b66\u4fdd\u771f\u5ea6\u548c\u5206\u5e03\u5339\u914d\u7b49\u6307\u6807\u4e0a\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u540c\u65f6\u6539\u5584\u4e86\u7528\u6237\u4f53\u9a8c\u3002", "conclusion": "DynFOA\u4e3aVR\u548c\u6c89\u6d78\u5f0f\u5a92\u4f53\u5e94\u7528\u63d0\u4f9b\u4e86\u5f3a\u5927\u4e14\u53ef\u6269\u5c55\u7684\u6e32\u67d3\u771f\u5b9e\u52a8\u6001\u7a7a\u95f4\u97f3\u9891\u7684\u65b9\u6cd5\u3002"}}
{"id": "2602.06937", "categories": ["cs.SD", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2602.06937", "abs": "https://arxiv.org/abs/2602.06937", "authors": ["Hugo Seut\u00e9", "Pranai Vasudev", "Etienne Richan", "Louis-Xavier Buffoni"], "title": "Reciprocal Latent Fields for Precomputed Sound Propagation", "comment": "Temporary pre-print, will be updated. In review at a conference", "summary": "Realistic sound propagation is essential for immersion in a virtual scene, yet physically accurate wave-based simulations remain computationally prohibitive for real-time applications. Wave coding methods address this limitation by precomputing and compressing impulse responses of a given scene into a set of scalar acoustic parameters, which can reach unmanageable sizes in large environments with many source-receiver pairs. We introduce Reciprocal Latent Fields (RLF), a memory-efficient framework for encoding and predicting these acoustic parameters. The RLF framework employs a volumetric grid of trainable latent embeddings decoded with a symmetric function, ensuring acoustic reciprocity. We study a variety of decoders and show that leveraging Riemannian metric learning leads to a better reproduction of acoustic phenomena in complex scenes. Experimental validation demonstrates that RLF maintains replication quality while reducing the memory footprint by several orders of magnitude. Furthermore, a MUSHRA-like subjective listening test indicates that sound rendered via RLF is perceptually indistinguishable from ground-truth simulations.", "AI": {"tldr": "RLF\u6846\u67b6\u4f7f\u7528\u53ef\u8bad\u7ec3\u7684\u6f5c\u5728\u5d4c\u5165\u4f53\u7f51\u683c\u548c\u5bf9\u79f0\u89e3\u7801\u51fd\u6570\uff0c\u901a\u8fc7Riemannian\u5ea6\u91cf\u5b66\u4e60\u9ad8\u6548\u7f16\u7801\u58f0\u5b66\u53c2\u6570\uff0c\u5927\u5e45\u51cf\u5c11\u5185\u5b58\u5360\u7528\uff0c\u540c\u65f6\u4fdd\u6301\u611f\u77e5\u4e0a\u65e0\u6cd5\u533a\u5206\u7684\u97f3\u8d28\u3002", "motivation": "\u57fa\u4e8e\u6ce2\u7684\u58f0\u4f20\u64ad\u6a21\u62df\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u800c\u73b0\u6709\u7684\u6ce2\u7f16\u7801\u65b9\u6cd5\u5728\u5927\u578b\u573a\u666f\u4e2d\u4f1a\u4ea7\u751f\u96be\u4ee5\u7ba1\u7406\u7684\u53c2\u6570\u89c4\u6a21\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u5185\u5b58\u7f16\u7801\u65b9\u6848\u3002", "method": "\u63d0\u51faReciprocal Latent Fields (RLF)\u6846\u67b6\uff0c\u4f7f\u7528\u53ef\u8bad\u7ec3\u7684\u6f5c\u5728\u5d4c\u5165\u4f53\u7f51\u683c\uff0c\u901a\u8fc7\u5bf9\u79f0\u89e3\u7801\u51fd\u6570\u786e\u4fdd\u58f0\u5b66\u4e92\u6613\u6027\uff0c\u5e76\u5229\u7528Riemannian\u5ea6\u91cf\u5b66\u4e60\u6765\u66f4\u597d\u5730\u590d\u73b0\u590d\u6742\u573a\u666f\u4e2d\u7684\u58f0\u5b66\u73b0\u8c61\u3002", "result": "RLF\u5728\u4fdd\u6301\u590d\u5236\u8d28\u91cf\u7684\u540c\u65f6\uff0c\u5c06\u5185\u5b58\u5360\u7528\u51cf\u5c11\u4e86\u6570\u4e2a\u6570\u91cf\u7ea7\u3002\u4e3b\u89c2\u542c\u529b\u6d4b\u8bd5\u8868\u660e\uff0c\u901a\u8fc7RLF\u6e32\u67d3\u7684\u58f0\u97f3\u5728\u611f\u77e5\u4e0a\u4e0e\u771f\u5b9e\u6a21\u62df\u65e0\u6cd5\u533a\u5206\u3002", "conclusion": "RLF\u4e3a\u5b9e\u65f6\u865a\u62df\u573a\u666f\u4e2d\u7684\u903c\u771f\u58f0\u4f20\u64ad\u63d0\u4f9b\u4e86\u4e00\u79cd\u5185\u5b58\u9ad8\u6548\u4e14\u611f\u77e5\u51c6\u786e\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6ce2\u7f16\u7801\u65b9\u6cd5\u5728\u5927\u578b\u73af\u5883\u4e2d\u7684\u53ef\u6269\u5c55\u6027\u95ee\u9898\u3002"}}

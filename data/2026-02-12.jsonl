{"id": "2602.10394", "categories": ["eess.SP", "physics.flu-dyn"], "pdf": "https://arxiv.org/pdf/2602.10394", "abs": "https://arxiv.org/abs/2602.10394", "authors": ["Jeffrey W. Utley", "Gregery T. Buzzard", "Charles A. Bouman", "Matthew R. Kemnetz"], "title": "Boiling flow parameter estimation from boundary layer data", "comment": "Published in Proc. SPIE 13619, 136190L (2025). Version of record: https://doi.org/10.1117/12.3063655", "summary": "Atmospheric turbulence and aero-optic effects cause phase aberrations in propagating light waves, thereby reducing effectiveness in transmitting and receiving coherent light from an aircraft. Existing optical sensors can measure the resulting phase aberrations, but the physical experiments required to induce these aberrations are expensive and time-intensive. Simulation methods could provide a less expensive alternative. For example, an existing simulation algorithm called boiling flow, which generalizes the Taylor frozen-flow method, can generate synthetic phase aberration data (i.e., phase screens) induced by atmospheric turbulence. However, boiling flow depends on physical parameters, such as the Fried coherence length r0, which are not well-defined for aero-optic effects. In this paper, we introduce a method to estimate the parameters of boiling flow from measured aero-optic phase aberration data. Our algorithm estimates these parameters to fit the spatial and temporal statistics of the measured data. This method is computationally efficient and our experiments show that the temporal power spectral density of the slopes of the synthetic phase screens reasonably matches that of the measured phase aberrations from two turbulent boundary layer data sets, with errors between 8-9%. However, the Kolmogorov spatial structure function of the phase screens does not match that of the measured phase aberrations, with errors above 28%. This suggests that, while the parameters of boiling flow can reasonably fit the temporal statistics of highly convective data, they cannot fit the complex spatial statistics of aero-optic phase aberrations."}
{"id": "2602.10417", "categories": ["eess.SP", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.10417", "abs": "https://arxiv.org/abs/2602.10417", "authors": ["Hongyu Deng", "He Chen"], "title": "RadarEye: Robust Liquid Level Tracking Using mmWave Radar in Robotic Pouring", "comment": "To appear in IEEE ICASSP 2026", "summary": "Transparent liquid manipulation in robotic pouring remains challenging for perception systems: specular/refraction effects and lighting variability degrade visual cues, undermining reliable level estimation. To address this challenge, we introduce RadarEye, a real-time mmWave radar signal processing pipeline for robust liquid level estimation and tracking during the whole pouring process. RadarEye integrates (i) a high-resolution range-angle beamforming module for liquid level sensing and (ii) a physics-informed mid-pour tracker that suppresses multipath to maintain lock on the liquid surface despite stream-induced clutter and source container reflections. The pipeline delivers sub-millisecond latency. In real-robot water-pouring experiments, RadarEye achieves a 0.35 cm median absolute height error at 0.62 ms per update, substantially outperforming vision and ultrasound baselines."}
{"id": "2602.10438", "categories": ["eess.SP", "cs.IT"], "pdf": "https://arxiv.org/pdf/2602.10438", "abs": "https://arxiv.org/abs/2602.10438", "authors": ["Chong Cao", "Zhuyu Liu", "Zheng Dong", "Yong Zhou", "He Chen"], "title": "Nonparametric Variational Bayesian Learning for Channel Estimation with OTFS Modulation", "comment": "To appear in IEEE WCNC 2026", "summary": "Orthogonal time frequency space (OTFS) modulation has demonstrated significant advantages in high-mobility scenarios in future 6G networks. However, existing channel estimation methods often overlook the structured sparsity and clustering characteristics inherent in realistic clustered delay line (CDL) channels, leading to degraded performance in practical systems. To address this issue, we propose a novel nonparametric Bayesian learning (NPBL) framework for OTFS channel estimation. Specifically, a stick-breaking process is introduced to automatically infer the number of multipath components and assign each path to its corresponding cluster. The channel coefficients within each cluster are modeled by a Gaussian mixture distribution to capture complex fading statistics. Furthermore, an effective pruning criterion is designed to eliminate spurious multipath components, thereby enhancing estimation accuracy and reducing computational complexity. Simulation results demonstrate that the proposed method achieves superior performance in terms of normalized mean squared error compared to existing methods."}
{"id": "2602.10537", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2602.10537", "abs": "https://arxiv.org/abs/2602.10537", "authors": ["Rang Liu", "Peishi Li", "Ming Li", "A. Lee Swindlehurst"], "title": "Clutter-Aware Integrated Sensing and Communication: Models, Methods, and Future Directions", "comment": "36 pages, 12 figures. Any feedback is greatly appreciated", "summary": "Integrated sensing and communication (ISAC) can substantially improve spectral, hardware, and energy efficiency by unifying radar sensing and data communications. In wideband and scattering-rich environments, clutter often dominates weak target reflections and becomes a fundamental bottleneck for reliable sensing. Practical ISAC clutter includes \"cold\" clutter arising from environmental backscatter of the probing waveform, and \"hot\" clutter induced by external interference and reflections from the environment whose statistics can vary rapidly over time. In this article, we develop a unified wideband multiple-input multiple-output orthogonal frequency-division multiplexing (MIMO-OFDM) signal model that captures both clutter types across the space, time, and frequency domains. Building on this model, we review clutter characterization at multiple levels, including amplitude statistics, robust spherically invariant random vector (SIRV) modeling, and structured covariance representations suitable for limited-snapshot regimes. We then summarize receiver-side suppression methods in the temporal and spatial domains, together with extensions to space-time adaptive processing (STAP) and space-frequency-time adaptive processing (SFTAP), and we provide guidance on selecting techniques under different waveform and interference conditions. To move beyond reactive suppression, we discuss clutter-aware transceiver co-design that couples beamforming and waveform optimization with practical communication quality-of-service (QoS) constraints to enable proactive clutter avoidance. We conclude with open challenges and research directions toward environment-adaptive and clutter-resilient ISAC for next-generation networks."}
{"id": "2602.10656", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2602.10656", "abs": "https://arxiv.org/abs/2602.10656", "authors": ["Jingru Lin", "Chen Zhang", "Tianrui Wang", "Haizhou Li"], "title": "AudioRAG: A Challenging Benchmark for Audio Reasoning and Information Retrieval", "comment": "Accepted by Audio-AAAI", "summary": "Due to recent advancements in Large Audio-Language Models (LALMs) that demonstrate remarkable performance across a range of sound-, speech- and music-related tasks, there is a growing interest in proposing benchmarks to assess these models. Existing benchmarks generally focus only on reasoning with internal knowledge, neglecting real-world scenarios that require external information grounding. To bridge this gap, we introduce AudioRAG, a novel benchmark designed to evaluate audio-based reasoning augmented by information retrieval in realistic web environments. This benchmark comprises both LLM-generated and manually curated question-answer pairs. Our evaluations reveal that even the state-of-the-art LALMs struggle to answer these questions. We therefore propose an agentic pipeline that integrates audio reasoning with retrieval-augmented generation, providing a stronger baseline for future research."}
{"id": "2602.10164", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2602.10164", "abs": "https://arxiv.org/abs/2602.10164", "authors": ["Raymond Chung"], "title": "Emotion-Coherent Speech Data Augmentation and Self-Supervised Contrastive Style Training for Enhancing Kids's Story Speech Synthesis", "comment": "Accepted at IEEE Spoken Language Technology Workshop 2024", "summary": "Expressive speech synthesis requires vibrant prosody and well-timed pauses. We propose an effective strategy to augment a small dataset to train an expressive end-to-end Text-to-Speech model. We merge audios of emotionally congruent text using a text emotion recognizer, creating augmented expressive speech data. By training with two-sentence audio, our model learns natural breaks between lines. We further apply self-supervised contrastive training to improve the speaking style embedding extraction from speech. During inference, our model produces multi-sentence speech in one step, guided by the text-predicted speaking style. Evaluations showcase the effectiveness of our proposed approach when compared to a baseline model trained with consecutive two-sentence audio. Our synthesized speeches give a closer inter-sentence pause distribution to the ground truth speech. Subjective evaluations reveal our synthesized speech scored higher in naturalness and style suitability than the baseline."}
{"id": "2602.10736", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2602.10736", "abs": "https://arxiv.org/abs/2602.10736", "authors": ["Wenlihan Lu", "Huacong Chen", "Ruiyang Duan", "Weijie Yuan", "Shijian Gao"], "title": "Transfer to Sky: Unveil Low-Altitude Route-Level Radio Maps via Ground Crowdsourced Data", "comment": null, "summary": "The expansion of the low-altitude economy is contingent on reliable cellular connectivity for unmanned aerial vehicles (UAVs). A key challenge in pre-flight planning is predicting communication link quality along proposed and pre-defined routes, a task hampered by sparse measurements that render existing radio map methods ineffective. This paper introduces a transfer learning framework for high-fidelity route-level radio map prediction. Our key insight is to leverage abundant crowdsourced ground signals as auxiliary supervision. To bridge the significant domain gap between ground and aerial data and address spatial sparsity, our framework learns general propagation priors from simulation, performs adversarial alignment of the feature spaces, and is fine-tuned on limited real UAV measurements. Extensive experiments on a real-world dataset from Meituan show that our method achieves over 50% higher accuracy in predicting Route RSRP compared to state-of-the-art baselines."}
{"id": "2602.10666", "categories": ["eess.AS", "cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2602.10666", "abs": "https://arxiv.org/abs/2602.10666", "authors": ["Riccardo Miccini", "Clément Laroche", "Tobias Piechowiak", "Xenofon Fafoutis", "Luca Pezzarossa"], "title": "From Diet to Free Lunch: Estimating Auxiliary Signal Properties using Dynamic Pruning Masks in Speech Enhancement Networks", "comment": "Accepted for publication at the 2026 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)", "summary": "Speech Enhancement (SE) in audio devices is often supported by auxiliary modules for Voice Activity Detection (VAD), SNR estimation, or Acoustic Scene Classification to ensure robust context-aware behavior and seamless user experience. Just like SE, these tasks often employ deep learning; however, deploying additional models on-device is computationally impractical, whereas cloud-based inference would introduce additional latency and compromise privacy. Prior work on SE employed Dynamic Channel Pruning (DynCP) to reduce computation by adaptively disabling specific channels based on the current input. In this work, we investigate whether useful signal properties can be estimated from these internal pruning masks, thus removing the need for separate models. We show that simple, interpretable predictors achieve up to 93% accuracy on VAD, 84% on noise classification, and an R2 of 0.86 on F0 estimation. With binary masks, predictions reduce to weighted sums, inducing negligible overhead. Our contribution is twofold: on one hand, we examine the emergent behavior of DynCP models through the lens of downstream prediction tasks, to reveal what they are learning; on the other, we repurpose and re-propose DynCP as a holistic solution for efficient SE and simultaneous estimation of signal properties."}
{"id": "2602.10439", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2602.10439", "abs": "https://arxiv.org/abs/2602.10439", "authors": ["Liyang Chen", "Hongkai Chen", "Yujun Cai", "Sifan Li", "Qingwen Ye", "Yiwei Wang"], "title": "AudioRouter: Data Efficient Audio Understanding via RL based Dual Reasoning", "comment": null, "summary": "Large Audio Language Models (LALMs) have demonstrated strong capabilities in audio understanding and reasoning. However, their performance on fine grained auditory perception remains unreliable, and existing approaches largely rely on data intensive training to internalize perceptual abilities. We propose AudioRouter, a reinforcement learning framework that enables LALMs to improve audio understanding by learning when and how to use external audio tools. Rather than tightly coupling tool usage with audio reasoning, AudioRouter formulates tool use as an explicit decision making problem and optimizes a lightweight routing policy while keeping the underlying reasoning model frozen. Experimental results show that AudioRouter achieves substantial improvements on standard audio understanding benchmarks while requiring up to 600x less training data to learn tool usage compared with conventional training paradigms. These findings suggest that learning effective tool usage offers a data efficient and scalable alternative to internalizing perceptual abilities in LALMs."}
{"id": "2602.10742", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2602.10742", "abs": "https://arxiv.org/abs/2602.10742", "authors": ["Muhammad Khalil", "Ke Wang", "Jinho Choi"], "title": "Stochastic Design of Active RIS-Assisted Satellite Downlinks under Interference, Folded Noise, and EIRP Constraints", "comment": null, "summary": "Active reconfigurable intelligent surfaces (RISs) can mitigate the double-fading loss of passive reflection in satellite downlinks. However, their gains are limited by random co-channel interference, gain-dependent amplifier noise, and regulatory emission constraints. This paper develops a stochastic reliability framework for active RIS-assisted satellite downlinks by modeling the desired and interfering channels, receiver noise, and RIS amplifier noise as random variables. The resulting instantaneous signal-to-interference-plus-noise ratio (SINR) model explicitly captures folded cascaded amplifier noise and reveals a finite high-gain SINR ceiling.\n  To guarantee a target outage level, we formulate a chance-constrained max-SINR design that jointly optimizes the binary RIS configuration and a common amplification gain. The chance constraint is handled using a sample-average approximation (SAA) with a violation budget. The resulting feasibility problem is solved as a mixed-integer second-order cone program (MISOCP) within a bisection search over the SINR threshold. Practical implementation is enforced by restricting the gain to an admissible range determined by small-signal stability and effective isotropic radiated power (EIRP) limits.\n  We also derive realization-wise SINR envelopes based on eigenvalue and l1-norm bounds, which provide interpretable performance limits and fast diagnostics. Monte Carlo results show that these envelopes tightly bound the simulated SINR, reproduce the predicted saturation behavior, and quantify performance degradation as interference increases. Overall, the paper provides a solver-ready, reliability-targeting design methodology whose achieved reliability is validated through out-of-sample Monte Carlo testing under realistic randomness and hardware constraints."}
{"id": "2602.10716", "categories": ["eess.AS", "cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2602.10716", "abs": "https://arxiv.org/abs/2602.10716", "authors": ["Jing-Han Chen", "Bo-Hao Su", "Ya-Tse Wu", "Chi-Chun Lee"], "title": "RE-LLM: Refining Empathetic Speech-LLM Responses by Integrating Emotion Nuance", "comment": "5 pages, 1 figure, 2 tables. Accepted at IEEE ASRU 2025", "summary": "With generative AI advancing, empathy in human-AI interaction is essential. While prior work focuses on emotional reflection, emotional exploration, key to deeper engagement, remains overlooked. Existing LLMs rely on text which captures limited emotion nuances. To address this, we propose RE-LLM, a speech-LLM integrating dimensional emotion embeddings and auxiliary learning. Experiments show statistically significant gains in empathy metrics across three datasets. RE-LLM relatively improves the Emotional Reaction score by 14.79% and 6.76% compared to text-only and speech-LLM baselines on ESD. Notably, it raises the Exploration score by 35.42% and 3.91% on IEMOCAP, 139.28% and 9.83% on ESD, and 60.95% and 22.64% on MSP-PODCAST. It also boosts unweighted accuracy by 5.4% on IEMOCAP, 2.3% on ESD, and 6.9% on MSP-PODCAST in speech emotion recognition. These results highlight the enriched emotional understanding and improved empathetic response generation of RE-LLM."}
{"id": "2602.10735", "categories": ["cs.SD", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.10735", "abs": "https://arxiv.org/abs/2602.10735", "authors": ["Hugo L. Hammer", "Vajira Thambawita", "Pål Halvorsen"], "title": "Calliope: A TTS-based Narrated E-book Creator Ensuring Exact Synchronization, Privacy, and Layout Fidelity", "comment": null, "summary": "A narrated e-book combines synchronized audio with digital text, highlighting the currently spoken word or sentence during playback. This format supports early literacy and assists individuals with reading challenges, while also allowing general readers to seamlessly switch between reading and listening. With the emergence of natural-sounding neural Text-to-Speech (TTS) technology, several commercial services have been developed to leverage these technology for converting standard text e-books into high-quality narrated e-books. However, no open-source solutions currently exist to perform this task. In this paper, we present Calliope, an open-source framework designed to fill this gap. Our method leverages state-of-the-art open-source TTS to convert a text e-book into a narrated e-book in the EPUB 3 Media Overlay format. The method offers several innovative steps: audio timestamps are captured directly during TTS, ensuring exact synchronization between narration and text highlighting; the publisher's original typography, styling, and embedded media are strictly preserved; and the entire pipeline operates offline. This offline capability eliminates recurring API costs, mitigates privacy concerns, and avoids copyright compliance issues associated with cloud-based services. The framework currently supports the state-of-the-art open-source TTS systems XTTS-v2 and Chatterbox. A potential alternative approach involves first generating narration via TTS and subsequently synchronizing it with the text using forced alignment. However, while our method ensures exact synchronization, our experiments show that forced alignment introduces drift between the audio and text highlighting significant enough to degrade the reading experience. Source code and usage instructions are available at https://github.com/hugohammer/TTS-Narrated-Ebook-Creator.git."}
{"id": "2602.10767", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2602.10767", "abs": "https://arxiv.org/abs/2602.10767", "authors": ["Athanasios T. Papadopoulos", "Thrassos K. Oikonomou", "Dimitrios Tyrovolas", "Sotiris A. Tegos", "Panagiotis D. Diamantoulakis", "Panagiotis Sarigiannidis", "George K. Karagiannidis"], "title": "Constellation Design for Robust Interference Mitigation", "comment": null, "summary": "This paper investigates symbol detection for single-carrier communication systems operating in the presence of additive interference with Nakagami-m statistics. Such interference departs from the assumptions underlying conventional detection methods based on Gaussian noise models and leads to detection mismatch that fundamentally affects symbol-level performance. In particular, the presence of random interference amplitude and non-uniform phase alters the structure of the optimal decision regions and renders standard Euclidean distance-based detectors suboptimal. To address this challenge, we develop the maximum-likelihood Gaussian-phase approximate (ML-G) detector, a low-complexity detection rule that accurately approximates maximum-likelihood detection while remaining suitable for practical implementation. The proposed detector explicitly incorporates the statistical properties of the interference and induces decision regions that differ significantly from those arising under conventional metrics. Building on the ML-G framework, we further investigate constellation design under interference-aware detection and formulate an optimization problem that seeks symbol placements that minimize the symbol error probability subject to an average energy constraint. The resulting constellations are obtained numerically and adapt to the interference environment, exhibiting non-standard and asymmetric structures as interference strength increases. Simulation results demonstrate clear symbol error probability gains over established benchmark schemes across a range of interference conditions, particularly in scenarios with dominant additive interference."}
{"id": "2602.10829", "categories": ["eess.AS", "cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2602.10829", "abs": "https://arxiv.org/abs/2602.10829", "authors": ["Theo Lepage", "Reda Dehak"], "title": "Self-Supervised Learning for Speaker Recognition: A study and review", "comment": "accepted for publication in Speech Communication", "summary": "Deep learning models trained in a supervised setting have revolutionized audio and speech processing. However, their performance inherently depends on the quantity of human-annotated data, making them costly to scale and prone to poor generalization under unseen conditions. To address these challenges, Self-Supervised Learning (SSL) has emerged as a promising paradigm, leveraging vast amounts of unlabeled data to learn relevant representations. The application of SSL for Automatic Speech Recognition (ASR) has been extensively studied, but research on other downstream tasks, notably Speaker Recognition (SR), remains in its early stages. This work describes major SSL instance-invariance frameworks (e.g., SimCLR, MoCo, and DINO), initially developed for computer vision, along with their adaptation to SR. Various SSL methods for SR, proposed in the literature and built upon these frameworks, are also presented. An extensive review of these approaches is then conducted: (1) the effect of the main hyperparameters of SSL frameworks is investigated; (2) the role of SSL components is studied (e.g., data-augmentation, projector, positive sampling); and (3) SSL frameworks are evaluated on SR with in-domain and out-of-domain data, using a consistent experimental setup, and a comprehensive comparison of SSL methods from the literature is provided. Specifically, DINO achieves the best downstream performance and effectively models intra-speaker variability, although it is highly sensitive to hyperparameters and training conditions, while SimCLR and MoCo provide robust alternatives that effectively capture inter-speaker variability and are less prone to collapse. This work aims to highlight recent trends and advancements, identifying current challenges in the field."}
{"id": "2602.10934", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2602.10934", "abs": "https://arxiv.org/abs/2602.10934", "authors": ["Yitian Gong", "Kuangwei Chen", "Zhaoye Fei", "Xiaogui Yang", "Ke Chen", "Yang Wang", "Kexin Huang", "Mingshu Chen", "Ruixiao Li", "Qingyuan Cheng", "Shimin Li", "Xipeng Qiu"], "title": "MOSS-Audio-Tokenizer: Scaling Audio Tokenizers for Future Audio Foundation Models", "comment": "27 pages, 8 figures", "summary": "Discrete audio tokenizers are fundamental to empowering large language models with native audio processing and generation capabilities. Despite recent progress, existing approaches often rely on pretrained encoders, semantic distillation, or heterogeneous CNN-based architectures. These designs introduce fixed inductive biases that limit reconstruction fidelity and hinder effective scaling. In this paper, we argue that discrete audio tokenization should be learned fully end-to-end using a homogeneous and scalable architecture. To this end, we first propose CAT (Causal Audio Tokenizer with Transformer), a purely Transformer-based architecture that jointly optimizes the encoder, quantizer, and decoder from scratch for high-fidelity reconstruction. Building on the CAT architecture, we develop MOSS-Audio-Tokenizer, a large-scale audio tokenizer featuring 1.6 billion parameters, pre-trained on 3 million hours of diverse, general audio data. We show that this simple, fully end-to-end approach built from homogeneous, causal Transformer blocks scales gracefully and supports high-fidelity reconstruction across diverse audio domains. Across speech, sound, and music, MOSS-Audio-Tokenizer consistently outperforms prior codecs over a wide range of bitrates, while exhibiting predictable improvements with increased scale. Notably, leveraging the discrete tokens from our model, we develop the first purely autoregressive TTS model that surpasses prior non-autoregressive and cascaded systems. Furthermore, MOSS-Audio-Tokenizer enables competitive ASR performance without auxiliary encoders. Our findings position the CAT architecture as a unified, scalable interface for the next generation of native audio foundation models."}
{"id": "2602.10792", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.10792", "abs": "https://arxiv.org/abs/2602.10792", "authors": ["Yi Zhang", "Rui Guo", "Yonina C. Eldar"], "title": "Bayesian Signal Component Decomposition via Diffusion-within-Gibbs Sampling", "comment": "13 pages, 2 figures. Submitted to journal", "summary": "In signal processing, the data collected from sensing devices is often a noisy linear superposition of multiple components, and the estimation of components of interest constitutes a crucial pre-processing step. In this work, we develop a Bayesian framework for signal component decomposition, which combines Gibbs sampling with plug-and-play (PnP) diffusion priors to draw component samples from the posterior distribution. Unlike many existing methods, our framework supports incorporating model-driven and data-driven prior knowledge into the diffusion prior in a unified manner. Moreover, the proposed posterior sampler allows component priors to be learned separately and flexibly combined without retraining. Under suitable assumptions, the proposed DiG sampler provably produces samples from the posterior distribution. We also show that DiG can be interpreted as an extension of a class of recently proposed diffusion-based samplers, and that, for suitable classes of sensing operators, DiG better exploits the structure of the measurement model. Numerical experiments demonstrate the superior performance of our method over existing approaches."}
{"id": "2602.10164", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2602.10164", "abs": "https://arxiv.org/abs/2602.10164", "authors": ["Raymond Chung"], "title": "Emotion-Coherent Speech Data Augmentation and Self-Supervised Contrastive Style Training for Enhancing Kids's Story Speech Synthesis", "comment": "Accepted at IEEE Spoken Language Technology Workshop 2024", "summary": "Expressive speech synthesis requires vibrant prosody and well-timed pauses. We propose an effective strategy to augment a small dataset to train an expressive end-to-end Text-to-Speech model. We merge audios of emotionally congruent text using a text emotion recognizer, creating augmented expressive speech data. By training with two-sentence audio, our model learns natural breaks between lines. We further apply self-supervised contrastive training to improve the speaking style embedding extraction from speech. During inference, our model produces multi-sentence speech in one step, guided by the text-predicted speaking style. Evaluations showcase the effectiveness of our proposed approach when compared to a baseline model trained with consecutive two-sentence audio. Our synthesized speeches give a closer inter-sentence pause distribution to the ground truth speech. Subjective evaluations reveal our synthesized speech scored higher in naturalness and style suitability than the baseline."}
{"id": "2602.11145", "categories": ["cs.SD", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2602.11145", "abs": "https://arxiv.org/abs/2602.11145", "authors": ["Christopher Mitcheltree", "Vincent Lostanlen", "Emmanouil Benetos", "Mathieu Lagrange"], "title": "SCRAPL: Scattering Transform with Random Paths for Machine Learning", "comment": "Accepted to ICLR 2026. Code, audio samples, and Python package provided at https://christhetree.github.io/scrapl/", "summary": "The Euclidean distance between wavelet scattering transform coefficients (known as paths) provides informative gradients for perceptual quality assessment of deep inverse problems in computer vision, speech, and audio processing. However, these transforms are computationally expensive when employed as differentiable loss functions for stochastic gradient descent due to their numerous paths, which significantly limits their use in neural network training. Against this problem, we propose \"Scattering transform with Random Paths for machine Learning\" (SCRAPL): a stochastic optimization scheme for efficient evaluation of multivariable scattering transforms. We implement SCRAPL for the joint time-frequency scattering transform (JTFS) which demodulates spectrotemporal patterns at multiple scales and rates, allowing a fine characterization of intermittent auditory textures. We apply SCRAPL to differentiable digital signal processing (DDSP), specifically, unsupervised sound matching of a granular synthesizer and the Roland TR-808 drum machine. We also propose an initialization heuristic based on importance sampling, which adapts SCRAPL to the perceptual content of the dataset, improving neural network convergence and evaluation performance. We make our code and audio samples available and provide SCRAPL as a Python package."}
{"id": "2602.10958", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2602.10958", "abs": "https://arxiv.org/abs/2602.10958", "authors": ["A. Abdelaziz Salem", "Saeed Abdallah", "Khawla Alnajjar", "Mahmoud A. Albreem", "Mohamed Saad", "Hayssam Dahrouj", "Hesham Elsawy"], "title": "Fluid-Antenna-Enabled Integrated Bistatic Sensing and Backscatter Communication Systems", "comment": null, "summary": "This paper studies a fluid-antenna-enabled integrated bistatic sensing and backscatter communication system for future networks where connectivity, power delivery, and environmental awareness are jointly supported by the same infrastructure. A multi-antenna base station (BS) with transmitting fluid antennas serves downlink users, energizes passive tags, and illuminates radar targets, while a spatially separated multi-antenna reader decodes tag backscatter and processes radar echoes to avoid the strong self-interference that would otherwise obscure weak returns at the BS. The coexistence of tags and targets, however, induces severe near--far disparities and multi-signal interference, which can be mitigated by fluid antennas through additional spatial degrees of freedom that reshape the multi-hop channels. We formulate a transmit-power minimization problem that jointly optimizes the BS information beamformers, sensing covariance matrix, reader receive beamformers, tag reflection coefficients, and fluid-antenna (FA) positions under heterogeneous quality of service constraints for communication, backscatter, and sensing, as well as energy-harvesting and FA geometry requirements. To tackle the resulting non-convex problem, we develop an alternating-optimization block-coordinate framework that solves four tractable subproblems using semidefinite relaxation, majorization--minimization, and successive convex approximation. Numerical results show consistent transmit-power savings over fixed-position antennas and zero-forcing baselines, achieving about 13.7% and 54.5% reductions, respectively."}
{"id": "2602.10439", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2602.10439", "abs": "https://arxiv.org/abs/2602.10439", "authors": ["Liyang Chen", "Hongkai Chen", "Yujun Cai", "Sifan Li", "Qingwen Ye", "Yiwei Wang"], "title": "AudioRouter: Data Efficient Audio Understanding via RL based Dual Reasoning", "comment": null, "summary": "Large Audio Language Models (LALMs) have demonstrated strong capabilities in audio understanding and reasoning. However, their performance on fine grained auditory perception remains unreliable, and existing approaches largely rely on data intensive training to internalize perceptual abilities. We propose AudioRouter, a reinforcement learning framework that enables LALMs to improve audio understanding by learning when and how to use external audio tools. Rather than tightly coupling tool usage with audio reasoning, AudioRouter formulates tool use as an explicit decision making problem and optimizes a lightweight routing policy while keeping the underlying reasoning model frozen. Experimental results show that AudioRouter achieves substantial improvements on standard audio understanding benchmarks while requiring up to 600x less training data to learn tool usage compared with conventional training paradigms. These findings suggest that learning effective tool usage offers a data efficient and scalable alternative to internalizing perceptual abilities in LALMs."}
{"id": "2602.10656", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2602.10656", "abs": "https://arxiv.org/abs/2602.10656", "authors": ["Jingru Lin", "Chen Zhang", "Tianrui Wang", "Haizhou Li"], "title": "AudioRAG: A Challenging Benchmark for Audio Reasoning and Information Retrieval", "comment": "Accepted by Audio-AAAI", "summary": "Due to recent advancements in Large Audio-Language Models (LALMs) that demonstrate remarkable performance across a range of sound-, speech- and music-related tasks, there is a growing interest in proposing benchmarks to assess these models. Existing benchmarks generally focus only on reasoning with internal knowledge, neglecting real-world scenarios that require external information grounding. To bridge this gap, we introduce AudioRAG, a novel benchmark designed to evaluate audio-based reasoning augmented by information retrieval in realistic web environments. This benchmark comprises both LLM-generated and manually curated question-answer pairs. Our evaluations reveal that even the state-of-the-art LALMs struggle to answer these questions. We therefore propose an agentic pipeline that integrates audio reasoning with retrieval-augmented generation, providing a stronger baseline for future research."}
{"id": "2602.10976", "categories": ["eess.SP", "cs.IT"], "pdf": "https://arxiv.org/pdf/2602.10976", "abs": "https://arxiv.org/abs/2602.10976", "authors": ["Georg Schwan", "Alexander Stutz-Tirri", "Christoph Studer"], "title": "Physically Consistent Evaluation of Commonly Used Near-Field Models", "comment": "Submitted to the 34th edition of EUSIPCO", "summary": "Near-field multi-antenna wireless communication has attracted growing research interest in recent years. Despite this development, most of the current literature on antennas and reflecting structures relies on simplified models, whose validity for real systems remains unclear. In this paper, we introduce a physically consistent near-field model, which we use to evaluate commonly used models. Our results indicate that common models are sufficient for basic beamfocusing, but fail to accurately predict the sidelobes and frequency dependence of reflecting structures."}
{"id": "2602.10934", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2602.10934", "abs": "https://arxiv.org/abs/2602.10934", "authors": ["Yitian Gong", "Kuangwei Chen", "Zhaoye Fei", "Xiaogui Yang", "Ke Chen", "Yang Wang", "Kexin Huang", "Mingshu Chen", "Ruixiao Li", "Qingyuan Cheng", "Shimin Li", "Xipeng Qiu"], "title": "MOSS-Audio-Tokenizer: Scaling Audio Tokenizers for Future Audio Foundation Models", "comment": "27 pages, 8 figures", "summary": "Discrete audio tokenizers are fundamental to empowering large language models with native audio processing and generation capabilities. Despite recent progress, existing approaches often rely on pretrained encoders, semantic distillation, or heterogeneous CNN-based architectures. These designs introduce fixed inductive biases that limit reconstruction fidelity and hinder effective scaling. In this paper, we argue that discrete audio tokenization should be learned fully end-to-end using a homogeneous and scalable architecture. To this end, we first propose CAT (Causal Audio Tokenizer with Transformer), a purely Transformer-based architecture that jointly optimizes the encoder, quantizer, and decoder from scratch for high-fidelity reconstruction. Building on the CAT architecture, we develop MOSS-Audio-Tokenizer, a large-scale audio tokenizer featuring 1.6 billion parameters, pre-trained on 3 million hours of diverse, general audio data. We show that this simple, fully end-to-end approach built from homogeneous, causal Transformer blocks scales gracefully and supports high-fidelity reconstruction across diverse audio domains. Across speech, sound, and music, MOSS-Audio-Tokenizer consistently outperforms prior codecs over a wide range of bitrates, while exhibiting predictable improvements with increased scale. Notably, leveraging the discrete tokens from our model, we develop the first purely autoregressive TTS model that surpasses prior non-autoregressive and cascaded systems. Furthermore, MOSS-Audio-Tokenizer enables competitive ASR performance without auxiliary encoders. Our findings position the CAT architecture as a unified, scalable interface for the next generation of native audio foundation models."}
{"id": "2602.10666", "categories": ["eess.AS", "cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2602.10666", "abs": "https://arxiv.org/abs/2602.10666", "authors": ["Riccardo Miccini", "Clément Laroche", "Tobias Piechowiak", "Xenofon Fafoutis", "Luca Pezzarossa"], "title": "From Diet to Free Lunch: Estimating Auxiliary Signal Properties using Dynamic Pruning Masks in Speech Enhancement Networks", "comment": "Accepted for publication at the 2026 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)", "summary": "Speech Enhancement (SE) in audio devices is often supported by auxiliary modules for Voice Activity Detection (VAD), SNR estimation, or Acoustic Scene Classification to ensure robust context-aware behavior and seamless user experience. Just like SE, these tasks often employ deep learning; however, deploying additional models on-device is computationally impractical, whereas cloud-based inference would introduce additional latency and compromise privacy. Prior work on SE employed Dynamic Channel Pruning (DynCP) to reduce computation by adaptively disabling specific channels based on the current input. In this work, we investigate whether useful signal properties can be estimated from these internal pruning masks, thus removing the need for separate models. We show that simple, interpretable predictors achieve up to 93% accuracy on VAD, 84% on noise classification, and an R2 of 0.86 on F0 estimation. With binary masks, predictions reduce to weighted sums, inducing negligible overhead. Our contribution is twofold: on one hand, we examine the emergent behavior of DynCP models through the lens of downstream prediction tasks, to reveal what they are learning; on the other, we repurpose and re-propose DynCP as a holistic solution for efficient SE and simultaneous estimation of signal properties."}
{"id": "2602.11140", "categories": ["eess.SP", "cond-mat.supr-con", "cs.AR"], "pdf": "https://arxiv.org/pdf/2602.11140", "abs": "https://arxiv.org/abs/2602.11140", "authors": ["Yerzhan Mustafa", "Berker Peköz", "Selçuk Köse"], "title": "Reed-Muller Error-Correction Code Encoder for SFQ-to-CMOS Interface Circuits", "comment": "Accepted for publication in IEEE Transactions on Applied Superconductivity", "summary": "Data transmission from superconducting digital electronics such as single flux quantum (SFQ) logic to semiconductor (CMOS) circuits is subject to bit errors due to, e.g., flux trapping, process parameter variations (PPV), and fabrication defects. In this paper, a lightweight hardware-efficient error-correction code encoder is designed and analyzed. Particularly, a Reed-Muller code RM(1,3) encoder is implemented with SFQ digital logic. The proposed RM(1,3) encoder converts a 4-bit message into an 8-bit codeword and can detect and correct up to 3- and 1-bit errors, respectively. This encoder circuit is designed using MIT-LL SFQ5ee process and SuperTools/ColdFlux RSFQ cell library. A simulation framework integrating JoSIM simulator and MATLAB script for automated data collection and analysis, is proposed to study the performance of RM(1,3) encoder. The proposed encoder improves the probability of having no bit errors by 6.7% as compared to an encoder-less design under $\\pm20\\%$ PPV. With $\\pm15\\%$ and lower PPV, the proposed encoder could correct all errors with at least 99.1% probability. The impact of fabrication defects such as open circuit faults on the encoder circuit is also studied using the proposed framework."}
{"id": "2602.11145", "categories": ["cs.SD", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2602.11145", "abs": "https://arxiv.org/abs/2602.11145", "authors": ["Christopher Mitcheltree", "Vincent Lostanlen", "Emmanouil Benetos", "Mathieu Lagrange"], "title": "SCRAPL: Scattering Transform with Random Paths for Machine Learning", "comment": "Accepted to ICLR 2026. Code, audio samples, and Python package provided at https://christhetree.github.io/scrapl/", "summary": "The Euclidean distance between wavelet scattering transform coefficients (known as paths) provides informative gradients for perceptual quality assessment of deep inverse problems in computer vision, speech, and audio processing. However, these transforms are computationally expensive when employed as differentiable loss functions for stochastic gradient descent due to their numerous paths, which significantly limits their use in neural network training. Against this problem, we propose \"Scattering transform with Random Paths for machine Learning\" (SCRAPL): a stochastic optimization scheme for efficient evaluation of multivariable scattering transforms. We implement SCRAPL for the joint time-frequency scattering transform (JTFS) which demodulates spectrotemporal patterns at multiple scales and rates, allowing a fine characterization of intermittent auditory textures. We apply SCRAPL to differentiable digital signal processing (DDSP), specifically, unsupervised sound matching of a granular synthesizer and the Roland TR-808 drum machine. We also propose an initialization heuristic based on importance sampling, which adapts SCRAPL to the perceptual content of the dataset, improving neural network convergence and evaluation performance. We make our code and audio samples available and provide SCRAPL as a Python package."}
{"id": "2602.10716", "categories": ["eess.AS", "cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2602.10716", "abs": "https://arxiv.org/abs/2602.10716", "authors": ["Jing-Han Chen", "Bo-Hao Su", "Ya-Tse Wu", "Chi-Chun Lee"], "title": "RE-LLM: Refining Empathetic Speech-LLM Responses by Integrating Emotion Nuance", "comment": "5 pages, 1 figure, 2 tables. Accepted at IEEE ASRU 2025", "summary": "With generative AI advancing, empathy in human-AI interaction is essential. While prior work focuses on emotional reflection, emotional exploration, key to deeper engagement, remains overlooked. Existing LLMs rely on text which captures limited emotion nuances. To address this, we propose RE-LLM, a speech-LLM integrating dimensional emotion embeddings and auxiliary learning. Experiments show statistically significant gains in empathy metrics across three datasets. RE-LLM relatively improves the Emotional Reaction score by 14.79% and 6.76% compared to text-only and speech-LLM baselines on ESD. Notably, it raises the Exploration score by 35.42% and 3.91% on IEMOCAP, 139.28% and 9.83% on ESD, and 60.95% and 22.64% on MSP-PODCAST. It also boosts unweighted accuracy by 5.4% on IEMOCAP, 2.3% on ESD, and 6.9% on MSP-PODCAST in speech emotion recognition. These results highlight the enriched emotional understanding and improved empathetic response generation of RE-LLM."}
{"id": "2602.10829", "categories": ["eess.AS", "cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2602.10829", "abs": "https://arxiv.org/abs/2602.10829", "authors": ["Theo Lepage", "Reda Dehak"], "title": "Self-Supervised Learning for Speaker Recognition: A study and review", "comment": "accepted for publication in Speech Communication", "summary": "Deep learning models trained in a supervised setting have revolutionized audio and speech processing. However, their performance inherently depends on the quantity of human-annotated data, making them costly to scale and prone to poor generalization under unseen conditions. To address these challenges, Self-Supervised Learning (SSL) has emerged as a promising paradigm, leveraging vast amounts of unlabeled data to learn relevant representations. The application of SSL for Automatic Speech Recognition (ASR) has been extensively studied, but research on other downstream tasks, notably Speaker Recognition (SR), remains in its early stages. This work describes major SSL instance-invariance frameworks (e.g., SimCLR, MoCo, and DINO), initially developed for computer vision, along with their adaptation to SR. Various SSL methods for SR, proposed in the literature and built upon these frameworks, are also presented. An extensive review of these approaches is then conducted: (1) the effect of the main hyperparameters of SSL frameworks is investigated; (2) the role of SSL components is studied (e.g., data-augmentation, projector, positive sampling); and (3) SSL frameworks are evaluated on SR with in-domain and out-of-domain data, using a consistent experimental setup, and a comprehensive comparison of SSL methods from the literature is provided. Specifically, DINO achieves the best downstream performance and effectively models intra-speaker variability, although it is highly sensitive to hyperparameters and training conditions, while SimCLR and MoCo provide robust alternatives that effectively capture inter-speaker variability and are less prone to collapse. This work aims to highlight recent trends and advancements, identifying current challenges in the field."}

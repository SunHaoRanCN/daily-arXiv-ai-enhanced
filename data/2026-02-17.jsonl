{"id": "2602.13265", "categories": ["eess.SP", "cs.IT"], "pdf": "https://arxiv.org/pdf/2602.13265", "abs": "https://arxiv.org/abs/2602.13265", "authors": ["Wenxuan Ma", "Bin Lin", "Hongyang Pan", "Geng Sun", "Enyu Shi", "Jiancheng An", "Chau Yuen"], "title": "SIM-assisted Secure Mobile Communications via Enhanced Proximal Policy Optimization Algorithm", "comment": null, "summary": "With the development of sixth-generation (6G) wireless communication networks, the security challenges are becoming increasingly prominent, especially for mobile users (MUs). As a promising solution, physical layer security (PLS) technology leverages the inherent characteristics of wireless channels to provide security assurance. Particularly, stacked intelligent metasurface (SIM) directly manipulates electromagnetic waves through their multilayer structures, offering significant potential for enhancing PLS performance in an energy efficient manner. Thus, in this work, we investigate an SIM-assisted secure communication system for MUs under the threat of an eavesdropper, addressing practical challenges such as channel uncertainty in mobile environments, multiple MU interference, and residual hardware impairments. Consequently, we formulate a joint power and phase shift optimization problem (JPPSOP), aiming at maximizing the achievable secrecy rate (ASR) of all MUs. Given the non-convexity and dynamic nature of this optimization problem, we propose an enhanced proximal policy optimization algorithm with a bidirectional long short-term memory mechanism, an off-policy data utilization mechanism, and a policy feedback mechanism (PPO-BOP). Through these mechanisms, the proposed algorithm can effectively capture short-term channel fading and long-term MU mobility, improve sample utilization efficiency, and enhance exploration capabilities. Extensive simulation results demonstrate that PPO-BOP significantly outperforms benchmark strategies and other deep reinforcement learning algorithms in terms of ASR.10.1109/TWC.2026.3658332"}
{"id": "2602.13447", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2602.13447", "abs": "https://arxiv.org/abs/2602.13447", "authors": ["Yuanyuan Yao", "Simon Geirnaert", "Tinne Tuytelaars", "Alexander Bertrand"], "title": "Sample-level EEG-based Selective Auditory Attention Decoding with Markov Switching Models", "comment": null, "summary": "Selective auditory attention decoding aims to identify the speaker of interest from listeners' neural signals, such as electroencephalography (EEG), in the presence of multiple concurrent speakers. Most existing methods operate at the window level, facing a trade-off between temporal resolution and decoding accuracy. Recent work has shown that hidden Markov model (HMM)-based post-processing can smooth window-level decoder outputs to improve this trade-off. Instead of using a separate smoothing step, we propose to integrate the decoding and smoothing components into a single probabilistic framework using a Markov switching model (MSM). It directly models the relationship between the EEG and speech envelopes under each attention state while incorporating the temporal dynamics of attention. This formulation enables sample-level attention decoding, with model parameters and attention states jointly estimated via the expectation-maximization algorithm. Experimental results demonstrate that this integrated MSM formulation achieves comparable decoding accuracy to HMM post-processing while providing faster attention switch detection. The code for the proposed method is available at https://github.com/YYao-42/MSM."}
{"id": "2602.13459", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2602.13459", "abs": "https://arxiv.org/abs/2602.13459", "authors": ["Farwa Abbas", "Wei Dai", "Zoran Cvetkovic", "Verity McClelland"], "title": "Towards Causality-Aware Modeling for Multimodal Brain-Muscle Interactions", "comment": null, "summary": "Robust characterization of dynamic causal interactions in multivariate biomedical signals is essential for advancing computational and algorithmic methods in biomedical imaging. Conventional approaches, such as Dynamic Bayesian Networks (DBNs), often assume linear or simple statistical dependencies, while manifold based techniques like Convergent Cross Mapping (CCM) capture nonlinear, lagged interactions but lack probabilistic quantification and interventional modeling. We introduce a DBN informed CCM framework that integrates geometric manifold reconstruction with probabilistic temporal modeling. Applied to multimodal EEG-EMG recordings from dystonic and neurotypical children, the method quantifies uncertainty, supports interventional simulation, and reveals distinct frequency specific reorganization of corticomuscular pathways in dystonia. Experimental results show marked improvements in predictive consistency and causal stability as compared to baseline approaches, demonstrating the potential of causality aware multimodal modeling for developing quantitative biomarkers and guiding targeted neuromodulatory interventions."}
{"id": "2602.13481", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2602.13481", "abs": "https://arxiv.org/abs/2602.13481", "authors": ["Humera Hameed", "Ali Ahmed"], "title": "Blind Deconvolution Demixing using Modulated Inputs", "comment": null, "summary": "This paper focuses on solving a challenging problem of blind deconvolution demixing involving modulated inputs. Specifically, multiple input signals $s_n(t)$, each bandlimited to $B$ Hz, are modulated with known random sequences $r_n(t)$ that alter at rate $Q$. Each modulated signal is convolved with a different M tap channel of impulse response $h_n(t)$, and the outputs of each channel are added at a common receiver to give the observed signal $y(t)=\\sum_{n=1}^N (r_n(t)\\odot s_n(t))\\circledast h_n(t)$, where $\\odot$ is the point wise multiplication, and $\\circledast$ is circular convolution. Given this observed signal $y(t)$, we are concerned with recovering $s_n(t)$ and $h_n(t)$. We employ deterministic subspace assumption for the input signal $s_n(t)$ and keep the channel impulse response $h_n(t)$ arbitrary. We show that if modulating sequence is altered at a rate $Q \\geq N^2 (B+M)$ and sample complexity bound is obeyed then all the signals and the channels, $\\{s_n(t),h_n(t)\\}_{n=1}^N$, can be estimated from the observed mixture $y(t)$ using gradient descent algorithm. We have performed extensive simulations that show the robustness of our algorithm and used phase transitions to numerically investigate the theoretical guarantees provided by our algorithm."}
{"id": "2602.13761", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2602.13761", "abs": "https://arxiv.org/abs/2602.13761", "authors": ["Amro Asali", "Yehuda Ben-Shimol", "Itshak Lapidot"], "title": "ELEAT-SAGA: Early & Late Integration with Evading Alternating Training for Spoof-Robust Speaker Verification", "comment": null, "summary": "Spoofing-robust automatic speaker verification (SASV) seeks to build automatic speaker verification systems that are robust against both zero-effort impostor attacks and sophisticated spoofing techniques such as voice conversion (VC) and text-to-speech (TTS). In this work, we propose a novel SASV architecture that introduces score-aware gated attention (SAGA), SASV-SAGA, enabling dynamic modulation of speaker embeddings based on countermeasure (CM) scores. By integrating speaker embeddings and CM scores from pre-trained ECAPA-TDNN and AASIST models respectively, we explore several integration strategies including early, late, and full integration. We further introduce alternating training for multi-module (ATMM) and a refined variant, evading alternating training (EAT). Experimental results on the ASVspoof 2019 Logical Access (LA) and Spoofceleb datasets demonstrate significant improvements over baselines, achieving a spoofing aware speaker verification equal error rate (SASV-EER) of 1.22% and minimum normalized agnostic detection cost function (min a-DCF) of 0.0304 on the ASVspoof 2019 evaluation set. These results confirm the effectiveness of score-aware attention mechanisms and alternating training strategies in enhancing the robustness of SASV systems."}
{"id": "2602.13259", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2602.13259", "abs": "https://arxiv.org/abs/2602.13259", "authors": ["Xu Zhang", "Longbing Cao", "Runze Yang", "Zhangkai Wu"], "title": "Learning Physiology-Informed Vocal Spectrotemporal Representations for Speech Emotion Recognition", "comment": "13 pages, 5 figures", "summary": "Speech emotion recognition (SER) is essential for humanoid robot tasks such as social robotic interactions and robotic psychological diagnosis, where interpretable and efficient models are critical for safety and performance. Existing deep models trained on large datasets remain largely uninterpretable, often insufficiently modeling underlying emotional acoustic signals and failing to capture and analyze the core physiology of emotional vocal behaviors. Physiological research on human voices shows that the dynamics of vocal amplitude and phase correlate with emotions through the vocal tract filter and the glottal source. However, most existing deep models solely involve amplitude but fail to couple the physiological features of and between amplitude and phase. Here, we propose PhysioSER, a physiology-informed vocal spectrotemporal representation learning method, to address these issues with a compact, plug-and-play design. PhysioSER constructs amplitude and phase views informed by voice anatomy and physiology (VAP) to complement SSL models for SER. This VAP-informed framework incorporates two parallel workflows: a vocal feature representation branch to decompose vocal signals based on VAP, embed them into a quaternion field, and use Hamilton-structured quaternion convolutions for modeling their dynamic interactions; and a latent representation branch based on a frozen SSL backbone. Then, utterance-level features from both workflows are aligned by a Contrastive Projection and Alignment framework, followed by a shallow attention fusion head for SER classification. PhysioSER is shown to be interpretable and efficient for SER through extensive evaluations across 14 datasets, 10 languages, and 6 backbones, and its practical efficacy is validated by real-time deployment on a humanoid robotic platform."}
{"id": "2602.13489", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2602.13489", "abs": "https://arxiv.org/abs/2602.13489", "authors": ["Parsa Razmara", "Takfarinas Medani", "Majid Abbasi Sisara", "Anand A. Joshi", "Rui Chen", "Woojae Jeong", "Ye Tian", "Krishna S. Nayak", "Richard M. Leahy"], "title": "Feasibility of simultaneous EEG-fMRI at 0.55 T: Recording, Denoising, and Functional Mapping", "comment": null, "summary": "Simultaneous recording of electroencephalography (EEG) and functional MRI (fMRI) can provide a more complete view of brain function by merging high temporal and spatial resolutions. High-field ($\\geq$3T) systems are standard, and require technical trade-offs, including artifacts in the EEG signal, reduced compatibility with metallic implants, high acoustic noise, and artifacts around high-susceptibility areas such as the optic nerve and nasal sinus. This proof-of-concept study demonstrates the feasibility of simultaneous EEG-fMRI at 0.55T in a visual task. We characterize the gradient and ballistocardiogram (BCG) artifacts inherent to this environment and observe reduced BCG magnitude consistent with the expected scaling of pulse-related artifacts with static magnetic field strength. This reduction shows promise for facilitating effective denoising while preserving the alpha rhythm and signal integrity. Furthermore, we tested a multimodal integration pipeline and demonstrated that the EEG power envelope corresponds with the hemodynamic BOLD response, supporting the potential to measure neurovascular coupling in this environment. We demonstrate that combined EEG-fMRI at 0.55T is feasible and represents a promising environment for multimodal neuroimaging."}
{"id": "2602.14584", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2602.14584", "abs": "https://arxiv.org/abs/2602.14584", "authors": ["Yacouba Kaloga", "Marina Laganaro", "Ina Kodrasi"], "title": "CLAP-Based Automatic Word Naming Recognition in Post-Stroke Aphasia", "comment": "Submitted to EUSIPCO 2026", "summary": "Conventional automatic word-naming recognition systems struggle to recognize words from post-stroke patients with aphasia because of disfluencies and mispronunciations, limiting reliable automated assessment in this population. In this paper, we propose a Contrastive Language-Audio Pretraining (CLAP) based approach for automatic word-naming recognition to address this challenge by leveraging text-audio alignment. Our approach treats word-naming recognition as an audio-text matching problem, projecting speech signals and textual prompts into a shared embedding space to identify intended words even in challenging recordings. Evaluated on two speech datasets of French post-stroke patients with aphasia, our approach achieves up to 90% accuracy, outperforming existing classification-based and automatic speech recognition-based baselines."}
{"id": "2602.13596", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2602.13596", "abs": "https://arxiv.org/abs/2602.13596", "authors": ["Zhe Ye", "Xiangui Kang", "Jiayi He", "Chengxin Chen", "Wei Zhu", "Kai Wu", "Yin Yang", "Jiwu Huang"], "title": "BreathNet: Generalizable Audio Deepfake Detection via Breath-Cue-Guided Feature Refinement", "comment": "Under Review", "summary": "As deepfake audio becomes more realistic and diverse, developing generalizable countermeasure systems has become crucial. Existing detection methods primarily depend on XLS-R front-end features to improve generalization. Nonetheless, their performance remains limited, partly due to insufficient attention to fine-grained information, such as physiological cues or frequency-domain features. In this paper, we propose BreathNet, a novel audio deepfake detection framework that integrates fine-grained breath information to improve generalization. Specifically, we design BreathFiLM, a feature-wise linear modulation mechanism that selectively amplifies temporal representations based on the presence of breathing sounds. BreathFiLM is trained jointly with the XLS-R extractor, in turn encouraging the extractor to learn and encode breath-related cues into the temporal features. Then, we use the frequency front-end to extract spectral features, which are then fused with temporal features to provide complementary information introduced by vocoders or compression artifacts. Additionally, we propose a group of feature losses comprising Positive-only Supervised Contrastive Loss (PSCL), center loss, and contrast loss. These losses jointly enhance the discriminative ability, encouraging the model to separate bona fide and deepfake samples more effectively in the feature space. Extensive experiments on five benchmark datasets demonstrate state-of-the-art (SOTA) performance. Using the ASVspoof 2019 LA training set, our method attains 1.99% average EER across four related eval benchmarks, with particularly strong performance on the In-the-Wild dataset, where it achieves 4.70% EER. Moreover, under the ASVspoof5 evaluation protocol, our method achieves an EER of 4.94% on this latest benchmark."}
{"id": "2602.13520", "categories": ["eess.SP", "physics.hist-ph"], "pdf": "https://arxiv.org/pdf/2602.13520", "abs": "https://arxiv.org/abs/2602.13520", "authors": ["Victor Lazzarini"], "title": "Sub Specie Aeternitatis: Fourier Transforms from the Theory of Heat to Musical Signals", "comment": null, "summary": "J. B. Fourier in his \\emph{Théorie Analytique de la Chaleur} of 1822 introduced, amongst other things, two ideas that have made a fundamental impact in fields as diverse as Mathematical Physics, Electrical Engineering, Computer Science, and Music. The first one of these, a method to find the coefficients for a trigonometric series describing an arbitrary function, was very early on picked up by G. Ohm and H. Helmholtz as the foundation for a theory of \\emph{musical tones}. The second one, which is described by Fourier's double integral, became the basis for treating certain kinds of infinity in discontinuous functions, as shown by A. De Morgan in his 1842 \\emph{The Differential and Integral Calculus}. Both make up the fundamental basis for what is now commonly known as the \\emph{Fourier theorem}. With the help of P. A. M. Dirac's insights into the nature of these infinities, we can have a compact description of the frequency spectrum of a function of time, or conversely of a waveform corresponding to a given function of frequency. This paper, using solely primary sources, takes us from the physics of heat propagation to the modern theory of musical signals. It concludes with some considerations on the inherent duality of time and frequency emerging from Fourier's theorem."}
{"id": "2602.14612", "categories": ["eess.AS", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.14612", "abs": "https://arxiv.org/abs/2602.14612", "authors": ["Naveen Vakada", "Kartik Hegde", "Arvind Krishna Sridhar", "Yinyi Guo", "Erik Visser"], "title": "LongAudio-RAG: Event-Grounded Question Answering over Multi-Hour Long Audio", "comment": null, "summary": "Long-duration audio is increasingly common in industrial and consumer settings, yet reviewing multi-hour recordings is impractical, motivating systems that answer natural-language queries with precise temporal grounding and minimal hallucination. Existing audio-language models show promise, but long-audio question answering remains difficult due to context-length limits. We introduce LongAudio-RAG (LA-RAG), a hybrid framework that grounds Large Language Model (LLM) outputs in retrieved, timestamped acoustic event detections rather than raw audio. Multi-hour streams are converted into structured event records stored in an SQL database, and at inference time the system resolves natural-language time references, classifies intent, retrieves only the relevant events, and generates answers using this constrained evidence. To evaluate performance, we construct a synthetic long-audio benchmark by concatenating recordings with preserved timestamps and generating template-based question-answer pairs for detection, counting, and summarization tasks. Finally, we demonstrate the practicality of our approach by deploying it in a hybrid edge-cloud environment, where the audio grounding model runs on-device on IoT-class hardware while the LLM is hosted on a GPU-backed server. This architecture enables low-latency event extraction at the edge and high-quality language reasoning in the cloud. Experiments show that structured, event-level retrieval significantly improves accuracy compared to vanilla Retrieval-Augmented Generation (RAG) or text-to-SQL approaches."}
{"id": "2602.13685", "categories": ["cs.SD", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.13685", "abs": "https://arxiv.org/abs/2602.13685", "authors": ["Siqian Tong", "Xuan Li", "Yiwei Wang", "Baolong Bi", "Yujun Cai", "Shenghua Liu", "Yuchen He", "Chengpeng Hao"], "title": "AuTAgent: A Reinforcement Learning Framework for Tool-Augmented Audio Reasoning", "comment": null, "summary": "Large Audio Language Models (LALMs) excel at perception but struggle with complex reasoning requiring precise acoustic measurements. While external tools can extract fine-grained features like exact tempo or pitch, effective integration remains challenging: naively using all tools causes information overload, while prompt-based selection fails to assess context-dependent utility. To address this, we propose AuTAgent (Audio Tool Agent), a reinforcement learning framework that learns when and which tools to invoke. By employing a sparse-feedback training strategy with a novel Differential Reward mechanism, the agent learns to filter out irrelevant tools and invokes external assistance only when it yields a net performance gain over the base model. Experimental results confirm that AuTAgent complements the representation bottleneck of LALMs by providing verifiable acoustic evidence. It improves accuracy by 4.20% / 6.20% and 9.80% / 8.00% for open-source and closed-source backbones on the MMAU Test-mini and the MMAR benchmarks, respectively. In addition, further experiments demonstrate exceptional transferability. We highlight the complementary role of external tools in augmenting audio model reasoning."}
{"id": "2602.13546", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2602.13546", "abs": "https://arxiv.org/abs/2602.13546", "authors": ["Yadang Alexis Rouzoumka", "Jean Pinsolle", "Eugénie Terreaux", "Christèle Morisseau", "Jean-Philippe Ovarlez", "Chengfang Ren"], "title": "DopplerGLRTNet for Radar Off-Grid Detection", "comment": "preprint", "summary": "Off-grid targets whose Doppler (or angle) does not lie on the discrete processing grid can severely degrade classical normalized matched-filter (NMF) detectors: even at high SNR, the detection probability may saturate at operationally relevant low false-alarm rates. A principled remedy is the continuous-parameter GLRT, which maximizes a normalized correlation over the parameter domain; however, dense scanning increases test-time cost and remains sensitive to covariance mismatch through whitening. We propose DopplerGLRTNet, an amortized off-grid GLRT: a lightweight regressor predicts the continuous Doppler within a resolution cell from the whitened observation, and the detector outputs a single GLRT/NMF-like score given by the normalized matched-filter energy at the predicted Doppler. Monte Carlo simulations in Gaussian and compound-Gaussian clutter show that DopplerGLRTNet mitigates off-grid saturation, approaches dense-scan performance at a fraction of its cost, and improves robustness to covariance estimation at the same empirically calibrated Pfa."}
{"id": "2602.14671", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2602.14671", "abs": "https://arxiv.org/abs/2602.14671", "authors": ["Mingchi Hou", "Enno Hermann", "Ina Kodrasi"], "title": "Data Augmentation for Pathological Speech Enhancement", "comment": null, "summary": "The performance of state-of-the-art speech enhancement (SE) models considerably degrades for pathological speech due to atypical acoustic characteristics and limited data availability. This paper systematically investigates data augmentation (DA) strategies to improve SE performance for pathological speakers, evaluating both predictive and generative SE models. We examine three DA categories, i.e., transformative, generative, and noise augmentation, assessing their impact with objective SE metrics. Experimental results show that noise augmentation consistently delivers the largest and most robust gains, transformative augmentations provide moderate improvements, while generative augmentation yields limited benefits and can harm performance as the amount of synthetic data increases. Furthermore, we show that the effectiveness of DA varies depending on the SE model, with DA being more beneficial for predictive SE models. While our results demonstrate that DA improves SE performance for pathological speakers, a performance gap between neurotypical and pathological speech persists, highlighting the need for future research on targeted DA strategies for pathological speech."}
{"id": "2602.13787", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2602.13787", "abs": "https://arxiv.org/abs/2602.13787", "authors": ["Lorenzo Picinali", "Robert Baumgartner", "Valerie Gaveau", "Antonino Greco", "Stefanie Liebe", "Paul Oomen", "Christoph Braun"], "title": "Enhancing spatial hearing with cochlear implants: exploring the role of AI, multimodal interaction and perceptual training", "comment": null, "summary": "Cochlear implants (CIs) have been developed to the point where they can restore hearing and speech understanding in a large proportion of patients. Although spatial hearing is central to controlling and directing attention and to enabling speech understanding in noisy environments, it has been largely neglected in the past. We propose here a multi-disciplinary research framework in which physicians, psychologists and engineers collaborate to improve spatial hearing for CI users."}
{"id": "2602.13863", "categories": ["eess.SP", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.13863", "abs": "https://arxiv.org/abs/2602.13863", "authors": ["Andreas Spanias"], "title": "Twenty-five years of J-DSP Online Labs for Signal Processing Classes and Workforce Development Programs", "comment": null, "summary": "This paper presents the history of the online simulation program Java-DSP (J-DSP) and the most recent function development and deployment. J-DSP was created to support online laboratories in DSP classes and was first deployed in our ASU DSP class in 2000. The development of the program and its extensions was supported by several NSF grants including CCLI and IUSE. The web-based software was developed by our team in Java and later transitioned to the more secure HTML5 environment. J-DSP supports laboratory exercises on: digital filters and their design, the FFT and its utility in spectral analysis, machine learning for signal classification, and more recently online simulations with the Quantum Fourier Transform. Throughout the J-DSP development and deployment of this tool and its associated laboratory exercises, we documented evaluations. Mobile versions of the program for iOS and Android were also developed. J-DSP is used to this day in several universities, and specific functions of the program have been used in NSF REU, IRES and RET workforce development and high school outreach."}
{"id": "2602.14686", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2602.14686", "abs": "https://arxiv.org/abs/2602.14686", "authors": ["Frederik Rautenberg", "Jana Wiechmann", "Petra Wagner", "Reinhold Haeb-Umbach"], "title": "Disentangling Pitch and Creak for Speaker Identity Preservation in Speech Synthesis", "comment": null, "summary": "We introduce a system capable of faithfully modifying the perceptual voice quality of creak while preserving the speaker's perceived identity. While it is well known that high creak probability is typically correlated with low pitch, it is important to note that this is a property observed on a population of speakers but does not necessarily hold across all situations. Disentanglement of pitch from creak is achieved by augmentation of the training dataset of a speech synthesis system with a speaker manipulation block based on conditional continuous normalizing flow. The experiments show greatly improved speaker verification performance over a range of creak manipulation strengths."}
{"id": "2602.13834", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2602.13834", "abs": "https://arxiv.org/abs/2602.13834", "authors": ["Minhui Lu", "Joshua D. Reiss"], "title": "Learning Vocal-Tract Area and Radiation with a Physics-Informed Webster Model", "comment": "Accepted at IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP) 2026", "summary": "We present a physics-informed voiced backend renderer for singing-voice synthesis. Given synthetic single-channel audio and a fund-amental--frequency trajectory, we train a time-domain Webster model as a physics-informed neural network to estimate an interpretable vocal-tract area function and an open-end radiation coefficient. Training enforces partial differential equation and boundary consistency; a lightweight DDSP path is used only to stabilize learning, while inference is purely physics-based. On sustained vowels (/a/, /i/, /u/), parameters rendered by an independent finite-difference time-domain Webster solver reproduce spectral envelopes competitively with a compact DDSP baseline and remain stable under changes in discretization, moderate source variations, and about ten percent pitch shifts. The in-graph waveform remains breathier than the reference, motivating periodicity-aware objectives and explicit glottal priors in future work."}
{"id": "2602.13988", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2602.13988", "abs": "https://arxiv.org/abs/2602.13988", "authors": ["Wenzhou Cao", "Yashuai Cao", "Tiejun Lv", "Mugen Peng"], "title": "Efficient Off-Grid Near-Field Cascade Channel Estimation for XL-IRS Systems via Tucker Decomposition", "comment": "This work has been accepted for publication in IEEE Transactions on Signal Processing", "summary": "Accurate cascaded channel state information is pivotal for extremely large-scale intelligent reflecting surfaces (XL-IRS) in next-generation wireless networks. However, the large XL-IRS aperture induces spherical wavefront propagation due to near-field (NF) effects, complicating cascaded channel estimation. Conventional dictionary-based methods suffer from cumulative quantization errors and high complexity, especially in uniform planar array (UPA) systems. To address these issues, we first propose a tensor modelization method for NF cascaded channels by exploiting the tensor product among the horizontal and vertical response vectors of the UPA-structured base station (BS) and the incident-reflective array response vector of the IRS. This structure leverages spatial characteristics, enabling independent estimation of factor matrices to improve efficiency. Meanwhile, to avoid quantization errors, we propose an off-grid cascaded channel estimation framework based on sparse Tucker decomposition. Specifically, we model the received signal as a Tucker tensor, where the sparse core tensor captures path gain-delay terms and three factor matrices are spanned by BS and NF IRS array responses. We then formulate a sparse core tensor minimization problem with tri-modal log-sum sparsity constraints to tackle the NP-hard challenge. Finally, the method is accelerated via higher-order singular value decomposition preprocessing, combined with majorization-minimization and a tailored tensor over-relaxation fast iterative shrinkage-thresholding technique. We derive the Cramér-Rao lower bound and conduct convergence analysis. Simulations show the proposed scheme achieves a 13.6 dB improvement in normalized mean square error over benchmarks with significantly reduced runtime."}
{"id": "2602.14785", "categories": ["eess.AS", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.14785", "abs": "https://arxiv.org/abs/2602.14785", "authors": ["Fengyuan Cao", "Xinyu Liang", "Fredrik Cumlin", "Victor Ungureanu", "Chandan K. A. Reddy", "Christian Schuldt", "Saikat Chatterjee"], "title": "SA-SSL-MOS: Self-supervised Learning MOS Prediction with Spectral Augmentation for Generalized Multi-Rate Speech Assessment", "comment": "Accepted at ICASSP 2026", "summary": "Designing a speech quality assessment (SQA) system for estimating mean-opinion-score (MOS) of multi-rate speech with varying sampling frequency (16-48 kHz) is a challenging task. The challenge arises due to the limited availability of a MOS-labeled training dataset comprising multi-rate speech samples. While self-supervised learning (SSL) models have been widely adopted in SQA to boost performance, a key limitation is that they are pretrained on 16 kHz speech and therefore discard high-frequency information present in higher sampling rates. To address this issue, we propose a spectrogram-augmented SSL method that incorporates high-frequency features (up to 48 kHz sampling rate) through a parallel-branch architecture. We further introduce a two-step training scheme: the model is first pre-trained on a large 48 kHz dataset and then fine-tuned on a smaller multi-rate dataset. Experimental results show that leveraging high-frequency information overlooked by SSL features is crucial for accurate multi-rate SQA, and that the proposed two-step training substantially improves generalization when multi-rate data is limited."}
{"id": "2602.13835", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2602.13835", "abs": "https://arxiv.org/abs/2602.13835", "authors": ["Sripathi Sridhar", "Prem Seetharaman", "Oriol Nieto", "Mark Cartwright", "Justin Salamon"], "title": "Audiocards: Structured Metadata Improves Audio Language Models For Sound Design", "comment": "Accepted at ICASSP 2026", "summary": "Sound designers search for sounds in large sound effects libraries using aspects such as sound class or visual context. However, the metadata needed for such search is often missing or incomplete, and requires significant manual effort to add. Existing solutions to automate this task by generating metadata, i.e. captioning, and search using learned embeddings, i.e. text-audio retrieval, are not trained on metadata with the structure and information pertinent to sound design. To this end we propose audiocards, structured metadata grounded in acoustic attributes and sonic descriptors, by exploiting the world knowledge of LLMs. We show that training on audiocards improves downstream text-audio retrieval, descriptive captioning, and metadata generation on professional sound effects libraries. Moreover, audiocards also improve performance on general audio captioning and retrieval over the baseline single-sentence captioning approach. We release a curated dataset of sound effects audiocards to invite further research in audio language modeling for sound design."}
{"id": "2602.14001", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2602.14001", "abs": "https://arxiv.org/abs/2602.14001", "authors": ["Huy Trinh", "George Shaker"], "title": "Lightweight Range-Angle Imaging Based Algorithm for Quasi-Static Human Detection on Low-Cost FMCW Radar", "comment": null, "summary": "Quasi-static human activities such as lying, standing or sitting produce very low Doppler shifts and highly spread radar signatures, making them difficult to detect with conventional constant-false-alarm rate (CFAR) detectors tuned for point targets. Moreover, privacy concerns and low lighting conditions limit the use of cameras in long-term care (LTC) facilities. This paper proposes a lightweight, non-visual image-based method for robust quasi-static human presence detection using a low-cost 60 GHz FMCW radar. On a dataset covering five semi-static activities, the proposed method improves average detection accuracy from 68.3% for Cell-Averaging CFAR (CA-CFAR) and 78.8% for Order-Statistics CFAR (OS-CFAR) to 93.24% for Subject 1, from 51.3%, 68.3% to 92.3% for Subject 2, and 57.72%, 69.94% to 94.82% for Subject 3, respectively. Finally, we benchmarked all three detectors across all activities on a Raspberry Pi 4B using a shared Range-Angle (RA) preprocessing pipeline. The proposed algorithm obtains an average 8.2 ms per frame, resulting in over 120 frames per second (FPS) and a 74 times speed-up over OS-CFAR. These results demonstrate that simple image-based processing can provide robust and deployable quasi-static human sensing in cluttered indoor environments."}
{"id": "2602.13259", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2602.13259", "abs": "https://arxiv.org/abs/2602.13259", "authors": ["Xu Zhang", "Longbing Cao", "Runze Yang", "Zhangkai Wu"], "title": "Learning Physiology-Informed Vocal Spectrotemporal Representations for Speech Emotion Recognition", "comment": "13 pages, 5 figures", "summary": "Speech emotion recognition (SER) is essential for humanoid robot tasks such as social robotic interactions and robotic psychological diagnosis, where interpretable and efficient models are critical for safety and performance. Existing deep models trained on large datasets remain largely uninterpretable, often insufficiently modeling underlying emotional acoustic signals and failing to capture and analyze the core physiology of emotional vocal behaviors. Physiological research on human voices shows that the dynamics of vocal amplitude and phase correlate with emotions through the vocal tract filter and the glottal source. However, most existing deep models solely involve amplitude but fail to couple the physiological features of and between amplitude and phase. Here, we propose PhysioSER, a physiology-informed vocal spectrotemporal representation learning method, to address these issues with a compact, plug-and-play design. PhysioSER constructs amplitude and phase views informed by voice anatomy and physiology (VAP) to complement SSL models for SER. This VAP-informed framework incorporates two parallel workflows: a vocal feature representation branch to decompose vocal signals based on VAP, embed them into a quaternion field, and use Hamilton-structured quaternion convolutions for modeling their dynamic interactions; and a latent representation branch based on a frozen SSL backbone. Then, utterance-level features from both workflows are aligned by a Contrastive Projection and Alignment framework, followed by a shallow attention fusion head for SER classification. PhysioSER is shown to be interpretable and efficient for SER through extensive evaluations across 14 datasets, 10 languages, and 6 backbones, and its practical efficacy is validated by real-time deployment on a humanoid robotic platform."}
{"id": "2602.13891", "categories": ["cs.SD", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.13891", "abs": "https://arxiv.org/abs/2602.13891", "authors": ["Maohao Shen", "Tejas Jayashankar", "Osama Hanna", "Naoyuki Kanda", "Yancheng Wang", "Kateřina Žmolíková", "Ruiming Xie", "Niko Moritz", "Anfeng Xu", "Yashesh Gaur", "Gregory Wornell", "Qing He", "Jilong Wu"], "title": "GSRM: Generative Speech Reward Model for Speech RLHF", "comment": null, "summary": "Recent advances in speech language models, such as GPT-4o Voice Mode and Gemini Live, have demonstrated promising speech generation capabilities. Nevertheless, the aesthetic naturalness of the synthesized audio still lags behind that of human speech. Enhancing generation quality requires a reliable evaluator of speech naturalness. However, existing naturalness evaluators typically regress raw audio to scalar scores, offering limited interpretability of the evaluation and moreover fail to generalize to speech across different taxonomies. Inspired by recent advances in generative reward modeling, we propose the Generative Speech Reward Model (GSRM), a reasoning-centric reward model tailored for speech. The GSRM is trained to decompose speech naturalness evaluation into an interpretable acoustic feature extraction stage followed by feature-grounded chain-of-thought reasoning, enabling explainable judgments. To achieve this, we curated a large-scale human feedback dataset comprising 31k expert ratings and an out-of-domain benchmark of real-world user-assistant speech interactions. Experiments show that GSRM substantially outperforms existing speech naturalness predictors, achieving model-human correlation of naturalness score prediction that approaches human inter-rater consistency. We further show how GSRM can improve the naturalness of speech LLM generations by serving as an effective verifier for online RLHF."}
{"id": "2602.14004", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2602.14004", "abs": "https://arxiv.org/abs/2602.14004", "authors": ["Zhongqin Wang", "J. Andrew Zhang", "Kai Wu", "Y. Jay Guo"], "title": "Rethinking RSSI for WiFi Sensing", "comment": null, "summary": "The Received Signal Strength Indicator (RSSI) is widely available on commodity WiFi devices but is commonly regarded as too coarse for fine-grained sensing. This paper revisits its sensing potential and presents WiRSSI, a bistatic WiFi sensing framework for passive human tracking using only RSSI measurements. WiRSSI adopts a 1Tx-3Rx configuration and is readily extensible to Multiple-Input Multiple-Output (MIMO) deployments. We first reveal how CSI power implicitly encodes phase-related information and how this relationship carries over to RSSI, showing that RSSI preserves exploitable Doppler, Angle-of-Arrival (AoA), and delay cues associated with human motion. WiRSSI then extracts Doppler-AoA features via a 2D Fast Fourier Transform and infers delay from amplitude-only information in the absence of subcarrier-level phase. The estimated AoA and delay are then mapped to Cartesian coordinates and denoised to recover motion trajectories. Experiments in practical environments show that WiRSSI achieves median XY localization errors of 0.905 m, 0.784 m, and 0.785 m for elliptical, linear, and rectangular trajectories, respectively. In comparison, a representative CSI-based method attains median errors of 0.574 m, 0.599 m, and 0.514 m, corresponding to an average accuracy gap of 0.26 m. These results demonstrate that, despite its lower resolution, RSSI can support practical passive sensing and offers a low-cost alternative to CSI-based WiFi sensing."}
{"id": "2602.13596", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2602.13596", "abs": "https://arxiv.org/abs/2602.13596", "authors": ["Zhe Ye", "Xiangui Kang", "Jiayi He", "Chengxin Chen", "Wei Zhu", "Kai Wu", "Yin Yang", "Jiwu Huang"], "title": "BreathNet: Generalizable Audio Deepfake Detection via Breath-Cue-Guided Feature Refinement", "comment": "Under Review", "summary": "As deepfake audio becomes more realistic and diverse, developing generalizable countermeasure systems has become crucial. Existing detection methods primarily depend on XLS-R front-end features to improve generalization. Nonetheless, their performance remains limited, partly due to insufficient attention to fine-grained information, such as physiological cues or frequency-domain features. In this paper, we propose BreathNet, a novel audio deepfake detection framework that integrates fine-grained breath information to improve generalization. Specifically, we design BreathFiLM, a feature-wise linear modulation mechanism that selectively amplifies temporal representations based on the presence of breathing sounds. BreathFiLM is trained jointly with the XLS-R extractor, in turn encouraging the extractor to learn and encode breath-related cues into the temporal features. Then, we use the frequency front-end to extract spectral features, which are then fused with temporal features to provide complementary information introduced by vocoders or compression artifacts. Additionally, we propose a group of feature losses comprising Positive-only Supervised Contrastive Loss (PSCL), center loss, and contrast loss. These losses jointly enhance the discriminative ability, encouraging the model to separate bona fide and deepfake samples more effectively in the feature space. Extensive experiments on five benchmark datasets demonstrate state-of-the-art (SOTA) performance. Using the ASVspoof 2019 LA training set, our method attains 1.99% average EER across four related eval benchmarks, with particularly strong performance on the In-the-Wild dataset, where it achieves 4.70% EER. Moreover, under the ASVspoof5 evaluation protocol, our method achieves an EER of 4.94% on this latest benchmark."}
{"id": "2602.13928", "categories": ["cs.SD", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.13928", "abs": "https://arxiv.org/abs/2602.13928", "authors": ["Aju Ani Justus", "Ruchit Agrawal", "Sudarsana Reddy Kadiri", "Shrikanth Narayanan"], "title": "voice2mode: Phonation Mode Classification in Singing using Self-Supervised Speech Models", "comment": "Accepted to the Speech, Music and Mind (SMM26) workshop at the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP 2026). This is the preprint version of the paper to appear in the proceedings", "summary": "We present voice2mode, a method for classification of four singing phonation modes (breathy, neutral (modal), flow, and pressed) using embeddings extracted from large self-supervised speech models. Prior work on singing phonation has relied on handcrafted signal features or task-specific neural nets; this work evaluates the transferability of speech foundation models to singing phonation classification. voice2mode extracts layer-wise representations from HuBERT and two wav2vec2 variants, applies global temporal pooling, and classifies the pooled embeddings with lightweight classifiers (SVM, XGBoost). Experiments on a publicly available soprano dataset (763 sustained vowel recordings, four labels) show that foundation-model features substantially outperform conventional spectral baselines (spectrogram, mel-spectrogram, MFCC). HuBERT embeddings obtained from early layers yield the best result (~95.7% accuracy with SVM), an absolute improvement of ~12-15% over the best traditional baseline. We also show layer-wise behaviour: lower layers, which retain acoustic/phonetic detail, are more effective than top layers specialized for Automatic Speech Recognition (ASR)."}
{"id": "2602.14018", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2602.14018", "abs": "https://arxiv.org/abs/2602.14018", "authors": ["Eunsoo Kim", "Yoon Huh", "Wan Choi"], "title": "Extended Universal Joint Source-Channel Coding for Digital Semantic Communications: Improving Channel-Adaptability", "comment": null, "summary": "Recent advances in deep learning (DL)-based joint source-channel coding (JSCC) have enabled efficient semantic communication in dynamic wireless environments. Among these approaches, vector quantization (VQ)-based JSCC effectively maps high-dimensional semantic feature vectors into compact codeword indices for digital modulation. However, existing methods, including universal JSCC (uJSCC), rely on fixed, modulation-specific encoders, decoders, and codebooks, limiting adaptability to fine-grained SNR variations. We propose an extended universal JSCC (euJSCC) framework that achieves SNR- and modulation-adaptive transmission within a single model. euJSCC employs a hypernetwork-based normalization layer for fine-grained feature vector normalization and a dynamic codebook generation (DCG) network that refines modulation-specific base codebooks according to block-wise SNR. To handle block fading channels, which consist of multiple coherence blocks, an inner-outer encoder-decoder architecture is adopted, where the outer encoder and decoder capture long-term channel statistics, and the inner encoder and decoder refine feature vectors to align with block-wise codebooks. A two-phase training strategy, i.e., pretraining on AWGN channels followed by finetuning on block fading channels, ensures stable convergence. Experiments on image transmission demonstrate that euJSCC consistently outperforms state-of-the-art channel-adaptive digital JSCC schemes under both block fading and AWGN channels."}
{"id": "2602.13787", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2602.13787", "abs": "https://arxiv.org/abs/2602.13787", "authors": ["Lorenzo Picinali", "Robert Baumgartner", "Valerie Gaveau", "Antonino Greco", "Stefanie Liebe", "Paul Oomen", "Christoph Braun"], "title": "Enhancing spatial hearing with cochlear implants: exploring the role of AI, multimodal interaction and perceptual training", "comment": null, "summary": "Cochlear implants (CIs) have been developed to the point where they can restore hearing and speech understanding in a large proportion of patients. Although spatial hearing is central to controlling and directing attention and to enabling speech understanding in noisy environments, it has been largely neglected in the past. We propose here a multi-disciplinary research framework in which physicians, psychologists and engineers collaborate to improve spatial hearing for CI users."}
{"id": "2602.13954", "categories": ["cs.SD", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.13954", "abs": "https://arxiv.org/abs/2602.13954", "authors": ["Dan Zhang", "Yishu Lei", "Jing Hu", "Shuwei He", "Songhe Deng", "Xianlong Luo", "Danxiang Zhu", "Shikun Feng", "Rui Liu", "Jingzhou He", "Yu Sun", "Hua Wu", "Haifeng Wang"], "title": "Eureka-Audio: Triggering Audio Intelligence in Compact Language Models", "comment": "23 pages, 4 figures", "summary": "We present Eureka-Audio, a compact yet high-performance audio language model that achieves competitive performance against models that are 4 to 18 times larger across a broad range of audio understanding benchmarks. Despite containing only 1.7B parameters, Eureka-Audio demonstrates strong performance on automatic speech recognition (ASR), audio understanding, and dense audio captioning, matching or surpassing multiple 7B to 30B audio and omni-modal baselines. The model adopts a unified end-to-end architecture composed of a lightweight language backbone, a Whisper-based audio encoder, and a sparsely activated Mixture-of-Experts (MoE) adapter that explicitly accounts for audio heterogeneity and alleviates cross-modal optimization conflicts under limited capacity. To further enhance paralinguistic reasoning, we introduce DataFlux, a closed loop audio instruction data synthesis and verification pipeline that constructs high quality, logically consistent supervision from raw audio. Extensive evaluations across ASR, knowledge reasoning, safety, instruction following, and paralinguistic benchmarks, demonstrate that Eureka-Audio achieves an efficient balance between computational cost and performance. These results establish Eureka Audio as a strong and practical baseline for lightweight audio understanding models."}
{"id": "2602.14063", "categories": ["eess.SP", "cs.IT"], "pdf": "https://arxiv.org/pdf/2602.14063", "abs": "https://arxiv.org/abs/2602.14063", "authors": ["Sajad Daei", "Gábor Fodor", "Mikael Skoglund"], "title": "Convexity Meets Curvature: Lifted Near-Field Super-Resolution", "comment": null, "summary": "Extra-large apertures, high carrier frequencies, and integrated sensing and communications (ISAC) are pushing array processing into the Fresnel region, where spherical wavefronts induce a range-dependent phase across the aperture. This curvature breaks the Fourier/Vandermonde structure behind classical subspace methods, and it is especially limiting with hybrid front-ends that provide only a small number of pilot measurements. Consequently, practical systems need continuous angle resolution and joint angle-range inference where many near-field approaches still rely on costly 2D gridding. We show that convexity can meet curvature via a lifted, gridless superresolution framework for near-field measurements. The key is a Bessel-Vandermonde factorization of the Fresnel-phase manifold that exposes a hidden Vandermonde structure in angle while isolating the range dependence into a compact coefficient map. Building on this, we introduce a lifting that maps each range bin and continuous angle to a structured rank-one atom, converting the nonlinear near-field model into a linear inverse problem over a row-sparse matrix. Recovery is posed as atomic-norm minimization and an explicit dual characterization via bounded trigonometric polynomials yields certificate-based localization that super-resolves off-grid angles and identifies active range bins. Simulations with strongly undersampled hybrid observations validate reliable joint angle-range recovery for next-generation wireless and ISAC systems."}
{"id": "2602.13834", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2602.13834", "abs": "https://arxiv.org/abs/2602.13834", "authors": ["Minhui Lu", "Joshua D. Reiss"], "title": "Learning Vocal-Tract Area and Radiation with a Physics-Informed Webster Model", "comment": "Accepted at IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP) 2026", "summary": "We present a physics-informed voiced backend renderer for singing-voice synthesis. Given synthetic single-channel audio and a fund-amental--frequency trajectory, we train a time-domain Webster model as a physics-informed neural network to estimate an interpretable vocal-tract area function and an open-end radiation coefficient. Training enforces partial differential equation and boundary consistency; a lightweight DDSP path is used only to stabilize learning, while inference is purely physics-based. On sustained vowels (/a/, /i/, /u/), parameters rendered by an independent finite-difference time-domain Webster solver reproduce spectral envelopes competitively with a compact DDSP baseline and remain stable under changes in discretization, moderate source variations, and about ten percent pitch shifts. The in-graph waveform remains breathier than the reference, motivating periodicity-aware objectives and explicit glottal priors in future work."}
{"id": "2602.14127", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2602.14127", "abs": "https://arxiv.org/abs/2602.14127", "authors": ["Reda Bensaid", "Amine Ouasfi", "Yassir Bendou", "Ilyass Moummad", "Vincent Gripon", "François Leduc-Primeau", "Adnane Boukhayma"], "title": "MUKA: Multi Kernel Audio Adaptation Of Audio-Language Models", "comment": null, "summary": "Multimodal foundation models have demonstrated impressive generalization capabilities, yet efficiently adapting them to new tasks in a few-shot setting remains a critical challenge. In this work, we investigate the few-shot adaptation of Large Audio-Language Models (ALMs) through both training-based and training-free approaches. We introduce MUKA, a multi-kernel adaptation framework that combines the fine-grained, context-dependent representations of instruction-tuning based models like Pengi with the global semantic representations of contrastive pretraining methods like CLAP. By constructing a product kernel that aligns local similarity with global semantics, MUKA enhances representational power while preserving the theoretical guarantees of kernel methods and avoiding additional training. Extensive experiments across 11 diverse audio datasets demonstrate that MUKA achieves state-of-the-art performance among training-free methods and even surpasses training-based adapters in several scenarios, offering a compelling balance between adaptability and efficiency."}
{"id": "2602.14094", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2602.14094", "abs": "https://arxiv.org/abs/2602.14094", "authors": ["Meng Hua", "Itsik Bergel", "Tolga Girici", "Marco Di Renzo", "Deniz Gunduz"], "title": "Wireless Physical Neural Networks (WPNNs): Opportunities and Challenges", "comment": null, "summary": "Wireless communication systems exhibit structural and functional similarities to neural networks: signals propagate through cascaded elements, interact with the environment, and undergo transformations. Building upon this perspective, we introduce a unified paradigm, termed \\textit{wireless physical neural networks (WPNNs)}, in which components of a wireless network, such as transceivers, relays, backscatter, and intelligent surfaces, are interpreted as computational layers within a learning architecture. By treating the wireless propagation environment and network elements as differentiable operators, new opportunities arise for joint communication-computation designs, where system optimization can be achieved through learning-based methods applied directly to the physical network. This approach may operate independently of, or in conjunction with, conventional digital neural layers, enabling hybrid communication learning pipelines. In the article, we outline representative architectures that embody this viewpoint and discuss the algorithmic and training considerations required to leverage the wireless medium as a computational resource. Through numerical examples, we highlight the potential performance gains in processing, adaptability, efficiency, and end-to-end optimization, demonstrating the promise of reconfiguring wireless systems as learning networks in next-generation communication frameworks."}
{"id": "2602.14291", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2602.14291", "abs": "https://arxiv.org/abs/2602.14291", "authors": ["H. M. Shadman Tabib", "Istiak Ahmmed Rifti", "Abdullah Muhammed Amimul Ehsan", "Somik Dasgupta", "Md Zim Mim Siddiqee Sowdha", "Abrar Jahin Sarker", "Md. Rafiul Islam Nijamy", "Tanvir Hossain", "Mst. Metaly Khatun", "Munzer Mahmood", "Rakesh Debnath", "Gourab Biswas", "Asif Karim", "Wahid Al Azad Navid", "Masnoon Muztahid", "Fuad Ahmed Udoy", "Shahad Shahriar Rahman", "Md. Tashdiqur Rahman Shifat", "Most. Sonia Khatun", "Mushfiqur Rahman", "Md. Miraj Hasan", "Anik Saha", "Mohammad Ninad Mahmud Nobo", "Soumik Bhattacharjee", "Tusher Bhomik", "Ahmmad Nur Swapnil", "Shahriar Kabir"], "title": "Bengali-Loop: Community Benchmarks for Long-Form Bangla ASR and Speaker Diarization", "comment": null, "summary": "Bengali (Bangla) remains under-resourced in long-form speech technology despite its wide use. We present Bengali-Loop, two community benchmarks to address this gap: (1) a long-form ASR corpus of 191 recordings (158.6 hours, 792k words) from 11 YouTube channels, collected via a reproducible subtitle-extraction pipeline and human-in-the-loop transcript verification; and (2) a speaker diarization corpus of 24 recordings (22 hours, 5,744 annotated segments) with fully manual speaker-turn labels in CSV format. Both benchmarks target realistic multi-speaker, long-duration content (e.g., Bangla drama/natok). We establish baselines (Tugstugi: 34.07% WER; pyannote.audio: 40.08% DER) and provide standardized evaluation protocols (WER/CER, DER), annotation rules, and data formats to support reproducible benchmarking and future model development for Bangla long-form ASR and diarization."}
{"id": "2602.14172", "categories": ["cs.SD", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.14172", "abs": "https://arxiv.org/abs/2602.14172", "authors": ["Keinichi Fujita", "Yusuke Ijima"], "title": "Investigation for Relative Voice Impression Estimation", "comment": "5 pages,3 figures, Accepted to Speech Prosody 2026", "summary": "Paralinguistic and non-linguistic aspects of speech strongly influence listener impressions. While most research focuses on absolute impression scoring, this study investigates relative voice impression estimation (RIE), a framework for predicting the perceptual difference between two utterances from the same speaker. The estimation target is a low-dimensional vector derived from subjective evaluations, quantifying the perceptual shift of the second utterance relative to the first along an antonymic axis (e.g., ``Dark--Bright''). To isolate expressive and prosodic variation, we used recordings of a professional speaker reading a text in various styles. We compare three modeling approaches: classical acoustic features commonly used for speech emotion recognition, self-supervised speech representations, and multimodal large language models (MLLMs). Our results demonstrate that models using self-supervised representations outperform methods with classical acoustic features, particularly in capturing complex and dynamic impressions (e.g., ``Cold--Warm'') where classical features fail. In contrast, current MLLMs prove unreliable for this fine-grained pairwise task. This study provides the first systematic investigation of RIE and demonstrates the strength of self-supervised speech models in capturing subtle perceptual variations."}
{"id": "2602.14152", "categories": ["eess.SP", "physics.app-ph"], "pdf": "https://arxiv.org/pdf/2602.14152", "abs": "https://arxiv.org/abs/2602.14152", "authors": ["Philipp del Hougne"], "title": "Electromagnetic Bounds on Realizing Targeted MIMO Transfer Functions in Real-World Systems with Wave-Domain Programmability", "comment": "12 pages with 3 figures", "summary": "A key question for most applications involving reconfigurable linear wave systems is how accurately a desired linear operator can be realized by configuring the system's tunable elements. The relevance of this question spans from hybrid-MIMO analog combiners via computational meta-imagers to programmable wave-domain signal processing. Yet, no electromagnetically consistent bounds have been derived for the fidelity with which a desired operator can be realized in a real-world reconfigurable wave system. Here, we derive such bounds based on an electromagnetically consistent multiport-network model (capturing mutual coupling between tunable elements) and accounting for real-world hardware constraints (lossy, 1-bit-programmable elements). Specifically, we formulate the operator-synthesis task as a quadratically constrained fractional-quadratic problem and compute rigorous fidelity upper bounds based on semidefinite relaxation. We apply our technique to three distinct experimental setups. The first two setups are, respectively, a free-space and a rich-scattering $4\\times 4$ MIMO channel at 2.45 GHz parameterized by a reconfigurable intelligent surface (RIS) comprising 100 1-bit-programmable elements. The third setup is a $4\\times 4$ MIMO channel at 19 GHz from four feeds of a dynamic metasurface antenna (DMA) to four users. We systematically study how the achievable fidelity scales with the number of tunable elements, and we probe the tightness of our bounds by trying to find optimized configurations approaching the bounds with standard discrete-optimization techniques. We observe a strong influence of the coupling strength between tunable elements on our fidelity bound. For the two RIS-based setups, our bound attests to insufficient wave-domain flexibility for the considered operator synthesis."}
{"id": "2602.14224", "categories": ["cs.SD", "cs.CL", "cs.MM"], "pdf": "https://arxiv.org/pdf/2602.14224", "abs": "https://arxiv.org/abs/2602.14224", "authors": ["Ziyang Ma", "Ruiyang Xu", "Yinghao Ma", "Chao-Han Huck Yang", "Bohan Li", "Jaeyeon Kim", "Jin Xu", "Jinyu Li", "Carlos Busso", "Kai Yu", "Eng Siong Chng", "Xie Chen"], "title": "The Interspeech 2026 Audio Reasoning Challenge: Evaluating Reasoning Process Quality for Audio Reasoning Models and Agents", "comment": "The official website of the Audio Reasoning Challenge: https://audio-reasoning-challenge.github.io", "summary": "Recent Large Audio Language Models (LALMs) excel in understanding but often lack transparent reasoning. To address this \"black-box\" limitation, we organized the Audio Reasoning Challenge at Interspeech 2026, the first shared task dedicated to evaluating Chain-of-Thought (CoT) quality in the audio domain. The challenge introduced MMAR-Rubrics, a novel instance-level protocol assessing the factuality and logic of reasoning chains. Featured Single Model and Agent tracks, the competition attracting 156 teams from 18 countries and regions. Results show agent systems currently lead in reasoning quality, utilizing iterative tool orchestration and cross-modal analysis. Besides, single models are rapidly advancing via reinforcement learning and sophisticated data pipeline. We details the challenge design, methodology, and a comprehensive analysis of state-of-the-art systems, providing new insights for explainable audio intelligence."}
{"id": "2602.14170", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2602.14170", "abs": "https://arxiv.org/abs/2602.14170", "authors": ["Yu Zhu", "Jiayang Guo", "Jun Jiang", "Peipei Gu", "Xin Shu", "Duo Chen"], "title": "Explainable Interictal Epileptiform Discharge Detection Method Based on Scalp EEG and Retrieval-Augmented Generation", "comment": null, "summary": "The detection of interictal epileptiform discharge (IED) is crucial for the diagnosis of epilepsy, but automated methods often lack interpretability. This study proposes IED-RAG, an explainable multimodal framework for joint IED detection and report generation. Our approach employs a dual-encoder to extract electrophysiological and semantic features, aligned via contrastive learning in a shared EEG-text embedding space. During inference, clinically relevant EEG-text pairs are retrieved from a vector database as explicit evidence to condition a large language model (LLM) for the generation of evidence-based reports. Evaluated on a private dataset from Wuhan Children's Hospital and the public TUH EEG Events Corpus (TUEV), the framework achieved balanced accuracies of 89.17\\% and 71.38\\%, with BLEU scores of 89.61\\% and 64.14\\%, respectively. The results demonstrate that retrieval of explicit evidence enhances both diagnostic performance and clinical interpretability compared to standard black-box methods."}
{"id": "2602.14291", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2602.14291", "abs": "https://arxiv.org/abs/2602.14291", "authors": ["H. M. Shadman Tabib", "Istiak Ahmmed Rifti", "Abdullah Muhammed Amimul Ehsan", "Somik Dasgupta", "Md Zim Mim Siddiqee Sowdha", "Abrar Jahin Sarker", "Md. Rafiul Islam Nijamy", "Tanvir Hossain", "Mst. Metaly Khatun", "Munzer Mahmood", "Rakesh Debnath", "Gourab Biswas", "Asif Karim", "Wahid Al Azad Navid", "Masnoon Muztahid", "Fuad Ahmed Udoy", "Shahad Shahriar Rahman", "Md. Tashdiqur Rahman Shifat", "Most. Sonia Khatun", "Mushfiqur Rahman", "Md. Miraj Hasan", "Anik Saha", "Mohammad Ninad Mahmud Nobo", "Soumik Bhattacharjee", "Tusher Bhomik", "Ahmmad Nur Swapnil", "Shahriar Kabir"], "title": "Bengali-Loop: Community Benchmarks for Long-Form Bangla ASR and Speaker Diarization", "comment": null, "summary": "Bengali (Bangla) remains under-resourced in long-form speech technology despite its wide use. We present Bengali-Loop, two community benchmarks to address this gap: (1) a long-form ASR corpus of 191 recordings (158.6 hours, 792k words) from 11 YouTube channels, collected via a reproducible subtitle-extraction pipeline and human-in-the-loop transcript verification; and (2) a speaker diarization corpus of 24 recordings (22 hours, 5,744 annotated segments) with fully manual speaker-turn labels in CSV format. Both benchmarks target realistic multi-speaker, long-duration content (e.g., Bangla drama/natok). We establish baselines (Tugstugi: 34.07% WER; pyannote.audio: 40.08% DER) and provide standardized evaluation protocols (WER/CER, DER), annotation rules, and data formats to support reproducible benchmarking and future model development for Bangla long-form ASR and diarization."}
{"id": "2602.14181", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2602.14181", "abs": "https://arxiv.org/abs/2602.14181", "authors": ["Isaac Skog", "Manon Kok", "Christophe Prieur", "Gustaf Hendeby"], "title": "Localization Exploiting Spatial Variations in the Magnetic Field: Principles and Challenges", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Signal processing has played, and continues to play, a fundamental role in the evolution of modern localization technologies. Localization using spatial variations in the Earth's magnetic field is no exception. It relies on signal-processing methods for statistical state inference, magnetic-field modeling, and sensor calibration. Contemporary localization techniques based on spatial variations in the magnetic field can provide decimeter-level indoor localization accuracy and outdoor localization accuracy on par with strategic-grade inertial navigation systems. This article provides a broad, high-level overview of current signal-processing principles and open research challenges in localization using spatial variations in the Earth's magnetic field. The aim is to provide the reader with an understanding of the similarities and differences among existing key technologies from a statistical signal-processing perspective. To that end, existing key technologies will be presented within a common parametric signal-model framework compatible with well-established statistical inference methods."}
{"id": "2602.14664", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2602.14664", "abs": "https://arxiv.org/abs/2602.14664", "authors": ["Parth Khadse", "Sunil Kumar Kopparapu"], "title": "Probing Human Articulatory Constraints in End-to-End TTS with Reverse and Mismatched Speech-Text Directions", "comment": "A shorter version of this paper appeared in ACPR 2025", "summary": "An end-to-end (e2e) text-to-speech (TTS) system is a deep architecture that learns to associate a text string with acoustic speech patterns from a curated dataset. It is expected that all aspects associated with speech production, such as phone duration, speaker characteristics, and intonation among other things are captured in the trained TTS model to enable the synthesized speech to be natural and intelligible. Human speech is complex, involving smooth transitions between articulatory configurations (ACs). Due to anatomical constraints, some ACs are challenging to mimic or transition between. In this paper, we experimentally study if the constraints imposed by human anatomy have an implication on training an e2e-TTS systems. We experiment with two e2e-TTS architectures, namely, Tacotron-2 an autoregressive model and VITS-TTS a non-autoregressive model. In this study, we build TTS systems using (a) forward text, forward speech (conventional, e2e-TTS), (b) reverse text, reverse speech (r-e2e-TTS), and (c) reverse text, forward speech (rtfs-e2e-TTS). Experiments demonstrate that e2e-TTS systems are purely data-driven. Interestingly, the generated speech by r-e2e-TTS systems exhibits better fidelity, better perceptual intelligibility, and better naturalness"}
{"id": "2602.14191", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2602.14191", "abs": "https://arxiv.org/abs/2602.14191", "authors": ["Hamid Reza Hashempour", "Le-Nam Tran", "Duy H. N. Nguyen", "Hien Quoc Ngo"], "title": "Robust SAC-Enabled UAV-RIS Assisted Secure MISO Systems With Untrusted EH Receivers", "comment": null, "summary": "This paper investigates secure downlink transmission in a UAV-assisted reconfigurable intelligent surface (RIS)-enabled multiuser multiple-input single-output network, where legitimate information-harvesting receivers coexist with untrusted energy-harvesting receivers (UEHRs) capable of eavesdropping. A UAV-mounted RIS provides blockage mitigation and passive beamforming, while the base station employs zero-forcing precoding for multiuser interference suppression. Due to limited feedback from UEHRs, their channel state information (CSI) is imperfect, leading to a worst-case secrecy energy efficiency (WCSEE) maximization problem. We jointly optimize the UAV horizontal position, RIS phase shifts, and transmit power allocation under both perfect and imperfect CSI, considering discrete RIS phases, UAV mobility, and energy-harvesting constraints. The resulting problem is highly nonconvex due to coupled channel geometry, robustness requirements, and discrete variables. To address this challenge, we propose a soft actor-critic (SAC)-based deep reinforcement learning framework that learns WCSEE-maximizing policies through interaction with the wireless environment. As a structured benchmark, a successive convex approximation (SCA) approach is developed for the perfect CSI case with continuous RIS phases. Simulation results show that the proposed SAC method achieves up to 28% and 16% secrecy energy efficiency gains over SCA and deep deterministic policy gradient baselines, respectively, while demonstrating superior robustness to CSI uncertainty and stable performance across varying transmit power levels and RIS sizes."}
{"id": "2602.14584", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2602.14584", "abs": "https://arxiv.org/abs/2602.14584", "authors": ["Yacouba Kaloga", "Marina Laganaro", "Ina Kodrasi"], "title": "CLAP-Based Automatic Word Naming Recognition in Post-Stroke Aphasia", "comment": "Submitted to EUSIPCO 2026", "summary": "Conventional automatic word-naming recognition systems struggle to recognize words from post-stroke patients with aphasia because of disfluencies and mispronunciations, limiting reliable automated assessment in this population. In this paper, we propose a Contrastive Language-Audio Pretraining (CLAP) based approach for automatic word-naming recognition to address this challenge by leveraging text-audio alignment. Our approach treats word-naming recognition as an audio-text matching problem, projecting speech signals and textual prompts into a shared embedding space to identify intended words even in challenging recordings. Evaluated on two speech datasets of French post-stroke patients with aphasia, our approach achieves up to 90% accuracy, outperforming existing classification-based and automatic speech recognition-based baselines."}
{"id": "2602.14292", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2602.14292", "abs": "https://arxiv.org/abs/2602.14292", "authors": ["Weijie Xiong", "Jian Yang", "Jingran Lin", "Hongli Liu", "Zhiling Xiao", "Qiang Li"], "title": "Low-Cost Physical-Layer Security Design for IRS-Assisted mMIMO Systems with One-Bit DACs", "comment": null, "summary": "Integrating massive multiple-input multiple-output (mMIMO) systems with intelligent reflecting surfaces (IRS) presents a promising paradigm for enhancing physical-layer security (PLS) in wireless communications. However, deploying high-resolution quantizers in large-scale mMIMO arrays, along with numerous IRS elements, leads to substantial hardware complexity. To address these challenges, this paper proposes a cost-effective PLS design for IRS-assisted mMIMO systems by employing one-bit digital-to-analog converters (DACs). The focus is on jointly optimizing one-bit quantized precoding at the transmitter and constant-modulus phase shifts at the IRS to maximize the secrecy rate. This leads to a highly non-convex fractional secrecy rate maximization (SRM) problem. To efficiently solve this problem, two algorithms are proposed: (1) the WMMSE-PDD algorithm, which reformulates the SRM problem into a sequence of non-fractional programs with auxiliary variables using the weighted minimum mean-square error (WMMSE) method and solves them via the penalty dual decomposition (PDD) approach, achieving superior secrecy performance; and (2) the exact penalty product Riemannian gradient descent (EPPRGD) algorithm, which transforms the SRM problem into an unconstrained optimization over a product Riemannian manifold, eliminating auxiliary variables and enabling faster convergence with a slight trade-off in secrecy performance. Both algorithms provide analytical solutions at each iteration and are proven to converge to Karush-Kuhn-Tucker (KKT) points. Simulation results confirm the effectiveness of the proposed methods and highlight their respective advantages."}
{"id": "2602.14411", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2602.14411", "abs": "https://arxiv.org/abs/2602.14411", "authors": ["Ayano Nakai-Kasai", "Yusuke Nakane", "Tadashi Wadayama"], "title": "Online Architecture Search for Compressed Sensing based on Hypergradient Descent", "comment": "Submitted to EUSIPCO 2026", "summary": "AS-ISTA (Architecture Searched-Iterative Shrinkage Thresholding Algorithm) and AS-FISTA (AS-Fast ISTA) are compressed sensing algorithms introducing structural parameters to ISTA and FISTA to enable architecture search within the iterative process. The structural parameters are determined using deep unfolding, but this approach requires training data and the large overhead of training time. In this paper, we propose HGD-AS-ISTA (Hypergradient Descent-AS-ISTA) and HGD-AS-FISTA that use hypergradient descent, which is an online hyperparameter optimization method, to determine the structural parameters. Experimental results show that the proposed method improves performance of the conventional ISTA/FISTA while avoiding the need for re-training when the environment changes."}
{"id": "2602.14415", "categories": ["eess.SP", "cs.IT"], "pdf": "https://arxiv.org/pdf/2602.14415", "abs": "https://arxiv.org/abs/2602.14415", "authors": ["Huyen-Trang Ta", "Ngoc-Son Duong", "Trung-Hieu Nguyen", "Van-Linh Nguyen", "Thai-Mai Dinh"], "title": "Reconfigurable Intelligent Surfaces-assisted Positioning in Integrated Sensing and Communication Systems", "comment": "accepted at INFOCOM 2026 Workshop ISAC-FutureG", "summary": "This paper investigates the problem of high-precision target localization in integrated sensing and communication (ISAC) systems, where the target is sensed via both a direct path and a reconfigurable intelligent surface (RIS)-assisted reflection path. We first develop a sequential matched-filter estimator to acquire coarse angular parameters, followed by a range recovery process based on subcarrier phase differences. Subsequently, we formulate the target localization problem as a non-linear least squares optimization, using the coarse estimates to initialize the target's position coordinates. To solve this efficiently, we introduce a fast iterative refinement algorithm tailored for RIS-aided ISAC environments. Recognizing that the signal model involves both linear path gains and non-linear geometric dependencies, we exploit the separable least-squares structure to decouple these parameters. Furthermore, we propose a modified Levenberg algorithm with an approximation strategy, which enables low-cost parameter updates without necessitating repeated evaluations of the full non-linear model. Simulation results show that the proposed refinement method achieves accuracy comparable to conventional approaches, while significantly reducing algorithmic complexity."}
{"id": "2602.14453", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2602.14453", "abs": "https://arxiv.org/abs/2602.14453", "authors": ["Haofan Dong", "Ozgur B. Akan"], "title": "Cramer--Rao Bounds for Magneto-Inductive Integrated Sensing and Communications", "comment": null, "summary": "Magnetic induction (MI) enables communication in RF-denied environments (underground, underwater, in-body), where the medium conductivity imprints a deterministic signature on the channel. This letter derives a closed-form Cramér--Rao bound (CRB) for the joint estimation of range and medium conductivity from MI pilot observations in an integrated sensing and communication (ISAC) framework. The Fisher information matrix reveals that the joint estimation penalty converges to 3\\,dB in the near-field regime, meaning conductivity sensing adds at most a factor-of-two loss in ranging precision. Monte Carlo maximum-likelihood simulations confirm that the CRB is achievable under practical operating conditions."}
{"id": "2602.14583", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2602.14583", "abs": "https://arxiv.org/abs/2602.14583", "authors": ["Rumeshika Pallewela", "Filip Elvander"], "title": "All-pole centroids in the Wasserstein metric with applications to clustering of spectral densities", "comment": "Submitted for EUSIPCO 2026", "summary": "In this work, we propose a method for computing centroids, or barycenters, in the spectral Wasserstein-2 metric for sets of power spectral densities, where the barycenters are restricted to belong to the set of all-pole spectra with a certain model order. This may be interpreted as finding an autoregressive representative for sets of second-order stationary Gaussian processes. While Wasserstein, or optimal transport, barycenters have been successfully used earlier in problems of spectral estimation and clustering, the resulting barycenters are non-parametric and the complexity of representing and storing them depends on, e.g., the choice of discretization grid. In contrast, the herein proposed method yields compact, low-dimensional, and interpretable spectral centroids that can be used in downstream tasks. Computing the all-pole centroids corresponds to solving a non-convex optimization problem in the model parameters, and we present a gradient descent scheme for addressing this. Although convergence to a globally optimal point cannot be guaranteed, the sub-optimality of the obtained centroids can be quantified. The proposed method is illustrated on a problem of phoneme classification."}
{"id": "2602.14590", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2602.14590", "abs": "https://arxiv.org/abs/2602.14590", "authors": ["Leonardo Di Nino", "Tiziana Cattai", "Sergio Barbarossa", "Ginestra Bianconi", "Paolo Di Lorenzo"], "title": "Learning Dirac Spectral Transforms for Topological Signals", "comment": null, "summary": "The Dirac operator provides a unified framework for processing signals defined over different order topological domains, such as node and edge signals. Its eigenmodes define a spectral representation that inherently captures cross-domain interactions, in contrast to conventional Hodge-Laplacian eigenmodes that operate within a single topological dimension. In this paper, we compare the two alternatives in terms of the distortion/sparsity trade-off and we show how an overcomplete basis built concatenating the two dictionaries can provide better performance with respect to each approach. Then, we propose a parameterized nonredundant transform whose eigenmodes incorporate a mode-specific mass parameter that captures the interplay between node and edge modes. Interestingly, we show that learning the mass parameters from data makes the proposed transform able to achieve the best distortion-sparsity tradeoff with respect to both complete and overcomplete bases."}
{"id": "2602.14629", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2602.14629", "abs": "https://arxiv.org/abs/2602.14629", "authors": ["Lucas Giroto", "Marcus Henninger", "Silvio Mandelli"], "title": "Synthetic Aperture Communication: Principles and Application to Massive IoT Satellite Uplink", "comment": null, "summary": "While synthetic aperture radar is widely adopted to provide high-resolution imaging at long distances using small arrays, the concept of coherent synthetic aperture communication (SAC) has not yet been explored. This article introduces the principles of SAC for direct satellite-to-device uplink, showcasing precise direction-of-arrival estimation for user equipment (UE) devices, facilitating spatial signal separation, localization, and easing link budget constraints. Simulations for a low Earth orbit satellite at 600 km orbit and two UE devices performing orthogonal frequency-division multiplexing-based transmission with polar coding at 3.5 GHz demonstrate block error rates below 0.1 with transmission powers as low as -10 dBm, even under strong interference when UE devices are resolved but fall on each other's strongest angular sidelobe. These results validate the ability of the proposed scheme to address mutual interference and stringent power limitations, paving the way for massive Internet of Things connectivity in non-terrestrial networks."}
{"id": "2602.14833", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.14833", "abs": "https://arxiv.org/abs/2602.14833", "authors": ["Hang Zou", "Yu Tian", "Bohao Wang", "Lina Bariah", "Samson Lasaulce", "Chongwen Huang", "Mérouane Debbah"], "title": "RF-GPT: Teaching AI to See the Wireless World", "comment": null, "summary": "Large language models (LLMs) and multimodal models have become powerful general-purpose reasoning systems. However, radio-frequency (RF) signals, which underpin wireless systems, are still not natively supported by these models. Existing LLM-based approaches for telecom focus mainly on text and structured data, while conventional RF deep-learning models are built separately for specific signal-processing tasks, highlighting a clear gap between RF perception and high-level reasoning. To bridge this gap, we introduce RF-GPT, a radio-frequency language model (RFLM) that utilizes the visual encoders of multimodal LLMs to process and understand RF spectrograms. In this framework, complex in-phase/quadrature (IQ) waveforms are mapped to time-frequency spectrograms and then passed to pretrained visual encoders. The resulting representations are injected as RF tokens into a decoder-only LLM, which generates RF-grounded answers, explanations, and structured outputs. To train RF-GPT, we perform supervised instruction fine-tuning of a pretrained multimodal LLM using a fully synthetic RF corpus. Standards-compliant waveform generators produce wideband scenes for six wireless technologies, from which we derive time-frequency spectrograms, exact configuration metadata, and dense captions. A text-only LLM then converts these captions into RF-grounded instruction-answer pairs, yielding roughly 12,000 RF scenes and 0.625 million instruction examples without any manual labeling. Across benchmarks for wideband modulation classification, overlap analysis, wireless-technology recognition, WLAN user counting, and 5G NR information extraction, RF-GPT achieves strong multi-task performance, whereas general-purpose VLMs with no RF grounding largely fail."}
{"id": "2602.14937", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2602.14937", "abs": "https://arxiv.org/abs/2602.14937", "authors": ["Taran Anusorn", "Byeongjin Kim", "Ian Anderson", "Ziqian Yao", "Ruochen Lu"], "title": "Lattice XBAR Filters in Thin-Film Lithium Niobate", "comment": null, "summary": "This work presents the demonstration of lattice filters based on laterally excited bulk acoustic resonators (XBARs). Two filter implementations, namely direct lattice and layout-balanced lattice topologies, are designed and fabricated in periodically poled piezoelectric film (P3F) thin-film lithium niobate (TFLN). By leveraging the strong electromechanical coupling of XBARs in P3F TFLN together with the inherently wideband nature of the lattice topology, 3-dB fractional bandwidths (FBWs) of 27.42\\% and 39.11\\% and low insertion losses (ILs) of 0.88 dB and 0.96 dB are achieved at approximately 20 GHz for the direct and layout-balanced lattice filters, respectively, under conjugate matching. Notably, all prototypes feature compact footprints smaller than 1.3 mm\\textsuperscript{2}. These results highlight the potential of XBAR-based lattice architectures to enable low-loss, wideband acoustic filters for compact, high-performance RF front ends in next-generation wireless communication and sensing systems, while also identifying key challenges and directions for further optimization."}
{"id": "2602.14985", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2602.14985", "abs": "https://arxiv.org/abs/2602.14985", "authors": ["Tara Esmaeilbeig", "Kartik Patel", "Traian E. Abrudan", "John Kimionis", "Eleftherios Kampianakis", "Michael S. Eggleston"], "title": "Real-time Range-Angle Estimation and Tag Localization for Multi-static Backscatter Systems", "comment": null, "summary": "Multi-static backscatter networks (BNs) are strong candidates for joint communication and localization in the ambient IoT paradigm for 6G. Enabling real-time localization in large-scale multi-static deployments with thousands of devices require highly efficient algorithms for estimating key parameters such as range and angle of arrival (AoA), and for fusing these parameters into location estimates. We propose two low-complexity algorithms, Joint Range-Angle Clustering (JRAC) and Stage-wise Range-Angle Estimation (SRAE). Both deliver range and angle estimation accuracy comparable to FFT- and subspace-based baselines while significantly reducing the computation. We then introduce two real-time localization algorithms that fuse the estimated ranges and AoAs: a maximum-likelihood (ML) method solved via gradient search and an iterative re-weighted least squares (IRLS) method. Both achieve localization accuracy comparable to ML-based brute force search albeit with far lower complexity. Experiments on a real-world large-scale multi-static testbed with 4 illuminators, 1 multi-antenna receiver, and 100 tags show that JRAC and SRAE reduce runtime by up to 40X and IRLS achieves up to 500X reduction over ML-based brute force search without degrading localization accuracy. The proposed methods achieve 3 m median localization error across all 100 tags in a sub-6GHz band with 40 MHz bandwidth. These results demonstrate that multi-static range-angle estimation and localization algorithms can make real-time, scalable backscatter localization practical for next-generation ambient IoT networks."}

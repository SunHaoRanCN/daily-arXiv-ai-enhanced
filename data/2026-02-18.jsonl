{"id": "2602.15307", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2602.15307", "abs": "https://arxiv.org/abs/2602.15307", "authors": ["Takao Kawamura", "Daisuke Niizumi", "Nobutaka Ono"], "title": "What Do Neurons Listen To? A Neuron-level Dissection of a General-purpose Audio Model", "comment": "5 pages, 8 figures. Submitted to EUSIPCO 2026", "summary": "In this paper, we analyze the internal representations of a general-purpose audio self-supervised learning (SSL) model from a neuron-level perspective. Despite their strong empirical performance as feature extractors, the internal mechanisms underlying the robust generalization of SSL audio models remain unclear. Drawing on the framework of mechanistic interpretability, we identify and examine class-specific neurons by analyzing conditional activation patterns across diverse tasks. Our analysis reveals that SSL models foster the emergence of class-specific neurons that provide extensive coverage across novel task classes. These neurons exhibit shared responses across different semantic categories and acoustic similarities, such as speech attributes and musical pitch. We also confirm that these neurons have a functional impact on classification performance. To our knowledge, this is the first systematic neuron-level analysis of a general-purpose audio SSL model, providing new insights into its internal representation."}
{"id": "2602.15484", "categories": ["eess.AS", "cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2602.15484", "abs": "https://arxiv.org/abs/2602.15484", "authors": ["Amartyaveer", "Murali Kadambi", "Chandra Mohan Sharma", "Anupam Mondal", "Prasanta Kumar Ghosh"], "title": "Bottleneck Transformer-Based Approach for Improved Automatic STOI Score Prediction", "comment": "7 pages, 7 tables, 2 figures, ASRU 2025", "summary": "In this study, we have presented a novel approach to predict the Short-Time Objective Intelligibility (STOI) metric using a bottleneck transformer architecture. Traditional methods for calculating STOI typically requires clean reference speech, which limits their applicability in the real world. To address this, numerous deep learning-based nonintrusive speech assessment models have garnered significant interest. Many studies have achieved commendable performance, but there is room for further improvement.\n  We propose the use of bottleneck transformer, incorporating convolution blocks for learning frame-level features and a multi-head self-attention (MHSA) layer to aggregate the information. These components enable the transformer to focus on the key aspects of the input data. Our model has shown higher correlation and lower mean squared error for both seen and unseen scenarios compared to the state-of-the-art model using self-supervised learning (SSL) and spectral features as inputs."}
{"id": "2602.15519", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2602.15519", "abs": "https://arxiv.org/abs/2602.15519", "authors": ["Yiming Yang", "Guangyong Wang", "Haixin Guan", "Yanhua Long"], "title": "Enroll-on-Wakeup: A First Comparative Study of Target Speech Extraction for Seamless Interaction in Real Noisy Human-Machine Dialogue Scenarios", "comment": "This paper is submitted to Interspeech 2026", "summary": "Target speech extraction (TSE) typically relies on pre-recorded high-quality enrollment speech, which disrupts user experience and limits feasibility in spontaneous interaction. In this paper, we propose Enroll-on-Wakeup (EoW), a novel framework where the wake-word segment, captured naturally during human-machine interaction, is automatically utilized as the enrollment reference. This eliminates the need for pre-collected speech to enable a seamless experience. We perform the first systematic study of EoW-TSE, evaluating advanced discriminative and generative models under real diverse acoustic conditions. Given the short and noisy nature of wake-word segments, we investigate enrollment augmentation using LLM-based TTS. Results show that while current TSE models face performance degradation in EoW-TSE, TTS-based assistance significantly enhances the listening experience, though gaps remain in speech recognition accuracy."}
{"id": "2602.15651", "categories": ["cs.SD", "cs.CV", "eess.AS"], "pdf": "https://arxiv.org/pdf/2602.15651", "abs": "https://arxiv.org/abs/2602.15651", "authors": ["Qiangong Zhou", "Nagasaka Tomohiro"], "title": "UniTAF: A Modular Framework for Joint Text-to-Speech and Audio-to-Face Modeling", "comment": "16 pages, 12 figures", "summary": "This work considers merging two independent models, TTS and A2F, into a unified model to enable internal feature transfer, thereby improving the consistency between audio and facial expressions generated from text. We also discuss the extension of the emotion control mechanism from TTS to the joint model. This work does not aim to showcase generation quality; instead, from a system design perspective, it validates the feasibility of reusing intermediate representations from TTS for joint modeling of speech and facial expressions, and provides engineering practice references for subsequent speech expression co-design. The project code has been open source at: https://github.com/GoldenFishes/UniTAF"}
{"id": "2602.15074", "categories": ["cs.SD", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.15074", "abs": "https://arxiv.org/abs/2602.15074", "authors": ["Wanyu Zang", "Yang Yu", "Meng Yu"], "title": "Structure-Aware Piano Accompaniment via Style Planning and Dataset-Aligned Pattern Retrieval", "comment": "12 pages", "summary": "We introduce a structure-aware approach for symbolic piano accompaniment that decouples high-level planning from note-level realization. A lightweight transformer predicts an interpretable, per-measure style plan conditioned on section/phrase structure and functional harmony, and a retriever then selects and reharmonizes human-performed piano patterns from a corpus. We formulate retrieval as pattern matching under an explicit energy with terms for harmonic feasibility, structural-role compatibility, voice-leading continuity, style preferences, and repetition control. Given a structured lead sheet and optional keyword prompts, the system generates piano-accompaniment MIDI. In our experiments, transformer style-planner-guided retrieval produces diverse long-form accompaniments with strong style realization. We further analyze planner ablations and quantify inter-style isolation. Experimental results demonstrate the effectiveness of our inference-time approach for piano accompaniment generation."}
{"id": "2602.15036", "categories": ["eess.SP", "cs.AI", "physics.app-ph"], "pdf": "https://arxiv.org/pdf/2602.15036", "abs": "https://arxiv.org/abs/2602.15036", "authors": ["Saumyadip Mukhopadhyay", "Kiho Yang", "Kasyap Thottasserymana Vasudevan", "Mounica Jyothi Divvela", "Selim Dogru", "Dilip Krishnamurthy", "Fergo Treska", "Werner Gillijns", "Ryan Ryoung han Kim", "Kumara Sastry", "Vivek Singh"], "title": "Transforming Computational Lithography with AC and AI -- Faster, More Accurate, and Energy-efficient", "comment": null, "summary": "From climate science to drug discovery, scientific computing demands have surged dramatically in recent years -- driven by larger datasets, more sophisticated models, and higher simulation fidelity. This growth rate far outpaces transistor scaling, leading to unsustainably rising costs, energy consumption, and emissions. Semiconductor manufacturing is no exception. Computational lithography -- involving transferring circuitry to silicon in diffraction-limited conditions -- is the largest workload in semiconductor manufacturing. It has also grown exceptionally complex as miniaturization has advanced in the angstrom-era, requiring more accurate modeling, intricate corrections, and broader solution-space exploration. Accelerated computing (AC) offers a solution by dramatically freeing up the compute and power envelope. AI augments these gains by serving as high-fidelity surrogates for compute-intensive steps. Together, they present a sustainable, next-generation computing platform for scientific workloads. This new paradigm needs a fundamental redesign of the software stack. For computational lithography, NVIDIA cuLitho reinvents the core primitives -- diffractive optics, computational geometry, multi-variant optimization, data processing -- to achieve a transformative 57X end-to-end acceleration. Beyond dramatically faster cycles, this expanded compute envelope enables more rigorous solutions, including curvilinear masks, high-numerical aperture extreme ultraviolet (high-NA EUV) lithography, and subatomic modeling. We reinvest a small fraction of the freed-up compute to include through-focus correction for better process resilience. Silicon experiments at IMEC show significant benefits compared to conventional methods -- 35% better process window and 19% better edge placement error. This is the first quantified chip-scale demonstration of the lithography benefits of AC and AI in silicon."}
{"id": "2602.15749", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2602.15749", "abs": "https://arxiv.org/abs/2602.15749", "authors": ["Jonah Casebeer", "Ge Zhu", "Zhepei Wang", "Nicholas J. Bryan"], "title": "A Generative-First Neural Audio Autoencoder", "comment": "ICASSP 2026", "summary": "Neural autoencoders underpin generative models. Practical, large-scale use of neural autoencoders for generative modeling necessitates fast encoding, low latent rates, and a single model across representations. Existing approaches are reconstruction-first: they incur high latent rates, slow encoding, and separate architectures for discrete vs. continuous latents and for different audio channel formats, hindering workflows from preprocessing to inference conditioning. We introduce a generative-first architecture for audio autoencoding that increases temporal downsampling from 2048x to 3360x and supports continuous and discrete representations and common audio channel formats in one model. By balancing compression, quality, and speed, it delivers 10x faster encoding, 1.6x lower rates, and eliminates channel-format-specific variants while maintaining competitive reconstruction quality. This enables applications previously constrained by processing costs: a 60-second mono signal compresses to 788 tokens, making generative modeling more tractable."}
{"id": "2602.15082", "categories": ["cs.SD", "cs.AI", "cs.MM"], "pdf": "https://arxiv.org/pdf/2602.15082", "abs": "https://arxiv.org/abs/2602.15082", "authors": ["Zineb Lahrichi", "Gaëtan Hadjeres", "Gaël Richard", "Geoffroy Peeters"], "title": "S-PRESSO: Ultra Low Bitrate Sound Effect Compression With Diffusion Autoencoders And Offline Quantization", "comment": null, "summary": "Neural audio compression models have recently achieved extreme compression rates, enabling efficient latent generative modeling. Conversely, latent generative models have been applied to compression, pushing the limits of continuous and discrete approaches. However, existing methods remain constrained to low-resolution audio and degrade substantially at very low bitrates, where audible artifacts are prominent. In this paper, we present S-PRESSO, a 48kHz sound effect compression model that produces both continuous and discrete embeddings at ultra-low bitrates, down to 0.096 kbps, via offline quantization. Our model relies on a pretrained latent diffusion model to decode compressed audio embeddings learned by a latent encoder. Leveraging the generative priors of the diffusion decoder, we achieve extremely low frame rates, down to 1Hz (750x compression rate), producing convincing and realistic reconstructions at the cost of exact fidelity. Despite operating at high compression rates, we demonstrate that S-PRESSO outperforms both continuous and discrete baselines in audio quality, acoustic similarity and reconstruction metrics."}
{"id": "2602.15042", "categories": ["eess.SP", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.15042", "abs": "https://arxiv.org/abs/2602.15042", "authors": ["Jiawei Wang", "Liang Xu", "Shuntian Zheng", "Yu Guan", "Kaichen Wang", "Ziqing Zhang", "Chen Chen", "Laurence T. Yang", "Sai Gu"], "title": "Combining scEEG and PPG for reliable sleep staging using lightweight wearables", "comment": null, "summary": "Reliable sleep staging remains challenging for lightweight wearable devices such as single-channel electroencephalography (scEEG) or photoplethysmography (PPG). scEEG offers direct measurement of cortical activity and serves as the foundation for sleep staging, yet exhibits limited performance on light sleep stages. PPG provides a low-cost complement that captures autonomic signatures effective for detecting light sleep. However, prior PPG-based methods rely on full night recordings (8 - 10 hours) as input context, which is less practical to provide timely feedback for sleep intervention. In this work, we investigate scEEG-PPG fusion for 4-class sleep staging under short-window (30 s - 30 min) constraints. First, we evaluate the temporal context required for each modality, to better understand the relationship of sleep staging performance with respect to monitoring window. Second, we investigate three fusion strategies: score-level fusion, cross-attention fusion enabling feature-level interactions, and Mamba-enhanced fusion incorporating temporal context modeling. Third, we train and evaluate on the Multi-Ethnic Study of Atherosclerosis (MESA) dataset and perform cross-dataset validation on the Cleveland Family Study (CFS) and the Apnea, Bariatric surgery, and CPAP (ABC) datasets. The Mamba-enhanced fusion achieves the best performance on MESA (Cohen's Kappa $κ$ = 0.798, Acc = 86.9%), with particularly notable improvement in light sleep classification (F1-score: 85.63% vs. 77.76%, recall: 82.85% vs. 69.95% for scEEG alone), and generalizes well to CFS and ABC datasets with different populations. These findings suggest that scEEG-PPG fusion is a promising approach for lightweight wearable based sleep monitoring, offering a pathway toward more accessible sleep health assessment. Source code of this project can be found at: https://github.com/DavyWJW/scEEG-PPGFusion"}
{"id": "2602.15491", "categories": ["cs.SD", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.15491", "abs": "https://arxiv.org/abs/2602.15491", "authors": ["Samir Sadok", "Laurent Girin", "Xavier Alameda-Pineda"], "title": "The Equalizer: Introducing Shape-Gain Decomposition in Neural Audio Codecs", "comment": "Neural audio codecs, shape-gain decomposition, vector quantization, speech coding", "summary": "Neural audio codecs (NACs) typically encode the short-term energy (gain) and normalized structure (shape) of speech/audio signals jointly within the same latent space. As a result, they are poorly robust to a global variation of the input signal level in the sense that such variation has strong influence on the embedding vectors at the output of the encoder and their quantization. This methodology is inherently inefficient, leading to codebook redundancy and suboptimal bitrate-distortion performance. To address these limitations, we propose to introduce shape-gain decomposition, widely used in classical speech/audio coding, into the NAC framework. The principle of the proposed Equalizer methodology is to decompose the input signal -- before the NAC encoder -- into gain and normalized shape vector on a short-term basis. The shape vector is processed by the NAC, while the gain is quantized with scalar quantization and transmitted separately. The output (decoded) signal is reconstructed from the normalized output of the NAC and the quantized gain. Our experiments conducted on speech signals show that this general methodology, easily applicable to any NAC, enables a substantial gain in bitrate-distortion performance, as well as a massive reduction in complexity."}
{"id": "2602.15174", "categories": ["eess.SP", "physics.med-ph"], "pdf": "https://arxiv.org/pdf/2602.15174", "abs": "https://arxiv.org/abs/2602.15174", "authors": ["Mick Gardner", "Michael L. Oelze"], "title": "Large elements and advanced beamformers for increased field of view in 2-D ultrasound matrix arrays", "comment": null, "summary": "Three-dimensional (3D) ultrasound promises various medical applications for abdominal, obstetrics, and cardiovascular imaging. However, ultrasound matrix arrays have extremely high element counts limiting their field of view (FOV). This work seeks to demonstrate an increased field-of-view using a reduced element count array design. The approach is to increase the element size and use advanced beamformers to maintain image quality. The delay and sum (DAS), Null Subtraction Imaging (NSI), directional coherence factor (DCF), and Minimum Variance (MV) beamformers were compared. K-wave simulations of the 3D point-spread functions (PSF) of NSI, DCF, and MV display reduced side lobes and narrowed main lobes compared to DAS. Experiments were conducted using a multiplexed 1024-element matrix array on a Verasonics 256 system. Elements were electronically coupled to imitate a larger pitch and element size. Then, a virtual large aperture was created by using a positioning system to collect data in sections with the matrix array. High-quality images were obtained using a coupling factor of two, doubling the FOV while maintaining the same element count in the virtual large aperture as the original matrix array. The NSI beamformer demonstrated the best resolution performance in simulations and on the large aperture, maintaining the same resolution as uncoupled DAS for coupling factors up to 4. Our results demonstrate how larger matrix arrays could be constructed with larger elements, with resolution maintained by advanced beamformers."}
{"id": "2602.15651", "categories": ["cs.SD", "cs.CV", "eess.AS"], "pdf": "https://arxiv.org/pdf/2602.15651", "abs": "https://arxiv.org/abs/2602.15651", "authors": ["Qiangong Zhou", "Nagasaka Tomohiro"], "title": "UniTAF: A Modular Framework for Joint Text-to-Speech and Audio-to-Face Modeling", "comment": "16 pages, 12 figures", "summary": "This work considers merging two independent models, TTS and A2F, into a unified model to enable internal feature transfer, thereby improving the consistency between audio and facial expressions generated from text. We also discuss the extension of the emotion control mechanism from TTS to the joint model. This work does not aim to showcase generation quality; instead, from a system design perspective, it validates the feasibility of reusing intermediate representations from TTS for joint modeling of speech and facial expressions, and provides engineering practice references for subsequent speech expression co-design. The project code has been open source at: https://github.com/GoldenFishes/UniTAF"}
{"id": "2602.15209", "categories": ["eess.SP", "cs.IT", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.15209", "abs": "https://arxiv.org/abs/2602.15209", "authors": ["Amirhossein Taherpour", "Abbas Taherpour", "Tamer Khattab"], "title": "Secure High-Resolution ISAC via Multi-Layer Intelligent Metasurfaces: A Layered Optimization Framework", "comment": null, "summary": "Integrated sensing and communication (ISAC) has emerged as a pivotal technology for next-generation wireless networks, enabling simultaneous data transmission and environmental sensing. However, existing ISAC systems face fundamental limitations in achieving high-resolution sensing while maintaining robust communication security and spectral efficiency. This paper introduces a transformative approach leveraging stacked intelligent metasurfaces (SIM) to overcome these challenges. We propose a multi-functional SIM-assisted system that jointly optimizes communication secrecy and sensing accuracy through a novel layered optimization framework. Our solution employs a multi-objective optimization formulation that balances secrecy rate maximization with sensing error minimization under practical hardware constraints. The proposed layered block coordinate descent algorithm efficiently coordinates sensing configuration, secure beamforming, communication metasurface optimization, and resource allocation while ensuring robustness to channel uncertainties. Extensive simulations demonstrate significant performance gains over conventional approaches, achieving 32-61\\% improvement in sensing accuracy and 15-35\\% enhancement in secrecy rates while maintaining computational efficiency. This work establishes a new paradigm for secure and high-precision multi-functional wireless systems."}
{"id": "2602.15749", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2602.15749", "abs": "https://arxiv.org/abs/2602.15749", "authors": ["Jonah Casebeer", "Ge Zhu", "Zhepei Wang", "Nicholas J. Bryan"], "title": "A Generative-First Neural Audio Autoencoder", "comment": "ICASSP 2026", "summary": "Neural autoencoders underpin generative models. Practical, large-scale use of neural autoencoders for generative modeling necessitates fast encoding, low latent rates, and a single model across representations. Existing approaches are reconstruction-first: they incur high latent rates, slow encoding, and separate architectures for discrete vs. continuous latents and for different audio channel formats, hindering workflows from preprocessing to inference conditioning. We introduce a generative-first architecture for audio autoencoding that increases temporal downsampling from 2048x to 3360x and supports continuous and discrete representations and common audio channel formats in one model. By balancing compression, quality, and speed, it delivers 10x faster encoding, 1.6x lower rates, and eliminates channel-format-specific variants while maintaining competitive reconstruction quality. This enables applications previously constrained by processing costs: a 60-second mono signal compresses to 788 tokens, making generative modeling more tractable."}
{"id": "2602.15218", "categories": ["eess.SP", "math.NA", "stat.CO"], "pdf": "https://arxiv.org/pdf/2602.15218", "abs": "https://arxiv.org/abs/2602.15218", "authors": ["L. Portella", "F. M. Bayer", "R. J. Cintra"], "title": "Multiplierless DFT Approximation Based on the Prime Factor Algorithm", "comment": "24 pages, 4 figures", "summary": "Matrix approximation methods have successfully produced efficient, low-complexity approximate transforms for the discrete cosine transforms and the discrete Fourier transforms. For the DFT case, literature archives approximations operating at small power-of-two blocklenghts, such as \\{8, 16, 32\\}, or at large blocklengths, such as 1024, which are obtained by means of the Cooley-Tukey-based approximation relying on the small-blocklength approximate transforms. Cooley-Tukey-based approximations inherit the intermediate multiplications by twiddled factors which are usually not approximated; otherwise the effected error propagation would prevent the overall good performance of the approximation. In this context, the prime factor algorithm can furnish the necessary framework for deriving fully multiplierless DFT approximations. We introduced an approximation method based on small prime-sized DFT approximations which entirely eliminates intermediate multiplication steps and prevents internal error propagation. To demonstrate the proposed method, we design a fully multiplierless 1023-point DFT approximation based on 3-, 11- and 31-point DFT approximations. The performance evaluation according to popular metrics showed that the proposed approximations not only presented a significantly lower arithmetic complexity but also resulted in smaller approximation error measurements when compared to competing methods."}
{"id": "2602.15766", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2602.15766", "abs": "https://arxiv.org/abs/2602.15766", "authors": ["Sonal Kumar", "Prem Seetharaman", "Ke Chen", "Oriol Nieto", "Jiaqi Su", "Zhepei Wang", "Rithesh Kumar", "Dinesh Manocha", "Nicholas J. Bryan", "Zeyu Jin", "Justin Salamon"], "title": "TAC: Timestamped Audio Captioning", "comment": null, "summary": "Large Audio Language Models struggle to disentangle overlapping events in complex acoustic scenes, yielding temporally inconsistent captions and frequent hallucinations. We introduce Timestamped Audio Captioner (TAC), a model that produces temporally grounded audio descriptions at varying degrees of detail and resolution. TAC is trained with a synthetic data pipeline that constructs challenging and dynamic mixtures from real-world audio sources, enabling robust learning under realistic polyphonic conditions. Across event detection and dense captioning, TAC outperforms all competing methods, with a low hallucination rate and accurate temporal grounding. We also introduce TAC-V, an audio-visual pipeline to generate semantically rich audio-visual descriptions. We then show that TAC and TAC-V serves as a \"semantic bridge\" for a text-only reasoner: a simple TAC$\\rightarrow$LLM and TAC-V$\\rightarrow$LLM cascade achieves state-of-the-art scores on benchmarks for both audio (MMAU-Pro, MMSU, MMAR) and audio-visual (DailyOmni, VideoHolmes) understanding and reasoning respectively."}
{"id": "2602.15326", "categories": ["eess.SP", "cs.AI", "cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.15326", "abs": "https://arxiv.org/abs/2602.15326", "authors": ["Hao Chen", "Zavareh Bozorgasl"], "title": "SCENE OTA-FD: Self-Centering Noncoherent Estimator for Over-the-Air Federated Distillation", "comment": "Work in progress. Codes will be available on: https://github.com/zavareh1", "summary": "We propose SCENE (Self-Centering Noncoherent Estimator), a pilot-free and phase-invariant aggregation primitive for over-the-air federated distillation (OTA-FD). Each device maps its soft-label (class-probability) vector to nonnegative transmit energies under constant per-round power and constant-envelope signaling (PAPR near 1). At the server, a self-centering energy estimator removes the noise-energy offset and yields an unbiased estimate of the weighted soft-label average, with variance decaying on the order of 1/(SM) in the number of receive antennas M and repetition factor S. We also develop a pilot-free ratio-normalized variant that cancels unknown large-scale gains, provide a convergence bound consistent with coherent OTA-FD analyses, and present an overhead-based crossover comparison. SCENE targets short-coherence and hardware-constrained regimes, where avoiding per-round CSI is essential: it trades a modest noncoherent variance constant for zero uplink pilots, unbiased aggregation, and hardware-friendly transmission, and can outperform coherent designs when pilot overhead is non-negligible."}
{"id": "2602.15307", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2602.15307", "abs": "https://arxiv.org/abs/2602.15307", "authors": ["Takao Kawamura", "Daisuke Niizumi", "Nobutaka Ono"], "title": "What Do Neurons Listen To? A Neuron-level Dissection of a General-purpose Audio Model", "comment": "5 pages, 8 figures. Submitted to EUSIPCO 2026", "summary": "In this paper, we analyze the internal representations of a general-purpose audio self-supervised learning (SSL) model from a neuron-level perspective. Despite their strong empirical performance as feature extractors, the internal mechanisms underlying the robust generalization of SSL audio models remain unclear. Drawing on the framework of mechanistic interpretability, we identify and examine class-specific neurons by analyzing conditional activation patterns across diverse tasks. Our analysis reveals that SSL models foster the emergence of class-specific neurons that provide extensive coverage across novel task classes. These neurons exhibit shared responses across different semantic categories and acoustic similarities, such as speech attributes and musical pitch. We also confirm that these neurons have a functional impact on classification performance. To our knowledge, this is the first systematic neuron-level analysis of a general-purpose audio SSL model, providing new insights into its internal representation."}
{"id": "2602.15530", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2602.15530", "abs": "https://arxiv.org/abs/2602.15530", "authors": ["Denis Esiunin", "Alexei Davydov"], "title": "Adaptive Selection of Codebook Using Assistance Information and Artificial Intelligence for 6G Systems", "comment": "6 pages, 3 figures, 3 tables", "summary": "This paper addresses the problem of adaptive codebook (CB) selection for downlink (DL) precoder quantization in channel state information (CSI) reporting. The accuracy of precoder quantization depends on propagation conditions, requiring independent parameter adaptation for each user equipment (UE). To enable optimal CB selection, this paper proposes UE-assisted CB selection at the base station (BS) using reported by the UE statistical channel properties across time, frequency, and spatial domains. The reported assistance information serves as input to a neural network (NN), which predicts the quantization accuracy of various CB types for each served user. The predicted accuracy is then used to select the optimal CB while considering the associated CSI reporting overhead and precoding performance. System-level simulations demonstrate that the proposed approach reduces total CSI overhead while maintaining the target system throughput performance."}
{"id": "2602.15519", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2602.15519", "abs": "https://arxiv.org/abs/2602.15519", "authors": ["Yiming Yang", "Guangyong Wang", "Haixin Guan", "Yanhua Long"], "title": "Enroll-on-Wakeup: A First Comparative Study of Target Speech Extraction for Seamless Interaction in Real Noisy Human-Machine Dialogue Scenarios", "comment": "This paper is submitted to Interspeech 2026", "summary": "Target speech extraction (TSE) typically relies on pre-recorded high-quality enrollment speech, which disrupts user experience and limits feasibility in spontaneous interaction. In this paper, we propose Enroll-on-Wakeup (EoW), a novel framework where the wake-word segment, captured naturally during human-machine interaction, is automatically utilized as the enrollment reference. This eliminates the need for pre-collected speech to enable a seamless experience. We perform the first systematic study of EoW-TSE, evaluating advanced discriminative and generative models under real diverse acoustic conditions. Given the short and noisy nature of wake-word segments, we investigate enrollment augmentation using LLM-based TTS. Results show that while current TSE models face performance degradation in EoW-TSE, TTS-based assistance significantly enhances the listening experience, though gaps remain in speech recognition accuracy."}
{"id": "2602.15544", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2602.15544", "abs": "https://arxiv.org/abs/2602.15544", "authors": ["Ngoc-Son Duong", "Huyen-Trang Ta", "Quang-Tang Ngo", "Thi-Hue Duong", "Van-Lap Nguyen", "Cong-Minh Nguyen", "Minh-Tran Nguyen", "Thai-Mai Dinh"], "title": "Waveform Design for ISAC System: A Consensus ADMM Approach", "comment": "accepted at IEEE WCNC 2026", "summary": "We study joint transmit-waveform and receive-filter design for a multi-user downlink integrated sensing and communication (ISAC) system under practical constant-modulus and similarity constraints. We cast the design as a unified multi-objective program that balances communication sum rate and sensing signal-to-interference-plus-noise ratio (SINR). To address this, we introduce an efficient algorithm that use consensus alternating direction method of multipliers (ADMM) framework to alternately update the transmit waveform and radar filter. The proposed method effectively handles the non-convex fractional sensing's SINR formulation and ensures fast convergence. Simulation results demonstrate that the proposed approach achieves better trade-offs between communication sum rate and sensing's SINR compared to existing benchmark schemes."}
{"id": "2602.15555", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2602.15555", "abs": "https://arxiv.org/abs/2602.15555", "authors": ["Ashwani Koul", "Gustaf Hendeby", "Isaac Skog"], "title": "Tracking Time-Varying Multipath Channels forActive Sonar Applications", "comment": "Submitted for possible publication in IEEE FUSION", "summary": "Reliable detection and tracking in active sonar require accurate and efficient learning of the acoustic multipath background environment. Conventionally, background learning is performed after transforming measurements into the range-Doppler domain, a step that is computationally expensive and can obscure phase-coherent structure useful for monitoring and tracking. This paper proposes a framework for learning and tracking the multipath background directly in the raw measurement domain. Starting from a wideband Doppler linearization of the impulse response of a time-varying multipath channel, a state-space model with a heteroscedastic measurement equation is derived. This model enables channel tracking using an extended Kalman filter (EKF), and unknown model parameters are learned from the marginalized likelihood. The statistical adequacy of the proposed models is assessed via a p-value significance test. Finally, this paper integrates the learned channel model into a sequential likelihood-ratio test for target detection. BELLHOP-based simulations show that the proposed model better captures channel dynamics induced by sea-surface fluctuations and transmitter and receiver drift, yielding more reliable detection in time-varying shallow-water environments"}
{"id": "2602.15618", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2602.15618", "abs": "https://arxiv.org/abs/2602.15618", "authors": ["Abdel Hakiem Mohamed Abbas Mohamed Ahmed", "Beth Jelfs", "Airlie Chapman", "Eric Schoof", "Christopher Gilliam"], "title": "Physics-Informed Anomaly Detection of Terrain Material Change in Radar Imagery", "comment": null, "summary": "In this paper we consider physics-informed detection of terrain material change in radar imagery (e.g., shifts in permittivity, roughness or moisture). We propose a lightweight electromagnetic (EM) forward model to simulate bi-temporal single-look complex (SLC) images from labelled material maps. On these data, we derive physics-aware feature stacks that include interferometric coherence, and evaluate unsupervised detectors: Reed-Xiaoli (RX)/Local-RX with robust scatter (Tyler's M-estimator), Coherent Change Detection (CCD), and a compact convolutional auto-encoder. Monte Carlo experiments sweep dielectric/roughness/moisture changes, number of looks and clutter regimes (gamma vs K-family) at fixed probability of false alarm. Results on synthetic but physically grounded scenes show that coherence and robust covariance markedly improve anomaly detection of material changes; a simple score-level fusion achieves the best F1 in heavy-tailed clutter."}
{"id": "2602.15623", "categories": ["eess.SP", "math-ph"], "pdf": "https://arxiv.org/pdf/2602.15623", "abs": "https://arxiv.org/abs/2602.15623", "authors": ["Zetao Fei", "Josselin Garnier"], "title": "Passive Imaging with Ambient Noise Under Wave Speed Mismatch: Mathematical Analysis and Wave Speed Estimation", "comment": null, "summary": "It is known that waves generated by ambient noise sources and recorded by passive receivers can be used to image the reflectivities of an unknown medium. However, reconstructing the reflectivity of the medium from partial boundary measurements remains a challenging problem, particularly when the background wave speed is unknown. In this paper, we investigate passive correlation-based imaging in the daylight configuration, where uncontrolled noise sources illuminate the medium and only ambient fields are recorded by a sensor array. We first analyze daylight migration for a point reflector embedded in a homogeneous background. By introducing a searching wave speed into the migration functional, we derive an explicit characterization of the deterministic shift and defocusing effects induced by wave-speed mismatch. We show that the maximum of the envelope of the resulting functional provides a reliable estimator of the true wave speed. We then extend the analysis to a random medium with correlation length smaller than the wavelength. Leveraging the shift formula obtained in the homogeneous case, we introduce a virtual guide star that remains fixed under migration with different searching speeds. This property enables an effective wave-speed estimation strategy based on spatial averaging around the virtual guide star. For both homogeneous and random media, we establish resolution analyses for the proposed wave-speed estimators. Numerical experiments are conducted to validate the theoretical result."}
{"id": "2602.15640", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.15640", "abs": "https://arxiv.org/abs/2602.15640", "authors": ["Peizheng Li", "Xinyi Lin", "Adnan Aijaz"], "title": "Latency-aware Human-in-the-Loop Reinforcement Learning for Semantic Communications", "comment": "6 pages, 8 figures. This paper has been accepted for publication in IEEE ICC 2026", "summary": "Semantic communication promises task-aligned transmission but must reconcile semantic fidelity with stringent latency guarantees in immersive and safety-critical services. This paper introduces a time-constrained human-in-the-loop reinforcement learning (TC-HITL-RL) framework that embeds human feedback, semantic utility, and latency control within a semantic-aware Open radio access network (RAN) architecture. We formulate semantic adaptation driven by human feedback as a constrained Markov decision process (CMDP) whose state captures semantic quality, human preferences, queue slack, and channel dynamics, and solve it via a primal--dual proximal policy optimization algorithm with action shielding and latency-aware reward shaping. The resulting policy preserves PPO-level semantic rewards while tightening the variability of both air-interface and near-real-time RAN intelligent controller processing budgets. Simulations over point-to-multipoint links with heterogeneous deadlines show that TC-HITL-RL consistently meets per-user timing constraints, outperforms baseline schedulers in reward, and stabilizes resource consumption, providing a practical blueprint for latency-aware semantic adaptation."}
{"id": "2602.15737", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2602.15737", "abs": "https://arxiv.org/abs/2602.15737", "authors": ["Isha Jariwala", "Xinquan Wang", "Bridget Meier", "Guanyue Qian", "Dipankar Shakya", "Mingjun Ying", "Homa Nikbakht", "Daniel Abraham", "Theodore S. Rappaport"], "title": "NYUSIM: A Roadmap to AI-Enabled Statistical Channel Modeling and Simulation", "comment": null, "summary": "Integrating artificial intelligence (AI) into wireless channel modeling requires large, accurate, and physically consistent datasets derived from real measurements. Such datasets are essential for training and validating models that learn spatio-temporal channel behavior across frequencies and environments. NYUSIM, introduced by NYU WIRELESS in 2016, generates realistic spatio-temporal channel data using extensive outdoor and indoor measurements between 28 and 142 GHz. To improve scalability and support 6G research, we migrated the complete NYUSIM framework from MATLAB to Python, and are incorporating new statistical model generation capabilities from extensive field measurements in the new 6G upper mid-band spectrum at 6.75 GHz (FR1(C)) and 16.95 GHz (FR3) [1]. The NYUSIM Python also incorporates a 3D antenna data format, referred to as Ant3D, which is a standardized, full-sphere format for defining canonical, commercial, or measured antenna patterns for any statistical or site-specific ray tracing modeling tool. Migration from MATLAB to Python was rigorously validated through Kolmogorov-Smirnov (K-S) tests, moment analysis, and end-to-end testing with unified randomness control, confirming statistical consistency and reproduction of spatio-temporal channel statistics, including spatial consistency with the open-source MATLAB NYUSIM v4.0 implementation. The NYUSIM Python version is designed to integrate with modern AI workflows and enable large-scale parallel data generation, establishing a robust, verified, and extensible foundation for future AI-enabled channel modeling."}
{"id": "2602.15808", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2602.15808", "abs": "https://arxiv.org/abs/2602.15808", "authors": ["Adam Umra", "Simon Tewes", "Niklas Beckmann", "Niels König", "Aydin Sezgin", "Robert Schmitt"], "title": "Measurement-Based Validation of Geometry-Driven RIS Beam Steering in Industrial Environments", "comment": "6 pages, 7 figures, submitted to 2026 EuCNC & 6G Summit", "summary": "Reconfigurable intelligent surfaces (RISs) offer programmable control of radio propagation for future wireless systems. For configuration, geometry-driven analytical approaches are appealing for their simplicity and real-time operation, but their performance in challenging environments such as industrial halls with dense multipath and metallic scattering is not well established. To this end, we present a measurement-based evaluation of geometry-driven RIS beam steering in a large industrial hall using a 5 GHz RIS prototype. A novel RIS configuration is proposed in which four patch antennas are mounted in close proximity in front of the RIS to steer the incident field and enable controlled reflection. For this setup, analytically computed, quantized configurations are implemented. Two-dimensional received power maps from two measurement areas reveal consistent, spatially selective focusing. Configurations optimized near the receiver produce clear power maxima, while steering to offset locations triggers a rapid 20-30 dB reduction. With increasing RIS-receiver distance, elevation selectivity broadens due to finite-aperture and geometric constraints, while azimuth steering remains robust. These results confirm the practical viability of geometry-driven RIS beam steering in industrial environments and support its use for spatial field control and localization under non-ideal propagation."}
{"id": "2602.15484", "categories": ["eess.AS", "cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2602.15484", "abs": "https://arxiv.org/abs/2602.15484", "authors": ["Amartyaveer", "Murali Kadambi", "Chandra Mohan Sharma", "Anupam Mondal", "Prasanta Kumar Ghosh"], "title": "Bottleneck Transformer-Based Approach for Improved Automatic STOI Score Prediction", "comment": "7 pages, 7 tables, 2 figures, ASRU 2025", "summary": "In this study, we have presented a novel approach to predict the Short-Time Objective Intelligibility (STOI) metric using a bottleneck transformer architecture. Traditional methods for calculating STOI typically requires clean reference speech, which limits their applicability in the real world. To address this, numerous deep learning-based nonintrusive speech assessment models have garnered significant interest. Many studies have achieved commendable performance, but there is room for further improvement.\n  We propose the use of bottleneck transformer, incorporating convolution blocks for learning frame-level features and a multi-head self-attention (MHSA) layer to aggregate the information. These components enable the transformer to focus on the key aspects of the input data. Our model has shown higher correlation and lower mean squared error for both seen and unseen scenarios compared to the state-of-the-art model using self-supervised learning (SSL) and spectral features as inputs."}

{"id": "2602.16008", "categories": ["cs.SD", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.16008", "abs": "https://arxiv.org/abs/2602.16008", "authors": ["Adnan El Assadi", "Isaac Chung", "Chenghao Xiao", "Roman Solomatin", "Animesh Jha", "Rahul Chand", "Silky Singh", "Kaitlyn Wang", "Ali Sartaz Khan", "Marc Moussa Nasser", "Sufen Fong", "Pengfei He", "Alan Xiao", "Ayush Sunil Munot", "Aditya Shrivastava", "Artem Gazizov", "Niklas Muennighoff", "Kenneth Enevoldsen"], "title": "MAEB: Massive Audio Embedding Benchmark", "comment": null, "summary": "We introduce the Massive Audio Embedding Benchmark (MAEB), a large-scale benchmark covering 30 tasks across speech, music, environmental sounds, and cross-modal audio-text reasoning in 100+ languages. We evaluate 50+ models and find that no single model dominates across all tasks: contrastive audio-text models excel at environmental sound classification (e.g., ESC50) but score near random on multilingual speech tasks (e.g., SIB-FLEURS), while speech-pretrained models show the opposite pattern. Clustering remains challenging for all models, with even the best-performing model achieving only modest results. We observe that models excelling on acoustic understanding often perform poorly on linguistic tasks, and vice versa. We also show that the performance of audio encoders on MAEB correlates highly with their performance when used in audio large language models. MAEB is derived from MAEB+, a collection of 98 tasks. MAEB is designed to maintain task diversity while reducing evaluation cost, and it integrates into the MTEB ecosystem for unified evaluation across text, image, and audio modalities. We release MAEB and all 98 tasks along with code and a leaderboard at https://github.com/embeddings-benchmark/mteb."}
{"id": "2602.16305", "categories": ["cs.SD", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.16305", "abs": "https://arxiv.org/abs/2602.16305", "authors": ["Houtan Ghaffari", "Lukas Rauch", "Christoph Scholz", "Paul Devos"], "title": "BAT: Better Audio Transformer Guided by Convex Gated Probing", "comment": null, "summary": "Probing is widely adopted in computer vision to faithfully evaluate self-supervised learning (SSL) embeddings, as fine-tuning may misrepresent their inherent quality. In contrast, audio SSL models still rely on fine-tuning because simple probing fails to unlock their full potential and alters their rankings when competing for SOTA on AudioSet. Hence, a robust and efficient probing mechanism is required to guide the trajectory of audio SSL towards reliable and reproducible methods. We introduce Convex Gated Probing (CGP), a prototype-based method that drastically closes the gap between fine-tuning and probing in audio. CGP efficiently utilizes all frozen layers via a gating mechanism and exposes the location of latent task-relevant information. Guided by CGP, we rework the entire SSL pipeline of current SOTA audio models that use legacy implementations of prior SSL methods. By refining data preprocessing, model architecture, and pre-training recipe, we introduce Better Audio Transformer (BAT), and establish new SOTA on audio benchmarks."}
{"id": "2602.16334", "categories": ["cs.SD", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.16334", "abs": "https://arxiv.org/abs/2602.16334", "authors": ["Arvind Krishna Sridhar", "Yinyi Guo", "Erik Visser"], "title": "Spatial Audio Question Answering and Reasoning on Dynamic Source Movements", "comment": null, "summary": "Spatial audio understanding aims to enable machines to interpret complex auditory scenes, particularly when sound sources move over time. In this work, we study Spatial Audio Question Answering (Spatial AQA) with a focus on movement reasoning, where a model must infer object motion, position, and directional changes directly from stereo audio. First, we introduce a movement-centric spatial audio augmentation framework that synthesizes diverse motion patterns from isolated mono audio events, enabling controlled and scalable training data generation. Second, we propose an end-to-end multimodal finetuning approach with a thinking mode, which allows audio-language models to produce explicit intermediate reasoning steps before predicting an answer. Third, we investigate the impact of query-conditioned source separation as a preprocessing stage and compare three inference regimes: no masking, an audio grounding model (AGM), and ground-truth masks. Our results show that reasoning amplifies the benefits of source separation, with thinking mode showing significant improvement of +5.1% when a single event is present in the question. These findings highlight the interplay between movement modeling, reasoning, and separation quality, offering new insights for advancing spatial audio understanding."}
{"id": "2602.16343", "categories": ["cs.SD", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.16343", "abs": "https://arxiv.org/abs/2602.16343", "authors": ["Yixuan Xiao", "Florian Lux", "Alejandro Pérez-González-de-Martos", "Ngoc Thang Vu"], "title": "How to Label Resynthesized Audio: The Dual Role of Neural Audio Codecs in Audio Deepfake Detection", "comment": "Accepted to ICASSP 2026", "summary": "Since Text-to-Speech systems typically don't produce waveforms directly, recent spoof detection studies use resynthesized waveforms from vocoders and neural audio codecs to simulate an attacker. Unlike vocoders, which are specifically designed for speech synthesis, neural audio codecs were originally developed for compressing audio for storage and transmission. However, their ability to discretize speech also sparked interest in language-modeling-based speech synthesis. Owing to this dual functionality, codec resynthesized data may be labeled as either bonafide or spoof. So far, very little research has addressed this issue. In this study, we present a challenging extension of the ASVspoof 5 dataset constructed for this purpose. We examine how different labeling choices affect detection performance and provide insights into labeling strategies."}
{"id": "2602.15909", "categories": ["eess.AS", "cs.AI", "cs.DB", "cs.HC", "cs.MA", "cs.SD"], "pdf": "https://arxiv.org/pdf/2602.15909", "abs": "https://arxiv.org/abs/2602.15909", "authors": ["Pengfei Zhang", "Tianxin Xie", "Minghao Yang", "Li Liu"], "title": "Resp-Agent: An Agent-Based System for Multimodal Respiratory Sound Generation and Disease Diagnosis", "comment": "24 pages, 3 figures. Published as a conference paper at ICLR 2026. Code and data available at https://github.com/zpforlove/Resp-Agent", "summary": "Deep learning-based respiratory auscultation is currently hindered by two fundamental challenges: (i) inherent information loss, as converting signals into spectrograms discards transient acoustic events and clinical context; (ii) limited data availability, exacerbated by severe class imbalance. To bridge these gaps, we present Resp-Agent, an autonomous multimodal system orchestrated by a novel Active Adversarial Curriculum Agent (Thinker-A$^2$CA). Unlike static pipelines, Thinker-A$^2$CA serves as a central controller that actively identifies diagnostic weaknesses and schedules targeted synthesis in a closed loop. To address the representation gap, we introduce a Modality-Weaving Diagnoser that weaves EHR data with audio tokens via Strategic Global Attention and sparse audio anchors, capturing both long-range clinical context and millisecond-level transients. To address the data gap, we design a Flow Matching Generator that adapts a text-only Large Language Model (LLM) via modality injection, decoupling pathological content from acoustic style to synthesize hard-to-diagnose samples. As a foundation for these efforts, we introduce Resp-229k, a benchmark corpus of 229k recordings paired with LLM-distilled clinical narratives. Extensive experiments demonstrate that Resp-Agent consistently outperforms prior approaches across diverse evaluation settings, improving diagnostic robustness under data scarcity and long-tailed class imbalance. Our code and data are available at https://github.com/zpforlove/Resp-Agent."}
{"id": "2602.15880", "categories": ["eess.SP", "cs.IT"], "pdf": "https://arxiv.org/pdf/2602.15880", "abs": "https://arxiv.org/abs/2602.15880", "authors": ["Ning Bian", "Zhong-Feng Sun", "Yun-Bin Zhao", "Jin-Chuan Zhou", "Nan Meng"], "title": "Newton-Direction-Based ReLU-Thresholding Methods for Nonnegative Sparse Signal Recovery", "comment": null, "summary": "Nonnegative sparse signal recovery has been extensively studied due to its broad applications. Recent work has integrated rectified linear unit (ReLU) techniques to enhance existing recovery algorithms. We merge Newton-type thresholding with ReLU-based approaches to propose two algorithms: Newton-Direction-Based ReLU-Thresholding (NDRT) and its enhanced variant, Newton-Direction-Based ReLU-Thresholding Pursuit (NDRTP). Theoretical analysis iindicates that both algorithms can guarantee exact recovery of nonnegative sparse signals when the measurement matrix satisfies a certain condition.. Numerical experiments demonstrate NDRTP achieves competitive performance compared to several existing methods in both noisy and noiseless scenarios."}
{"id": "2602.16687", "categories": ["cs.SD", "cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2602.16687", "abs": "https://arxiv.org/abs/2602.16687", "authors": ["Potsawee Manakul", "Woody Haosheng Gan", "Martijn Bartelds", "Guangzhi Sun", "William Held", "Diyi Yang"], "title": "Scaling Open Discrete Audio Foundation Models with Interleaved Semantic, Acoustic, and Text Tokens", "comment": null, "summary": "Current audio language models are predominantly text-first, either extending pre-trained text LLM backbones or relying on semantic-only audio tokens, limiting general audio modeling. This paper presents a systematic empirical study of native audio foundation models that apply next-token prediction to audio at scale, jointly modeling semantic content, acoustic details, and text to support both general audio generation and cross-modal capabilities. We provide comprehensive empirical insights for building such models: (1) We systematically investigate design choices -- data sources, text mixture ratios, and token composition -- establishing a validated training recipe. (2) We conduct the first scaling law study for discrete audio models via IsoFLOP analysis on 64 models spanning $3{\\times}10^{18}$ to $3{\\times}10^{20}$ FLOPs, finding that optimal data grows 1.6$\\times$ faster than optimal model size. (3) We apply these lessons to train SODA (Scaling Open Discrete Audio), a suite of models from 135M to 4B parameters on 500B tokens, comparing against our scaling predictions and existing models. SODA serves as a flexible backbone for diverse audio/text tasks -- we demonstrate this by fine-tuning for voice-preserving speech-to-speech translation, using the same unified architecture."}
{"id": "2602.16253", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2602.16253", "abs": "https://arxiv.org/abs/2602.16253", "authors": ["Kevin Wilkinghoff", "Keisuke Imoto", "Zheng-Hua Tan"], "title": "How Much Does Machine Identity Matter in Anomalous Sound Detection at Test Time?", "comment": null, "summary": "Anomalous sound detection (ASD) benchmarks typically assume that the identity of the monitored machine is known at test time and that recordings are evaluated in a machine-wise manner. However, in realistic monitoring scenarios with multiple known machines operating concurrently, test recordings may not be reliably attributable to a specific machine, and requiring machine identity imposes deployment constraints such as dedicated sensors per machine. To reveal performance degradations and method-specific differences in robustness that are hidden under standard machine-wise evaluation, we consider a minimal modification of the ASD evaluation protocol in which test recordings from multiple machines are merged and evaluated jointly without access to machine identity at inference time. Training data and evaluation metrics remain unchanged, and machine identity labels are used only for post hoc evaluation. Experiments with representative ASD methods show that relaxing this assumption reveals performance degradations and method-specific differences in robustness that are hidden under standard machine-wise evaluation, and that these degradations are strongly related to implicit machine identification accuracy."}
{"id": "2602.15888", "categories": ["eess.SP", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.15888", "abs": "https://arxiv.org/abs/2602.15888", "authors": ["Boyu Li", "Xingchun Zhu", "Yonghui Wu"], "title": "NeuroSleep: Neuromorphic Event-Driven Single-Channel EEG Sleep Staging for Edge-Efficient Sensing", "comment": "14 pages, 5 figures, under review at Journal of Neural Engineering", "summary": "Reliable, continuous neural sensing on wearable edge platforms is fundamental to long-term health monitoring; however, for electroencephalography (EEG)-based sleep monitoring, dense high-frequency processing is often computationally prohibitive under tight energy budgets. To address this bottleneck, this paper proposes NeuroSleep, an integrated event-driven sensing and inference system for energy-efficient sleep staging. NeuroSleep first converts raw EEG into complementary multi-scale bipolar event streams using Residual Adaptive Multi-Scale Delta Modulation (R-AMSDM), enabling an explicit fidelity-sparsity trade-off at the sensing front end. Furthermore, NeuroSleep adopts a hierarchical inference architecture that comprises an Event-based Adaptive Multi-scale Response (EAMR) module for local feature extraction, a Local Temporal-Attention Module (LTAM) for context aggregation, and an Epoch-Leaky Integrate-and-Fire (ELIF) module to capture long-term state persistence. Experimental results using subject-independent 5-fold cross-validation on the Sleep-EDF Expanded dataset demonstrate that NeuroSleep achieves a mean accuracy of 74.2% with only 0.932 M parameters while reducing sparsity-adjusted effective operations by approximately 53.6% relative to dense processing. Compared with the representative dense Transformer baseline, NeuroSleep improves accuracy by 7.5% with a 45.8% reduction in computational load. By bridging neuromorphic encoding with state-aware modeling, NeuroSleep provides a scalable solution for always-on sleep analysis in resource-constrained wearable scenarios."}
{"id": "2602.15909", "categories": ["eess.AS", "cs.AI", "cs.DB", "cs.HC", "cs.MA", "cs.SD"], "pdf": "https://arxiv.org/pdf/2602.15909", "abs": "https://arxiv.org/abs/2602.15909", "authors": ["Pengfei Zhang", "Tianxin Xie", "Minghao Yang", "Li Liu"], "title": "Resp-Agent: An Agent-Based System for Multimodal Respiratory Sound Generation and Disease Diagnosis", "comment": "24 pages, 3 figures. Published as a conference paper at ICLR 2026. Code and data available at https://github.com/zpforlove/Resp-Agent", "summary": "Deep learning-based respiratory auscultation is currently hindered by two fundamental challenges: (i) inherent information loss, as converting signals into spectrograms discards transient acoustic events and clinical context; (ii) limited data availability, exacerbated by severe class imbalance. To bridge these gaps, we present Resp-Agent, an autonomous multimodal system orchestrated by a novel Active Adversarial Curriculum Agent (Thinker-A$^2$CA). Unlike static pipelines, Thinker-A$^2$CA serves as a central controller that actively identifies diagnostic weaknesses and schedules targeted synthesis in a closed loop. To address the representation gap, we introduce a Modality-Weaving Diagnoser that weaves EHR data with audio tokens via Strategic Global Attention and sparse audio anchors, capturing both long-range clinical context and millisecond-level transients. To address the data gap, we design a Flow Matching Generator that adapts a text-only Large Language Model (LLM) via modality injection, decoupling pathological content from acoustic style to synthesize hard-to-diagnose samples. As a foundation for these efforts, we introduce Resp-229k, a benchmark corpus of 229k recordings paired with LLM-distilled clinical narratives. Extensive experiments demonstrate that Resp-Agent consistently outperforms prior approaches across diverse evaluation settings, improving diagnostic robustness under data scarcity and long-tailed class imbalance. Our code and data are available at https://github.com/zpforlove/Resp-Agent."}
{"id": "2602.16256", "categories": ["eess.AS", "cs.AI", "cs.SD"], "pdf": "https://arxiv.org/pdf/2602.16256", "abs": "https://arxiv.org/abs/2602.16256", "authors": ["Ryotaro Nagase", "Ryoichi Takashima", "Yoichi Yamashita"], "title": "Color-based Emotion Representation for Speech Emotion Recognition", "comment": "Submitted to EUSIPCO2026", "summary": "Speech emotion recognition (SER) has traditionally relied on categorical or dimensional labels. However, this technique is limited in representing both the diversity and interpretability of emotions. To overcome this limitation, we focus on color attributes, such as hue, saturation, and value, to represent emotions as continuous and interpretable scores. We annotated an emotional speech corpus with color attributes via crowdsourcing and analyzed them. Moreover, we built regression models for color attributes in SER using machine learning and deep learning, and explored the multitask learning of color attribute regression and emotion classification. As a result, we demonstrated the relationship between color attributes and emotions in speech, and successfully developed color attribute regression models for SER. We also showed that multitask learning improved the performance of each task."}
{"id": "2602.16108", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2602.16108", "abs": "https://arxiv.org/abs/2602.16108", "authors": ["Muhammad Fasih Waheed", "Shonda Bernadin", "Ali Hassan"], "title": "Advancing Industry 4.0: Multimodal Sensor Fusion for AI-Based Fault Detection in 3D Printing", "comment": "International Journal of Engineering Research and Innovation | v17, n2, Fall/Winter 2025", "summary": "Additive manufacturing, particularly fused deposition modeling, is transforming modern production by enabling rapid prototyping and complex part fabrication. However, its layer-by-layer process remains vulnerable to faults such as nozzle clogging, filament runout, and layer misalignment, which compromise print quality and reliability. Traditional inspection methods are costly, time-intensive, and often limited to post-process analysis, making them unsuitable for real-time intervention. In this current study, the authors developed a novel, low-cost, and portable faultdetection system that leverages multimodal sensor fusion and artificial intelligence for real-time monitoring in FDM-based 3D printing. The system integrates acoustic, vibration, and thermal sensing into a non-intrusive architecture, capturing complementary data streams that reflect both mechanical and process-related anomalies. Acoustic and thermal sensors operate in a fully contactless manner, while the vibration sensor requires minimal attachment such that it will not interfere with printer hardware, thereby preserving portability and ease of deployment. The multimodal signals are processed into spectrograms and time-frequency features, which are classified using convolutional neural networks for intelligent fault detection. The proposed system advances Industry 4.0 objectives by offering an affordable, scalable, and practical monitoring solution that improves faultdetection accuracy, reduces waste, and supports sustainable, adaptive manufacturing."}
{"id": "2602.16118", "categories": ["eess.SP", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2602.16118", "abs": "https://arxiv.org/abs/2602.16118", "authors": ["Muhammad Fasih Waheed", "Shonda Bernadin"], "title": "Real time fault detection in 3D printers using Convolutional Neural Networks and acoustic signals", "comment": "6 pages", "summary": "The reliability and quality of 3D printing processes are critically dependent on the timely detection of mechanical faults. Traditional monitoring methods often rely on visual inspection and hardware sensors, which can be both costly and limited in scope. This paper explores a scalable and contactless method for the use of real-time audio signal analysis for detecting mechanical faults in 3D printers. By capturing and classifying acoustic emissions during the printing process, we aim to identify common faults such as nozzle clogging, filament breakage, pully skipping and various other mechanical faults. Utilizing Convolutional neural networks, we implement algorithms capable of real-time audio classification to detect these faults promptly. Our methodology involves conducting a series of controlled experiments to gather audio data, followed by the application of advanced machine learning models for fault detection. Additionally, we review existing literature on audio-based fault detection in manufacturing and 3D printing to contextualize our research within the broader field. Preliminary results demonstrate that audio signals, when analyzed with machine learning techniques, provide a reliable and cost-effective means of enhancing real-time fault detection."}
{"id": "2602.16399", "categories": ["eess.AS", "cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2602.16399", "abs": "https://arxiv.org/abs/2602.16399", "authors": ["Michael Neri", "Tuomas Virtanen"], "title": "Multi-Channel Replay Speech Detection using Acoustic Maps", "comment": "Submitted to EUSIPCO 2026", "summary": "Replay attacks remain a critical vulnerability for automatic speaker verification systems, particularly in real-time voice assistant applications. In this work, we propose acoustic maps as a novel spatial feature representation for replay speech detection from multi-channel recordings. Derived from classical beamforming over discrete azimuth and elevation grids, acoustic maps encode directional energy distributions that reflect physical differences between human speech radiation and loudspeaker-based replay. A lightweight convolutional neural network is designed to operate on this representation, achieving competitive performance on the ReMASC dataset with approximately 6k trainable parameters. Experimental results show that acoustic maps provide a compact and physically interpretable feature space for replay attack detection across different devices and acoustic environments."}
{"id": "2602.16118", "categories": ["eess.SP", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2602.16118", "abs": "https://arxiv.org/abs/2602.16118", "authors": ["Muhammad Fasih Waheed", "Shonda Bernadin"], "title": "Real time fault detection in 3D printers using Convolutional Neural Networks and acoustic signals", "comment": "6 pages", "summary": "The reliability and quality of 3D printing processes are critically dependent on the timely detection of mechanical faults. Traditional monitoring methods often rely on visual inspection and hardware sensors, which can be both costly and limited in scope. This paper explores a scalable and contactless method for the use of real-time audio signal analysis for detecting mechanical faults in 3D printers. By capturing and classifying acoustic emissions during the printing process, we aim to identify common faults such as nozzle clogging, filament breakage, pully skipping and various other mechanical faults. Utilizing Convolutional neural networks, we implement algorithms capable of real-time audio classification to detect these faults promptly. Our methodology involves conducting a series of controlled experiments to gather audio data, followed by the application of advanced machine learning models for fault detection. Additionally, we review existing literature on audio-based fault detection in manufacturing and 3D printing to contextualize our research within the broader field. Preliminary results demonstrate that audio signals, when analyzed with machine learning techniques, provide a reliable and cost-effective means of enhancing real-time fault detection."}
{"id": "2602.16253", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2602.16253", "abs": "https://arxiv.org/abs/2602.16253", "authors": ["Kevin Wilkinghoff", "Keisuke Imoto", "Zheng-Hua Tan"], "title": "How Much Does Machine Identity Matter in Anomalous Sound Detection at Test Time?", "comment": null, "summary": "Anomalous sound detection (ASD) benchmarks typically assume that the identity of the monitored machine is known at test time and that recordings are evaluated in a machine-wise manner. However, in realistic monitoring scenarios with multiple known machines operating concurrently, test recordings may not be reliably attributable to a specific machine, and requiring machine identity imposes deployment constraints such as dedicated sensors per machine. To reveal performance degradations and method-specific differences in robustness that are hidden under standard machine-wise evaluation, we consider a minimal modification of the ASD evaluation protocol in which test recordings from multiple machines are merged and evaluated jointly without access to machine identity at inference time. Training data and evaluation metrics remain unchanged, and machine identity labels are used only for post hoc evaluation. Experiments with representative ASD methods show that relaxing this assumption reveals performance degradations and method-specific differences in robustness that are hidden under standard machine-wise evaluation, and that these degradations are strongly related to implicit machine identification accuracy."}
{"id": "2602.16416", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2602.16416", "abs": "https://arxiv.org/abs/2602.16416", "authors": ["Andreas Jonas Fuglsig", "Mads Græsbøll Christensen", "Jesper Rindom Jensen"], "title": "Online Single-Channel Audio-Based Sound Speed Estimation for Robust Multi-Channel Audio Control", "comment": "Preprint submitted to EUSIPCO 2026, under review", "summary": "Robust spatial audio control relies on accurate acoustic propagation models, yet environmental variations, especially changes in the speed of sound, cause systematic mismatches that degrade performance. Existing methods either assume known sound speed, require multiple microphones, or rely on separate calibration, making them impractical for systems with minimal sensing. We propose an online sound speed estimator that operates during general multichannel audio playback and requires only a single observation microphone. The method exploits the structured effect of sound speed on the reproduced signal and estimates it by minimizing the mismatch between the measured audio and a parametric acoustic model. Simulations show accurate tracking of sound speed for diverse input signals and improved spatial control performance when the estimates are used to compensate propagation errors in a sound zone control framework."}
{"id": "2602.16119", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2602.16119", "abs": "https://arxiv.org/abs/2602.16119", "authors": ["Muhammad Fasih Waheed", "Shonda Bernadin"], "title": "In-Situ Analysis of Vibration and Acoustic Data in Additive Manufacturing", "comment": null, "summary": "Vibration from an erroneous disturbance harms the manufactured components and lowers the output quality of an FDM printer. For moving machinery, vibration analysis and control are crucial. Additive manufacturing is the basis of 3D printing, which utilizes mechanical movement of the extruder to fabricate objects, and faults occur due to unwanted vibrations. Therefore, it is vital to examine the vibration patterns of a 3D printer. In this work, we observe these parameters of an FDM printer, exemplified by the MakerBot Method X. To analyze the system, it is necessary to understand the motion it generates and select appropriate sensors to detect those motions. The sensor measurement values can be used to determine the condition of the printer. We used an accelerometer and an acoustic sensor to measure the vibration and sound produced by the printer. The outputs from these sensors were examined individually. The findings show that vibration occurs at relatively low levels during continuous motion because it mainly appears at component transition edges. Due to abrupt acceleration and deceleration during zigzag motion, vibration reaches its peak."}
{"id": "2602.16256", "categories": ["eess.AS", "cs.AI", "cs.SD"], "pdf": "https://arxiv.org/pdf/2602.16256", "abs": "https://arxiv.org/abs/2602.16256", "authors": ["Ryotaro Nagase", "Ryoichi Takashima", "Yoichi Yamashita"], "title": "Color-based Emotion Representation for Speech Emotion Recognition", "comment": "Submitted to EUSIPCO2026", "summary": "Speech emotion recognition (SER) has traditionally relied on categorical or dimensional labels. However, this technique is limited in representing both the diversity and interpretability of emotions. To overcome this limitation, we focus on color attributes, such as hue, saturation, and value, to represent emotions as continuous and interpretable scores. We annotated an emotional speech corpus with color attributes via crowdsourcing and analyzed them. Moreover, we built regression models for color attributes in SER using machine learning and deep learning, and explored the multitask learning of color attribute regression and emotion classification. As a result, we demonstrated the relationship between color attributes and emotions in speech, and successfully developed color attribute regression models for SER. We also showed that multitask learning improved the performance of each task."}
{"id": "2602.16421", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2602.16421", "abs": "https://arxiv.org/abs/2602.16421", "authors": ["Natsuki Akaishi", "Nicki Holighaus", "Kohei Yatabe"], "title": "SELEBI: Percussion-aware Time Stretching via Selective Magnitude Spectrogram Compression by Nonstationary Gabor Transform", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Phase vocoder-based time-stretching is a widely used technique for the time-scale modification of audio signals. However, conventional implementations suffer from ``percussion smearing,'' a well-known artifact that significantly degrades the quality of percussive components. We attribute this artifact to a fundamental time-scale mismatch between the temporally smeared magnitude spectrogram and the localized, newly generated phase. To address this, we propose SELEBI, a signal-adaptive phase vocoder algorithm that significantly reduces percussion smearing while preserving stability and the perfect reconstruction property. Unlike conventional methods that rely on heuristic processing or component separation, our approach leverages the nonstationary Gabor transform. By dynamically adapting analysis window lengths to assign short windows to intervals containing significant energy associated with percussive components, we directly compute a temporally localized magnitude spectrogram from the time-domain signal. This approach ensures greater consistency between the temporal structures of the magnitude and phase. Furthermore, the perfect reconstruction property of the nonstationary Gabor transform guarantees stable, high-fidelity signal synthesis, in contrast to previous heuristic approaches. Experimental results demonstrate that the proposed method effectively mitigates percussion smearing and yields natural sound quality."}
{"id": "2602.16244", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2602.16244", "abs": "https://arxiv.org/abs/2602.16244", "authors": ["Shan Shan", "Chongjun Ouyang", "Xiaohang Yang", "Yong Li", "Zhiqin Wang", "Yuanwei Liu"], "title": "Pinching Antennas-Aided Integrated Sensing and Multicast Communication Systems", "comment": null, "summary": "A pinching antennas (PAs)-aided integrated sensing and multicast communication framework is proposed. In this framework, the communication performance is measured by the multicast rate considering max-min fairness. Moreover, the sensing performance is quantified by the Bayesian Cramér-Rao bound (BCRB), where a Gauss-Hermite quadrature-based approach is proposed to compute the Bayesian Fisher information matrix. Based on these metrics, PA placement is optimized under three criteria: communications-centric (C-C), sensing-centric (S-C), and Pareto-optimal designs. These designs are investigated in two scenarios: the single-PA case and the multi-PA case. 1) For the single-PA case, a closed-form solution is derived for the location of the C-C transmit PA, while the S-C design yields optimal transmit and receive PA placements that are symmetric about the target location. Leveraging this geometric insight, the Pareto-optimal design is solved by enforcing this PA placement symmetry, thereby reducing the joint transmit and receive PA placement to the transmit PA optimization. 2) For the general multi-PA case, the PA placements constitute a highly non-convex optimization problem. To solve this, an element-wise alternating optimization-based method is proposed to sequentially optimize all PA placements for the S-C design, and is further incorporated into an augmented Lagrangian (AL) framework and a rate-profile formulation to solve the C-C and Pareto-optimal design problems, respectively. Numerical results show that: i) PASS substantially outperforms fixed-antenna baselines in both multicast rate and sensing accuracy; ii) the multicasting gain becomes more pronounced as the user density increases; and iii) the sensing accuracy improves with the number of deployed PAs."}
{"id": "2602.16399", "categories": ["eess.AS", "cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2602.16399", "abs": "https://arxiv.org/abs/2602.16399", "authors": ["Michael Neri", "Tuomas Virtanen"], "title": "Multi-Channel Replay Speech Detection using Acoustic Maps", "comment": "Submitted to EUSIPCO 2026", "summary": "Replay attacks remain a critical vulnerability for automatic speaker verification systems, particularly in real-time voice assistant applications. In this work, we propose acoustic maps as a novel spatial feature representation for replay speech detection from multi-channel recordings. Derived from classical beamforming over discrete azimuth and elevation grids, acoustic maps encode directional energy distributions that reflect physical differences between human speech radiation and loudspeaker-based replay. A lightweight convolutional neural network is designed to operate on this representation, achieving competitive performance on the ReMASC dataset with approximately 6k trainable parameters. Experimental results show that acoustic maps provide a compact and physically interpretable feature space for replay attack detection across different devices and acoustic environments."}
{"id": "2602.16118", "categories": ["eess.SP", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2602.16118", "abs": "https://arxiv.org/abs/2602.16118", "authors": ["Muhammad Fasih Waheed", "Shonda Bernadin"], "title": "Real time fault detection in 3D printers using Convolutional Neural Networks and acoustic signals", "comment": "6 pages", "summary": "The reliability and quality of 3D printing processes are critically dependent on the timely detection of mechanical faults. Traditional monitoring methods often rely on visual inspection and hardware sensors, which can be both costly and limited in scope. This paper explores a scalable and contactless method for the use of real-time audio signal analysis for detecting mechanical faults in 3D printers. By capturing and classifying acoustic emissions during the printing process, we aim to identify common faults such as nozzle clogging, filament breakage, pully skipping and various other mechanical faults. Utilizing Convolutional neural networks, we implement algorithms capable of real-time audio classification to detect these faults promptly. Our methodology involves conducting a series of controlled experiments to gather audio data, followed by the application of advanced machine learning models for fault detection. Additionally, we review existing literature on audio-based fault detection in manufacturing and 3D printing to contextualize our research within the broader field. Preliminary results demonstrate that audio signals, when analyzed with machine learning techniques, provide a reliable and cost-effective means of enhancing real-time fault detection."}
{"id": "2602.16257", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2602.16257", "abs": "https://arxiv.org/abs/2602.16257", "authors": ["Jón Winkel", "Tom Willems", "Cillian O'Driscoll", "Ignacio Fernandez-Hernandez"], "title": "SeaSpoofFinder -- Potential GNSS Spoofing Event Detection Using AIS", "comment": null, "summary": "This paper investigates whether large-scale GNSS spoofing activity can be inferred from maritime Automatic Identification System (AIS) position reports. A data-processing framework, called SeaSpoofFinder, available here: seaspooffinder.github.io/ais_data, was developed to ingest and post-process global AIS streams and to detect candidate anomalies through a two-stage procedure. In Stage 1, implausible position jumps are identified using kinematic and data-quality filters; in Stage 2, events are retained only when multiple vessels exhibit spatially consistent source and target clustering, thereby reducing false positives from single-vessel artifacts. The resulting final potential spoofing events (FPSEs) reveal recurrent patterns in several regions, including the Baltic Sea, the Black Sea, Murmansk, Moscow, and the Haifa area, with affected footprints that can span large maritime areas. The analysis also highlights recurring non-spoofing artifacts (e.g., back-to-port jumps and data gaps) that can still pass heuristic filters in dense traffic regions. These results indicate that AIS-based monitoring can provide useful evidence for identifying and characterizing potential spoofing activity at scale, while emphasizing that AIS-only evidence does not provide definitive attribution."}
{"id": "2602.16416", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2602.16416", "abs": "https://arxiv.org/abs/2602.16416", "authors": ["Andreas Jonas Fuglsig", "Mads Græsbøll Christensen", "Jesper Rindom Jensen"], "title": "Online Single-Channel Audio-Based Sound Speed Estimation for Robust Multi-Channel Audio Control", "comment": "Preprint submitted to EUSIPCO 2026, under review", "summary": "Robust spatial audio control relies on accurate acoustic propagation models, yet environmental variations, especially changes in the speed of sound, cause systematic mismatches that degrade performance. Existing methods either assume known sound speed, require multiple microphones, or rely on separate calibration, making them impractical for systems with minimal sensing. We propose an online sound speed estimator that operates during general multichannel audio playback and requires only a single observation microphone. The method exploits the structured effect of sound speed on the reproduced signal and estimates it by minimizing the mismatch between the measured audio and a parametric acoustic model. Simulations show accurate tracking of sound speed for diverse input signals and improved spatial control performance when the estimates are used to compensate propagation errors in a sound zone control framework."}
{"id": "2602.16687", "categories": ["cs.SD", "cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2602.16687", "abs": "https://arxiv.org/abs/2602.16687", "authors": ["Potsawee Manakul", "Woody Haosheng Gan", "Martijn Bartelds", "Guangzhi Sun", "William Held", "Diyi Yang"], "title": "Scaling Open Discrete Audio Foundation Models with Interleaved Semantic, Acoustic, and Text Tokens", "comment": null, "summary": "Current audio language models are predominantly text-first, either extending pre-trained text LLM backbones or relying on semantic-only audio tokens, limiting general audio modeling. This paper presents a systematic empirical study of native audio foundation models that apply next-token prediction to audio at scale, jointly modeling semantic content, acoustic details, and text to support both general audio generation and cross-modal capabilities. We provide comprehensive empirical insights for building such models: (1) We systematically investigate design choices -- data sources, text mixture ratios, and token composition -- establishing a validated training recipe. (2) We conduct the first scaling law study for discrete audio models via IsoFLOP analysis on 64 models spanning $3{\\times}10^{18}$ to $3{\\times}10^{20}$ FLOPs, finding that optimal data grows 1.6$\\times$ faster than optimal model size. (3) We apply these lessons to train SODA (Scaling Open Discrete Audio), a suite of models from 135M to 4B parameters on 500B tokens, comparing against our scaling predictions and existing models. SODA serves as a flexible backbone for diverse audio/text tasks -- we demonstrate this by fine-tuning for voice-preserving speech-to-speech translation, using the same unified architecture."}
{"id": "2602.16271", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2602.16271", "abs": "https://arxiv.org/abs/2602.16271", "authors": ["Omid Abbassi Aghda", "Slavisa Tomic", "Oussama Ben Haj Belkacem", "Joao Guerreiro", "Nuno Souto", "Michal Szczachor", "Rui Dinis"], "title": "Impact of Preprocessing on Neural Network-Based RSS/AoA Positioning", "comment": null, "summary": "Hybrid received signal strength (RSS)-angle of arrival (AoA)-based positioning offers low-cost distance estimation and high-resolution angular measurements. Still, it comes at a cost of inherent nonlinearities, geometry-dependent noise, and suboptimal weighting in conventional linear estimators that might limit accuracy. In this paper, we propose a neural network-based approach using a multilayer perceptron (MLP) to directly map RSS-AoA measurements to 3D positions, capturing nonlinear relationships that are difficult to model with traditional methods. We evaluate the impact of input representation by comparing networks trained on raw measurements versus preprocessed features derived from a linearization method. Simulation results show that the learning-based approach consistently outperforms existing linear methods under RSS noise across all noise levels, and matches or surpasses state-of-the-art performance under increasing AoA noise. Furthermore, preprocessing measurements using the linearization method provides a clear advantage over raw data, demonstrating the benefit of geometry-aware feature extraction."}
{"id": "2602.16421", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2602.16421", "abs": "https://arxiv.org/abs/2602.16421", "authors": ["Natsuki Akaishi", "Nicki Holighaus", "Kohei Yatabe"], "title": "SELEBI: Percussion-aware Time Stretching via Selective Magnitude Spectrogram Compression by Nonstationary Gabor Transform", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Phase vocoder-based time-stretching is a widely used technique for the time-scale modification of audio signals. However, conventional implementations suffer from ``percussion smearing,'' a well-known artifact that significantly degrades the quality of percussive components. We attribute this artifact to a fundamental time-scale mismatch between the temporally smeared magnitude spectrogram and the localized, newly generated phase. To address this, we propose SELEBI, a signal-adaptive phase vocoder algorithm that significantly reduces percussion smearing while preserving stability and the perfect reconstruction property. Unlike conventional methods that rely on heuristic processing or component separation, our approach leverages the nonstationary Gabor transform. By dynamically adapting analysis window lengths to assign short windows to intervals containing significant energy associated with percussive components, we directly compute a temporally localized magnitude spectrogram from the time-domain signal. This approach ensures greater consistency between the temporal structures of the magnitude and phase. Furthermore, the perfect reconstruction property of the nonstationary Gabor transform guarantees stable, high-fidelity signal synthesis, in contrast to previous heuristic approaches. Experimental results demonstrate that the proposed method effectively mitigates percussion smearing and yields natural sound quality."}
{"id": "2602.16383", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2602.16383", "abs": "https://arxiv.org/abs/2602.16383", "authors": ["Ziming Liu", "Tao Chen", "Giacinto Gelli", "Vincenzo Galdi", "Francesco Verde"], "title": "Joint beamforming and mode optimization for multi-functional STAR-RIS-aided integrated sensing and communication networks", "comment": "17 pages, 8 figures, journal paper", "summary": "This paper investigates the design of integrated sensing and communication (ISAC) systems assisted by simultaneously transmitting and reflecting reconfigurable intelligent surfaces (STAR-RISs), which act as multi-functional programmable metasurfaces capable of supporting concurrent communication and sensing within a unified architecture. We propose a two-stage ISAC protocol, in which the preparation phase performs direction estimation for outdoor users located in the reflection space, while maintaining communication with both outdoor and indoor users in the transmission space. The subsequent communication phase exploits the estimated directions to enhance information transfer. The directions of outdoor users are modeled as Gaussian random variables to capture estimation uncertainty, and the corresponding average communication performance is incorporated into the design. Building on this framework, we formulate a performance-balanced optimization problem that maximizes the communication sum-rate while guaranteeing the required sensing accuracy, jointly determining the beamforming vectors at the base station (BS), the STAR-RIS transmission and reflection coefficients, and the metasurface partition between energy-splitting and transmit-only modes. The physical constraints of STAR-RIS elements and the required sensing performance are explicitly enforced. To address the non-convex nature of the problem, we combine fractional programming, Lagrangian dual reformulation, and successive convex approximation. The binary metasurface partition is ultimately recovered via continuous relaxation followed by projection-based binarization. Numerical results demonstrate that the proposed design achieves an effective trade-off between sensing accuracy and communication throughput, by significantly outperforming conventional STAR-RIS-aided ISAC schemes."}
{"id": "2602.16418", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2602.16418", "abs": "https://arxiv.org/abs/2602.16418", "authors": ["Haruka Kobayashi", "Ryo Hayakawa"], "title": "Reconstruction of Piecewise-Constant Sparse Signals for Modulo Sampling", "comment": "This work will be submitted to the IEEE for possible publication", "summary": "Modulo sampling is a promising technology to preserve amplitude information that exceeds the observable range of analog-to-digital converters during the digitization of analog signals. Since conventional methods typically reconstruct the original signal by estimating the differences of the residual signal and computing their cumulative sum, each estimation error inevitably propagates through subsequent time samples. In this paper, to eliminate this error-propagation problem, we propose an algorithm that reconstructs the residual signal directly. The proposed method takes advantage of the high-frequency characteristics of the modulo samples and the sparsity of both the residual signal and its difference. Simulation results show that the proposed method reconstructs the original signal more accurately than a conventional method based on the differences of the residual signal."}
{"id": "2602.16441", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2602.16441", "abs": "https://arxiv.org/abs/2602.16441", "authors": ["Carl Collmann", "Ahmad Nimr", "Gerhard Fettweis"], "title": "Proof of Concept: Local TX Real-Time Phase Calibration in MIMO Systems", "comment": "7 pages, 12 figures, 3 tables", "summary": "Channel measurements in MIMO systems hinge on precise synchronization. While methods for time and frequency synchronization are well established, maintaining real-time phase coherence remains an open requirement for many MIMO systems. Phase coherence in MIMO systems is crucial for beamforming in digital arrays and enables precise parameter estimates such as Angle-of-Arrival/Departure. This work presents and validates a simple local real-time phase calibration method for a digital array. We compare two different approaches, instantaneous and smoothed calibration, to determine the optimal interval between synchronization procedures. To quantitatively assess calibration performance, we use two metrics: the average beamforming power loss and the RMS cycle-to-cycle jitter. Our results indicate that both approaches for phase calibration are effective and yield RMS of jitter in the 2.1 ps to 124 fs range for different SDR models. This level of precision enables coherent transmission on commonly available SDR platforms, allowing investigation on advanced MIMO techniques and transmit beamforming in practical testbeds."}
{"id": "2602.16546", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2602.16546", "abs": "https://arxiv.org/abs/2602.16546", "authors": ["Mostafa Rahmani Ghourtani", "Junbo Zhao", "Yi Chu", "Hamed Ahmadi", "David Grace", "Alister G. Burr"], "title": "Failure-Aware Access Point Selection for Resilient Cell-Free Massive MIMO Networks", "comment": "7 Pages, 3 figures", "summary": "This paper presents a Failure-Aware Access Point Selection (FAAS) method aimed at improving hardware resilience in cell-free massive MIMO (CF-mMIMO) networks. FAAS selects APs for each user by jointly considering channel strength and the failure probability of each AP. A tunable parameter \\(α\\in [0,1]\\) scales these failure probabilities to model different levels of network stress. We evaluate resilience using two key metrics: the minimum-user spectral efficiency, which captures worst-case user performance, and the outage probability, defined as the fraction of users left without any active APs. Simulation results show that FAAS maintains significantly better performance under failure conditions compared to failure-agnostic clustering. At high failure levels, FAAS reduces outage by over 85\\% and improves worst-case user rates. These results confirm that FAAS is a practical and efficient solution for building more reliable CF-mMIMO networks."}
{"id": "2602.16621", "categories": ["eess.SP", "physics.flu-dyn", "physics.optics"], "pdf": "https://arxiv.org/pdf/2602.16621", "abs": "https://arxiv.org/abs/2602.16621", "authors": ["Karl J. Weisenburger", "Gregery T. Buzzard", "Charles A. Bouman", "Matthew R. Kemnetz"], "title": "WindDensity-MBIR: Model-Based Iterative Reconstruction for Wind Tunnel 3D Density Estimation", "comment": "Submitted to the Unconventional Imaging, Sensing, and Adaptive Optics special session of Optical Engineering", "summary": "Experimentalists often use wind tunnels to study aerodynamic turbulence, but most wind tunnel imaging techniques are limited in their ability to take non-invasive 3D density measurements of turbulence. Wavefront tomography is a technique that uses multiple wavefront measurements from various viewing angles to non-invasively measure the 3D density field of a turbulent medium. Existing methods make strong assumptions, such as a spline basis representation, to address the ill-conditioned nature of this problem. We formulate this problem as a Bayesian, sparse-view tomographic reconstruction problem and develop a model-based iterative reconstruction algorithm for measuring the volumetric 3D density field inside a wind tunnel. We call this method WindDensity-MBIR and apply it using simulated data to difficult reconstruction scenarios with sparse data, small projection field of view, and limited angular extent. WindDensity-MBIR can recover high-order features in these scenarios within 10% to 25% error even when the tip, tilt, and piston are removed from the wavefront measurements."}
{"id": "2602.16637", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2602.16637", "abs": "https://arxiv.org/abs/2602.16637", "authors": ["De-Ming Chian", "Chao-Kai Wen", "Feng-Ji Chen", "Yi-Jie Sun", "Fu-Kang Wang"], "title": "Active RIS-Assisted MIMO System for Vital Signs Extraction: ISAC Modeling, Deep Learning, and Prototype Measurements", "comment": null, "summary": "We present the RIS-VSign system, an active reconfigurable intelligent surface (RIS)-assisted multiple-input multiple-output orthogonal frequency division multiplexing (MIMO-OFDM) framework for vital signs extraction under an integrated sensing and communication (ISAC) model. The system consists of two stages: the phase selector of RIS and the extraction of respiration rate. To mitigate synchronization-induced common phase drifts, the difference of Möbius transformation (DMT) is integrated into the deep learning framework, named DMTNet, to jointly configure multiple active RIS elements. Notably, the training data are generated in simulation without collecting real-world measurements, and the resulting phase selector is validated experimentally. For sensing, multi-antenna measurements are fused by the DC-offset calibration and the DeepMining-MMV processing with CA-CFAR detection and Newton's refinements. Prototype experiments indicate that active RIS deployment improves respiration detectability while simultaneously enabling higher-order modulation; without RIS, respiration detection is unreliable and only lower-order modulation is supported."}

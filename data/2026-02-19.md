<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 14]
- [eess.AS](#eess.AS) [Total: 6]
- [cs.SD](#cs.SD) [Total: 5]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [Newton-Direction-Based ReLU-Thresholding Methods for Nonnegative Sparse Signal Recovery](https://arxiv.org/abs/2602.15880)
*Ning Bian,Zhong-Feng Sun,Yun-Bin Zhao,Jin-Chuan Zhou,Nan Meng*

Main category: eess.SP

TL;DR: 提出两种基于牛顿方向和ReLU的非负稀疏信号恢复算法：NDRT和NDRTP，理论证明在特定测量矩阵条件下能精确恢复，实验显示NDRTP在噪声和无噪声场景下具有竞争力。


<details>
  <summary>Details</summary>
Motivation: 非负稀疏信号恢复在多个领域有广泛应用，现有方法虽然有效但仍有改进空间。ReLU技术已被引入增强恢复算法，但结合牛顿型阈值方法的研究不足，需要开发更高效的非负稀疏恢复算法。

Method: 将牛顿型阈值与基于ReLU的方法结合，提出两种算法：牛顿方向ReLU阈值法(NDRT)及其增强版本牛顿方向ReLU阈值追踪法(NDRTP)。算法利用ReLU处理非负约束，结合牛顿方向优化阈值过程。

Result: 理论分析表明，当测量矩阵满足特定条件时，两种算法都能保证精确恢复非负稀疏信号。数值实验显示NDRTP在噪声和无噪声场景下，与多种现有方法相比具有竞争性性能。

Conclusion: 成功开发了结合牛顿方向和ReLU的非负稀疏恢复算法，理论保证精确恢复条件，实验验证了算法有效性，NDRTP在性能上具有竞争力，为非负稀疏信号恢复提供了新方法。

Abstract: Nonnegative sparse signal recovery has been extensively studied due to its broad applications. Recent work has integrated rectified linear unit (ReLU) techniques to enhance existing recovery algorithms. We merge Newton-type thresholding with ReLU-based approaches to propose two algorithms: Newton-Direction-Based ReLU-Thresholding (NDRT) and its enhanced variant, Newton-Direction-Based ReLU-Thresholding Pursuit (NDRTP). Theoretical analysis iindicates that both algorithms can guarantee exact recovery of nonnegative sparse signals when the measurement matrix satisfies a certain condition.. Numerical experiments demonstrate NDRTP achieves competitive performance compared to several existing methods in both noisy and noiseless scenarios.

</details>


### [2] [NeuroSleep: Neuromorphic Event-Driven Single-Channel EEG Sleep Staging for Edge-Efficient Sensing](https://arxiv.org/abs/2602.15888)
*Boyu Li,Xingchun Zhu,Yonghui Wu*

Main category: eess.SP

TL;DR: NeuroSleep是一个集成的事件驱动传感和推理系统，用于高效能睡眠分期，通过神经形态编码和状态感知建模，在资源受限的可穿戴设备上实现持续睡眠监测。


<details>
  <summary>Details</summary>
Motivation: 可穿戴边缘平台上可靠、连续的神经传感对于长期健康监测至关重要，但基于脑电图（EEG）的睡眠监测需要进行密集的高频处理，这在严格的能量预算下计算成本过高。需要解决这一瓶颈问题。

Method: 1. 使用残差自适应多尺度Delta调制（R-AMSDM）将原始EEG转换为互补的多尺度双极事件流，实现传感前端的显式保真度-稀疏性权衡。2. 采用分层推理架构：基于事件的自适应多尺度响应（EAMR）模块用于局部特征提取，局部时间注意力模块（LTAM）用于上下文聚合，以及Epoch-Leaky Integrate-and-Fire（ELIF）模块用于捕获长期状态持续性。

Result: 在Sleep-EDF Expanded数据集上使用受试者独立的5折交叉验证，NeuroSleep实现了74.2%的平均准确率，仅需0.932M参数，同时相对于密集处理减少了约53.6%的稀疏性调整有效操作。与代表性的密集Transformer基线相比，NeuroSleep准确率提高了7.5%，计算负载减少了45.8%。

Conclusion: 通过将神经形态编码与状态感知建模相结合，NeuroSleep为资源受限的可穿戴场景中的始终在线睡眠分析提供了一个可扩展的解决方案。

Abstract: Reliable, continuous neural sensing on wearable edge platforms is fundamental to long-term health monitoring; however, for electroencephalography (EEG)-based sleep monitoring, dense high-frequency processing is often computationally prohibitive under tight energy budgets. To address this bottleneck, this paper proposes NeuroSleep, an integrated event-driven sensing and inference system for energy-efficient sleep staging. NeuroSleep first converts raw EEG into complementary multi-scale bipolar event streams using Residual Adaptive Multi-Scale Delta Modulation (R-AMSDM), enabling an explicit fidelity-sparsity trade-off at the sensing front end. Furthermore, NeuroSleep adopts a hierarchical inference architecture that comprises an Event-based Adaptive Multi-scale Response (EAMR) module for local feature extraction, a Local Temporal-Attention Module (LTAM) for context aggregation, and an Epoch-Leaky Integrate-and-Fire (ELIF) module to capture long-term state persistence. Experimental results using subject-independent 5-fold cross-validation on the Sleep-EDF Expanded dataset demonstrate that NeuroSleep achieves a mean accuracy of 74.2% with only 0.932 M parameters while reducing sparsity-adjusted effective operations by approximately 53.6% relative to dense processing. Compared with the representative dense Transformer baseline, NeuroSleep improves accuracy by 7.5% with a 45.8% reduction in computational load. By bridging neuromorphic encoding with state-aware modeling, NeuroSleep provides a scalable solution for always-on sleep analysis in resource-constrained wearable scenarios.

</details>


### [3] [Advancing Industry 4.0: Multimodal Sensor Fusion for AI-Based Fault Detection in 3D Printing](https://arxiv.org/abs/2602.16108)
*Muhammad Fasih Waheed,Shonda Bernadin,Ali Hassan*

Main category: eess.SP

TL;DR: 开发了一种基于多模态传感器融合和人工智能的低成本便携式故障检测系统，用于FDM 3D打印的实时监控。


<details>
  <summary>Details</summary>
Motivation: FDM 3D打印的逐层制造过程容易受到喷嘴堵塞、耗材耗尽和层错位等故障影响，传统检测方法成本高、耗时长且无法实时干预，需要一种实时监控解决方案。

Method: 集成声学、振动和热传感器构建非侵入式架构，将多模态信号处理成频谱图和时间-频率特征，使用卷积神经网络进行智能故障检测。

Result: 系统实现了实时故障检测，提高了检测准确性，减少了浪费，支持可持续的自适应制造，符合工业4.0目标。

Conclusion: 该研究提出了一种经济实惠、可扩展且实用的监控解决方案，通过多模态传感器融合和人工智能技术，显著提升了FDM 3D打印的可靠性和质量。

Abstract: Additive manufacturing, particularly fused deposition modeling, is transforming modern production by enabling rapid prototyping and complex part fabrication. However, its layer-by-layer process remains vulnerable to faults such as nozzle clogging, filament runout, and layer misalignment, which compromise print quality and reliability. Traditional inspection methods are costly, time-intensive, and often limited to post-process analysis, making them unsuitable for real-time intervention. In this current study, the authors developed a novel, low-cost, and portable faultdetection system that leverages multimodal sensor fusion and artificial intelligence for real-time monitoring in FDM-based 3D printing. The system integrates acoustic, vibration, and thermal sensing into a non-intrusive architecture, capturing complementary data streams that reflect both mechanical and process-related anomalies. Acoustic and thermal sensors operate in a fully contactless manner, while the vibration sensor requires minimal attachment such that it will not interfere with printer hardware, thereby preserving portability and ease of deployment. The multimodal signals are processed into spectrograms and time-frequency features, which are classified using convolutional neural networks for intelligent fault detection. The proposed system advances Industry 4.0 objectives by offering an affordable, scalable, and practical monitoring solution that improves faultdetection accuracy, reduces waste, and supports sustainable, adaptive manufacturing.

</details>


### [4] [Real time fault detection in 3D printers using Convolutional Neural Networks and acoustic signals](https://arxiv.org/abs/2602.16118)
*Muhammad Fasih Waheed,Shonda Bernadin*

Main category: eess.SP

TL;DR: 利用卷积神经网络对3D打印过程中的音频信号进行实时分析，实现机械故障的无接触检测


<details>
  <summary>Details</summary>
Motivation: 传统3D打印故障监测方法依赖视觉检查和硬件传感器，成本高且范围有限，需要更经济有效的实时监测方案

Method: 通过控制实验收集音频数据，使用卷积神经网络进行实时音频分类，检测喷嘴堵塞、线材断裂、皮带打滑等机械故障

Result: 初步结果表明，结合机器学习技术的音频分析能够提供可靠且经济高效的实时故障检测手段

Conclusion: 音频信号分析为3D打印机械故障检测提供了一种可扩展、无接触的解决方案，有望提升打印过程的可靠性和质量

Abstract: The reliability and quality of 3D printing processes are critically dependent on the timely detection of mechanical faults. Traditional monitoring methods often rely on visual inspection and hardware sensors, which can be both costly and limited in scope. This paper explores a scalable and contactless method for the use of real-time audio signal analysis for detecting mechanical faults in 3D printers. By capturing and classifying acoustic emissions during the printing process, we aim to identify common faults such as nozzle clogging, filament breakage, pully skipping and various other mechanical faults. Utilizing Convolutional neural networks, we implement algorithms capable of real-time audio classification to detect these faults promptly. Our methodology involves conducting a series of controlled experiments to gather audio data, followed by the application of advanced machine learning models for fault detection. Additionally, we review existing literature on audio-based fault detection in manufacturing and 3D printing to contextualize our research within the broader field. Preliminary results demonstrate that audio signals, when analyzed with machine learning techniques, provide a reliable and cost-effective means of enhancing real-time fault detection.

</details>


### [5] [In-Situ Analysis of Vibration and Acoustic Data in Additive Manufacturing](https://arxiv.org/abs/2602.16119)
*Muhammad Fasih Waheed,Shonda Bernadin*

Main category: eess.SP

TL;DR: 研究使用加速度计和声学传感器监测FDM 3D打印机的振动模式，发现连续运动时振动较低，而在锯齿形运动时由于突然加减速导致振动达到峰值


<details>
  <summary>Details</summary>
Motivation: 3D打印中的振动会损害制造部件质量并降低输出质量，对于移动机械来说振动分析和控制至关重要。FDM打印机利用挤出机的机械运动制造物体，不希望的振动会导致故障，因此需要研究3D打印机的振动模式

Method: 使用MakerBot Method X FDM打印机作为示例，采用加速度计和声学传感器分别测量打印机产生的振动和声音，分析传感器输出数据来评估打印机状态

Result: 研究发现：连续运动时振动水平相对较低，因为振动主要出现在组件过渡边缘；而在锯齿形运动期间，由于突然的加速和减速，振动达到峰值

Conclusion: 通过传感器监测可以识别3D打印机的振动模式，这对于理解打印机状态和潜在故障检测很重要，特别是在运动模式变化时振动特性会显著改变

Abstract: Vibration from an erroneous disturbance harms the manufactured components and lowers the output quality of an FDM printer. For moving machinery, vibration analysis and control are crucial. Additive manufacturing is the basis of 3D printing, which utilizes mechanical movement of the extruder to fabricate objects, and faults occur due to unwanted vibrations. Therefore, it is vital to examine the vibration patterns of a 3D printer. In this work, we observe these parameters of an FDM printer, exemplified by the MakerBot Method X. To analyze the system, it is necessary to understand the motion it generates and select appropriate sensors to detect those motions. The sensor measurement values can be used to determine the condition of the printer. We used an accelerometer and an acoustic sensor to measure the vibration and sound produced by the printer. The outputs from these sensors were examined individually. The findings show that vibration occurs at relatively low levels during continuous motion because it mainly appears at component transition edges. Due to abrupt acceleration and deceleration during zigzag motion, vibration reaches its peak.

</details>


### [6] [Pinching Antennas-Aided Integrated Sensing and Multicast Communication Systems](https://arxiv.org/abs/2602.16244)
*Shan Shan,Chongjun Ouyang,Xiaohang Yang,Yong Li,Zhiqin Wang,Yuanwei Liu*

Main category: eess.SP

TL;DR: 提出基于可移动天线（PAs）的集成感知与组播通信框架，优化天线布局以平衡通信与感知性能


<details>
  <summary>Details</summary>
Motivation: 传统固定天线系统在集成感知与通信（ISAC）中性能受限，需要探索可移动天线布局优化来提升组播通信速率和感知精度

Method: 提出三种设计准则：通信中心（C-C）、感知中心（S-C）和帕累托最优设计；针对单PA和多PA场景分别采用闭式解和交替优化方法

Result: PAs系统显著优于固定天线基线，组播增益随用户密度增加而增强，感知精度随PA数量增加而提高

Conclusion: 可移动天线布局优化能有效提升ISAC系统的通信和感知性能，为未来无线系统设计提供了新思路

Abstract: A pinching antennas (PAs)-aided integrated sensing and multicast communication framework is proposed. In this framework, the communication performance is measured by the multicast rate considering max-min fairness. Moreover, the sensing performance is quantified by the Bayesian Cramér-Rao bound (BCRB), where a Gauss-Hermite quadrature-based approach is proposed to compute the Bayesian Fisher information matrix. Based on these metrics, PA placement is optimized under three criteria: communications-centric (C-C), sensing-centric (S-C), and Pareto-optimal designs. These designs are investigated in two scenarios: the single-PA case and the multi-PA case. 1) For the single-PA case, a closed-form solution is derived for the location of the C-C transmit PA, while the S-C design yields optimal transmit and receive PA placements that are symmetric about the target location. Leveraging this geometric insight, the Pareto-optimal design is solved by enforcing this PA placement symmetry, thereby reducing the joint transmit and receive PA placement to the transmit PA optimization. 2) For the general multi-PA case, the PA placements constitute a highly non-convex optimization problem. To solve this, an element-wise alternating optimization-based method is proposed to sequentially optimize all PA placements for the S-C design, and is further incorporated into an augmented Lagrangian (AL) framework and a rate-profile formulation to solve the C-C and Pareto-optimal design problems, respectively. Numerical results show that: i) PASS substantially outperforms fixed-antenna baselines in both multicast rate and sensing accuracy; ii) the multicasting gain becomes more pronounced as the user density increases; and iii) the sensing accuracy improves with the number of deployed PAs.

</details>


### [7] [SeaSpoofFinder -- Potential GNSS Spoofing Event Detection Using AIS](https://arxiv.org/abs/2602.16257)
*Jón Winkel,Tom Willems,Cillian O'Driscoll,Ignacio Fernandez-Hernandez*

Main category: eess.SP

TL;DR: 开发SeaSpoofFinder框架，通过AIS数据分析检测大规模GNSS欺骗活动，识别出多个区域的潜在欺骗事件模式。


<details>
  <summary>Details</summary>
Motivation: 研究是否可以通过海事自动识别系统(AIS)位置报告推断大规模GNSS欺骗活动，为海上导航安全提供监控手段。

Method: 开发SeaSpoofFinder数据处理框架，采用两阶段检测：第一阶段通过运动学和数据质量过滤器识别不合理位置跳跃；第二阶段通过多船舶空间一致性聚类减少误报。

Result: 在波罗的海、黑海、摩尔曼斯克、莫斯科和海法地区发现重复出现的潜在欺骗事件模式，同时识别出非欺骗性伪影（如回港跳跃和数据缺口）。

Conclusion: AIS监控可为大规模潜在欺骗活动识别提供有用证据，但仅凭AIS证据不能提供明确归因，需要结合其他数据源。

Abstract: This paper investigates whether large-scale GNSS spoofing activity can be inferred from maritime Automatic Identification System (AIS) position reports. A data-processing framework, called SeaSpoofFinder, available here: seaspooffinder.github.io/ais_data, was developed to ingest and post-process global AIS streams and to detect candidate anomalies through a two-stage procedure. In Stage 1, implausible position jumps are identified using kinematic and data-quality filters; in Stage 2, events are retained only when multiple vessels exhibit spatially consistent source and target clustering, thereby reducing false positives from single-vessel artifacts. The resulting final potential spoofing events (FPSEs) reveal recurrent patterns in several regions, including the Baltic Sea, the Black Sea, Murmansk, Moscow, and the Haifa area, with affected footprints that can span large maritime areas. The analysis also highlights recurring non-spoofing artifacts (e.g., back-to-port jumps and data gaps) that can still pass heuristic filters in dense traffic regions. These results indicate that AIS-based monitoring can provide useful evidence for identifying and characterizing potential spoofing activity at scale, while emphasizing that AIS-only evidence does not provide definitive attribution.

</details>


### [8] [Impact of Preprocessing on Neural Network-Based RSS/AoA Positioning](https://arxiv.org/abs/2602.16271)
*Omid Abbassi Aghda,Slavisa Tomic,Oussama Ben Haj Belkacem,Joao Guerreiro,Nuno Souto,Michal Szczachor,Rui Dinis*

Main category: eess.SP

TL;DR: 提出基于多层感知器的神经网络方法，直接将RSS-AoA测量映射到3D位置，优于传统线性方法


<details>
  <summary>Details</summary>
Motivation: 混合RSS-AoA定位虽然成本低且角度分辨率高，但存在固有非线性、几何相关噪声和传统线性估计器次优加权等问题，限制了定位精度

Method: 使用多层感知器神经网络直接映射RSS-AoA测量到3D位置，比较原始测量与基于线性化方法预处理特征两种输入表示

Result: 在所有RSS噪声水平下，学习方法始终优于现有线性方法；在AoA噪声增加时，匹配或超越最先进性能；预处理测量相比原始数据有明显优势

Conclusion: 基于神经网络的方法能有效捕捉传统方法难以建模的非线性关系，几何感知的特征提取能提升性能，为混合RSS-AoA定位提供了更优解决方案

Abstract: Hybrid received signal strength (RSS)-angle of arrival (AoA)-based positioning offers low-cost distance estimation and high-resolution angular measurements. Still, it comes at a cost of inherent nonlinearities, geometry-dependent noise, and suboptimal weighting in conventional linear estimators that might limit accuracy. In this paper, we propose a neural network-based approach using a multilayer perceptron (MLP) to directly map RSS-AoA measurements to 3D positions, capturing nonlinear relationships that are difficult to model with traditional methods. We evaluate the impact of input representation by comparing networks trained on raw measurements versus preprocessed features derived from a linearization method. Simulation results show that the learning-based approach consistently outperforms existing linear methods under RSS noise across all noise levels, and matches or surpasses state-of-the-art performance under increasing AoA noise. Furthermore, preprocessing measurements using the linearization method provides a clear advantage over raw data, demonstrating the benefit of geometry-aware feature extraction.

</details>


### [9] [Joint beamforming and mode optimization for multi-functional STAR-RIS-aided integrated sensing and communication networks](https://arxiv.org/abs/2602.16383)
*Ziming Liu,Tao Chen,Giacinto Gelli,Vincenzo Galdi,Francesco Verde*

Main category: eess.SP

TL;DR: 本文研究了STAR-RIS辅助的集成感知与通信系统设计，提出两阶段协议，通过优化基站波束成形、STAR-RIS系数和模式划分，在保证感知精度的同时最大化通信和速率。


<details>
  <summary>Details</summary>
Motivation: 传统ISAC系统在同时支持通信和感知方面存在性能折衷，STAR-RIS作为多功能可编程超表面，能够通过同时透射和反射特性，为室内外用户提供统一的感知通信架构，但需要解决感知不确定性和资源分配的联合优化问题。

Method: 提出两阶段ISAC协议：准备阶段进行反射空间户外用户方向估计，同时维持传输空间通信；通信阶段利用估计方向增强信息传输。将方向建模为高斯随机变量，考虑估计不确定性，采用分数规划、拉格朗日对偶重构和逐次凸逼近方法解决非凸优化问题，通过连续松弛和投影二值化恢复超表面划分。

Result: 数值结果表明，所提设计在感知精度和通信吞吐量之间实现了有效权衡，显著优于传统的STAR-RIS辅助ISAC方案，通过联合优化基站波束成形、STAR-RIS系数和模式划分，提升了系统整体性能。

Conclusion: STAR-RIS辅助的ISAC系统通过提出的两阶段协议和联合优化框架，能够有效平衡感知与通信性能，为未来集成感知通信网络提供了有前景的解决方案，特别是在支持室内外混合用户场景方面表现出优越性能。

Abstract: This paper investigates the design of integrated sensing and communication (ISAC) systems assisted by simultaneously transmitting and reflecting reconfigurable intelligent surfaces (STAR-RISs), which act as multi-functional programmable metasurfaces capable of supporting concurrent communication and sensing within a unified architecture. We propose a two-stage ISAC protocol, in which the preparation phase performs direction estimation for outdoor users located in the reflection space, while maintaining communication with both outdoor and indoor users in the transmission space. The subsequent communication phase exploits the estimated directions to enhance information transfer. The directions of outdoor users are modeled as Gaussian random variables to capture estimation uncertainty, and the corresponding average communication performance is incorporated into the design. Building on this framework, we formulate a performance-balanced optimization problem that maximizes the communication sum-rate while guaranteeing the required sensing accuracy, jointly determining the beamforming vectors at the base station (BS), the STAR-RIS transmission and reflection coefficients, and the metasurface partition between energy-splitting and transmit-only modes. The physical constraints of STAR-RIS elements and the required sensing performance are explicitly enforced. To address the non-convex nature of the problem, we combine fractional programming, Lagrangian dual reformulation, and successive convex approximation. The binary metasurface partition is ultimately recovered via continuous relaxation followed by projection-based binarization. Numerical results demonstrate that the proposed design achieves an effective trade-off between sensing accuracy and communication throughput, by significantly outperforming conventional STAR-RIS-aided ISAC schemes.

</details>


### [10] [Reconstruction of Piecewise-Constant Sparse Signals for Modulo Sampling](https://arxiv.org/abs/2602.16418)
*Haruka Kobayashi,Ryo Hayakawa*

Main category: eess.SP

TL;DR: 提出一种直接重构残差信号的模数采样方法，避免传统方法中误差传播问题


<details>
  <summary>Details</summary>
Motivation: 传统模数采样方法通过估计残差信号的差值并计算累积和来重构原始信号，但每个估计误差会不可避免地传播到后续时间样本，存在误差传播问题

Method: 提出直接重构残差信号的算法，利用模数样本的高频特性以及残差信号及其差值的稀疏性

Result: 仿真结果表明，所提方法比基于残差信号差值的传统方法能更准确地重构原始信号

Conclusion: 通过直接重构残差信号而非估计其差值，有效消除了误差传播问题，提高了模数采样信号重构的准确性

Abstract: Modulo sampling is a promising technology to preserve amplitude information that exceeds the observable range of analog-to-digital converters during the digitization of analog signals. Since conventional methods typically reconstruct the original signal by estimating the differences of the residual signal and computing their cumulative sum, each estimation error inevitably propagates through subsequent time samples. In this paper, to eliminate this error-propagation problem, we propose an algorithm that reconstructs the residual signal directly. The proposed method takes advantage of the high-frequency characteristics of the modulo samples and the sparsity of both the residual signal and its difference. Simulation results show that the proposed method reconstructs the original signal more accurately than a conventional method based on the differences of the residual signal.

</details>


### [11] [Proof of Concept: Local TX Real-Time Phase Calibration in MIMO Systems](https://arxiv.org/abs/2602.16441)
*Carl Collmann,Ahmad Nimr,Gerhard Fettweis*

Main category: eess.SP

TL;DR: 提出并验证了一种用于数字阵列的简单本地实时相位校准方法，比较了瞬时和平滑校准两种方法，实现了2.1 ps到124 fs范围内的抖动精度，使商用SDR平台能够支持相干传输和波束成形。


<details>
  <summary>Details</summary>
Motivation: MIMO系统中的信道测量依赖于精确同步。虽然时间和频率同步方法已经很成熟，但保持实时相位相干性仍然是许多MIMO系统的开放需求。相位相干性对于数字阵列中的波束成形至关重要，并能够实现精确的参数估计，如到达角/离开角。

Method: 提出并验证了一种简单的本地实时相位校准方法，比较了两种不同方法：瞬时校准和平滑校准，以确定同步程序之间的最佳间隔。使用两个指标定量评估校准性能：平均波束成形功率损失和RMS周期到周期抖动。

Result: 结果表明两种相位校准方法都有效，对于不同的SDR模型，抖动的RMS在2.1 ps到124 fs范围内。这种精度水平使得在常见的SDR平台上能够实现相干传输，允许在实际测试平台上研究先进的MIMO技术和发射波束成形。

Conclusion: 该研究提出的简单本地实时相位校准方法有效解决了MIMO系统中的相位相干性问题，使商用SDR平台能够支持相干传输和波束成形，为实际测试平台中的先进MIMO技术研究提供了可行方案。

Abstract: Channel measurements in MIMO systems hinge on precise synchronization. While methods for time and frequency synchronization are well established, maintaining real-time phase coherence remains an open requirement for many MIMO systems. Phase coherence in MIMO systems is crucial for beamforming in digital arrays and enables precise parameter estimates such as Angle-of-Arrival/Departure. This work presents and validates a simple local real-time phase calibration method for a digital array. We compare two different approaches, instantaneous and smoothed calibration, to determine the optimal interval between synchronization procedures. To quantitatively assess calibration performance, we use two metrics: the average beamforming power loss and the RMS cycle-to-cycle jitter. Our results indicate that both approaches for phase calibration are effective and yield RMS of jitter in the 2.1 ps to 124 fs range for different SDR models. This level of precision enables coherent transmission on commonly available SDR platforms, allowing investigation on advanced MIMO techniques and transmit beamforming in practical testbeds.

</details>


### [12] [Failure-Aware Access Point Selection for Resilient Cell-Free Massive MIMO Networks](https://arxiv.org/abs/2602.16546)
*Mostafa Rahmani Ghourtani,Junbo Zhao,Yi Chu,Hamed Ahmadi,David Grace,Alister G. Burr*

Main category: eess.SP

TL;DR: FAAS方法通过联合考虑信道强度和AP故障概率来选择接入点，显著提升了无蜂窝大规模MIMO网络的硬件弹性，在高故障率下可将中断概率降低85%以上。


<details>
  <summary>Details</summary>
Motivation: 无蜂窝大规模MIMO网络中接入点故障会严重影响网络性能，现有方法通常忽略故障概率，导致在硬件故障条件下网络弹性不足。

Method: 提出故障感知接入点选择方法，为每个用户选择接入点时同时考虑信道强度和每个AP的故障概率，通过可调参数α缩放故障概率来模拟不同网络压力水平。

Result: 仿真结果显示，与故障无关的聚类方法相比，FAAS在故障条件下保持显著更好的性能，在高故障水平下可将中断概率降低85%以上，并改善最差用户速率。

Conclusion: FAAS是构建更可靠无蜂窝大规模MIMO网络的实用高效解决方案，通过考虑故障概率显著提升了网络弹性。

Abstract: This paper presents a Failure-Aware Access Point Selection (FAAS) method aimed at improving hardware resilience in cell-free massive MIMO (CF-mMIMO) networks. FAAS selects APs for each user by jointly considering channel strength and the failure probability of each AP. A tunable parameter \(α\in [0,1]\) scales these failure probabilities to model different levels of network stress. We evaluate resilience using two key metrics: the minimum-user spectral efficiency, which captures worst-case user performance, and the outage probability, defined as the fraction of users left without any active APs. Simulation results show that FAAS maintains significantly better performance under failure conditions compared to failure-agnostic clustering. At high failure levels, FAAS reduces outage by over 85\% and improves worst-case user rates. These results confirm that FAAS is a practical and efficient solution for building more reliable CF-mMIMO networks.

</details>


### [13] [WindDensity-MBIR: Model-Based Iterative Reconstruction for Wind Tunnel 3D Density Estimation](https://arxiv.org/abs/2602.16621)
*Karl J. Weisenburger,Gregery T. Buzzard,Charles A. Bouman,Matthew R. Kemnetz*

Main category: eess.SP

TL;DR: 提出WindDensity-MBIR方法，一种基于贝叶斯框架的稀疏视角断层扫描重建算法，用于风洞中非侵入式3D密度场测量，即使在数据稀疏、视场小、角度受限等困难场景下也能恢复高阶特征。


<details>
  <summary>Details</summary>
Motivation: 传统风洞成像技术难以进行非侵入式3D密度测量，现有波前断层扫描方法需要强假设条件（如样条基表示）来处理病态问题，需要更鲁棒的方法来应对稀疏数据、小视场和有限角度等实际限制。

Method: 将问题建模为贝叶斯稀疏视角断层扫描重建问题，开发模型驱动的迭代重建算法WindDensity-MBIR，即使从波前测量中移除倾斜和活塞项后，仍能处理稀疏数据、小投影视场和有限角度范围等挑战性场景。

Result: 在模拟数据测试中，WindDensity-MBIR在困难重建场景下（稀疏数据、小视场、有限角度）能够恢复高阶特征，误差控制在10%到25%之间，即使移除了波前测量的倾斜和活塞项后仍能保持良好性能。

Conclusion: WindDensity-MBIR为风洞湍流研究提供了一种有效的非侵入式3D密度场测量方法，能够处理实际实验中的各种限制条件，为空气动力学研究提供了新的工具。

Abstract: Experimentalists often use wind tunnels to study aerodynamic turbulence, but most wind tunnel imaging techniques are limited in their ability to take non-invasive 3D density measurements of turbulence. Wavefront tomography is a technique that uses multiple wavefront measurements from various viewing angles to non-invasively measure the 3D density field of a turbulent medium. Existing methods make strong assumptions, such as a spline basis representation, to address the ill-conditioned nature of this problem. We formulate this problem as a Bayesian, sparse-view tomographic reconstruction problem and develop a model-based iterative reconstruction algorithm for measuring the volumetric 3D density field inside a wind tunnel. We call this method WindDensity-MBIR and apply it using simulated data to difficult reconstruction scenarios with sparse data, small projection field of view, and limited angular extent. WindDensity-MBIR can recover high-order features in these scenarios within 10% to 25% error even when the tip, tilt, and piston are removed from the wavefront measurements.

</details>


### [14] [Active RIS-Assisted MIMO System for Vital Signs Extraction: ISAC Modeling, Deep Learning, and Prototype Measurements](https://arxiv.org/abs/2602.16637)
*De-Ming Chian,Chao-Kai Wen,Feng-Ji Chen,Yi-Jie Sun,Fu-Kang Wang*

Main category: eess.SP

TL;DR: RIS-VSign系统：基于主动可重构智能表面的MIMO-OFDM框架，用于集成感知与通信下的生命体征提取，通过DMTNet深度学习框架配置RIS相位，实验验证可提高呼吸检测可靠性并支持高阶调制。


<details>
  <summary>Details</summary>
Motivation: 在集成感知与通信（ISAC）场景下，传统方法在呼吸检测可靠性和通信调制阶数方面存在限制，需要一种能够同时提升感知性能和通信能力的解决方案。

Method: 采用两阶段系统：1）RIS相位选择器，集成Möbius变换差异（DMT）的深度学习框架DMTNet，通过仿真数据训练配置多个主动RIS元件；2）呼吸率提取，通过DC偏移校准、DeepMining-MMV处理、CA-CFAR检测和牛顿优化融合多天线测量。

Result: 原型实验表明，主动RIS部署显著提高了呼吸检测的可靠性，同时支持更高阶的调制方案；在没有RIS的情况下，呼吸检测不可靠且仅支持低阶调制。

Conclusion: RIS-VSign系统成功实现了主动RIS辅助的ISAC框架，通过深度学习配置RIS相位，在无需真实测量数据训练的情况下，有效提升了生命体征感知性能和通信能力，为未来智能无线系统提供了有前景的解决方案。

Abstract: We present the RIS-VSign system, an active reconfigurable intelligent surface (RIS)-assisted multiple-input multiple-output orthogonal frequency division multiplexing (MIMO-OFDM) framework for vital signs extraction under an integrated sensing and communication (ISAC) model. The system consists of two stages: the phase selector of RIS and the extraction of respiration rate. To mitigate synchronization-induced common phase drifts, the difference of Möbius transformation (DMT) is integrated into the deep learning framework, named DMTNet, to jointly configure multiple active RIS elements. Notably, the training data are generated in simulation without collecting real-world measurements, and the resulting phase selector is validated experimentally. For sensing, multi-antenna measurements are fused by the DC-offset calibration and the DeepMining-MMV processing with CA-CFAR detection and Newton's refinements. Prototype experiments indicate that active RIS deployment improves respiration detectability while simultaneously enabling higher-order modulation; without RIS, respiration detection is unreliable and only lower-order modulation is supported.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [15] [Resp-Agent: An Agent-Based System for Multimodal Respiratory Sound Generation and Disease Diagnosis](https://arxiv.org/abs/2602.15909)
*Pengfei Zhang,Tianxin Xie,Minghao Yang,Li Liu*

Main category: eess.AS

TL;DR: Resp-Agent是一个自主多模态呼吸听诊系统，通过主动对抗课程代理解决信息丢失和数据稀缺问题，在数据稀缺和类别不平衡情况下显著提升诊断性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于深度学习的呼吸听诊面临两个基本挑战：(1) 将信号转换为频谱图时丢失瞬态声学事件和临床上下文信息；(2) 数据可用性有限，且存在严重的类别不平衡问题。

Method: 提出Resp-Agent系统，包含三个核心组件：1) Thinker-A²CA主动对抗课程代理，作为中央控制器识别诊断弱点并安排针对性合成；2) Modality-Weaving Diagnoser，通过战略全局注意力和稀疏音频锚点将EHR数据与音频令牌编织，捕获长期临床上下文和毫秒级瞬态；3) Flow Matching Generator，通过模态注入适配纯文本LLM，解耦病理内容和声学风格以合成难以诊断的样本。

Result: 在Resp-229k基准语料库（包含229k录音和LLM提炼的临床叙述）上进行广泛实验，Resp-Agent在不同评估设置中始终优于先前方法，在数据稀缺和长尾类别不平衡情况下提高了诊断鲁棒性。

Conclusion: Resp-Agent通过主动对抗课程代理、多模态编织诊断器和流匹配生成器的协同工作，有效解决了呼吸听诊中的信息丢失和数据稀缺问题，为临床诊断提供了更可靠的解决方案。

Abstract: Deep learning-based respiratory auscultation is currently hindered by two fundamental challenges: (i) inherent information loss, as converting signals into spectrograms discards transient acoustic events and clinical context; (ii) limited data availability, exacerbated by severe class imbalance. To bridge these gaps, we present Resp-Agent, an autonomous multimodal system orchestrated by a novel Active Adversarial Curriculum Agent (Thinker-A$^2$CA). Unlike static pipelines, Thinker-A$^2$CA serves as a central controller that actively identifies diagnostic weaknesses and schedules targeted synthesis in a closed loop. To address the representation gap, we introduce a Modality-Weaving Diagnoser that weaves EHR data with audio tokens via Strategic Global Attention and sparse audio anchors, capturing both long-range clinical context and millisecond-level transients. To address the data gap, we design a Flow Matching Generator that adapts a text-only Large Language Model (LLM) via modality injection, decoupling pathological content from acoustic style to synthesize hard-to-diagnose samples. As a foundation for these efforts, we introduce Resp-229k, a benchmark corpus of 229k recordings paired with LLM-distilled clinical narratives. Extensive experiments demonstrate that Resp-Agent consistently outperforms prior approaches across diverse evaluation settings, improving diagnostic robustness under data scarcity and long-tailed class imbalance. Our code and data are available at https://github.com/zpforlove/Resp-Agent.

</details>


### [16] [How Much Does Machine Identity Matter in Anomalous Sound Detection at Test Time?](https://arxiv.org/abs/2602.16253)
*Kevin Wilkinghoff,Keisuke Imoto,Zheng-Hua Tan*

Main category: eess.AS

TL;DR: 本文提出对异常声音检测（ASD）评估协议进行最小修改：在测试时将多台机器的录音合并评估，无需机器身份信息，以揭示标准机器级评估下隐藏的性能下降和鲁棒性差异。


<details>
  <summary>Details</summary>
Motivation: 现实监控场景中，多台已知机器同时运行，测试录音可能无法可靠归属于特定机器，而标准ASD基准假设测试时已知机器身份，这在实际部署中需要每台机器专用传感器等约束。标准评估隐藏了性能下降和鲁棒性差异。

Method: 对ASD评估协议进行最小修改：将多台机器的测试录音合并并联合评估，无需推理时访问机器身份信息。训练数据和评估指标保持不变，机器身份标签仅用于事后评估。通过代表性ASD方法进行实验验证。

Result: 实验显示，放宽机器身份假设揭示了标准机器级评估下隐藏的性能下降和鲁棒性差异，这些性能下降与隐式机器识别准确性密切相关。

Conclusion: 标准ASD评估协议假设测试时已知机器身份，这在现实多机器监控场景中不切实际。通过最小修改评估协议，揭示了方法鲁棒性的重要差异，这些差异与机器识别能力相关，为更现实的ASD评估提供了新视角。

Abstract: Anomalous sound detection (ASD) benchmarks typically assume that the identity of the monitored machine is known at test time and that recordings are evaluated in a machine-wise manner. However, in realistic monitoring scenarios with multiple known machines operating concurrently, test recordings may not be reliably attributable to a specific machine, and requiring machine identity imposes deployment constraints such as dedicated sensors per machine. To reveal performance degradations and method-specific differences in robustness that are hidden under standard machine-wise evaluation, we consider a minimal modification of the ASD evaluation protocol in which test recordings from multiple machines are merged and evaluated jointly without access to machine identity at inference time. Training data and evaluation metrics remain unchanged, and machine identity labels are used only for post hoc evaluation. Experiments with representative ASD methods show that relaxing this assumption reveals performance degradations and method-specific differences in robustness that are hidden under standard machine-wise evaluation, and that these degradations are strongly related to implicit machine identification accuracy.

</details>


### [17] [Color-based Emotion Representation for Speech Emotion Recognition](https://arxiv.org/abs/2602.16256)
*Ryotaro Nagase,Ryoichi Takashima,Yoichi Yamashita*

Main category: eess.AS

TL;DR: 该论文提出使用颜色属性（色调、饱和度、明度）作为连续可解释的情感表示方法，替代传统的情感分类或维度标签，并构建了基于机器学习和深度学习的语音情感识别回归模型。


<details>
  <summary>Details</summary>
Motivation: 传统语音情感识别依赖分类或维度标签，但这种方法在表示情感多样性和可解释性方面存在局限。为了克服这些限制，研究者探索使用颜色属性作为连续且可解释的情感表示方式。

Method: 1. 通过众包方式为情感语音语料库标注颜色属性；2. 分析颜色属性与情感的关系；3. 使用机器学习和深度学习构建颜色属性回归模型；4. 探索颜色属性回归和情感分类的多任务学习。

Result: 1. 证明了语音中颜色属性与情感之间的关系；2. 成功开发了用于语音情感识别的颜色属性回归模型；3. 多任务学习提高了每个任务的性能。

Conclusion: 颜色属性可以作为连续且可解释的情感表示方法，多任务学习能有效提升语音情感识别中颜色属性回归和情感分类的性能。

Abstract: Speech emotion recognition (SER) has traditionally relied on categorical or dimensional labels. However, this technique is limited in representing both the diversity and interpretability of emotions. To overcome this limitation, we focus on color attributes, such as hue, saturation, and value, to represent emotions as continuous and interpretable scores. We annotated an emotional speech corpus with color attributes via crowdsourcing and analyzed them. Moreover, we built regression models for color attributes in SER using machine learning and deep learning, and explored the multitask learning of color attribute regression and emotion classification. As a result, we demonstrated the relationship between color attributes and emotions in speech, and successfully developed color attribute regression models for SER. We also showed that multitask learning improved the performance of each task.

</details>


### [18] [Multi-Channel Replay Speech Detection using Acoustic Maps](https://arxiv.org/abs/2602.16399)
*Michael Neri,Tuomas Virtanen*

Main category: eess.AS

TL;DR: 提出声学地图作为多通道录音中重放语音检测的新型空间特征表示方法，通过轻量级CNN实现高效检测


<details>
  <summary>Details</summary>
Motivation: 重放攻击是自动说话人验证系统的关键漏洞，特别是在实时语音助手应用中，需要有效的检测方法

Method: 提出声学地图作为空间特征表示，基于离散方位角和仰角网格的经典波束形成，编码方向性能量分布；设计轻量级卷积神经网络处理该表示

Result: 在ReMASC数据集上实现竞争性性能，仅需约6k可训练参数；声学地图为不同设备和声学环境下的重放攻击检测提供紧凑且物理可解释的特征空间

Conclusion: 声学地图是一种有效的重放语音检测特征表示方法，具有物理可解释性和高效性

Abstract: Replay attacks remain a critical vulnerability for automatic speaker verification systems, particularly in real-time voice assistant applications. In this work, we propose acoustic maps as a novel spatial feature representation for replay speech detection from multi-channel recordings. Derived from classical beamforming over discrete azimuth and elevation grids, acoustic maps encode directional energy distributions that reflect physical differences between human speech radiation and loudspeaker-based replay. A lightweight convolutional neural network is designed to operate on this representation, achieving competitive performance on the ReMASC dataset with approximately 6k trainable parameters. Experimental results show that acoustic maps provide a compact and physically interpretable feature space for replay attack detection across different devices and acoustic environments.

</details>


### [19] [Online Single-Channel Audio-Based Sound Speed Estimation for Robust Multi-Channel Audio Control](https://arxiv.org/abs/2602.16416)
*Andreas Jonas Fuglsig,Mads Græsbøll Christensen,Jesper Rindom Jensen*

Main category: eess.AS

TL;DR: 提出一种在线声速估计方法，仅需单个麦克风，在音频播放过程中实时估计声速，改善空间音频控制性能


<details>
  <summary>Details</summary>
Motivation: 环境变化（特别是声速变化）会导致声学传播模型失配，降低空间音频控制性能。现有方法要么假设已知声速，要么需要多个麦克风或单独校准，不适用于传感资源有限的系统

Method: 利用声速对重放信号的结构化影响，通过最小化测量音频与参数化声学模型之间的失配来估计声速。该方法在通用多通道音频播放期间运行，仅需单个观测麦克风

Result: 仿真显示该方法能准确跟踪不同输入信号的声速变化，在声区控制框架中使用估计值补偿传播误差时，显著改善了空间控制性能

Conclusion: 该方法提供了一种实用的在线声速估计解决方案，适用于传感资源有限的系统，能有效补偿环境变化对空间音频控制的影响

Abstract: Robust spatial audio control relies on accurate acoustic propagation models, yet environmental variations, especially changes in the speed of sound, cause systematic mismatches that degrade performance. Existing methods either assume known sound speed, require multiple microphones, or rely on separate calibration, making them impractical for systems with minimal sensing. We propose an online sound speed estimator that operates during general multichannel audio playback and requires only a single observation microphone. The method exploits the structured effect of sound speed on the reproduced signal and estimates it by minimizing the mismatch between the measured audio and a parametric acoustic model. Simulations show accurate tracking of sound speed for diverse input signals and improved spatial control performance when the estimates are used to compensate propagation errors in a sound zone control framework.

</details>


### [20] [SELEBI: Percussion-aware Time Stretching via Selective Magnitude Spectrogram Compression by Nonstationary Gabor Transform](https://arxiv.org/abs/2602.16421)
*Natsuki Akaishi,Nicki Holighaus,Kohei Yatabe*

Main category: eess.AS

TL;DR: SELEBI是一种基于非平稳Gabor变换的信号自适应相位声码器算法，通过动态调整分析窗长来减少打击乐成分的时间拉伸伪影，同时保持完美重构特性。


<details>
  <summary>Details</summary>
Motivation: 传统相位声码器在时间拉伸音频时存在"打击乐模糊"伪影，这是由于时间模糊的幅度谱与局部化新生成相位之间的时间尺度不匹配造成的，严重影响打击乐成分的质量。

Method: 提出SELEBI算法，利用非平稳Gabor变换，动态适应分析窗长度：为包含显著打击乐能量的区间分配短窗口，直接从时域信号计算时间局部化的幅度谱，确保幅度和相位时间结构的一致性。

Result: 实验结果表明，该方法能有效减轻打击乐模糊伪影，产生自然的音质，同时保持稳定性和完美重构特性。

Conclusion: SELEBI通过信号自适应窗长调整和非平稳Gabor变换，解决了传统相位声码器的打击乐模糊问题，在保持完美重构的同时显著提升了打击乐成分的时间拉伸质量。

Abstract: Phase vocoder-based time-stretching is a widely used technique for the time-scale modification of audio signals. However, conventional implementations suffer from ``percussion smearing,'' a well-known artifact that significantly degrades the quality of percussive components. We attribute this artifact to a fundamental time-scale mismatch between the temporally smeared magnitude spectrogram and the localized, newly generated phase. To address this, we propose SELEBI, a signal-adaptive phase vocoder algorithm that significantly reduces percussion smearing while preserving stability and the perfect reconstruction property. Unlike conventional methods that rely on heuristic processing or component separation, our approach leverages the nonstationary Gabor transform. By dynamically adapting analysis window lengths to assign short windows to intervals containing significant energy associated with percussive components, we directly compute a temporally localized magnitude spectrogram from the time-domain signal. This approach ensures greater consistency between the temporal structures of the magnitude and phase. Furthermore, the perfect reconstruction property of the nonstationary Gabor transform guarantees stable, high-fidelity signal synthesis, in contrast to previous heuristic approaches. Experimental results demonstrate that the proposed method effectively mitigates percussion smearing and yields natural sound quality.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [21] [MAEB: Massive Audio Embedding Benchmark](https://arxiv.org/abs/2602.16008)
*Adnan El Assadi,Isaac Chung,Chenghao Xiao,Roman Solomatin,Animesh Jha,Rahul Chand,Silky Singh,Kaitlyn Wang,Ali Sartaz Khan,Marc Moussa Nasser,Sufen Fong,Pengfei He,Alan Xiao,Ayush Sunil Munot,Aditya Shrivastava,Artem Gazizov,Niklas Muennighoff,Kenneth Enevoldsen*

Main category: cs.SD

TL;DR: MAEB是一个大规模音频嵌入基准测试，涵盖30个任务、100+语言，评估50+模型发现没有单一模型在所有任务上表现最优，音频-文本对比模型在环境音分类表现好但在多语言语音任务表现差，语音预训练模型则相反。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏统一的音频嵌入评估基准，现有模型在不同音频任务上表现差异大，需要系统评估音频嵌入模型在各种任务上的性能，并为音频大语言模型提供参考。

Method: 构建MAEB基准测试，包含30个任务覆盖语音、音乐、环境音和跨模态音频-文本推理，评估50+模型，从更大的MAEB+（98个任务）中精选而来，集成到MTEB生态系统中。

Result: 没有单一模型在所有任务上表现最优：音频-文本对比模型在环境音分类（如ESC50）表现好但在多语言语音任务表现差，语音预训练模型则相反；聚类任务对所有模型都具挑战性；音频编码器在MAEB上的表现与其在音频大语言模型中的表现高度相关。

Conclusion: MAEB为音频嵌入模型提供了全面的评估基准，揭示了当前模型的局限性，特别是声学理解和语言理解之间的权衡，为未来模型开发提供了重要参考，并已集成到多模态评估生态系统中。

Abstract: We introduce the Massive Audio Embedding Benchmark (MAEB), a large-scale benchmark covering 30 tasks across speech, music, environmental sounds, and cross-modal audio-text reasoning in 100+ languages. We evaluate 50+ models and find that no single model dominates across all tasks: contrastive audio-text models excel at environmental sound classification (e.g., ESC50) but score near random on multilingual speech tasks (e.g., SIB-FLEURS), while speech-pretrained models show the opposite pattern. Clustering remains challenging for all models, with even the best-performing model achieving only modest results. We observe that models excelling on acoustic understanding often perform poorly on linguistic tasks, and vice versa. We also show that the performance of audio encoders on MAEB correlates highly with their performance when used in audio large language models. MAEB is derived from MAEB+, a collection of 98 tasks. MAEB is designed to maintain task diversity while reducing evaluation cost, and it integrates into the MTEB ecosystem for unified evaluation across text, image, and audio modalities. We release MAEB and all 98 tasks along with code and a leaderboard at https://github.com/embeddings-benchmark/mteb.

</details>


### [22] [BAT: Better Audio Transformer Guided by Convex Gated Probing](https://arxiv.org/abs/2602.16305)
*Houtan Ghaffari,Lukas Rauch,Christoph Scholz,Paul Devos*

Main category: cs.SD

TL;DR: 论文提出了Convex Gated Probing (CGP)方法，显著缩小了音频自监督学习中微调与探测之间的性能差距，并基于此改进了音频SSL流程，推出了Better Audio Transformer (BAT)，在音频基准测试中取得了新的SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 当前音频自监督学习模型仍依赖微调进行评估，因为简单的探测方法无法充分发挥其潜力，且在AudioSet等基准上会改变模型排名。需要一种鲁棒且高效的探测机制来指导音频SSL向可靠和可复现的方法发展。

Method: 提出了Convex Gated Probing (CGP)，一种基于原型的探测方法，通过门控机制有效利用所有冻结层，并揭示潜在任务相关信息的位置。基于CGP的指导，重新设计了当前SOTA音频模型的整个SSL流程，包括数据预处理、模型架构和预训练策略。

Result: CGP显著缩小了音频SSL中微调与探测之间的性能差距。通过改进的SSL流程，推出了Better Audio Transformer (BAT)，在音频基准测试中建立了新的SOTA结果。

Conclusion: CGP为音频自监督学习提供了可靠的评估机制，指导了更有效的SSL流程设计，推动了音频SSL向可靠和可复现的方向发展，并通过BAT模型展示了实际性能提升。

Abstract: Probing is widely adopted in computer vision to faithfully evaluate self-supervised learning (SSL) embeddings, as fine-tuning may misrepresent their inherent quality. In contrast, audio SSL models still rely on fine-tuning because simple probing fails to unlock their full potential and alters their rankings when competing for SOTA on AudioSet. Hence, a robust and efficient probing mechanism is required to guide the trajectory of audio SSL towards reliable and reproducible methods. We introduce Convex Gated Probing (CGP), a prototype-based method that drastically closes the gap between fine-tuning and probing in audio. CGP efficiently utilizes all frozen layers via a gating mechanism and exposes the location of latent task-relevant information. Guided by CGP, we rework the entire SSL pipeline of current SOTA audio models that use legacy implementations of prior SSL methods. By refining data preprocessing, model architecture, and pre-training recipe, we introduce Better Audio Transformer (BAT), and establish new SOTA on audio benchmarks.

</details>


### [23] [Spatial Audio Question Answering and Reasoning on Dynamic Source Movements](https://arxiv.org/abs/2602.16334)
*Arvind Krishna Sridhar,Yinyi Guo,Erik Visser*

Main category: cs.SD

TL;DR: 该论文提出了一种用于空间音频问答（Spatial AQA）的框架，专注于运动推理，通过数据增强、思维模式和源分离技术提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 空间音频理解旨在让机器能够解释复杂的听觉场景，特别是当声源随时间移动时。目前需要解决从立体声音频中推断物体运动、位置和方向变化的问题。

Method: 1. 提出基于运动的空间音频增强框架，从单声道音频事件合成多样化运动模式；2. 提出端到端多模态微调方法，包含思维模式，让音频-语言模型在预测答案前产生显式中间推理步骤；3. 研究查询条件源分离作为预处理阶段，比较无掩码、音频定位模型和真实掩码三种推理机制。

Result: 结果显示推理放大了源分离的益处，当问题中只存在单个事件时，思维模式带来了+5.1%的显著改进。这些发现突出了运动建模、推理和分离质量之间的相互作用。

Conclusion: 该研究为推进空间音频理解提供了新见解，强调了运动建模、显式推理和源分离技术的重要性，特别是在处理移动声源场景时。

Abstract: Spatial audio understanding aims to enable machines to interpret complex auditory scenes, particularly when sound sources move over time. In this work, we study Spatial Audio Question Answering (Spatial AQA) with a focus on movement reasoning, where a model must infer object motion, position, and directional changes directly from stereo audio. First, we introduce a movement-centric spatial audio augmentation framework that synthesizes diverse motion patterns from isolated mono audio events, enabling controlled and scalable training data generation. Second, we propose an end-to-end multimodal finetuning approach with a thinking mode, which allows audio-language models to produce explicit intermediate reasoning steps before predicting an answer. Third, we investigate the impact of query-conditioned source separation as a preprocessing stage and compare three inference regimes: no masking, an audio grounding model (AGM), and ground-truth masks. Our results show that reasoning amplifies the benefits of source separation, with thinking mode showing significant improvement of +5.1% when a single event is present in the question. These findings highlight the interplay between movement modeling, reasoning, and separation quality, offering new insights for advancing spatial audio understanding.

</details>


### [24] [How to Label Resynthesized Audio: The Dual Role of Neural Audio Codecs in Audio Deepfake Detection](https://arxiv.org/abs/2602.16343)
*Yixuan Xiao,Florian Lux,Alejandro Pérez-González-de-Martos,Ngoc Thang Vu*

Main category: cs.SD

TL;DR: 该研究探讨了语音合成系统中神经音频编解码器重合成数据的标签歧义问题，构建了ASVspoof 5数据集的扩展版本，分析了不同标签策略对欺骗检测性能的影响。


<details>
  <summary>Details</summary>
Motivation: 神经音频编解码器最初用于音频压缩，但也可用于语音合成，导致其重合成数据在欺骗检测中标签模糊（既可能是真实语音也可能是欺骗语音），目前对此问题研究较少。

Method: 构建了ASVspoof 5数据集的挑战性扩展版本，用于研究神经音频编解码器重合成数据的标签问题，并系统分析了不同标签选择对检测性能的影响。

Result: 研究发现不同的标签策略会显著影响欺骗检测性能，为相关领域提供了关于如何标注神经音频编解码器重合成数据的实践指导。

Conclusion: 神经音频编解码器重合成数据的标签歧义是欺骗检测中的重要问题，需要仔细考虑标签策略，该研究为此提供了数据集和分析框架。

Abstract: Since Text-to-Speech systems typically don't produce waveforms directly, recent spoof detection studies use resynthesized waveforms from vocoders and neural audio codecs to simulate an attacker. Unlike vocoders, which are specifically designed for speech synthesis, neural audio codecs were originally developed for compressing audio for storage and transmission. However, their ability to discretize speech also sparked interest in language-modeling-based speech synthesis. Owing to this dual functionality, codec resynthesized data may be labeled as either bonafide or spoof. So far, very little research has addressed this issue. In this study, we present a challenging extension of the ASVspoof 5 dataset constructed for this purpose. We examine how different labeling choices affect detection performance and provide insights into labeling strategies.

</details>


### [25] [Scaling Open Discrete Audio Foundation Models with Interleaved Semantic, Acoustic, and Text Tokens](https://arxiv.org/abs/2602.16687)
*Potsawee Manakul,Woody Haosheng Gan,Martijn Bartelds,Guangzhi Sun,William Held,Diyi Yang*

Main category: cs.SD

TL;DR: 本文系统研究了原生音频基础模型，通过大规模音频的下一个token预测，联合建模语义内容、声学细节和文本，支持通用音频生成和跨模态能力，并训练了SODA模型套件。


<details>
  <summary>Details</summary>
Motivation: 当前音频语言模型主要是文本优先的，要么扩展预训练的文本LLM骨干，要么依赖纯语义音频token，限制了通用音频建模能力。需要研究原生音频基础模型来支持更全面的音频理解和生成。

Method: 1) 系统研究设计选择：数据源、文本混合比例、token组成，建立验证的训练方案；2) 通过IsoFLOP分析对64个模型进行离散音频模型的首次缩放定律研究；3) 应用这些经验训练SODA模型套件（135M到4B参数，500B token）。

Result: 发现最优数据增长速度比最优模型大小快1.6倍；SODA模型在缩放预测和现有模型对比中表现良好；SODA作为灵活骨干支持多样化音频/文本任务，通过微调实现语音保持的语音到语音翻译。

Conclusion: 通过系统实证研究，建立了原生音频基础模型的有效训练方法，发现了离散音频模型的缩放规律，并展示了SODA模型作为统一架构在多任务音频处理中的灵活性和实用性。

Abstract: Current audio language models are predominantly text-first, either extending pre-trained text LLM backbones or relying on semantic-only audio tokens, limiting general audio modeling. This paper presents a systematic empirical study of native audio foundation models that apply next-token prediction to audio at scale, jointly modeling semantic content, acoustic details, and text to support both general audio generation and cross-modal capabilities. We provide comprehensive empirical insights for building such models: (1) We systematically investigate design choices -- data sources, text mixture ratios, and token composition -- establishing a validated training recipe. (2) We conduct the first scaling law study for discrete audio models via IsoFLOP analysis on 64 models spanning $3{\times}10^{18}$ to $3{\times}10^{20}$ FLOPs, finding that optimal data grows 1.6$\times$ faster than optimal model size. (3) We apply these lessons to train SODA (Scaling Open Discrete Audio), a suite of models from 135M to 4B parameters on 500B tokens, comparing against our scaling predictions and existing models. SODA serves as a flexible backbone for diverse audio/text tasks -- we demonstrate this by fine-tuning for voice-preserving speech-to-speech translation, using the same unified architecture.

</details>
